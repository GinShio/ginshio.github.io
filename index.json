[{"categories":["Programming"],"content":"GinShio | CS61A Study Notes - Sequences and Containers","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/"},{"categories":["Programming"],"content":" 容器","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:1:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#容器"},{"categories":["Programming"],"content":" Lists在 scheme 中一类最基础的异构数据结构即 list (list 1 2 3 \"str\") '(1 2 3 \"str\") 当然 list 可以看作是个二元组 pair，也有称作 dotlist (cons 1 2) ;; '(1 . 2) (cons 1 '(2)) ;; '(1 2) (cons* 1 2 3 4) ;; '(1 2 3 . 4) '(1 . (2 . (3 . 4))) (cons* 1 2 '(3 4)) ;; '(1 2 3 4) '(1 . ()) ;; '(1) '(1 . (2 . (3 . ()))) ;; '(1 2 3) '(1 . (2 3)) ;; '(1 2 3) '((1 2) . (3 4)) ;; '((1 2) 3 4) 访问 list 的首个元素使用 car，而获取尾元素使用 cdr (car '(1 2 3)) ;; 1 (cdr '(1 2 3)) ;; (2 3) (car '(1)) ;; 1 (car '(1)) ;; () 获取整个 list 的长度则是使用 length 方法 (length '()) ;; 0 (length '(1 2)) ;; 2 (length '(1 . (2 3))) ;; 3 随机访问 list 中的元素 (list-ref '(1 2 3 4 5) 3) ;; 4 (list-ref '(1 . (a . ())) 1) ;; a 更多 list 的操作可以翻阅 Lists ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:1:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#lists"},{"categories":["Programming"],"content":" Mapping-ReductionGoogle 大名鼎鼎的 MapReduce 就是来源于此。Mapping / Reduction 是一类高阶函数，其作用是抽象针对 List 的不同操作。 Map: 对容器中的每个元素进行一个相同的操作 Reduce: 根据一个相同的规则对容器中的元素进行规约 Mapping map 无副作用的对 lists 进行从左至右计算，但是对每个元素的操作顺序是未指定的 (map (lambda (n) (expt n n)) '(1 2 3 4)) ;; (1 4 27 256) (map + '(1 2 3) '(4 5 6) '(7 8 9)) ;; (12 15 18) map* 将初值附加在结果之后的 map (map* '() + '(1 2 3) '(4 5 6)) ;; (5 7 9) (map* '(1 2 3) + '(4 5 6)) ;; (4 5 6 1 2 3) for-each 是可以带有副作用的 map，因为保证每个元素按顺序执行 (let ((v '((1 2) (3 4) (5 6)))) (for-each (lambda (x) (set-cdr! x (list (+ (car x) (cadr x)) (* (car x) (cadr x))))) v) v) ;; ((1 3 2) (3 7 12) (5 11 30)) Reduction规约通常有一个初始值，并将集合中的每个元素与初始值进行一定操作 reduce 左结合地进行规约操作，仅在 list 为空时使用初始值 (reduce + 0 '(1 2 3 4)) ;; 10 (reduce + 0 '(foo)) ;; foo (reduce + 0 '()) ;; 0 (reduce list '() '(1 2 3 4)) ;; (4 (3 (2 1))) reduce-right 右结合地进行规约操作，仅在 list 为空时使用初始值 (reduce-right + 0 '(1 2 3 4)) ;; 10 (reduce-right + 0 '(foo)) ;; foo (reduce-right + 0 '()) ;; 0 (reduce-right list '() '(1 2 3 4)) ;; (1 (2 (3 4))) fold-left 左结合地进行规约操作，且总是使用初始值 (fold-left + 0 '(1 2 3 4)) ;; 10 ;; (fold-left + 0 '(foo)) ;; Error: is not the correct type (fold-left + 0 '()) ;; 0 (fold-left list '() '(1 2 3 4)) ;; ((((() 1) 2) 3) 4) fold-right 右结合地进行规约操作，且总是使用初始值 (fold-right + 0 '(1 2 3 4)) ;; 10 ;; (fold-right + 0 '(foo)) ;; Error: is not the correct type (fold-right + 0 '()) ;; 0 (fold-right list '() '(1 2 3 4)) ;; (1 (2 (3 (4 ())))) there-exists? 类似其他语言中的 any? (define (any? set predicate) (fold-left (lambda (acc v) (or acc (predicate v))) #f set)) (there-exists? '(1 2 3) (lambda (v) (= 0 (remainder v 2)))) ;; #t (there-exists? '(1 3 5) (lambda (v) (= 0 (remainder v 2)))) ;; #f for-all? 类似于其他语言中的 all? (define (all? set predicate) (fold-left (lambda (acc v) (and acc (predicate v))) #t set)) (for-all? '(1 2 3) (lambda (v) (= 1 (remainder v 2)))) ;; #f (for-all? '(1 3 5) (lambda (v) (= 1 (remainder v 2)))) ;; #t ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#mapping-reduction"},{"categories":["Programming"],"content":" Mapping-ReductionGoogle 大名鼎鼎的 MapReduce 就是来源于此。Mapping / Reduction 是一类高阶函数，其作用是抽象针对 List 的不同操作。 Map: 对容器中的每个元素进行一个相同的操作 Reduce: 根据一个相同的规则对容器中的元素进行规约 Mapping map 无副作用的对 lists 进行从左至右计算，但是对每个元素的操作顺序是未指定的 (map (lambda (n) (expt n n)) '(1 2 3 4)) ;; (1 4 27 256) (map + '(1 2 3) '(4 5 6) '(7 8 9)) ;; (12 15 18) map* 将初值附加在结果之后的 map (map* '() + '(1 2 3) '(4 5 6)) ;; (5 7 9) (map* '(1 2 3) + '(4 5 6)) ;; (4 5 6 1 2 3) for-each 是可以带有副作用的 map，因为保证每个元素按顺序执行 (let ((v '((1 2) (3 4) (5 6)))) (for-each (lambda (x) (set-cdr! x (list (+ (car x) (cadr x)) (* (car x) (cadr x))))) v) v) ;; ((1 3 2) (3 7 12) (5 11 30)) Reduction规约通常有一个初始值，并将集合中的每个元素与初始值进行一定操作 reduce 左结合地进行规约操作，仅在 list 为空时使用初始值 (reduce + 0 '(1 2 3 4)) ;; 10 (reduce + 0 '(foo)) ;; foo (reduce + 0 '()) ;; 0 (reduce list '() '(1 2 3 4)) ;; (4 (3 (2 1))) reduce-right 右结合地进行规约操作，仅在 list 为空时使用初始值 (reduce-right + 0 '(1 2 3 4)) ;; 10 (reduce-right + 0 '(foo)) ;; foo (reduce-right + 0 '()) ;; 0 (reduce-right list '() '(1 2 3 4)) ;; (1 (2 (3 4))) fold-left 左结合地进行规约操作，且总是使用初始值 (fold-left + 0 '(1 2 3 4)) ;; 10 ;; (fold-left + 0 '(foo)) ;; Error: is not the correct type (fold-left + 0 '()) ;; 0 (fold-left list '() '(1 2 3 4)) ;; ((((() 1) 2) 3) 4) fold-right 右结合地进行规约操作，且总是使用初始值 (fold-right + 0 '(1 2 3 4)) ;; 10 ;; (fold-right + 0 '(foo)) ;; Error: is not the correct type (fold-right + 0 '()) ;; 0 (fold-right list '() '(1 2 3 4)) ;; (1 (2 (3 (4 ())))) there-exists? 类似其他语言中的 any? (define (any? set predicate) (fold-left (lambda (acc v) (or acc (predicate v))) #f set)) (there-exists? '(1 2 3) (lambda (v) (= 0 (remainder v 2)))) ;; #t (there-exists? '(1 3 5) (lambda (v) (= 0 (remainder v 2)))) ;; #f for-all? 类似于其他语言中的 all? (define (all? set predicate) (fold-left (lambda (acc v) (and acc (predicate v))) #t set)) (for-all? '(1 2 3) (lambda (v) (= 1 (remainder v 2)))) ;; #f (for-all? '(1 3 5) (lambda (v) (= 1 (remainder v 2)))) ;; #t ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#mapping"},{"categories":["Programming"],"content":" Mapping-ReductionGoogle 大名鼎鼎的 MapReduce 就是来源于此。Mapping / Reduction 是一类高阶函数，其作用是抽象针对 List 的不同操作。 Map: 对容器中的每个元素进行一个相同的操作 Reduce: 根据一个相同的规则对容器中的元素进行规约 Mapping map 无副作用的对 lists 进行从左至右计算，但是对每个元素的操作顺序是未指定的 (map (lambda (n) (expt n n)) '(1 2 3 4)) ;; (1 4 27 256) (map + '(1 2 3) '(4 5 6) '(7 8 9)) ;; (12 15 18) map* 将初值附加在结果之后的 map (map* '() + '(1 2 3) '(4 5 6)) ;; (5 7 9) (map* '(1 2 3) + '(4 5 6)) ;; (4 5 6 1 2 3) for-each 是可以带有副作用的 map，因为保证每个元素按顺序执行 (let ((v '((1 2) (3 4) (5 6)))) (for-each (lambda (x) (set-cdr! x (list (+ (car x) (cadr x)) (* (car x) (cadr x))))) v) v) ;; ((1 3 2) (3 7 12) (5 11 30)) Reduction规约通常有一个初始值，并将集合中的每个元素与初始值进行一定操作 reduce 左结合地进行规约操作，仅在 list 为空时使用初始值 (reduce + 0 '(1 2 3 4)) ;; 10 (reduce + 0 '(foo)) ;; foo (reduce + 0 '()) ;; 0 (reduce list '() '(1 2 3 4)) ;; (4 (3 (2 1))) reduce-right 右结合地进行规约操作，仅在 list 为空时使用初始值 (reduce-right + 0 '(1 2 3 4)) ;; 10 (reduce-right + 0 '(foo)) ;; foo (reduce-right + 0 '()) ;; 0 (reduce-right list '() '(1 2 3 4)) ;; (1 (2 (3 4))) fold-left 左结合地进行规约操作，且总是使用初始值 (fold-left + 0 '(1 2 3 4)) ;; 10 ;; (fold-left + 0 '(foo)) ;; Error: is not the correct type (fold-left + 0 '()) ;; 0 (fold-left list '() '(1 2 3 4)) ;; ((((() 1) 2) 3) 4) fold-right 右结合地进行规约操作，且总是使用初始值 (fold-right + 0 '(1 2 3 4)) ;; 10 ;; (fold-right + 0 '(foo)) ;; Error: is not the correct type (fold-right + 0 '()) ;; 0 (fold-right list '() '(1 2 3 4)) ;; (1 (2 (3 (4 ())))) there-exists? 类似其他语言中的 any? (define (any? set predicate) (fold-left (lambda (acc v) (or acc (predicate v))) #f set)) (there-exists? '(1 2 3) (lambda (v) (= 0 (remainder v 2)))) ;; #t (there-exists? '(1 3 5) (lambda (v) (= 0 (remainder v 2)))) ;; #f for-all? 类似于其他语言中的 all? (define (all? set predicate) (fold-left (lambda (acc v) (and acc (predicate v))) #t set)) (for-all? '(1 2 3) (lambda (v) (= 1 (remainder v 2)))) ;; #f (for-all? '(1 3 5) (lambda (v) (= 1 (remainder v 2)))) ;; #t ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#reduction"},{"categories":["Programming"],"content":" VectorVector 同样是异构结构 (vector 1 2 3 \"str\") ;; #(1 2 3 \"str\") (make-vector 3 1) ;; #(1 1 1) (make-vector 3 '(1 2)) ;; #((1 2) (1 2) (1 2)) '#(1 #\\A \"str\") ;; #(1 #\\A \"str\") (make-initialized-vector 5 -) ;; #(0 -1 -2 -3 -4) (make-initialized-vector 5 (lambda (x) (* x x))) ;; #(0 1 4 9 16) 当然 list 和 vector 是可以互相转换的 (vector-\u003elist '#(1 2 3)) ;; (1 2 3) (list-\u003evector '(1 2 3)) ;; #(1 2 3) 根据 MIT-scheme 的说法，vector 性能更好，且占用的内存更少。不过其操作方法也更少。 ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:1:3","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#vector"},{"categories":["Programming"],"content":" Bit Strings在某些语言中 BitStrings 也被称作 bitmap (位图)。 '#*11001100 ;; #*11001100 (make-bit-string 7 #t) ;; #*1111111 (bit-string-allocate 7) ;; #*0000000 通常位图都支持按位运算 (bit-string-not #*0011) ;; #*1100 (bit-string-and #*0011 #*1010) ;; #*0010 (bit-string-andc #*0011 #*1010) ;; #*0001 (and #*0011 (not #*1010)) (bit-string-or #*0011 #*1010) ;; #*1011 (bit-string-xor #*0011 #*1010) ;; #*1001 常见的修改操作也是支持的 (let ((x #*1010)) (bit-string-set! x 2) x) ;; #*1110 (let ((x #*1010)) (bit-string-clear! x 1) x) ;; #*1000 (let ((x #*1010)) (bit-string-fill! x #f) x) ;; #*0000 (let ((x #*1010)) (bit-string-move! x #*0011) x) ;; #*0000 由于 lisp 中整型是无上限的，因此可以将任意大的整型与位图进行互转换 (unsigned-integer-\u003ebit-string 16 10000) ;; #*0010011100010000 (signed-integer-\u003ebit-string 16 -10000) ;; #*1101100011110000 (bit-string-\u003eunsigned-integer #*0010011100010000) ;; 10000 (bit-string-\u003esigned-integer #*1101100011110000) ;; -10000 ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:1:4","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#bit-strings"},{"categories":["Programming"],"content":" 数据抽象有时我们需要对基本数据进行更高层次的抽象，处理与数据相关的各种问题。因此数据抽象就显得尤为重要了。 ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:2:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#数据抽象"},{"categories":["Programming"],"content":" 抽象屏障比方说，要实现一个有理数的计算，首先要有对有理数这一数据的抽象。 (make-rat n d)，构造一个分子为 n 分母为 d 的有理数 (numer x) 获取有理数的分子 (denom x) 获取有理数的分母 现在我们可以轻松实现有理数的相关运算 (add-rat x y) 两个有理数相加 \\[\\frac{n_{1}}{d_{1}}+\\frac{n_{2}}{d_{2}}=\\frac{n_{1}d_{2}+n_{2}d_{1}}{d_{1}d_{2}}\\] (define (add-rat x y) (make-rat (+ (* (numer x) (denom y)) (* (numer y) (denom x))) (* (denom x) (denom y)))) (sub-rat x y) 两个有理数相减 \\[\\frac{n_{1}}{d_{1}}-\\frac{n_{2}}{d_{2}}=\\frac{n_{1}d_{2}-n_{2}d_{1}}{d_{1}d_{2}}\\] (define (sub-rat x y) (make-rat (- (* (numer x) (denom y)) (* (numer y) (denom x))) (* (denom x) (denom y)))) (mul-rat x y) 两有理数相乘 \\[\\frac{n_{1}}{d_{1}}\\cdot\\frac{n_{2}}{d_{2}}=\\frac{n_{1}n_{2}}{d_{1}d_{2}}\\] (define (mul-rat x y) (make-rat (* (numer x) (numer y)) (* (denom x) (denom y)))) (div-rat x y) 两有理数相除 \\[\\frac{n_{1}}{d_{1}}\\div\\frac{n_{2}}{d_{2}}=\\frac{n_{1}d_{2}}{d_{1}n_{2}}\\] (define (div-rat x y) (make-rat (* (numer x) (denom y)) (* (denom x) (numer y)))) (eq-rat x y) 判断两有理数是否相等 \\[\\frac{n_{1}}{d_{1}}=\\frac{n_{2}}{d_{2}},\\ \\texttt{if}\\ n_{1}d_{2}=n_{2}d_{1}\\] (define (eq-rat? x y) (= (* (numer x) (denom y)) (* (numer y) (denom x)))) 现在只抽象了方法，甚至还没考虑如何实现有理数，但已经将各个有理数的使用方法完成了。 构造有理数可以选用 pair 进行构建 (define make-rat cons) (define numer car) (define denom cdr) 比如示例中的有理数，使用了 make-rat 等方法对有理数进行构造，而使用 add-rat 等方法对有理数进行操作，而这些操作使用 make-rat 进行构造。也就是说，从上层看没有任何与 pair 相关的操作，使用时并不需要关心这些有理数是如何构建、使用的，因此这种方式也被称作 抽象屏障。 ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:2:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#抽象屏障"},{"categories":["Programming"],"content":" 程序接口一般来说，可以将 数据 定义为一组适当的选择函数与构造函数，以及为这些过程合法表示而指定的一组特定规则。数据抽象在复合数据的使用上是十分重要的，可以利用数据抽象的理论设计出不会被实现细节纠缠的程序。另一个重要原则就是程序接口。 如计算一棵树中，值为奇数的结点的平方和。 (define (sum-odd-squares tree) (cond ((null? tree) 0) ((not (pair? tree)) (if (odd? tree) (square tree) 0)) (else (+ (sum-odd-squares (car tree)) (sum-odd-squares (cdr tree)))))) 实际上这与级联处理中处理信号的方式相似，是从一个枚举器(enumerate) 开始，它产生出所有给定树的树叶的信号；接下来信号经过过滤器(filter)，将不是奇数的叶结点全部去除；之后信号会进入一个转换器(converter) 对其进行结果映射(map)；最终信号被输入到累加器(accumulator) 将所有元素组合起来。 不过 sum-odd-squares 并没有展现出这种信号流结构，其中一部分 enumerate 是由 null? 和 pair? 实现的，而另一部分是由树形递归实现的，这种结构并不如信号流清晰。 首先最简单的是 Mapping-Reduction 以及 filter，这是 scheme 原本就提供的。我们只需要实现对应的枚举操作即可 (define (enumerate tree) (cond ((null? tree) '()) ((not (pair? tree)) (list tree)) (else (append (enumerate (car tree)) (enumerate (cdr tree)))))) 最终可以用这些操作组合起来，实现我们需要的求和函数。可以看到组合而来的函数既简单又清晰 (define (sum-odd-squares tree) (reduce + 0 (map square (filter odd? (enumerate tree))))) ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:2:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#程序接口"},{"categories":["Programming"],"content":" 多重表示有时一个抽象数据可以有多种表示方法，如复数，就可以有直角坐标形式 (实部和虚部) 或者极坐标形式 (模和幅角)。 如果同时使用多种表示，那么就需要一个标志来确定当前数据是被哪种形式所表示的。另外，由选择函数和构造函数形成的抽象屏障，可以将选择具体形式的时间向后拖延，并保持系统设计的灵活性。 对此，我们只需提供从对象中提取类型标志以及对应的谓词，就可以方便分辨出现在到底使用的是哪种形式的抽象。 (define (type-tag datum) (if (pair? datum) (car datum) (error \"Bad tagged datum -- TYPE_TAG\" datum))) (define (contents datum) (if (pair? datum) (cdr datum) (error \"Bad tagged datum -- CONTENTS\" datum))) (define (rectangular? complex) (eq? (type-tag complex) 'rectangular)) (define (polar? complex) (eq? (type-tag complex) 'polar)) 这样我们可以很轻松地在系统中对复数多重表示进行不同的实现。 (define (real-part complex) (cond ((rectangular? complex) (real-part-rectangular (contents complex))) ((polar? complex) (real-part-polar (contents complex))) (else (error \"Unknow type -- REAL-PART\" complex)))) (define (real-part-rectangular complex) (car complex)) (define (real-part-polar complex) (* (mangnitude-polar complex) (cos (angle-polar complex)))) (define (mangnitude-polar complex) (car complex)) (define (angle-polar complex) (cdr complex)) 如果用 C++ 实现，可以用泛型的方式 template \u003cForm f\u003e auto real_part(Complex\u003cf\u003e const\u0026) -\u003e double { static_assert(false, \"Unknow type -- REAL-PART\"); } template \u003c\u003e auto real_part(Complex\u003cForm::Rectangular\u003e const\u0026 z) -\u003e double { return ::std::get\u003c0\u003e(z); } template \u003c\u003e auto real_part(Complex\u003cForm::Polar\u003e const\u0026 z) -\u003e double { return ::std::get\u003c0\u003e(z) * ::std::cos(::std::get\u003c1\u003e(z)); } 显然，使用泛型实现的 real_part 比 scheme 中的 real_part 要清晰的多。在 scheme 中我们实现了基于形式的派发，很明显接口中需要知道所有已知的形式；而 C++ 中，完全可以通过类型区分形式，即基于类型的派发。但是在 scheme 中，这是不现实的，需要手动测试传入接口的类型，并且保证每个形式的实现不会重名。而 C++ 中使用泛型的缺点就是，类型系统不认为这两个形式的复数是同一种类型，也就是说如果想用容器来存储复数类型的数据是不可能的。 因此在 scheme 中，以数据导向的程序设计方法进一步将系统模块化。好的方法是做一个类似注册的做法，用类似 get / set 的方法将每个模块的函数注册到表中。 操作 / 类型 Polar Rectangular real-part real-part-polar real-part-rectangular imag-part imag-part-polar imag-part-rectangular magnitude magnitude-polar magnitude-rectangular angle angle-polar angle-rectangular 这样只需要在对应模块中进行操作，这样不但不会造成名称污染，还方便对代码进行扩展。 (define (apply-generic op . args) (let* ((type-args (map type-tag args)) (proc (get op type-tags))) (if proc (apply proc (map contents args)) (error \"No method for these types -- APPLY-GENERIC\" (list op type-tags))))) (define (real-part complex) (apply-generic 'real-part complex)) ","date":"02-12","objectID":"/2023/cs61a_03_data_abstractions/:2:3","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"数据抽象","uri":"/2023/cs61a_03_data_abstractions/#多重表示"},{"categories":["API"],"content":"GinShio | libarchive Learning and Development Guide -- Introduction","date":"01-28","objectID":"/2023/libarchive_development_001/","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/"},{"categories":["API"],"content":"libarchive 是一个可以创建和读取多种不同流式归档格式的程序库，包含了最流行的 tar 格式变体、一些 cpio 格式，以及所有的 BSD 和 GNU ar 变体。bsdtar 是一个使用 libarchive 的 tar 实现。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:0:0","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#"},{"categories":["API"],"content":" 简介","date":"01-28","objectID":"/2023/libarchive_development_001/:1:0","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#简介"},{"categories":["API"],"content":" 为什么实现大约在 2001 年的某个时间，邮件列表中出现了一些关于 FreeBSD 打包工具的辩论，辩论主要涉及两个相关的问题： 对打包工具来说什么是 正确 (right) 的格式？ 为什么 FreeBSD 的打包工具比其他发行版的打包工具慢？ 在仔细研究之后，Kientzle 认为 tar/gzip 和 tar/bzip2 依然是很好的格式，性能问题纯粹是实现的原因。 因此，Kientzle 开启了一个从 pkg_add 开始重写打包工具的项目，关键是这个项目是一个了解 tar/gzip 和 tar/bzip2 的库。他最终在 2003 年时完成了 libtarfile，并意识到许多核心的基础设施都要简单通用地处理其他格式，因此这个库被重命名为 libarchive。一次在 Kientzle 构建 libarchive 时，他意识到他早期测试套件更接近于一个 GNU tar 的完全 BSD 许可的替代品，即 bsdtar。FreeBSD 项目采用了 bsdtar 和 libarchive，并允许他继续在 FreeBSD 源码树中开发。大约在 2007 年，libarchive 被移植到其他平台，并将主要开发工作转移到了独立的仓库，刚开始在 GoogleCode，之后转到了 GitHub 直到今天。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:1:1","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#为什么实现"},{"categories":["API"],"content":" 谁在用 操作系统 freeBSD 2004 年 11 月发布的 FreeBSD 5.3 中首次包含 libarchive FreeBSD 6 起将 bsdtar 设置为默认的 tar 工具 FreeBSD 8 起将 bsdcpio 设置为默认的 cpio 工具 FreeBSD 8 起将 Kai Wang 的 ar 和 Dag-Erling Smørgrav 的 unzip 标准化 NetBSD 从 NetBSD 5.0 开始 libarchive 就是其一部分 NetBSD 9.0 开始使用 bsdtar 和 bsdcpio 作为默认的系统工具 MacOS 2009 的起 libarchive 就作为 MacOS 的一部分 bsdtar 和 bsdcpio 也是 MacOS 的默认系统工具 Windows Windows 10 insider build 17063 开始，libarchive 和 bsdtar 作为系统默认工具被提供 包管理器 Arch Linux 的 Pacman Void Linux 的 XBPS CRUX Linux 的 pkgutils Gentoo 和 Exherbo 的多格式包管理器 Paludis CMake 压缩软件和文件浏览器 Tarsnap Springy 用于处理 TAR, PAX 和 CPIO 格式的文件 Archivemount 用于挂载归档文件 KDE 的 ark 更多的用户可以参见 LibarchiveUsers ","date":"01-28","objectID":"/2023/libarchive_development_001/:1:2","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#谁在用"},{"categories":["API"],"content":" 有哪些功能libarchive 支持读取多种不同的格式，包括 tar、pax、cpio、7zip、zip、xar、mtree、rar 以及 ISO 映像等；可以写入 tar、pax、cpio、7zip、zip、xar、mtree、ISO 以及 shar 等格式。更多格式可以查看文档。 在读取归档文件时，libarchive 有一个健壮的格式自动检测器用以识别归档文件的格式，也可以识别 gzip、bzip2、xz、lzip 以及多种流行的压缩算法，包括像 tar.gz 这样的归档、压缩组合。 libarchive 允许灵活地处理数据的来源和去处。有一个方便的包装器可以读取、写入数据到磁盘上的常规文件或内存中，也可以注册你自己的 IO 函数来读取、写入磁带设备、网络套接字或任何数据源的归档文件。libarchive 的内部 IO 模型是零拷贝设计的，在操作非常大的归档文件时，可以最小化拷贝数据，以获取最佳性能。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:1:3","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#有哪些功能"},{"categories":["API"],"content":" 获取更多信息 官方网站 仓库 Wiki 邮件列表 手册 Issues PR ","date":"01-28","objectID":"/2023/libarchive_development_001/:1:4","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#获取更多信息"},{"categories":["API"],"content":" 如何贡献libarchive 欢迎大家贡献，贡献前请先阅读相关文档，以免造成不必要的麻烦： 贡献到 Libarchive 愿望单 内部资料 如果您无法帮助编写 C 代码，开发者们依然欢迎其他帮忙： 改进 Wiki 文档 在邮件列表中解决问题 随着 libarchive 的发展进行测试 ","date":"01-28","objectID":"/2023/libarchive_development_001/:1:5","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#如何贡献"},{"categories":["API"],"content":" 归档与压缩一个归档文件可以分为两个部分，归档 (仅存储) 和压缩。通常将需要添加到压缩包的文件先归档为一个文件，再对这个文件进行压缩。 像 tar、7z、zip 等被 libarchive 称为格式的就是归档文件，这些格式规定了压缩包内文件怎么存储，怎么记录词条，等等信息。 压缩就是对这个已经归档的文件选用什么算法进行操作。像 lzma、xz、zstd 等被 libarchive 称为 filter 的就是压缩算法。 一般来说，压缩包就是格式与压缩算法的组合，像常见的 7zip，就是 7zip 格式与默认算法 lzma2 的组合，而有些 7zip 文件也会不进行压缩，也就是用 7zip 格式将待打包的文件归档在一个文件里。而 unix-like 世界中常见的 tar/gzip、tar/bzip2 等就是 tar 格式与 gz、bz2 算法的组合。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:2:0","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#归档与压缩"},{"categories":["API"],"content":" 编译libarchive 用 format 和 filter 区分格式与压缩。格式通常只需要 libarchive 根据其文档实现就 okay。压缩由于算法基本都公开，因此有大量优秀的库来实现这些压缩算法，因此 libarchive 并不实现压缩算法。因此 libarchive 编译时有很多关于算法的开关 压缩算法开关 算法名称 仓库地址 开关名称 默认状态 LZ4 GitHub ENABLE_LZ4 ON LZO None ENABLE_LZO OFF LZMA xz-utils ENABLE_LZMA ON Zstandard GitHub ENABLE_ZSTD ON Bzip2 sourceware or GitLab ENABLE_Bzip2 ON 加密库开关 库名称 仓库地址 开关名称 默认状态 备注 Mbed TLS GitHub ENABLE_MBEDTLS OFF GNU Nettle Lysator ENABLE_NETTLE OFF OpenSSL GitHub ENABLE_OPENSSL ON Windows CNG None ENABLE_CNG ON 用于 Windows 上的 Zip 加密解密 其他开关 开关名称 默认状态 用途 ENABLE_TAR ON 编译 bsdtar ENABLE_CPIO ON 编译 bsdcpio ENABLE_CAT ON 编译 bsdcat ENABLE_LIBXML2 ON 使用 libxml2 解析 xar 格式 ENABLE_EXPAT ON 使用 Expat 解析 xar 格式 ENABLE_PCREPOSIX ON 在 bsdtar 中使用 pcreposix 解析 regex ENABLE_ACL ON 支持读写 POSIX.1e ACLs。Linux 需要 libacl ENABLE_XATTR ON 支持读写 POSIX.1e 风格文件扩展属性。Linux 需要 libxattr；FreeBSD 如果使用 ZFS 并 hang 过，建议关闭 编译变量 POSIX_REGEX_LIB，使用哪个库解析 POSIX 正则表达式 AUTO (Default) LIBC LIBREGEX ENABLE_SAFESEH WINDOWS_VERSION ","date":"01-28","objectID":"/2023/libarchive_development_001/:3:0","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#编译"},{"categories":["API"],"content":" 简单的入门Libarchive 使用时需要两个对用户透明的基础类型对象：struct archive 指针和 struct archive_entry 指针。在 libarchive 中 archive 对象的生命周期是十分简单的： 使用 archive_xxx_new 创建一个对象 使用 support 或 set 对 archive 对象进行配置 support 允许库决定何时启用功能 set 无条件的启用功能 open 打开一个数据源 迭代读取内容：从 entry 中获取 archive 词条的 header 信息和数据。 结束时，使用 close 写入信息，free 则会释放 archive 对象 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:0","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#简单的入门"},{"categories":["API"],"content":" 读取归档中的文件名 struct archive *a = archive_read_new(); archive_read_support_filter_all(a); archive_read_support_format_all(a); if (archive_read_open_filename(a, \"archive.tar\", 10240) != ARCHIVE_OK) { exit(EXIT_FAILURE); } struct archive_entry *entry; while (archive_read_next_header(a, \u0026entry) == ARCHIVE_OK) { printf(\"%s\\n\", archive_entry_pathname(entry)); archive_read_data_skip(a); // Note 2 } if (archive_read_free(a) != ARCHIVE_OK) { exit(EXIT_FAILURE); } 代码开始时的 support_filter_all 与 support_format_all 就是 libacrhive 所说的自动推导数据源的压缩算法与格式。 这里使用的 open 是 filename，即打开一个磁盘上的文件作为数据源，另外 libarchive 还可以对内存 (memory) 进行读写，或文件指针 (FILE*)、文件描述符 (fd) 进行读写。 archive_read_open_memory(a, buff, sizeof(buff)); archive_read_open_FILE(a, fileptr); archive_read_open_fd(a, fd, 10240); 之后在 loop 中使用 next_header 来获取归档中的文件信息，也就是之前说到的迭代读取内容。该函数可以将目前待读取的文件信息写入到 entry 结构中，以便用户之后进行操作。如 archive_entry_pathname 获取文件名称。实际上 archive_read_data_skip 并不需要调用，这里作为不对数据进行任何处理的标志。如果没有使用数据，在 next_header 中 libarchive 会自动调用该函数。 最后调用了 archive_read_free 来释放掉 archive 结构，这也会自动关闭已经打开的数据源。如果你还有其他用途，可以使用 close 关闭打开的数据源，并重新打开新的数据源，防止重复分配释放 archive 以获得更高的性能。另外显式调用 close 有一个好处是你可以获取到错误状态，而 free 中隐式调用 close 则用户无法接收到 close 的状态。 另外需要注意的是，只有从磁盘上打开的数据源，libarchive 会真正的 close 掉，内存、文件指针以及文件描述符，libarchive 并不会 close 它们，因为这是不属于 libarchive 的资源，需要调用者自己承担这些资源的释放。但是也不能因为 libarchive 不会释放它们而不调用 close 函数，因为该函数中会释放一些 libarchive 自己申请的一些资源。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:1","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#读取归档中的文件名"},{"categories":["API"],"content":" 注册自己的函数 read / write / close callbacklibarchive 提供了更低级的 open 函数，该函数接收 3 个回调函数和你定义的数据类型： 一个打开数据源的 open 回调，不过这是个遗留参数，不需要也不应该被使用 一个 read / write 的回调函数 一个 close 的回调函数 因此你可以定制一个如从 HTTP 中读取归档文件的实现。 回调函数应该遵循基本的 libarchive 约定： open 与 close 函数在成功时应该返回 ARCHIVE_OK (0)，而失败时应该返回一个负数。通常来说使用 ARCHIVE_WARN 表示有问题的情况，而 ARCHIVE_FATAL 表示不能恢复或不能重试的问题。 read 与 write 返回成功读取、写入的字节数，0 表示 EOF，出现错误与上一回调函数一样。read 回调还会返回一个指向读取的数据的指针。 Libarchive 不在意数据的块大小，在访问下一个块前会完成当前块，因此回调函数中不需要处理这些块。唯一要求时块的大小必须不为 0，因为 0 字节大小表示 EOF。 ssize_t myread(struct archive *a, void *client_data, const void **buff) { struct mydata *mydata = (struct mydata *) client_data; *buff = mydata-\u003ebuff; return (read(mydata-\u003efd, mydata-\u003ebuff, 1024)); } int myclose(struct archive *a, void *client_data) { struct mydata *mydata = (struct mydata *) client_data; if (mydata-\u003efd \u003e 0) { close(mydata-\u003efd); } free(mydata); return (ARCHIVE_OK); } // in main struct mydata mydata = { .fd = open(name, O_RDONLY), }; archive_read_open(a, \u0026mydata, NULL, myread, myclose); skip callbackLibarhive 提供了和 open 类似的函数 open2，额外提供了 skip 回调函数。一般来说时用不到该回调的，但有时 libarchive 可以利用该回调优化某些格式的读取，快速搜索整个正文条目。当然也必须满足一些必要条件： 必须返回实际跳过的字节数，如果不能跳过则应该返回一个负数 可以跳过比请求更少的字节，但不能跳过超过请求的字节数 只有向前 (正向) 跳过才被允许 如果未提供 skip 回调或失败，libarchive 将调用 read() 简单地忽略不需要的数据 seek callbackLibarchive 3.0 支持了 seek 回调，该回调用于读取不适合流式传输的格式，如 7zip 和某些 zip 的变种。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:2","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#注册自己的函数"},{"categories":["API"],"content":" 注册自己的函数 read / write / close callbacklibarchive 提供了更低级的 open 函数，该函数接收 3 个回调函数和你定义的数据类型： 一个打开数据源的 open 回调，不过这是个遗留参数，不需要也不应该被使用 一个 read / write 的回调函数 一个 close 的回调函数 因此你可以定制一个如从 HTTP 中读取归档文件的实现。 回调函数应该遵循基本的 libarchive 约定： open 与 close 函数在成功时应该返回 ARCHIVE_OK (0)，而失败时应该返回一个负数。通常来说使用 ARCHIVE_WARN 表示有问题的情况，而 ARCHIVE_FATAL 表示不能恢复或不能重试的问题。 read 与 write 返回成功读取、写入的字节数，0 表示 EOF，出现错误与上一回调函数一样。read 回调还会返回一个指向读取的数据的指针。 Libarchive 不在意数据的块大小，在访问下一个块前会完成当前块，因此回调函数中不需要处理这些块。唯一要求时块的大小必须不为 0，因为 0 字节大小表示 EOF。 ssize_t myread(struct archive *a, void *client_data, const void **buff) { struct mydata *mydata = (struct mydata *) client_data; *buff = mydata-\u003ebuff; return (read(mydata-\u003efd, mydata-\u003ebuff, 1024)); } int myclose(struct archive *a, void *client_data) { struct mydata *mydata = (struct mydata *) client_data; if (mydata-\u003efd \u003e 0) { close(mydata-\u003efd); } free(mydata); return (ARCHIVE_OK); } // in main struct mydata mydata = { .fd = open(name, O_RDONLY), }; archive_read_open(a, \u0026mydata, NULL, myread, myclose); skip callbackLibarhive 提供了和 open 类似的函数 open2，额外提供了 skip 回调函数。一般来说时用不到该回调的，但有时 libarchive 可以利用该回调优化某些格式的读取，快速搜索整个正文条目。当然也必须满足一些必要条件： 必须返回实际跳过的字节数，如果不能跳过则应该返回一个负数 可以跳过比请求更少的字节，但不能跳过超过请求的字节数 只有向前 (正向) 跳过才被允许 如果未提供 skip 回调或失败，libarchive 将调用 read() 简单地忽略不需要的数据 seek callbackLibarchive 3.0 支持了 seek 回调，该回调用于读取不适合流式传输的格式，如 7zip 和某些 zip 的变种。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:2","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#read-write-close-callback"},{"categories":["API"],"content":" 注册自己的函数 read / write / close callbacklibarchive 提供了更低级的 open 函数，该函数接收 3 个回调函数和你定义的数据类型： 一个打开数据源的 open 回调，不过这是个遗留参数，不需要也不应该被使用 一个 read / write 的回调函数 一个 close 的回调函数 因此你可以定制一个如从 HTTP 中读取归档文件的实现。 回调函数应该遵循基本的 libarchive 约定： open 与 close 函数在成功时应该返回 ARCHIVE_OK (0)，而失败时应该返回一个负数。通常来说使用 ARCHIVE_WARN 表示有问题的情况，而 ARCHIVE_FATAL 表示不能恢复或不能重试的问题。 read 与 write 返回成功读取、写入的字节数，0 表示 EOF，出现错误与上一回调函数一样。read 回调还会返回一个指向读取的数据的指针。 Libarchive 不在意数据的块大小，在访问下一个块前会完成当前块，因此回调函数中不需要处理这些块。唯一要求时块的大小必须不为 0，因为 0 字节大小表示 EOF。 ssize_t myread(struct archive *a, void *client_data, const void **buff) { struct mydata *mydata = (struct mydata *) client_data; *buff = mydata-\u003ebuff; return (read(mydata-\u003efd, mydata-\u003ebuff, 1024)); } int myclose(struct archive *a, void *client_data) { struct mydata *mydata = (struct mydata *) client_data; if (mydata-\u003efd \u003e 0) { close(mydata-\u003efd); } free(mydata); return (ARCHIVE_OK); } // in main struct mydata mydata = { .fd = open(name, O_RDONLY), }; archive_read_open(a, \u0026mydata, NULL, myread, myclose); skip callbackLibarhive 提供了和 open 类似的函数 open2，额外提供了 skip 回调函数。一般来说时用不到该回调的，但有时 libarchive 可以利用该回调优化某些格式的读取，快速搜索整个正文条目。当然也必须满足一些必要条件： 必须返回实际跳过的字节数，如果不能跳过则应该返回一个负数 可以跳过比请求更少的字节，但不能跳过超过请求的字节数 只有向前 (正向) 跳过才被允许 如果未提供 skip 回调或失败，libarchive 将调用 read() 简单地忽略不需要的数据 seek callbackLibarchive 3.0 支持了 seek 回调，该回调用于读取不适合流式传输的格式，如 7zip 和某些 zip 的变种。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:2","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#skip-callback"},{"categories":["API"],"content":" 注册自己的函数 read / write / close callbacklibarchive 提供了更低级的 open 函数，该函数接收 3 个回调函数和你定义的数据类型： 一个打开数据源的 open 回调，不过这是个遗留参数，不需要也不应该被使用 一个 read / write 的回调函数 一个 close 的回调函数 因此你可以定制一个如从 HTTP 中读取归档文件的实现。 回调函数应该遵循基本的 libarchive 约定： open 与 close 函数在成功时应该返回 ARCHIVE_OK (0)，而失败时应该返回一个负数。通常来说使用 ARCHIVE_WARN 表示有问题的情况，而 ARCHIVE_FATAL 表示不能恢复或不能重试的问题。 read 与 write 返回成功读取、写入的字节数，0 表示 EOF，出现错误与上一回调函数一样。read 回调还会返回一个指向读取的数据的指针。 Libarchive 不在意数据的块大小，在访问下一个块前会完成当前块，因此回调函数中不需要处理这些块。唯一要求时块的大小必须不为 0，因为 0 字节大小表示 EOF。 ssize_t myread(struct archive *a, void *client_data, const void **buff) { struct mydata *mydata = (struct mydata *) client_data; *buff = mydata-\u003ebuff; return (read(mydata-\u003efd, mydata-\u003ebuff, 1024)); } int myclose(struct archive *a, void *client_data) { struct mydata *mydata = (struct mydata *) client_data; if (mydata-\u003efd \u003e 0) { close(mydata-\u003efd); } free(mydata); return (ARCHIVE_OK); } // in main struct mydata mydata = { .fd = open(name, O_RDONLY), }; archive_read_open(a, \u0026mydata, NULL, myread, myclose); skip callbackLibarhive 提供了和 open 类似的函数 open2，额外提供了 skip 回调函数。一般来说时用不到该回调的，但有时 libarchive 可以利用该回调优化某些格式的读取，快速搜索整个正文条目。当然也必须满足一些必要条件： 必须返回实际跳过的字节数，如果不能跳过则应该返回一个负数 可以跳过比请求更少的字节，但不能跳过超过请求的字节数 只有向前 (正向) 跳过才被允许 如果未提供 skip 回调或失败，libarchive 将调用 read() 简单地忽略不需要的数据 seek callbackLibarchive 3.0 支持了 seek 回调，该回调用于读取不适合流式传输的格式，如 7zip 和某些 zip 的变种。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:2","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#seek-callback"},{"categories":["API"],"content":" 写入文件 struct archive *a = archive_write_new(); archive_write_add_filter_gzip(a); // gzip compression archive_write_add_format_pax_restricted(a); // tar format (pax extensions) // archive_write_add_format_ustar(a); // tar format (POSIX.1-1988) // archive_write_add_format_gnutar(a); // tar format (GNU extensions) archive_write_open_filename(a, outname); char buff[8192]; while (*filelist) { struct stat st; stat(*filelist, \u0026st); struct archive_entry *entry = archive_entry_new(); archive_entry_set_pathname(entry, *filelist); archive_entry_set_size(entry, st.st_size); archive_entry_set_filetype(entry, AE_IFREG); archive_entry_set_perm(entry, 0644); archive_write_header(a, entry); int fd = open(*filename, O_RDONLY); ssize_t len = read(fd, buff, sizeof(buff)); while (len \u003e 0) { archive_write_data(a, buff, len); len = read(fd, buff, sizeof(buff)); } close(fd); archive_entry_free(entry); filename++; } archive_write_free(a); 将文件加入归档时，每次都会申请一份新的 entry 结构，而在处理下一个文件前会释放掉它。如果为了更高的性能，可以不释放掉该结构，而是采用 archive_entry_clear 来清除掉其中的数据，以安全地复用该结构。 对于写入归档的 entry 来说，文件的大小、类型和路径是必要属性。如果比较懒也可以使用 archive_entry_copy_stat 从文件的 struct stat 中来拷贝属性，也包括 ACL 与 xattr。拷贝属性这个函数也可以在 Windows 下使用。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:3","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#写入文件"},{"categories":["API"],"content":" 在磁盘上构造对象Libarchive 提供了一个 archive_write_disk 这样的直接在磁盘上构建文件对象的方式，而不是将文件构造在归档中。并且对于如常规文件 (regular)、目录 (directory)、符号链接 (symlink)、硬链接 (hard link) 等不同的磁盘对象都可以正确处理。 struct archive *a = archive_write_disk_new(); archive_write_disk_set_options(a, ARCHIVE_EXTRACT_TIME); struct archive_entry *entry = archive_entry_new(); archive_entry_set_pathname(entry, \"my_file.txt\"); archive_entry_set_filetype(entry, AE_IFREG); archive_entry_set_size(ae, 5); archive_entry_set_mtime(ae, 123456789, 0); archive_write_header(a, entry); archive_write_data(a, \"abcde\", 5); archive_write_finish_entry(a); archive_write_free(a); archive_entry_free(entry); 如果在 entry 中使用 archive_entry_set_size 设置了大小的话，写入磁盘将会强制使用该大小。如果对数据实际的写入 archive_write_data 大小多于 entry 中设置的大小，那会将后面的数据强制截取掉；如果不足 entry 设置的大小的话，那也会填充 0 补齐文件大小。 虽然可以处理不同的磁盘对象，但是对于符号链接和硬链接还是需要特殊的操作： 符号链接需要设置文件类型为 AE_IFLNK 并使用 archive_entry_set_symlink 硬链接同样需要使用 archive_entry_set_hardlink。调用了该函数的话，常规文件类型会被忽略；如果设置了大小，那么需要写入文件数据，如果不希望覆盖文件内容则不要设置硬链接大小。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:4","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#在磁盘上构造对象"},{"categories":["API"],"content":" 错误处理 ARCHIVE_EOF 只会在 archive_read_data 到达数据结尾时，或 archive_read_next_header 词条到达归档文件末尾时，被返回。 ARCHIVE_OK 在操作成功时被返回 ARCHIVE_WARN 在操作完成并有些问题时返回。你可以将这个问题报告给用户，archive_error_string 可以获取到对应的字符串信息，而 archive_errno 可以返回关联的系统 errno 值。（并非所有错误都有系统调用引起，因此 archive_errno 并不总是返回有效值） ARCHIVE_FAILED 在操作失败时返回。通常来说该状态意味着目前的词条无法再进行下一步操作。比如写不支持的归档格式。通常恢复的手段就是操作下一个词条。 ARCHIVE_FATAL 通常在 archive 对象无法使用时返回，典型原因就是 IO 错误或内存分配失败。通常你需要调用 archive_write_free 来释放掉这个对象。 通常一些极端情况下 libarchive 会调用 abort 终止程序，这通常只发生 libarchive 的内部一致性检查检测到自身存在严重错误时才会发生。 ","date":"01-28","objectID":"/2023/libarchive_development_001/:4:5","series":["libarchive 学习与开发指南"],"tags":["C","Archive","Library"],"title":"libarchive 介绍与入门","uri":"/2023/libarchive_development_001/#错误处理"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第九章笔记","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/"},{"categories":["Algorithm⁄DataStructure"],"content":" Graphs stand or fall by their choice of nodes and edges. — Watts \u0026 Strogatz 信息 对于图的学习推荐使用 Rocs。什么？你说你是 Windows？那也不知道用什么啊，欢迎推荐其他工具。另外，KDE 天下第一！ ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:0:0","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#"},{"categories":["Algorithm⁄DataStructure"],"content":" 图的定义与表示图 (graph) 是有序对 \\(G = (V, E)\\)，其中 V 是点集 (Vertex)，点的个数用 \\(\\lvert{V}\\rvert\\) 表示；\\(E \\subseteq \\{ \\{ x, y \\}: (x, y) \\in V^{2}, x \\ne y \\}\\) 是边集 (Edge)，边的个数用 \\(\\lvert{E}\\rvert\\) 表示。如果点对是有序的，那么这个图称为有向图 (directed graph / digraph)。当然有向图的边，如果去掉方向限制所对应的无向图，称为该有向图的基础图 (underlying graph)。有时边还有一个属性称为权重 (weight)，表示使用这条边的代价 (cost)。如果任意两个顶点之间都有一条边的话，那么这个图被称作完全图 (complete graph)。 图中的一条路径 (path) 是一个顶点序列 \\(v_{1}, v_{2}, \\cdots, v_{n}\\) (其中 \\(v_{i}, v_{i+1} \\in E, i \\le i \u003c n\\))，一条路径的长 (length) 是这条路径上的边的数量。如果图中含有一个顶点到它自身的路径，则这个路径称为环 (loop)，另外环上所有顶点是互异的。有向图中的环通常被称为回路 (cycle)，没有回路的有向图是无环的 (acyclic)，也被称为有向无环图 (DAG, Directed Acyclic Graph)。 如果无向图中从任意顶点到其他顶点都存在一条路径，那么该图是连通的 (connected)；如果是具有这样性质的有向图，则被称为强连通的 (strongly connected)；如果没有这样性质的有向图，但其基础图具有这种性质，那么该图被称为弱连通的 (weakly connected)。 根据图的邻接表表示，可以轻松的实现出图的结点 class Node { public: int val; ::std::vector\u003cNode *\u003e neighbors; Node() : val{0}, neighbors{} {} Node(int _val) : val{_val}, neighbors{} {} Node(int _val, ::std::vector\u003cNode *\u003e _neighbors) : val{_val}, neighbors{::std::move(_neighbors)} {} }; ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:1:0","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#图的定义与表示"},{"categories":["Algorithm⁄DataStructure"],"content":" 深度优先搜索深度优先搜索 (DFS, depth-first search) 是前序遍历的推广，从任一顶点 v 开始处理，然后遍历与 v 相接的所有顶点。如果对于树进行 DFS，可以在总时间 \\(\\Theta(\\lvert{V}\\rvert)\\) 内遍历完成。但图的遍历需要小心其中的环，因此需要一个 visited 标记来表明该结点是否已被遍历过，防止进入无限的循环中。 void dfs(Vertex const \u0026v) { v.visited = true; for (Vertex const \u0026w : v.neighbors) { if (!w.visited) { dfs(w); } } } 如果想克隆图，则以 DFS 为模板就可以轻易实现 ::std::unordered_map\u003cVertex const, Vertex\u003e vertices; Vertex clone_graph(Vertex const v) { if (auto it = vertices.find(v); it != vertices.end()) { return it-\u003esecond; } auto clone = get_from_source(v); vertices.emplace(v, clone); for (auto\u0026 other : v.neighbors) { clone.adjacent.emplace_back(clone_graph(other)); } return clone; } 深度优先生成树 (depth-first spanning tree) 可以描述 DFS 的过程。如果一条边 (v, w) 发现没有被遍历，那就用树上的一条边表示；否则在树上用虚线表示，这条边称之为后向边 (back edge)，实际上这并不是树的一部分。 如果无向图是不连通的或有向图是非强连通的，那么需要遍历整个图，查找该图还有哪些结点没有被遍历。对于不连通的图，每次 DFS 生成的树的集合，就是一个深度优先生成森林。 ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:2:0","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#深度优先搜索"},{"categories":["Algorithm⁄DataStructure"],"content":" 双连通性一个连通的无向图中删除任一顶点后，剩下的图如果依然连通，那么这样的无向连通图就是双连通的 (biconnected)，即点双连通。如果图不是双连通的，那删除后图不再连通的顶点称之为割点 (articulation point)。如图顶点 C 和 D 是割点，删除顶点 C 使顶点 G 不连通；删除顶点 D 使顶点 E、F 不和图其余部分连通。 深度优先遍历提供了寻找割点的线性时间复杂度的方法 – Tarjan。首先从任一顶点开始 DFS，并按照被访问的顺序对其进行编号，记作 \\(Num(v)\\)。对每个顶点计算能到达的最低的顶点编号，记作 \\(Low(v)\\)，其值由以下情况中的最小值定义 \\(Num(v)\\) 所有后向边 \\((v, w)\\) 中的最低的 \\(Num(w)\\) 生成树的边 \\((v, w)\\) 中的最低的 \\(Low(w)\\) 当且仅当根有多个儿子时，此时根是割点；当且仅当生成树中的一个结点有某个儿子 w，且 \\(Low(w) \\ge Num(v)\\)，此时这个结点为割点。 int num{0}; ::std::set\u003cVertex\u003e ans; int tarjan(Vertex \u0026v) { v.visit(++num); int child{0}; for (auto const \u0026w : v.neighbors) { bool flag{!w.visited}; v.low = ::std::min(v.low, flag ? (++child, tarjan(w)) : w.num); if (flag \u0026\u0026 w.low \u003e= v.num \u0026\u0026 (v.num != 1 || child \u003e 1)) { static_cast\u003cvoid\u003e(ans.emplace(v)); } } return v.low; } 一个连通的无向图中删除任一边后，剩下的图如果依然连通，那么这样的无向连通图就是边双连通的。如果图不是双连通的，那删除后图不再连通的边称为割边，通常称为桥 (bridge)。 相比于割点，桥的计算更为简单，不需要在考虑生成树根结点的问题。如果顶点 w 不能回到祖先也没有另外一条回到父亲的路，那么 \\(v-w\\) 这条边就是割边。 ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:2:1","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#双连通性"},{"categories":["Algorithm⁄DataStructure"],"content":" 欧拉回路哥尼斯堡的七桥问题引出了图论和几何拓扑学，欧拉解决了该问题 (符合条件的走法不存在) 并解决了一笔画问题：对于一个给定的图，怎样判断是否存在着一个恰好包含了所有的边，并且没有重复的路径？ 这样的图现称为欧拉图。这时遍历的路径称作欧拉路径，如果路径闭合则称为欧拉回路。欧拉给出了一笔画问题的两个判定准则： 欧拉图的必要条件是 G 中奇顶点 (度为奇数的顶点) 的数目必须是 0 或 2 形成欧拉回路的充要条件是 G 中的所有顶点度是偶数 如果无向图 G 有 2k 个奇顶点，那么可以用 k 笔画成，并且至少要用 k 笔画成 现在问题是，如何在线性时间复杂度内寻找出这条欧拉回路。这都在 DFS 下了，还用想， DFS 就完了！ 选择一个起点出发，可能遍历之后提前回到了起点，此时如果起点的所有边都已访问，那图的其他部分就不会被访问到了。此时可以在已经访问的路径上，查找还有路径没有访问的顶点，从这个新顶点开始重复刚刚的操作。直到新的路径也回到起点并没有可以继续访问的边，就将这个新的路径插入到之前的路径中。直到所有边都被访问，这样得到的路径就是一个欧拉回路。这种算法被称为 Hierholzer。 正常来说算法的时间复杂度约 \\(\\mathcal{O}(\\lvert{V}\\rvert+\\lvert{E}\\rvert)\\)，但是需要特别注意使用适当的数据结构。比如路径作为一个链表保留，这样方便后续路径的插入于替换。 ::std::list\u003cVertex *\u003e circuit; using Iter = decltype(circuit)::iterator; Iter hierholzer(Vertex \u0026v, Iter it = circuit.end()) { Iter next = circuit.emplace(it, \u0026v); Iter curr = next++; while (!v.edges.empty()) { auto wit = v.edges.begin(); auto\u0026 w = **wit; w.edges.erase(\u0026v); v.edges.erase(wit); next = hierholzer(w, next); } return curr; } 与欧拉回路相似的是哈密顿回路，即仅通过图中所有顶点一次的回路。但是这个问题并没有已知的有效算法。 ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:2:2","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#欧拉回路"},{"categories":["Algorithm⁄DataStructure"],"content":" 有向图深度优先遍历可以与遍历无向图类似的方法遍历有向图。如果图不是强连通的，那么从某个结点开始的 DFS 可能不能访问所有结点。 给定任意一个有向图，根据深度优先遍历得到一棵生成树，与无向图得到的生成树不同的是，这棵树上有一些后向边 (back edge)，即访问祖先结点，如图中的 (A,B)；一些前向边 (forward edge)，这些边从树的一个结点通向其后裔，如图中的 (C,D)；还可能有一些交叉边 (cross edge)，即将两棵直接不相关的树连接起来的边，如图中的 (F, C)、(H, F) 等。 深度优先生成森林会将遍历的先后顺序反映在森林中，左边的树总比右边的树先访问到。因此交叉边总是右边的树指向左边的树；从右向左依次遍历也就是在后续遍历这幅图。 ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:2:3","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#有向图"},{"categories":["Algorithm⁄DataStructure"],"content":" 查找强连通分量强连通分量 (SCC, Strongly Connected Component) 一个极大的强连通子图。通过执行两次 DFS 可以检测一个图是否是强连通的，如果不是强连通的，实际上得到的是顶点的子集，因为顶点到其自身是强连通的。该算法即 Kosaraju 算法，时间复杂度为 \\(\\mathcal{O}(\\lvert{E}\\rvert+\\lvert{V}\\rvert)\\)。 第一次 DFS，选取任意点为起点遍历所有未访问过的顶点，并在回溯之前给顶点编号，即后序遍历。 第二次 DFS，对于反向后的图，以编号最大的顶点开始进行 DFS。这样遍历得到的顶点集合就是一个 SCC。对所有没有访问的结点，重复此过程。 ::std::stack\u003cVertex *\u003e s; void dfs1(Vertex \u0026v) { v.visited = true; for (auto \u0026w : v.neighbors) { if (!w.visited) { dfs1(w); } } s.push(\u0026v); } void dfs2(Vertex \u0026v, ::std::list\u003cVertex *\u003e \u0026ans) { ans.emplace_front(\u0026v); v.trans_visited = true; for (auto \u0026w : v.trans_neighbors) { if (!w.trans_visited) { dfs2(w); } } } ::std::vector\u003c::std::list\u003cVertex *\u003e\u003e kosaraju() { for (auto \u0026v : graph) { if (!v.visited) { dfs1(v); } } ::std::vector\u003c::std::list\u003cVertex *\u003e\u003e ans; while (!s.empty()) { auto \u0026v = *s.top(); s.pop(); if (!v.trans_visited) { ::std::list\u003cVertex *\u003e scc; dfs2(v, scc); ans.emplace_back(scc); } } return ans; } Garbow 算法是 Tarjan 算法在 SCC 问题上的实现，其维护两个栈，一个是生成树结点栈，另一个是确定何时弹出第一个栈中同属于同一 SCC 的结点的栈。 int num{0}; ::std::stack\u003cVertex *\u003e s1, s2; ::std::vector\u003c::std::list\u003cVertex *\u003e\u003e ans; void garbow(Vertex \u0026v) { s1.push(\u0026v); s2.push(\u0026v); v.visit(++num); for (auto const \u0026w : v.neighbors) { if (!w.visited) { garbow(w); } else if (!w.in_scc) { while (s2.top()-\u003elow \u003e w.low) { s2.pop(); } } } if (s2.top() == \u0026v) { s2.pop(); ::std::list\u003cVertex *\u003e scc; Vertex *top; do { top = s1.top(); s1.pop(); scc.emplace_front(top); } while (top != \u0026v); ans.emplace_back(scc); } } ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:2:4","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#查找强连通分量"},{"categories":["Algorithm⁄DataStructure"],"content":" 拓扑排序拓扑排序 (topological sorting) 是对 DAG 顶点的一种排序，如果存在一条从 \\(v_{i}\\) 到 \\(v_{j}\\) 的路径，那么在排序中 \\(v_{j}\\) 一定出现在 \\(v_{i}\\) 之后。排序不必是唯一的，任何满足要求的排序都被认为是正确的解。 另外，存在回路的图是无法进行拓扑排序的，如果有边 \\(\u003cv_{i}, v_{j}\u003e\\) 和 \\(\u003cv_{j}, v_{i}\u003e\\)，无法同时满足 \\(v_{i}\\) 在 \\(v_{j}\\) 之前且 \\(v_{j}\\) 在 \\(v_{i}\\) 之前。 拓扑排序经典的示例是，在一系列课程的学习中，根据前置课程的关系，给出这一系列课程的正确学习顺序。 显然这是一个 DAG，对其进行拓扑排序就可以找出一条合理的学习路线，在学习某一课程之前会先学习完所有前置课程。 拓扑排序用 Kahn 算法实现，这是一个很容易理解的算法： 寻找一个度为 0 的结点 删除该结点的所有边，并将其加入到已排序队列中 重复步骤 1、2，直到没有度为 0 的结点 如果此时所有结点都已被排序，那么该图是一个 DAG，且已排序队列就是拓扑排序的结果 如果此时还有结点没有被排序，那么剩余的子图中必然存在回路 为了方便实现，这里使用了一个入度为 0 的结点的集合 starts ::std::vector\u003cVertex\u003e topological_sorting(::std::set\u003cVertex *\u003e \u0026starts) { ::std::vector\u003cVertex\u003e ans; while (!starts.empty()) { auto \u0026v = **starts.begin(); for (auto \u0026w : v.neighbors) { --w.indegree; if (w.indegree == 0) { starts.emplace(\u0026w); } } v.neighbors.clear(); ans.emplace_back(v); starts.erase(\u0026v); } return ans; } ","date":"10-07","objectID":"/2022/data_strucures_and_algorithm_analysis_008_graph/:3:0","series":["数据结构与算法分析"],"tags":["Note","Graph"],"title":"图结构","uri":"/2022/data_strucures_and_algorithm_analysis_008_graph/#拓扑排序"},{"categories":["Programming"],"content":"GinShio | CS61A Study Notes - Control, and Higher-Order Functions","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/"},{"categories":["Programming"],"content":" 控制流解释器所执行语句来执行某些操作。 比如这整个复合语句 (compound statement)，在 Python 中由 def 声明；标头 header 确定了一个简易语句 (clause) 的类型，这个语句中跟随了一个语句序列 (suite)。解释器会按一定顺序执行这个语句序列。 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:1:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#控制流"},{"categories":["Programming"],"content":" 条件语句条件语句在大部分语言中以 if 关键字呈现。 在 Python 中 True 和 False 分别表示真或假，if 引导条件语句及其真分支，零或一个 else 引导假分支，其中还可能会有零或多个 elif 进行嵌套。 def absolute_value(n): if n \u003c 0: return -n elif n == 0: return 0 else: return n 在 scheme 中 #t 和 #f 分别表示真或假，语法的话就不能 elif 进行嵌套了 (if test consequent alternative) (define (absolute-value n) (if (positive? n) n (- n))) FP 中一般都会提供一套类似 guard 的语法，即该条件语句可以接受任意多的条件判断，由上至下进行条件判断，在条件为真时执行语句块并退出条件语句，如果所有条件都不符合将有一个默认块进行兜底处理。其实这个语句更像是 if-then-elif-else 的变体。 (cond ((\u003c n 0) (- n)) ((= n 0) n) (else n)) 在 erlang 中，if 就是 cond。 absolute_value(N) -\u003e if N \u003c 0 -\u003e -N; N =:= 0 -\u003e N; true -\u003e N end. ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:1:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#条件语句"},{"categories":["Programming"],"content":" 迭代语句迭代语句也称循环语句，在满足条件的情况下运行循环体，直到不满足的情况下退出。 i = 0 total = 0 while i \u003c 3: total = total + i i = i + 1 print(total) print(total) # 0 # 1 # 3 scheme 中的循环与正常语言中的 while 差距有点大，虽然更像是 C 语言中的 for (也可能是 C 的 for 更像是 lisp 的 do)，无论怎么说，都是一种带变量变化的循环语句。 (do ((i 0 (+ i 1)) (total 0 (+ total i))) ;; assignment ((\u003e i 2) total) ;; exit (display total) ;; body (newline)) ;; 0 ;; 0 ;; 1 ;; 3 -- not print erlang 作为一种 pure 的 FP 并没有可用的 while 语句，需要使用尾递归来模拟。 loop(3, Total) -\u003e Total; loop(I, Total) -\u003e loop(I + 1, Total + I). ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#迭代语句"},{"categories":["Programming"],"content":" 示例：质因数分解质因数分解是一个典型的需要用循环、条件来查找出答案的问题。对于每个正整数 N，我们都可以分解出它的所有质因数集合： 8 = 2 * 2 * 2 9 = 3 * 3 10 = 2 * 5 11 = 11 … 比较好的一种方式是，寻找这个正整数的最小质因数，之后再继续分解剩余的部分，直到分解完成。 \\[858\\ =\\ 2 * 429\\ =\\ 2 * 3 * 143\\ =\\ 2 * 3 * 11 * 13.\\] (define (get-primes n) (let ((bits (make-vector (+ n 1) #t))) (let loop ((p 2) (ps '())) (cond ((\u003c n p) (reverse ps)) ((vector-ref bits p) (do ((i (+ p p) (+ i p))) ((\u003c n i)) (vector-set! bits i #f)) (loop (+ p 1) (cons p ps))) (else (loop (+ p 1) ps)))))) (define (get-factorization n primes) (let ((prime (car primes)) (others (cdr primes))) (if (= (remainder n prime) 0) prime (get-factorization n others)))) (define (prime-factorization n) (if (\u003c n 3) (list n) (let ((primes (get-primes n))) (let loop ((num n) (ans '())) (cond ((= num 1) (reverse ans)) (else (let ((prime (get-factorization num primes))) (loop (quotient num prime) (cons prime ans))))))))) ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:1:3","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#示例-质因数分解"},{"categories":["Programming"],"content":" 递归递归 (Recursion) 在数学与计算机科学中，是指在函数的定义中使用函数自身的方法。递归一词还较常用于描述以自相似方法重复事物的过程。例如，当两面镜子相互之间近似平行时，镜中嵌套的图像是以无限递归的形式出现的。也可以理解为自我复制的过程。 以下是一个可能更有利于理解递归过程的解释： 我们已经完成了吗？如果完成了，返回结果。如果没有这样的终止条件，递归将会永远地继续下去。 如果没有，则简化问题，解决较容易的问题，并将结果组装成原始问题的解决办法。然后返回该解决办法。 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:2:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#递归"},{"categories":["Programming"],"content":" 示例：阶乘\\[\\begin{aligned} Factorial(0) \u0026= 1,\\\\ Factorial(1) \u0026= 1,\\\\ Factorial(N) \u0026= N \\times Factorial(N - 1). \\end{aligned}\\] (define (factorial n) (cond ((= n 0) 1) (else (* n (factorial (- n 1)))))) 这个计算过程中，通过代换模型可以看出计算是一种先逐步展开而后收缩的形状，计算过程构造起一个推迟进行的操作所形成的链条，收缩阶段表现为这些运算的实际执行，这种计算过程被称为递归计算过程。如果要执行这个过程，解释器就必须维护好以后要执行的操作的轨迹，这个例子中推迟执行的乘法链条的长度也就是为保存其轨迹需要保存的信息量，这个长度随着 n 值的增加而线性增长，这个过程被称为线性递归计算。 (factorial 5) (* 5 (factorial 4)) (* 5 (* 4 (factorial 3))) (* 5 (* 4 (* 3 (factorial 2)))) (* 5 (* 4 (* 3 (* 2 (factorial 1))))) (* 5 (* 4 (* 3 (* 2 (* 1 (factorial 0)))))) (* 5 (* 4 (* 3 (* 2 (* 1 1))))) (* 5 (* 4 (* 3 (* 2 1)))) (* 5 (* 4 (* 3 2))) (* 5 (* 4 6)) (* 5 24) 120 来看另一种实现 (define (factorial n) (let factorial-iter ((product 1) (counter 1)) (if (\u003e counter n) product (factorial-iter (* counter product) (1+ counter))))) 这个计算过程中没有任何增长或收缩，计算过程的每一步，需要保存的轨迹就是变量 product 和 counter 的当前值，我们称这个过程为迭代计算过程。迭代计算过程就是那种其状态可以用固定数目的状态变量描述的计算过程，同时又存在一套固定的规则描述了计算过程从一个状态到另一个状态转换时状态变量的更新方式，还有一个结束状态的检测用以描述计算过程如何终止。 (factorial-iter 1 1 5) (factorial-iter 1 2 5) (factorial-iter 2 3 5) (factorial-iter 6 4 5) (factorial-iter 24 5 5) (factorial-iter 120 6 5) 120 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:2:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#示例-阶乘"},{"categories":["Programming"],"content":" 示例：斐波那契数列\\[\\begin{aligned} Fibonacci(0) \u0026= 0,\\\\ Fibonacci(1) \u0026= 1,\\\\ Fibonacci(N) \u0026= Fibonacci(N - 1) + Fibonacci(N - 2). \\end{aligned}\\] 可以看出斐波那契数列是一个天然递归的函数，函数递归在之前的代码中已经遇到了，直接看代码实现。 (define (fibonacci n) (cond ((= n 0) 0) ((= n 1) 1) (else (+ (fibonacci (- n 1)) (fibonacci (- n 2)))))) 如果将 fibonacci 函数的调用图画出来，可以看到它就像一棵树一样，这样的递归被称为 树形递归。但是其中有大量的重复计算，会导致无意义的计算从而浪费 CPU 的性能。 由于这种计算斐波那契数列的方法很糟糕，做了很多冗余计算，其递归次数跟随 n 的大小指数增加，因此我们需要使用迭代的方法来优化这个求解过程。 (define (fibonacci n) (let fibonacci-iter ((a 1) (b 0) (counter 1)) (if (\u003e counter n) b (fibonacci-iter (+ a b) a (+ counter 1))))) 树形递归计算过程并不是无用的，当考虑在层次结构性的数据上操作，而不是对数操作时，树形递归计算过程是一种自然、威力强大的工具，可以帮助我们理解与设计程序。 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:2:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#示例-斐波那契数列"},{"categories":["Programming"],"content":" 示例：判断奇偶数如果不能直接使用取模的方法判断奇偶数，那么有一个简单且明了的方式 – 询问前一个数是否是奇 / 偶数。显然这是个递归问题，我们需要不断向前询问，直到得到答案。 (define (odd? n) (if (= n 0) #f (even? (- n 1)))) (define (even? n) (if (= n 0) #t (odd? (- n 1)))) 这种有多个函数互相递归调用的方式，称其为间接递归。 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:2:3","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#示例-判断奇偶数"},{"categories":["Programming"],"content":" 高阶函数高阶函数 (Higher-Order Functions) 是一类将函数作为参数、返回值进行传递的函数，这种特性多发生在具有 FP 特性的语言中，往往这些语言还会同时提供 lambda。 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:3:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#高阶函数"},{"categories":["Programming"],"content":" lambda 表达式lambda 表达式 是一种简化的定义函数的方式，可以捕获当前环境中的一些变量，也被称为闭包 (clause)。lambda 往往伴随着高阶函数出现，通常是传递条件谓词时，创建一个对应的 lambda 对象，而不是创建一个函数传递。 (lambda (x) (= x 0)) lambda 与正常函数无异，由三部分组成 – 标头 (lambda 关键字)、参数列表和函数体组成。另外一点，lambda 无法递归，如果想要递归 lambda 需要使用 Y 组合子。1 , 2 lambda 中捕获的环境变量是可以直接使用的 (define (zero? x) ((lambda () (= x 0)))) (zero? 0) ;; #t (zero? 1) ;; #f ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:3:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#lambda-表达式"},{"categories":["Programming"],"content":" 设计函数对于一个函数，在设计一个函数时，需要注意三个方面： 每个函数应该只有一个精确的任务，执行多个任务的函数应该被拆分为多个函数 不要重复自己 (DRY, Don’t repeat yourself)，如果你发现自己在复制粘贴一段代码，你可能已经发现了一个机会用函数抽象 函数应该被设计的更通用，比如不提供 square 和 cube，而是提供 pow，指定幂来分别实现 square 和 cube ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:3:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#设计函数"},{"categories":["Programming"],"content":" 示例：累加计算比如你现在需要计算累和，包括但不限于 \\(\\sum_{i=1}^{n}{i}\\)、\\(\\sum_{i=1}^{n}{i^{2}}\\) 等，根据设计函数的 3 个方面，我们需要设计一个用于累加的函数！另外，这个函数需要有足够的抽象，来提供泛用性。 那么我可以定义两个参数 start 和 end 用于标识累加函数的上下限，那么最重要的如何累加应该怎么告诉这个函数呢？将这个函数设计为高阶函数！ (define (summation start end term) (let summation-iter ((counter start) (value 0)) (if (\u003e counter end) value (summation-iter (+ counter 1) (+ value (term counter)))))) (summation 0 10 (lambda (x) x)) ;; sum (i), 55 (summation 0 10 (lambda (x) (* x x))) ;; sum (i^2), 385 (summation 0 10 (lambda (x) (sqrt x))) ;; sum (sqrt(i)), 22.4682 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:3:3","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#示例-累加计算"},{"categories":["Programming"],"content":" 示例：柯里化柯里化 (currying) 是数学和 FP 的重要特性，将接收多个参数的函数转换为接收一个参数的函数，并且返回接收余下的参数而且返回结果的新函数的技术。所以这三个表达式是等价的。 \\[ \\begin{align} x \u0026= f(a, b, c)\\\\ \u0026 \\left\\{\\begin{aligned} h \u0026= g(a)\\\\ i \u0026= h(b)\\\\ x \u0026= i(c) \\end{aligned}\\right.\\\\ x \u0026= g(a)(b)(c) \\end{align}\\] (define (sum a b) (+ a b)) (define (sum-curry a) (lambda (b) (+ a b))) (define add10 (sum-curry 10)) (add10 5) ;; 15 (sum 10 5) ;; 15 ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:3:4","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#示例-柯里化"},{"categories":["Programming"],"content":" Lab 1: Functions, Control 计算排列 数学概念排列 \\(P_{k}^{n} = \\frac{n!}{(n-k)!}\\)，数学概念组合 \\(C_{k}^{n} = \\binom{n}{k} = \\frac{P_{k}^{n}}{k!} = \\frac{n!}{k!(n-k)!}\\)。实现一个计算排列的函数 (define (falling n k) \"Compute the falling factorial of n to depth k.\" (define factorial (lambda (n) (cond ((= n 0) 1) ((= n 1) 1) (else (* n (factorial (- n 1))))))) (/ (factorial n) (factorial (- n k)))) 计算整数 n 的每位数字之和 (define (sum-digits n) \"Sum all the digits of y.\" (let sum-digits-iter ((num n) (val 0)) (if (= num 0) val (sum-digits-iter (quotient num 10) (+ val (remainder num 10)))))) 查询整数 n 是否有两个连续的 8 (define (double-eights n) \"Return true if n has two eights in a row.\" (let double-eights-iter ((num n) (prev #f)) (if (= num 0) #f (let ((curr (= (remainder num 10) 8))) (or (and curr prev) (double-eights-iter (quotient num 10) curr)))))) ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:4:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#lab-1-functions-control"},{"categories":["Programming"],"content":" Homework 2: Higher Order Functions product 计算 \\(term(1) \\times term(2) \\times \\cdots \\times term(n)\\) (define (product n term) \"Return the product of the first n terms in a sequence.\" (let product-iter ((counter 1) (init 1)) (if (\u003e counter n) init (product-iter (+ counter 1) (* init (term counter)))))) accumulate 累加函数 (define (accumulate merger init n term) \"Return the result of merging the first n terms in a sequence and start. The terms to be merged are term(1), term(2), ..., term(n). merger is a two-argument commutative function.\" (let accumulate-iter ((counter 1) (value init)) (if (\u003e counter n) value (accumulate-iter (+ counter 1) (merger value (term counter)))))) ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:5:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#homework-2-higher-order-functions"},{"categories":["Programming"],"content":" Project 1: The Game of Hog I know! I’ll use my Higher-order functions to Order higher rolls. 在 Hog 中，两个玩家轮流尝试接近目标，成为第一个总分至少到达目标的玩家胜利，默认目标为 100 分。每次尝试，玩家选择至多十个色子进行投掷。玩家的得分是本轮所有骰子点数之和。一名玩家如果投掷太多骰子将会面临一定风险： Sow Sad，如果任何骰子的结果为 1，则当前玩家的回合得分为 1。 在一局正常的 Hog 游戏中，这就是所有的规则了。为了增加一些游戏特色，我们将添加一些特殊规则： Pig Tail，一名玩家如果选择投掷 0 个骰子，他将得 \\(2 \\times \\lvert{tens - ones}\\rvert + 1\\) 分。其中 tens 和 ones 指对手分数的的十位数字和个位数字。 Square Swine，当一名玩家在他的回合获得了分数，且最终结果是一个完全平方数，那么将他的分数设置为下一个完全平方数。 Why oh why Y? ↩︎ 为什么是Y? ↩︎ ","date":"09-25","objectID":"/2022/cs61a_02_control_and_higher_order_functions/:6:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"控制流、递归、高阶函数","uri":"/2022/cs61a_02_control_and_higher_order_functions/#project-1-the-game-of-hog"},{"categories":["CompilerPrinciple"],"content":"GinShio | 橡书第 8 章：优化简介","date":"09-23","objectID":"/2022/introduction_to_optimization/","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/"},{"categories":["CompilerPrinciple"],"content":" 优化背景上世纪 80 年代早期优化在编译器开发中还是一个可选特性，一般在其他部分都完成后才会添加到编译器中。因此出现了调试编译器和优化编译器的区别，即前者强调编译速度，因此可执行代码与源码之间存在较强的对应关系；后者强调最小化或最大化可执行程序的某些属性。因此优化编译器会花费更多时间来编译，生成质量更好的代码，通常这个过程伴随着大量移动操作，使调试变得困难。 从 RISC 开始流行，运行时性能开始需要编译器的优化。分支指令的延迟槽、非阻塞内存操作、流水线使用的增多以及功能单元数目的增加等，这些特性使得处理器性能不仅收程序布局和结构方面的制约，还受到指令调度和资源分配等底层细节的限制。 优化编译器现在变得司空见惯 (反而 go 是异类)，进而使编译器改变成了前端、后端的架构，优化将前端与性能问题分割开来。优化假定后端会处理资源分配的问题，因而假定针对具有无限寄存器、内存和功能单元的理想机器进行优化。这也对编译器后端产生了更大压力。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:1:0","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#优化背景"},{"categories":["CompilerPrinciple"],"content":" 示例：改进数组的地址计算如果编译器前端对数组的引用 m[i, j] 生成的 IR 没有关于 m、i、j 的信息或不了解外围的上下文，编译器如果按照默认的行主序处理地址。生成的表达式类似 \\[m + (i - low_{1}(m)) \\times (high_{2}(m) - low_{2}(m) + 1) \\times w + (j - low_{2}(m)) \\times w.\\] m 是数组的首地址，\\(low_{i}(m)\\) 和 \\(high_{i}(m)\\) 分别表示 m 的第 i 维的下界和上界，w 是 m 中一个元素的字节长度。如何降低该计算的代价，直接取决于对该数组变量极其上下文的分析。如果数组 m 是局部变量并各维度下界均从 1 开始，且上界已知，那么就可以将计算简化为 \\[m + (i - 1) \\times high_{2}(m) \\times w + (j - 1) \\times w.\\] 如果引用出现在循环内部，且循环中 i 从 1 变动到 I，那么编译器可以使用运算符强度折减 (OSR, Operator Strength Reduction) 将 \\((i - 1) \\times high_{2}(m) \\times w\\) 替换为 \\(i^{’}_{x} = i^{’}_{x - 1} + high_{2}(m) \\times w\\) (其中 \\(i^{’}_{1} = 0\\))。同样地，如果 j 也是个循环的归纳变量 (IV, Induction Variable)，且 j 从 1 变动到 J，那么经过 OSR 后就有了 \\(j^{’}_{y} = j^{’}_{y - 1} + w\\) (其中 \\(j^{’}_{1} = 0\\))。经过两次 OSR 后，只需要计算此式 \\[m + i^{’} + j^{’}.\\] ","date":"09-23","objectID":"/2022/introduction_to_optimization/:1:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#示例-改进数组的地址计算"},{"categories":["CompilerPrinciple"],"content":" 示例：改进循环嵌套LINPACK 是一个线性代数库的数值计算库，当然现在也是一个不错的性能测试库。其中的 dmxpy 是一个经典的能够说明上下文作用的例子。源码可以在这里找到。 代码主要是 dmxpy 的主循环部分 (这个小括号，它是 lisp!)。 /** * @prototype * void dmxpy(n1, y, n2, ldm, x, m) * REAL y[], x[], m[]; * int n1, n2, ldm; * * @purpose * multiply matrix m times vector x and add the result to vector y. * * @parameters * n1 integer, number of elements in vector y, and number of rows in * matrix m * y double [n1], vector of length n1 to which is added * the product m*x * n2 integer, number of elements in vector x, and number of columns * in matrix m * ldm integer, leading dimension of array m * x double [n2], vector of length n2 * m double [ldm][n2], matrix of n1 rows and n2 columns * * @note * We would like to declare m[][ldm], but c does not allow it. In this * function, references to m[i][j] are written m[ldm*i+j]. */ /* main loop - groups of sixteen vectors */ jmin = (n2 % 16) + 16; for (j = jmin - 1; j \u003c n2; j = j + 16) { for (i = 0; i \u003c n1; i++) y[i] = ((((((((((((((((y[i]) + x[j - 15] * m[ldm * (j - 15) + i]) + x[j - 14] * m[ldm * (j - 14) + i]) + x[j - 13] * m[ldm * (j - 13) + i]) + x[j - 12] * m[ldm * (j - 12) + i]) + x[j - 11] * m[ldm * (j - 11) + i]) + x[j - 10] * m[ldm * (j - 10) + i]) + x[j - 9] * m[ldm * (j - 9) + i]) + x[j - 8] * m[ldm * (j - 8) + i]) + x[j - 7] * m[ldm * (j - 7) + i]) + x[j - 6] * m[ldm * (j - 6) + i]) + x[j - 5] * m[ldm * (j - 5) + i]) + x[j - 4] * m[ldm * (j - 4) + i]) + x[j - 3] * m[ldm * (j - 3) + i]) + x[j - 2] * m[ldm * (j - 2) + i]) + x[j - 1] * m[ldm * (j - 1) + i]) + x[j] * m[ldm * j + i]; } 作者为提高性能将外层的循环展开了 16 次，这消除了 15 次额外的加法与绝大多数 load / store 操作。为了应对其他情况，在主循环之上，还分别处理了 1、2、4、8 列的情况，保证最终待处理的列为 16 的倍数。 当然，可以将其简化为我们常写的样子。 for (j = 0; j \u003c n2; j++) { for (i = 0; i \u003c n1; i++) y[i] = y[i] + x[j] * m[ldm * j + i]; } 理想情况下编译器可以将普通的循环变换为这种高效的版本，或某种适用于目标机的形式。但是，编译器不能保证有目标机所需的所有优化。进行手工循环展开可以为多种目标机提供良好的性能。但从编译器的角度看，循环展开有 16 个关于 m 的表达式、15 个关于 x 的表达式，以及一个关于 y 的表达式。如果编译器不能简化地址计算，将产生大量的整数计算。 由于循环中并不会改变 x 的值，因此可以将 x 的地址计算与 load 移出内循环。另外就是将 x 的基址保存在寄存器中，也能节省很大一部分开销。对引用 x 中的元素 \\(x[j - k]\\)，地址计算就是 \\(x + (j - k) * w\\)，进一步化简 \\(x + jw - kw\\)，也就是说循环展开后，每次 x 的基址为 \\(x + jw\\)，load 操作将有相同的基址和不同的偏移量。 虽然 m 的元素也不会被改变，但是每次内循环都会改变引用的元素，因此无法将 load 运算外提来削减 load 带来的开销。但是同理可以使用相同基址的方法减少计算的消耗。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:1:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#示例-改进循环嵌套"},{"categories":["CompilerPrinciple"],"content":" 对优化的考虑优化变换的核心就是两个问题 – 安全性 和 可获利性。安全性就是保证变换将保持程序原有的语义，而可获利性就是保证变换是有利可图的。如果不能同时满足时，那么编译器就不应该采用该变换。 一般来说可供优化编译器利用的时机有几种不同的来源 减少抽象开销，程序设计语言引入的数据结构和类型需要运行时支持，优化器可以通过分析和变换来减少这种开销。 利用特性，通常编译器可以利用操作执行时所处上下文的相关信息，来特化该操作。 将代码与系统资源匹配 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:1:3","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#对优化的考虑"},{"categories":["CompilerPrinciple"],"content":" 优化的范围优化可以在不同粒度或范围上运行。主要有四种范围：局部的、区域性的、全局的和整个程序。 局部方法 局部方法作用于单个 BB，在不考虑异常的情况下，BB 有两个重要性质：语句是顺序执行的；如果任一语句执行，那么整个块必将执行。因此编译器可以在局部获得更有利于优化的信息。 区域性方法 区域性方法的作用范围大于单个 BB，但小于一个完整的过程。编译器通常采用 扩展基本程序块 (EBB, Extended Basic Block) 的 BB 集合来考察一个区域，从而得出一些有利于优化的信息。 区域性方法将变换的范围限制到小于整个过程的区域上，使得编译器将工作重点集中在频繁执行的语句上。并且可以针对不同的区域采用不同的优化策略。 全局方法 (过程内方法) 局部最优解在全局范围下不一定是全局最优解，过程为编译器提供了一个自然的边界，封装和隔离了运行时环境，并且有些系统中过程也充当了编译的单位。通常过程内方法构建过程的表示 (如 CFG)，并分析该表示，在分析之后根据信息来完成具体的变换。借助过程内视图，可以发现一些局部方法和区域性方法都无发发现的优化时机。 全程序方法 (过程间方法) 通常过程间分析和优化作用于调用图，经典的例子是 内联替换 (inline substitution) 和 过程间常数传递 (interprocedural constant propagation)。 信息 扩展基本程序块 (EBB, Extended Basic Block) 是一组基本程序块的最大集合： 只有第一个 BB 可以拥有多个前驱结点 集合中的其余结点只能拥有一个前驱结点 如图，可以将这个 CFG 划分为 3 个 EBB：\\(\\{B_{0}, B_{1}, B_{2}, B_{3}, B_{4}\\}\\)、 \\(\\{B_{5}\\}\\) 和 \\(\\{B_{6}\\}\\)。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:2:0","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#优化的范围"},{"categories":["CompilerPrinciple"],"content":" 局部优化局部优化是编译器能够使用的最简单且非常有效的优化方法。常用的手法是 局部值编号 (local value numbering)，通过重用此前计算过的值来替换冗余的求值 树高平衡 (tree-height balancing)，用于重新组织表达式树，揭示更多指令层级的并行性 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:0","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#局部优化"},{"categories":["CompilerPrinciple"],"content":" 局部值编号就像之前提到的名字对编译器的影响，消除冗余计算的同时，也会扩展或缩短相关变量的生命周期。假定所有冗余消除是有利可图的，最古老且强大的方法就是局部值编号 (LVN, Local Value Numbering)。 另外需要主要的是，LVN 旨在消除冗余计算，因此每次对相应值的使用都会对生命周期进行延长或缩短。如将 \\(d\\leftarrow{}a-d\\) 替换为 \\(d\\leftarrow{}b\\) 会增长 b 的生命周期，但会减少 a 或 d 的生命周期。 LVN 算法算法遍历 BB，并为程序块计算的每个值分配一个不同的编号。算法会为值选择编号，使得给定两个表达式 \\(e_{i}\\) 和 \\(e_{j}\\)，当且仅当表达式的所有可能的运算对象，都可以验证 \\(e_{i}\\) 和 \\(e_{j}\\) 具有相等的值时，二者具有相同的值编号。 LVN 算法的输入是一个具有 n 个二元运算的基本程序块，每个运算形如 \\(T_{i} \\leftarrow{} L_{i}\\ [Op_{i}]\\ [R_{i}]\\)，算法按顺序考察每个运算。通常使用散列表将名字、常数和表达式映射到不同的值编号。为处理第 i 个运算，LVN 在散列表中查找 \\(L_{i}\\) 和 \\(R_{i}\\)，并获取二者对应的值编号。如果找到对应的表项就使用该值编号；否则，创建一个新的表项并分配一个新的值编号。 \\(L_{i}\\) 和 \\(R_{i}\\) 的值编号分别记作 \\(VN(L_{i})\\) 和 \\(VN(R_{i})\\)，LVN 基于表达式 \\(\u003cVN(L_{i}),\\ Op_{i},\\ VN(R_{i})\u003e\\) 构造散列键，并查找该键。如果存在对应的表项则说明该表达式是冗余的；否则认为是第一次计算该表达式，算法为对应的表达式键创建对应的表项，并分配一个新的值编号。 for expr in BasicBlock do get the value number \\(VN(L_{i})\\) and \\(VN(R_{i})\\) construct a hash key h from expr (using \\(Op_{i}\\), \\(VN(L_{i})\\) and \\(VN(R_{i})\\)) if h is already present in the table then replace operation value into \\(T_{i}\\) associate the value number with \\(T_{i}\\) else insert a new value number into table at the hash key location record the new value for \\(T_{i}\\) end end 扩展 LVN 算法LVN 还可以进行其他几种局部优化 交换运算，对于可交换的运算来说，如果仅运算操作数的顺序不同，它们将分配相同的值编号 常量合并，如果一个运算的所有运算对象都是已知的常数项，那么 LVN 可以在编译时计算并将结果进行合并。 代数恒等式，LVN 可以用代数恒等式来简化代码。如 \\(x+0\\) 和 \\(x\\) 应该分配相同的编号。 for expr in BasicBlock, do get the value number \\(VN(L_{i})\\) and \\(VN(R_{i})\\) if \\(L_{i}\\) and \\(R_{i}\\) are both constant then evaluate \\(L_{i}\\) and \\(R_{i}\\) assign the result to \\(T_{i}\\)适当的特定邮寄气得方法进行编码 if h is already present in the table, then replace operation value into \\(T_{i}\\) associate the value number with \\(T_{i}\\) else insert a new value number into table at the hash key location record the new value for \\(T_{i}\\) end end ","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#局部值编号"},{"categories":["CompilerPrinciple"],"content":" 局部值编号就像之前提到的名字对编译器的影响，消除冗余计算的同时，也会扩展或缩短相关变量的生命周期。假定所有冗余消除是有利可图的，最古老且强大的方法就是局部值编号 (LVN, Local Value Numbering)。 另外需要主要的是，LVN 旨在消除冗余计算，因此每次对相应值的使用都会对生命周期进行延长或缩短。如将 \\(d\\leftarrow{}a-d\\) 替换为 \\(d\\leftarrow{}b\\) 会增长 b 的生命周期，但会减少 a 或 d 的生命周期。 LVN 算法算法遍历 BB，并为程序块计算的每个值分配一个不同的编号。算法会为值选择编号，使得给定两个表达式 \\(e_{i}\\) 和 \\(e_{j}\\)，当且仅当表达式的所有可能的运算对象，都可以验证 \\(e_{i}\\) 和 \\(e_{j}\\) 具有相等的值时，二者具有相同的值编号。 LVN 算法的输入是一个具有 n 个二元运算的基本程序块，每个运算形如 \\(T_{i} \\leftarrow{} L_{i}\\ [Op_{i}]\\ [R_{i}]\\)，算法按顺序考察每个运算。通常使用散列表将名字、常数和表达式映射到不同的值编号。为处理第 i 个运算，LVN 在散列表中查找 \\(L_{i}\\) 和 \\(R_{i}\\)，并获取二者对应的值编号。如果找到对应的表项就使用该值编号；否则，创建一个新的表项并分配一个新的值编号。 \\(L_{i}\\) 和 \\(R_{i}\\) 的值编号分别记作 \\(VN(L_{i})\\) 和 \\(VN(R_{i})\\)，LVN 基于表达式 \\(\\) 构造散列键，并查找该键。如果存在对应的表项则说明该表达式是冗余的；否则认为是第一次计算该表达式，算法为对应的表达式键创建对应的表项，并分配一个新的值编号。 for expr in BasicBlock do get the value number \\(VN(L_{i})\\) and \\(VN(R_{i})\\) construct a hash key h from expr (using \\(Op_{i}\\), \\(VN(L_{i})\\) and \\(VN(R_{i})\\)) if h is already present in the table then replace operation value into \\(T_{i}\\) associate the value number with \\(T_{i}\\) else insert a new value number into table at the hash key location record the new value for \\(T_{i}\\) end end 扩展 LVN 算法LVN 还可以进行其他几种局部优化 交换运算，对于可交换的运算来说，如果仅运算操作数的顺序不同，它们将分配相同的值编号 常量合并，如果一个运算的所有运算对象都是已知的常数项，那么 LVN 可以在编译时计算并将结果进行合并。 代数恒等式，LVN 可以用代数恒等式来简化代码。如 \\(x+0\\) 和 \\(x\\) 应该分配相同的编号。 for expr in BasicBlock, do get the value number \\(VN(L_{i})\\) and \\(VN(R_{i})\\) if \\(L_{i}\\) and \\(R_{i}\\) are both constant then evaluate \\(L_{i}\\) and \\(R_{i}\\) assign the result to \\(T_{i}\\)适当的特定邮寄气得方法进行编码 if h is already present in the table, then replace operation value into \\(T_{i}\\) associate the value number with \\(T_{i}\\) else insert a new value number into table at the hash key location record the new value for \\(T_{i}\\) end end ","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#lvn-算法"},{"categories":["CompilerPrinciple"],"content":" 局部值编号就像之前提到的名字对编译器的影响，消除冗余计算的同时，也会扩展或缩短相关变量的生命周期。假定所有冗余消除是有利可图的，最古老且强大的方法就是局部值编号 (LVN, Local Value Numbering)。 另外需要主要的是，LVN 旨在消除冗余计算，因此每次对相应值的使用都会对生命周期进行延长或缩短。如将 \\(d\\leftarrow{}a-d\\) 替换为 \\(d\\leftarrow{}b\\) 会增长 b 的生命周期，但会减少 a 或 d 的生命周期。 LVN 算法算法遍历 BB，并为程序块计算的每个值分配一个不同的编号。算法会为值选择编号，使得给定两个表达式 \\(e_{i}\\) 和 \\(e_{j}\\)，当且仅当表达式的所有可能的运算对象，都可以验证 \\(e_{i}\\) 和 \\(e_{j}\\) 具有相等的值时，二者具有相同的值编号。 LVN 算法的输入是一个具有 n 个二元运算的基本程序块，每个运算形如 \\(T_{i} \\leftarrow{} L_{i}\\ [Op_{i}]\\ [R_{i}]\\)，算法按顺序考察每个运算。通常使用散列表将名字、常数和表达式映射到不同的值编号。为处理第 i 个运算，LVN 在散列表中查找 \\(L_{i}\\) 和 \\(R_{i}\\)，并获取二者对应的值编号。如果找到对应的表项就使用该值编号；否则，创建一个新的表项并分配一个新的值编号。 \\(L_{i}\\) 和 \\(R_{i}\\) 的值编号分别记作 \\(VN(L_{i})\\) 和 \\(VN(R_{i})\\)，LVN 基于表达式 \\(\\) 构造散列键，并查找该键。如果存在对应的表项则说明该表达式是冗余的；否则认为是第一次计算该表达式，算法为对应的表达式键创建对应的表项，并分配一个新的值编号。 for expr in BasicBlock do get the value number \\(VN(L_{i})\\) and \\(VN(R_{i})\\) construct a hash key h from expr (using \\(Op_{i}\\), \\(VN(L_{i})\\) and \\(VN(R_{i})\\)) if h is already present in the table then replace operation value into \\(T_{i}\\) associate the value number with \\(T_{i}\\) else insert a new value number into table at the hash key location record the new value for \\(T_{i}\\) end end 扩展 LVN 算法LVN 还可以进行其他几种局部优化 交换运算，对于可交换的运算来说，如果仅运算操作数的顺序不同，它们将分配相同的值编号 常量合并，如果一个运算的所有运算对象都是已知的常数项，那么 LVN 可以在编译时计算并将结果进行合并。 代数恒等式，LVN 可以用代数恒等式来简化代码。如 \\(x+0\\) 和 \\(x\\) 应该分配相同的编号。 for expr in BasicBlock, do get the value number \\(VN(L_{i})\\) and \\(VN(R_{i})\\) if \\(L_{i}\\) and \\(R_{i}\\) are both constant then evaluate \\(L_{i}\\) and \\(R_{i}\\) assign the result to \\(T_{i}\\)适当的特定邮寄气得方法进行编码 if h is already present in the table, then replace operation value into \\(T_{i}\\) associate the value number with \\(T_{i}\\) else insert a new value number into table at the hash key location record the new value for \\(T_{i}\\) end end ","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#扩展-lvn-算法"},{"categories":["CompilerPrinciple"],"content":" 树高平衡编译器对一个计算进行编码的具体细节会影响到编译器优化该计算的能力，许多现代处理器有多个功能单元，因而可以在每个周期中执行多个独立的操作。如果编译器可以通过对指令流的编排使之包含独立的多个操作，并适合于特定机器，那么应用程序会运行得更快。 如代码 a + b + c + d + e + f + g + h ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 对于该代码，可以左递归求值生成一棵左结合树，亦或右递归语法建立右结合树。但是建立一棵平衡树可以减少递归求值的约束，比如左结合树中 \\(a+b\\) 必须在涉及 g 或 h 的加法之前执行。 如果处理器每次可执行多个加法，左、右结合的树只能依次调度，而平衡树代码可以并行调度。这种优化利用了结合律和交换律，揭示表达式求值中的指令级并行，从而改进执行时间。 如果将树高平衡转换为算法，算法分为 分析 和 转换 两个步骤： 识别程序块中的候选表达式树。候选表达式树的运算符必须是相同的，且必须是可交换的和可结合的。同样，候选表达式树内部结点的每个名字都必须刚好是用一次。 对于每个候选树，算法将找到所有的运算对象，并将所有运算对象输入到一个优先队列，按等级递增的次序排列。 寻找候选树一个基本程序块由一个或多个混合计算组成，编译器可以将其中的 IR 解释成一个 DDG，该图记录了值的流动和对各个操作的执行顺序约束。 ;; LLVM IR %t = mul nsw i32 %a, %b %u = sub nsw i32 %c, %d %v = add nsw i32 %t, %u %w = mul nsw i32 %t, %u 一般地，DDG 不会形成一棵树，而是由多棵树交织组成。平衡算法所需的各种候选表达式树都是 DDG 中的不同子集。 在算法重排各个运算对象时，规模较大的候选树能够提供更多的重排机会。因此，算法试图构造最大规模的候选树。概念上算法找到每个候选树都可以看作是一个 n 元运算符 (n 尽可能大)。因此某些因素会限制候选树的规模 树不可能大过它表示的程序块 重写无法改变程序程序块的 可观察量，即程序块以外使用的任何值都必须像原来的代码中那样计算，且保留其值。类似地，任何在程序块中使用多次的值都必须保留 树反向扩展时不能超过程序块的起始位置 信息 如果一个值在某个代码片段之外是可读取的，那么该值相对于该代码片段是可观察的。 在查找树阶段，对程序中定义的名字 \\(T_{i}\\) 都需要知道何处引用了 \\(T_{i}\\)，因此算法包括一个 \\(Uses(T_{i})\\) 的集合，即使用了 \\(T_{i}\\) 的操作、指令的索引。 算法首先遍历程序块中的各个操作，判断每一个操作是否一定要将该操作作为其自身所属树的根结点。找到根节点时，会将该操作定义的名字添加到一个由名字组成的优先队列中，该队列按根结点运算符的优先级排序。假定操作 i 形如 \\(T_{i}\\leftarrow{}L_{i}\\,Op_{i}\\,R_{i}\\)，且 \\(Op_{i}\\) 是可交换和可结合的。那么下列条件之一成立时，将 \\(Op_{i}\\) 标记的为根结点，并将其加入优先队列。 如果 \\(T_{i}\\) 使用多次，那么操作 i 必须被标记为根结点，以确保对所有使用 \\(T_{i}\\) 的操作，\\(T_{i}\\) 都是可用的，对 \\(T_{i}\\) 的多次使用使之成为一个可观察量。 如果 \\(T_{i}\\) 只在操作 j 中使用一次，但 \\(Op_{i}\\,\\ne\\,Op_{j}\\)，那么操作 i 必是根结点，因为它不可能是包含 \\(Op_{j}\\) 的树的一部分。 roots = priority queue of names for i in range(0, n - 1), rank(\\(T_{i}\\)) = -1 do if \\(Op_{i}\\) is commutative and associative and (\\(\\lvert{Uses(T_{i})}\\rvert\\) \u003e 1 or (\\(\\lvert{Uses(T_{i})}\\rvert\\) = 1 and \\(Op_{Uses(T_{i})} \\ne Op_{i}\\))) then mark \\(T_{i}\\) as a root Enqueue(roots, \\(T_{i}\\), precedence of \\(Op_{i}\\)) end end 重构程序块使之平衡接下来算法以候选树根结点的队列作为输入，并根据每个根结点建立一个大体的平衡树。这个阶段使用三个模块：Balance、Flatten、Rebuild。 while Roots is not empty do var = Dequeue(Roots) Balance(var) end Balance 对根结点进行操作，分配一个新的优先队列来容纳当前树的所有操作数，使用 Flatten 递归遍历树，为每个操作数指派等级并将其添加到队列中。 Balance(root) if Rank(root) \u003e= 0 then return end q = new queue of names Rank(root) = Flatten(\\(L_{i}\\), q) + Flatten(\\(R_{i}\\), q) Rebuild(q, \\(Op_{i}\\)) end Flatten(var, q) if var is a constant then Rank(var) = 0 Enqueue(q, var, Rank(var)) else if var \\(\\in\\) UEVar(b) then Rank(var) = 1 Enqueue(q, var, Rank(var)) else if var is a root then Balance(var) Enqueue(q, var, Rank(var)) else Flatten(\\(L_{j}\\), q) Flatten(\\(R_{j}\\), q) end return Rank(var) end Rebuild 使用了一个简单的算法来构造新的代码序列，它重复从树中移除两个等级最低的项。该函数将输出一个操作来合并这两项。它会为结果分配一个等级，然后将结果插回到优先队列中，直到队列为空。 Rebuild(q, op) while q is not empty do NL = Dequeue(q) NR = Dequeue(q) if NL and NR are both constants then NT = Fold(op, NL, NR) if q is empty then # root = NT Rank(root) = 0 else Enqueue(q, NT, 0) Rank(NT) = 0 end else if q is empty then NT = root else NT = new name end # NT = NL op NR Rank(NT) = Rank(NL) + Rank(NR) if q is not empty then Enqueue(q, NT, Rank(NT)) end end end end 在该算法中，有些细节： 在遍历候选树时，Flatten 可能会遇到另一棵树的根结点。它会递归调用 Balance 而非 Flatten，一边为候选子树的根结点创建一个新的优先队列，并确保编译器在输出引用子树值的代码之前，先对优先级较高的子树输出代码。 程序块由三种引用：常数、本程序块中先定义后使用的名字、向上展现的名字。 Flatten 例程分别处理每种情形。 算法设计常数等级为零，因此常数可以移动到队列前端，用 Fold 进行编译期对常数进行计算，并到新的名字加入到树中。叶结点的等级为 1，内部结点的等级等于其所在子树所有结点等级之和。这种指派等级的方法将生成一种近似于平衡二叉树的树状结构。 树高平衡算法的例子回到表达式 \\(a + b + c + d + e + f + g + h\\)，重新看看 IR ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 假设只有 %7 在程序块之外使用，那么只有 Uses(%7) \u003e 1，因此只有 %7 被作为候选树的根。平衡时扁平化树将会得到以下队列 \\[\u003ch, 1\u003e, \u003cg, 1\u003e, \u003cf, 1\u003e, \u003ce, 1\u003e, \u003cd, 1\u003e, \u003cc, 1\u003e, \u003cb, 1\u003e, \u003ca, 1\u003e.\\] Rebuild 会从队列中取出 \\(\u003ch, 1\u003e\\) 和 \\(\u003cg, 1\u003e\\) 然后将 \\(\u003cn_{0}, 2\u003e\\) 加入队列。 \\[\u003cf, 1\u003e, \u003ce, 1\u003e, \u003cd, 1\u003e, \u003cc, 1\u003e, \u003cb, 1\u003e, \u003ca, 1\u003e, \u003cn_{0}, 2\u003e.\\] Rebuild 构建四次后，队列将成为以下 \\[\u003cn_","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#树高平衡"},{"categories":["CompilerPrinciple"],"content":" 树高平衡编译器对一个计算进行编码的具体细节会影响到编译器优化该计算的能力，许多现代处理器有多个功能单元，因而可以在每个周期中执行多个独立的操作。如果编译器可以通过对指令流的编排使之包含独立的多个操作，并适合于特定机器，那么应用程序会运行得更快。 如代码 a + b + c + d + e + f + g + h ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 对于该代码，可以左递归求值生成一棵左结合树，亦或右递归语法建立右结合树。但是建立一棵平衡树可以减少递归求值的约束，比如左结合树中 \\(a+b\\) 必须在涉及 g 或 h 的加法之前执行。 如果处理器每次可执行多个加法，左、右结合的树只能依次调度，而平衡树代码可以并行调度。这种优化利用了结合律和交换律，揭示表达式求值中的指令级并行，从而改进执行时间。 如果将树高平衡转换为算法，算法分为 分析 和 转换 两个步骤： 识别程序块中的候选表达式树。候选表达式树的运算符必须是相同的，且必须是可交换的和可结合的。同样，候选表达式树内部结点的每个名字都必须刚好是用一次。 对于每个候选树，算法将找到所有的运算对象，并将所有运算对象输入到一个优先队列，按等级递增的次序排列。 寻找候选树一个基本程序块由一个或多个混合计算组成，编译器可以将其中的 IR 解释成一个 DDG，该图记录了值的流动和对各个操作的执行顺序约束。 ;; LLVM IR %t = mul nsw i32 %a, %b %u = sub nsw i32 %c, %d %v = add nsw i32 %t, %u %w = mul nsw i32 %t, %u 一般地，DDG 不会形成一棵树，而是由多棵树交织组成。平衡算法所需的各种候选表达式树都是 DDG 中的不同子集。 在算法重排各个运算对象时，规模较大的候选树能够提供更多的重排机会。因此，算法试图构造最大规模的候选树。概念上算法找到每个候选树都可以看作是一个 n 元运算符 (n 尽可能大)。因此某些因素会限制候选树的规模 树不可能大过它表示的程序块 重写无法改变程序程序块的 可观察量，即程序块以外使用的任何值都必须像原来的代码中那样计算，且保留其值。类似地，任何在程序块中使用多次的值都必须保留 树反向扩展时不能超过程序块的起始位置 信息 如果一个值在某个代码片段之外是可读取的，那么该值相对于该代码片段是可观察的。 在查找树阶段，对程序中定义的名字 \\(T_{i}\\) 都需要知道何处引用了 \\(T_{i}\\)，因此算法包括一个 \\(Uses(T_{i})\\) 的集合，即使用了 \\(T_{i}\\) 的操作、指令的索引。 算法首先遍历程序块中的各个操作，判断每一个操作是否一定要将该操作作为其自身所属树的根结点。找到根节点时，会将该操作定义的名字添加到一个由名字组成的优先队列中，该队列按根结点运算符的优先级排序。假定操作 i 形如 \\(T_{i}\\leftarrow{}L_{i}\\,Op_{i}\\,R_{i}\\)，且 \\(Op_{i}\\) 是可交换和可结合的。那么下列条件之一成立时，将 \\(Op_{i}\\) 标记的为根结点，并将其加入优先队列。 如果 \\(T_{i}\\) 使用多次，那么操作 i 必须被标记为根结点，以确保对所有使用 \\(T_{i}\\) 的操作，\\(T_{i}\\) 都是可用的，对 \\(T_{i}\\) 的多次使用使之成为一个可观察量。 如果 \\(T_{i}\\) 只在操作 j 中使用一次，但 \\(Op_{i}\\,\\ne\\,Op_{j}\\)，那么操作 i 必是根结点，因为它不可能是包含 \\(Op_{j}\\) 的树的一部分。 roots = priority queue of names for i in range(0, n - 1), rank(\\(T_{i}\\)) = -1 do if \\(Op_{i}\\) is commutative and associative and (\\(\\lvert{Uses(T_{i})}\\rvert\\) \u003e 1 or (\\(\\lvert{Uses(T_{i})}\\rvert\\) = 1 and \\(Op_{Uses(T_{i})} \\ne Op_{i}\\))) then mark \\(T_{i}\\) as a root Enqueue(roots, \\(T_{i}\\), precedence of \\(Op_{i}\\)) end end 重构程序块使之平衡接下来算法以候选树根结点的队列作为输入，并根据每个根结点建立一个大体的平衡树。这个阶段使用三个模块：Balance、Flatten、Rebuild。 while Roots is not empty do var = Dequeue(Roots) Balance(var) end Balance 对根结点进行操作，分配一个新的优先队列来容纳当前树的所有操作数，使用 Flatten 递归遍历树，为每个操作数指派等级并将其添加到队列中。 Balance(root) if Rank(root) \u003e= 0 then return end q = new queue of names Rank(root) = Flatten(\\(L_{i}\\), q) + Flatten(\\(R_{i}\\), q) Rebuild(q, \\(Op_{i}\\)) end Flatten(var, q) if var is a constant then Rank(var) = 0 Enqueue(q, var, Rank(var)) else if var \\(\\in\\) UEVar(b) then Rank(var) = 1 Enqueue(q, var, Rank(var)) else if var is a root then Balance(var) Enqueue(q, var, Rank(var)) else Flatten(\\(L_{j}\\), q) Flatten(\\(R_{j}\\), q) end return Rank(var) end Rebuild 使用了一个简单的算法来构造新的代码序列，它重复从树中移除两个等级最低的项。该函数将输出一个操作来合并这两项。它会为结果分配一个等级，然后将结果插回到优先队列中，直到队列为空。 Rebuild(q, op) while q is not empty do NL = Dequeue(q) NR = Dequeue(q) if NL and NR are both constants then NT = Fold(op, NL, NR) if q is empty then # root = NT Rank(root) = 0 else Enqueue(q, NT, 0) Rank(NT) = 0 end else if q is empty then NT = root else NT = new name end # NT = NL op NR Rank(NT) = Rank(NL) + Rank(NR) if q is not empty then Enqueue(q, NT, Rank(NT)) end end end end 在该算法中，有些细节： 在遍历候选树时，Flatten 可能会遇到另一棵树的根结点。它会递归调用 Balance 而非 Flatten，一边为候选子树的根结点创建一个新的优先队列，并确保编译器在输出引用子树值的代码之前，先对优先级较高的子树输出代码。 程序块由三种引用：常数、本程序块中先定义后使用的名字、向上展现的名字。 Flatten 例程分别处理每种情形。 算法设计常数等级为零，因此常数可以移动到队列前端，用 Fold 进行编译期对常数进行计算，并到新的名字加入到树中。叶结点的等级为 1，内部结点的等级等于其所在子树所有结点等级之和。这种指派等级的方法将生成一种近似于平衡二叉树的树状结构。 树高平衡算法的例子回到表达式 \\(a + b + c + d + e + f + g + h\\)，重新看看 IR ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 假设只有 %7 在程序块之外使用，那么只有 Uses(%7) \u003e 1，因此只有 %7 被作为候选树的根。平衡时扁平化树将会得到以下队列 \\[, , , , , , , .\\] Rebuild 会从队列中取出 \\(\\) 和 \\(\\) 然后将 \\(\\) 加入队列。 \\[, , , , , , .\\] Rebuild 构建四次后，队列将成为以下 \\[","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#寻找候选树"},{"categories":["CompilerPrinciple"],"content":" 树高平衡编译器对一个计算进行编码的具体细节会影响到编译器优化该计算的能力，许多现代处理器有多个功能单元，因而可以在每个周期中执行多个独立的操作。如果编译器可以通过对指令流的编排使之包含独立的多个操作，并适合于特定机器，那么应用程序会运行得更快。 如代码 a + b + c + d + e + f + g + h ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 对于该代码，可以左递归求值生成一棵左结合树，亦或右递归语法建立右结合树。但是建立一棵平衡树可以减少递归求值的约束，比如左结合树中 \\(a+b\\) 必须在涉及 g 或 h 的加法之前执行。 如果处理器每次可执行多个加法，左、右结合的树只能依次调度，而平衡树代码可以并行调度。这种优化利用了结合律和交换律，揭示表达式求值中的指令级并行，从而改进执行时间。 如果将树高平衡转换为算法，算法分为 分析 和 转换 两个步骤： 识别程序块中的候选表达式树。候选表达式树的运算符必须是相同的，且必须是可交换的和可结合的。同样，候选表达式树内部结点的每个名字都必须刚好是用一次。 对于每个候选树，算法将找到所有的运算对象，并将所有运算对象输入到一个优先队列，按等级递增的次序排列。 寻找候选树一个基本程序块由一个或多个混合计算组成，编译器可以将其中的 IR 解释成一个 DDG，该图记录了值的流动和对各个操作的执行顺序约束。 ;; LLVM IR %t = mul nsw i32 %a, %b %u = sub nsw i32 %c, %d %v = add nsw i32 %t, %u %w = mul nsw i32 %t, %u 一般地，DDG 不会形成一棵树，而是由多棵树交织组成。平衡算法所需的各种候选表达式树都是 DDG 中的不同子集。 在算法重排各个运算对象时，规模较大的候选树能够提供更多的重排机会。因此，算法试图构造最大规模的候选树。概念上算法找到每个候选树都可以看作是一个 n 元运算符 (n 尽可能大)。因此某些因素会限制候选树的规模 树不可能大过它表示的程序块 重写无法改变程序程序块的 可观察量，即程序块以外使用的任何值都必须像原来的代码中那样计算，且保留其值。类似地，任何在程序块中使用多次的值都必须保留 树反向扩展时不能超过程序块的起始位置 信息 如果一个值在某个代码片段之外是可读取的，那么该值相对于该代码片段是可观察的。 在查找树阶段，对程序中定义的名字 \\(T_{i}\\) 都需要知道何处引用了 \\(T_{i}\\)，因此算法包括一个 \\(Uses(T_{i})\\) 的集合，即使用了 \\(T_{i}\\) 的操作、指令的索引。 算法首先遍历程序块中的各个操作，判断每一个操作是否一定要将该操作作为其自身所属树的根结点。找到根节点时，会将该操作定义的名字添加到一个由名字组成的优先队列中，该队列按根结点运算符的优先级排序。假定操作 i 形如 \\(T_{i}\\leftarrow{}L_{i}\\,Op_{i}\\,R_{i}\\)，且 \\(Op_{i}\\) 是可交换和可结合的。那么下列条件之一成立时，将 \\(Op_{i}\\) 标记的为根结点，并将其加入优先队列。 如果 \\(T_{i}\\) 使用多次，那么操作 i 必须被标记为根结点，以确保对所有使用 \\(T_{i}\\) 的操作，\\(T_{i}\\) 都是可用的，对 \\(T_{i}\\) 的多次使用使之成为一个可观察量。 如果 \\(T_{i}\\) 只在操作 j 中使用一次，但 \\(Op_{i}\\,\\ne\\,Op_{j}\\)，那么操作 i 必是根结点，因为它不可能是包含 \\(Op_{j}\\) 的树的一部分。 roots = priority queue of names for i in range(0, n - 1), rank(\\(T_{i}\\)) = -1 do if \\(Op_{i}\\) is commutative and associative and (\\(\\lvert{Uses(T_{i})}\\rvert\\) \u003e 1 or (\\(\\lvert{Uses(T_{i})}\\rvert\\) = 1 and \\(Op_{Uses(T_{i})} \\ne Op_{i}\\))) then mark \\(T_{i}\\) as a root Enqueue(roots, \\(T_{i}\\), precedence of \\(Op_{i}\\)) end end 重构程序块使之平衡接下来算法以候选树根结点的队列作为输入，并根据每个根结点建立一个大体的平衡树。这个阶段使用三个模块：Balance、Flatten、Rebuild。 while Roots is not empty do var = Dequeue(Roots) Balance(var) end Balance 对根结点进行操作，分配一个新的优先队列来容纳当前树的所有操作数，使用 Flatten 递归遍历树，为每个操作数指派等级并将其添加到队列中。 Balance(root) if Rank(root) \u003e= 0 then return end q = new queue of names Rank(root) = Flatten(\\(L_{i}\\), q) + Flatten(\\(R_{i}\\), q) Rebuild(q, \\(Op_{i}\\)) end Flatten(var, q) if var is a constant then Rank(var) = 0 Enqueue(q, var, Rank(var)) else if var \\(\\in\\) UEVar(b) then Rank(var) = 1 Enqueue(q, var, Rank(var)) else if var is a root then Balance(var) Enqueue(q, var, Rank(var)) else Flatten(\\(L_{j}\\), q) Flatten(\\(R_{j}\\), q) end return Rank(var) end Rebuild 使用了一个简单的算法来构造新的代码序列，它重复从树中移除两个等级最低的项。该函数将输出一个操作来合并这两项。它会为结果分配一个等级，然后将结果插回到优先队列中，直到队列为空。 Rebuild(q, op) while q is not empty do NL = Dequeue(q) NR = Dequeue(q) if NL and NR are both constants then NT = Fold(op, NL, NR) if q is empty then # root = NT Rank(root) = 0 else Enqueue(q, NT, 0) Rank(NT) = 0 end else if q is empty then NT = root else NT = new name end # NT = NL op NR Rank(NT) = Rank(NL) + Rank(NR) if q is not empty then Enqueue(q, NT, Rank(NT)) end end end end 在该算法中，有些细节： 在遍历候选树时，Flatten 可能会遇到另一棵树的根结点。它会递归调用 Balance 而非 Flatten，一边为候选子树的根结点创建一个新的优先队列，并确保编译器在输出引用子树值的代码之前，先对优先级较高的子树输出代码。 程序块由三种引用：常数、本程序块中先定义后使用的名字、向上展现的名字。 Flatten 例程分别处理每种情形。 算法设计常数等级为零，因此常数可以移动到队列前端，用 Fold 进行编译期对常数进行计算，并到新的名字加入到树中。叶结点的等级为 1，内部结点的等级等于其所在子树所有结点等级之和。这种指派等级的方法将生成一种近似于平衡二叉树的树状结构。 树高平衡算法的例子回到表达式 \\(a + b + c + d + e + f + g + h\\)，重新看看 IR ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 假设只有 %7 在程序块之外使用，那么只有 Uses(%7) \u003e 1，因此只有 %7 被作为候选树的根。平衡时扁平化树将会得到以下队列 \\[, , , , , , , .\\] Rebuild 会从队列中取出 \\(\\) 和 \\(\\) 然后将 \\(\\) 加入队列。 \\[, , , , , , .\\] Rebuild 构建四次后，队列将成为以下 \\[","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#重构程序块使之平衡"},{"categories":["CompilerPrinciple"],"content":" 树高平衡编译器对一个计算进行编码的具体细节会影响到编译器优化该计算的能力，许多现代处理器有多个功能单元，因而可以在每个周期中执行多个独立的操作。如果编译器可以通过对指令流的编排使之包含独立的多个操作，并适合于特定机器，那么应用程序会运行得更快。 如代码 a + b + c + d + e + f + g + h ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 对于该代码，可以左递归求值生成一棵左结合树，亦或右递归语法建立右结合树。但是建立一棵平衡树可以减少递归求值的约束，比如左结合树中 \\(a+b\\) 必须在涉及 g 或 h 的加法之前执行。 如果处理器每次可执行多个加法，左、右结合的树只能依次调度，而平衡树代码可以并行调度。这种优化利用了结合律和交换律，揭示表达式求值中的指令级并行，从而改进执行时间。 如果将树高平衡转换为算法，算法分为 分析 和 转换 两个步骤： 识别程序块中的候选表达式树。候选表达式树的运算符必须是相同的，且必须是可交换的和可结合的。同样，候选表达式树内部结点的每个名字都必须刚好是用一次。 对于每个候选树，算法将找到所有的运算对象，并将所有运算对象输入到一个优先队列，按等级递增的次序排列。 寻找候选树一个基本程序块由一个或多个混合计算组成，编译器可以将其中的 IR 解释成一个 DDG，该图记录了值的流动和对各个操作的执行顺序约束。 ;; LLVM IR %t = mul nsw i32 %a, %b %u = sub nsw i32 %c, %d %v = add nsw i32 %t, %u %w = mul nsw i32 %t, %u 一般地，DDG 不会形成一棵树，而是由多棵树交织组成。平衡算法所需的各种候选表达式树都是 DDG 中的不同子集。 在算法重排各个运算对象时，规模较大的候选树能够提供更多的重排机会。因此，算法试图构造最大规模的候选树。概念上算法找到每个候选树都可以看作是一个 n 元运算符 (n 尽可能大)。因此某些因素会限制候选树的规模 树不可能大过它表示的程序块 重写无法改变程序程序块的 可观察量，即程序块以外使用的任何值都必须像原来的代码中那样计算，且保留其值。类似地，任何在程序块中使用多次的值都必须保留 树反向扩展时不能超过程序块的起始位置 信息 如果一个值在某个代码片段之外是可读取的，那么该值相对于该代码片段是可观察的。 在查找树阶段，对程序中定义的名字 \\(T_{i}\\) 都需要知道何处引用了 \\(T_{i}\\)，因此算法包括一个 \\(Uses(T_{i})\\) 的集合，即使用了 \\(T_{i}\\) 的操作、指令的索引。 算法首先遍历程序块中的各个操作，判断每一个操作是否一定要将该操作作为其自身所属树的根结点。找到根节点时，会将该操作定义的名字添加到一个由名字组成的优先队列中，该队列按根结点运算符的优先级排序。假定操作 i 形如 \\(T_{i}\\leftarrow{}L_{i}\\,Op_{i}\\,R_{i}\\)，且 \\(Op_{i}\\) 是可交换和可结合的。那么下列条件之一成立时，将 \\(Op_{i}\\) 标记的为根结点，并将其加入优先队列。 如果 \\(T_{i}\\) 使用多次，那么操作 i 必须被标记为根结点，以确保对所有使用 \\(T_{i}\\) 的操作，\\(T_{i}\\) 都是可用的，对 \\(T_{i}\\) 的多次使用使之成为一个可观察量。 如果 \\(T_{i}\\) 只在操作 j 中使用一次，但 \\(Op_{i}\\,\\ne\\,Op_{j}\\)，那么操作 i 必是根结点，因为它不可能是包含 \\(Op_{j}\\) 的树的一部分。 roots = priority queue of names for i in range(0, n - 1), rank(\\(T_{i}\\)) = -1 do if \\(Op_{i}\\) is commutative and associative and (\\(\\lvert{Uses(T_{i})}\\rvert\\) \u003e 1 or (\\(\\lvert{Uses(T_{i})}\\rvert\\) = 1 and \\(Op_{Uses(T_{i})} \\ne Op_{i}\\))) then mark \\(T_{i}\\) as a root Enqueue(roots, \\(T_{i}\\), precedence of \\(Op_{i}\\)) end end 重构程序块使之平衡接下来算法以候选树根结点的队列作为输入，并根据每个根结点建立一个大体的平衡树。这个阶段使用三个模块：Balance、Flatten、Rebuild。 while Roots is not empty do var = Dequeue(Roots) Balance(var) end Balance 对根结点进行操作，分配一个新的优先队列来容纳当前树的所有操作数，使用 Flatten 递归遍历树，为每个操作数指派等级并将其添加到队列中。 Balance(root) if Rank(root) \u003e= 0 then return end q = new queue of names Rank(root) = Flatten(\\(L_{i}\\), q) + Flatten(\\(R_{i}\\), q) Rebuild(q, \\(Op_{i}\\)) end Flatten(var, q) if var is a constant then Rank(var) = 0 Enqueue(q, var, Rank(var)) else if var \\(\\in\\) UEVar(b) then Rank(var) = 1 Enqueue(q, var, Rank(var)) else if var is a root then Balance(var) Enqueue(q, var, Rank(var)) else Flatten(\\(L_{j}\\), q) Flatten(\\(R_{j}\\), q) end return Rank(var) end Rebuild 使用了一个简单的算法来构造新的代码序列，它重复从树中移除两个等级最低的项。该函数将输出一个操作来合并这两项。它会为结果分配一个等级，然后将结果插回到优先队列中，直到队列为空。 Rebuild(q, op) while q is not empty do NL = Dequeue(q) NR = Dequeue(q) if NL and NR are both constants then NT = Fold(op, NL, NR) if q is empty then # root = NT Rank(root) = 0 else Enqueue(q, NT, 0) Rank(NT) = 0 end else if q is empty then NT = root else NT = new name end # NT = NL op NR Rank(NT) = Rank(NL) + Rank(NR) if q is not empty then Enqueue(q, NT, Rank(NT)) end end end end 在该算法中，有些细节： 在遍历候选树时，Flatten 可能会遇到另一棵树的根结点。它会递归调用 Balance 而非 Flatten，一边为候选子树的根结点创建一个新的优先队列，并确保编译器在输出引用子树值的代码之前，先对优先级较高的子树输出代码。 程序块由三种引用：常数、本程序块中先定义后使用的名字、向上展现的名字。 Flatten 例程分别处理每种情形。 算法设计常数等级为零，因此常数可以移动到队列前端，用 Fold 进行编译期对常数进行计算，并到新的名字加入到树中。叶结点的等级为 1，内部结点的等级等于其所在子树所有结点等级之和。这种指派等级的方法将生成一种近似于平衡二叉树的树状结构。 树高平衡算法的例子回到表达式 \\(a + b + c + d + e + f + g + h\\)，重新看看 IR ;; LLVM IR %1 = add nsw i32 %a, %b %2 = add nsw i32 %1, %c %3 = add nsw i32 %2, %d %4 = add nsw i32 %3, %e %5 = add nsw i32 %4, %f %6 = add nsw i32 %5, %g %7 = add nsw i32 %6, %h 假设只有 %7 在程序块之外使用，那么只有 Uses(%7) \u003e 1，因此只有 %7 被作为候选树的根。平衡时扁平化树将会得到以下队列 \\[, , , , , , , .\\] Rebuild 会从队列中取出 \\(\\) 和 \\(\\) 然后将 \\(\\) 加入队列。 \\[, , , , , , .\\] Rebuild 构建四次后，队列将成为以下 \\[","date":"09-23","objectID":"/2022/introduction_to_optimization/:3:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#树高平衡算法的例子"},{"categories":["CompilerPrinciple"],"content":" 区域优化低效性不止出现在单个 BB 中，一个 BB 可能为改进另一个 BB 提供上下文环境。因此大多数优化也会考察多个 BB 的上下文，这也就是区域优化。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:4:0","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#区域优化"},{"categories":["CompilerPrinciple"],"content":" 超局部值编号命名一直是编译器中的一个重点项目，LVN 是在单个 BB 内的命名方法，超局部值编号 (SVN, Superlocal Value Numbering) 则是扩展到 EBB 中进行命名的方法。 回到优化的范围所提到的 EBB 示例，先聚焦到第一个 EBB \\(\\{B_{0}, B_{1}, B_{2}, B_{3}, B_{4}\\}\\) 上，SVN 可以将 3 条路径中的每一条路径都当作一个单个 BB 进行处理，也就是说，在处理时 \\(\\{B_{0}, B_{1}\\}\\)、\\(\\{B_{0}, B_{2}, B_{3}\\}\\) 和 \\(\\{B_{0}, B_{2}, B_{4}\\}\\) 都被当作线性代码。比如在处理 \\(\\{B_{0}, B_{1}\\}\\) 时，编译器先将 LVN 算法应用到 \\(B_{0}\\) 上，然后将生成的散列表按 BB 顺序用 LVN 算法应用到 \\(B_{1}\\) 上。 警告 因此考虑，为何 EBB 只允许第一个 BB 可以有多个前驱，而其他 BB 只允许有一个前驱。 SVN 可以发现 LVN 可能错过的冗余和常量表达式。但对于分支上的 BB 来说，SVN 算法可能会将一个 BB 分析多次，例如 EBB \\(\\{B_{0}, B_{1}, B_{2}, B_{3}, B_{4}\\}\\) 的 3 个分支，会将 \\(B_{0}\\) 分析 3 次，将 \\(B_{2}\\) 分析 2 次。 为了 SVN 的高效运行，算法必须有一种重用分析结果的方法，比如处理分支 \\(\\{B_{0}, B_{2}, B_{4}\\}\\) 时，需要重用 \\(\\{B_{0}, B_{2}\\}\\) 结束时的状态来处理 \\(B_{4}\\)。而在重用之前，必须撤销 \\(\\{B_{0}, B_{2}, B_{3}\\}\\) 所带来的影响。为了高效的撤销，使用作用域化散列表可以有效解决这个问题。在处理每个 BB 时为其分配一个值表，将其连接到前驱程序块的值表 (将前驱块的值表当作外层作用域)，并用这个新的值表与程序块 b 作为参数使用 LVN 算法。 WorkList = { entry block } Empty = new table while WorkList is not empty do remove b from WorkList SVN(b, Empty) end SVN(BB, Table) t = new table for BB link Table as the surrounding scope for t LVN(BB, t) for each successor s of BB do if s has only 1 predecessor then SVN(s, t) else if s has not been processed then add s to WorkList end end end 还有一个问题，就是名字的值编号是由 EBB 中定义该名字的第一个操作相关联的值表记录的，那么在优化的范围示例中的 CFG，如果 \\(B_{0}\\)、\\(B_{3}\\) 和 \\(B_{4}\\) 中都定义了名字 x，那么其值编号将记录在 \\(B_{0}\\) 中的作用域化值表中。在处理 \\(B_{3}\\) 时，会将它的 x 的新的值编号记录到对应于 \\(B_{0}\\) 的表中，删除对应于 \\(B_{3}\\) 的表并开始处理 \\(B_{4}\\) 时，由 \\(B_{3}\\) 定义的值编号依然保留在 \\(B_{0}\\) 的表中。为了避免这种复杂的情况，编译器可以使用只定义每个名字一次的表示法，也就是 SSA 所具有的性质。使用 SSA 时可以撤销一个程序块值表的所有影响，恢复到前驱程序块退出时的状态，并且 SSA 还可以使 LVN 更加高效。 需要注意的是，SVN 虽然可以发现 EBB 中的冗余，但也有局限，例如当一个 BB 有多个前驱时，SVN 无法将上下文信息传入其中。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:4:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#超局部值编号"},{"categories":["CompilerPrinciple"],"content":" 循环展开循环展开时最古老、最著名的循环变换，展开一个循环，复制循环体并调整迭代执行数目的逻辑。 for j = 1, n2, 1 do for i = 1, n1, 1 do y[i] = y[i] + x[j] * m[i][j] end end 编译器可以展开内层循环或外层循环，例如展开内层循环会复制循环体，而展开外层循环时会复制多次内层循环。如果编译器之后合并这些内层循环 (循环融合，loop fusion)，先展开外层循环再融合内层循环的变换组合被称为 展开-轧挤 (unroll-and-jam)。 循环展开对编译器为给定循环生成的代码有着直接或间接的影响。展开循环可以减少完成循环所需操作的数目，控制流的改变减少了判断和分支代码序列的总数。展开还可以在循环体内部产生重用，减少内存访问。最后，如果循环包含一个复制操作的有环链，那么展开可以消除这些复制。但展开会增大程序的长度，这样可能会增加编译时间，但展开的循环体内部可能影响 Cache，从而导致性能降低。 循环展开的关键副效应时增加了循环内部的操作数目，一些优化可以利用这些 增加循环体中独立操作的数目，可以生成更好的指令调度。在操作更多的情况下，指令调度器有更高的几率使多个功能单元保持忙碌，并隐藏长耗时操作 (如分支和访存) 的延迟。 循环展开可以将连续的内存访问移动到同一迭代中，编译器可以调度这些操作一同执行。这可以提高内存访问的局部性，或利用多字操作进行内存访问。 展开可以暴露跨迭代的冗余，而这在原来的代码中可能是难以发现的。展开循环后 LVN 算法可以找到这些冗余并消除。 与原来的循环相比，展开后的循环能以不同的方式进行优化。如增加一个变量在循环内部出现的次数，可以改变寄存器分配器内部逐出代码选择中使用的权重。改变寄存器逐出的模式，可能在根本上影响到为循环生成的最终代码的速度。 与原来的循环体相比，展开后的循环体可能会对寄存器有更大的需求。如果对寄存器增加的需求会导致额外的寄存器逐出 (存储到内存和从内存重新加载)，那么由此导致的内存访问代价可能会超出循环展开带来的收益。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:4:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#循环展开"},{"categories":["CompilerPrinciple"],"content":" 全局优化全局优化处理整个过程或方法，其作用域包括有环的控制流结构 (如循环)，全局优化在修改代码前通常会有一个分析阶段。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:5:0","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#全局优化"},{"categories":["CompilerPrinciple"],"content":" 利用活动信息查找未初始化变量如果过程 p 在为某个变量 v 分配一个值之前能够使用 v 的值，那么就说 v 在这次使用时是为初始化的。通过计算活动情况的信息，可以找到对未初始化变量的潜在使用。当且仅当 CFG 中存在一条从 p 到使用 v 的某个位置之间的路径，且 v 在该路径中没有被重新定义，变量 v 在位置 p 处是活动的。通过计算将过程中的每个 BB 对应的活动信息编码到集合 LiveOut(bb) 中，该集合包含在 BB 退出时的所有活动的变量。 给定 CFG 入口结点 \\(n_{0}\\) 的 LiveOut 集合，\\(LiveOut(n_{0})\\) 中的每个变量都有一次潜在的未初始化使用。 定义数据流问题为了计算 CFG 中结点 n 的 LiveOut，需要使用其后继结点的 LiveOut，以及另外两个集合 UEVar 和 VarKill。定义 LiveOut 的方程如下： \\[LiveOut(n) = \\cup_{m\\in{}succ(n)} \\left( UEVar(m) \\cup \\left( LiveOut(m) \\cap \\overline{VarKill(m)} \\right) \\right).\\] UEVar(m) 包含了 m 中向上展现的变量，即那些在 m 中重新定义之前就开始使用的变量。VarKill(m) 包含了 m 中定义的所有变量，\\(\\overline{VarKill(m)}\\) 则是其补集，即未在 m 中定义的变量的集合。由于 LiveOut(n) 是利用 n 的后继结点来定义的，因此该方程描述了一个反向数据流问题。 解决这个数据流问题对一个过程及其 CFG 计算各个结点的 LiveOut 集合，编译器可以使用一个三步算法 构建 CFG，这个步骤在概念上很简单 收集初始信息，分析程序在一趟简单的遍历中分别为每个程序块 b 计算一个 UEVar 和 VarKill 集合 为了计算 LiveOut 集合，分析程序需要每个程序块的 UEVar 和 VarKill 集合。一趟处理即可计算出这两个集合。对于每个程序块，分析程序将两个集合都初始化为 \\(\\emptyset\\)。接下来按从上到下顺序遍历，并适当地更新 UEVar 和 VarKill 集合，以反映程序块的每个操作的影响。 // x = y op z for each block b do Init(b) end Init(b) UEVar(b) = \\(\\emptyset\\) VarKill(b) = \\(\\emptyset\\) for i in range(1, k) do if y \\(\\notin\\) VarKill(b) then add y to UEVar(b) end if z \\(\\notin\\) VarKill(b) then add z to UEVar(b) end add x to VarKill(b) end end 求解方程式，为每个程序块生成 LiveOut 集合 求解过程需要反复进行，直到所有 LiveOut 集合不再改变为止 for i in range(0, N - 1), LiveOut(i) = \\(\\emptyset\\) do changed = true while changed do changed = false for i in range(0, N - 1) do recompute LiveOut(i) if LiveOut(i) changed then changed = true end end end end 比如对于一个控制流图 ;; IR b0: %i = i32 1 b1: br i32 %i, label %b3, label %b2 b2: %s = i32 0 b3: %s = add nsw i32 %s, %i %i = add nsw i32 %i, 1 br i32 %i, label %b1, label %b4 b4: print i32 %s 根据控制流信息可以轻松计算出 VarKill 和 UEVar UEVar VarKill b0 \\(\\emptyset\\) {i} b1 {i} \\(\\emptyset\\) b2 \\(\\emptyset\\) {s} b3 {s, i} {s, i} b4 {s} \\(\\emptyset\\) 在这个例子中，开始计算每个程序块的 LiveOut。 迭代次数 LiveOut(b0) LiveOut(b1) LiveOut(b2) LiveOut(b3) LiveOut(b4) 初始 \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) 1 {i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 2 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 3 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 查找未初始化的变量计算出 CFG 的每个结点的 LiveOut 集合后，查找未初始化变量的使用就变得简单了。如果有一个变量 v，且 \\(v \\in LiveOut(n_{0})\\)，\\(n_{0}\\) 为 CFG 的入口结点，那么一定存在一条从 \\(n_{0}\\) 到 v 的某个使用之处的路径，v 在该路径上未被定义。因此编译器可以识别处其中未被初始化的变量。但也有几种可能导致编译器错误的识别。 v 通过另一个名字初始化 v 在当前过程被调用之前就已存在 v 在路径上没有被初始化，但实际上该路径总是不会出现 如果过程包含对另一过程的调用，且 v 通过允许修改的方式传递给后者，那么分析程序必须考虑调用可能带来的副效应。在缺少被调用者的具体信息时，就需要假定其总是被修改。 对活动变量的其他使用除了查找未初始化变量外，编译器还可以在许多上下文中使用活动变量 全局寄存器分配中，活动变量会发挥关键作用，除非值是活动的，否则寄存器分配器不必将其保持在寄存器中；当值从活动转变为不活动时，分配器可以因其他用途重用该寄存器。 活动变量可以用于改进 SSA 构建：对一个值来说，它不活动的任何程序块中都不需要 \\(\\phi\\) 函数。用活动变量信息可以显著减少编译器构建程序 SSA 时必须插入 \\(\\phi\\) 函数的数目。 编译器可以使用活动变量信息发现无用的 store 操作。如果一个操作将 v 存在内从中，如果 v 是不活动的，那么该 store 操作是无用的。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:5:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#利用活动信息查找未初始化变量"},{"categories":["CompilerPrinciple"],"content":" 利用活动信息查找未初始化变量如果过程 p 在为某个变量 v 分配一个值之前能够使用 v 的值，那么就说 v 在这次使用时是为初始化的。通过计算活动情况的信息，可以找到对未初始化变量的潜在使用。当且仅当 CFG 中存在一条从 p 到使用 v 的某个位置之间的路径，且 v 在该路径中没有被重新定义，变量 v 在位置 p 处是活动的。通过计算将过程中的每个 BB 对应的活动信息编码到集合 LiveOut(bb) 中，该集合包含在 BB 退出时的所有活动的变量。 给定 CFG 入口结点 \\(n_{0}\\) 的 LiveOut 集合，\\(LiveOut(n_{0})\\) 中的每个变量都有一次潜在的未初始化使用。 定义数据流问题为了计算 CFG 中结点 n 的 LiveOut，需要使用其后继结点的 LiveOut，以及另外两个集合 UEVar 和 VarKill。定义 LiveOut 的方程如下： \\[LiveOut(n) = \\cup_{m\\in{}succ(n)} \\left( UEVar(m) \\cup \\left( LiveOut(m) \\cap \\overline{VarKill(m)} \\right) \\right).\\] UEVar(m) 包含了 m 中向上展现的变量，即那些在 m 中重新定义之前就开始使用的变量。VarKill(m) 包含了 m 中定义的所有变量，\\(\\overline{VarKill(m)}\\) 则是其补集，即未在 m 中定义的变量的集合。由于 LiveOut(n) 是利用 n 的后继结点来定义的，因此该方程描述了一个反向数据流问题。 解决这个数据流问题对一个过程及其 CFG 计算各个结点的 LiveOut 集合，编译器可以使用一个三步算法 构建 CFG，这个步骤在概念上很简单 收集初始信息，分析程序在一趟简单的遍历中分别为每个程序块 b 计算一个 UEVar 和 VarKill 集合 为了计算 LiveOut 集合，分析程序需要每个程序块的 UEVar 和 VarKill 集合。一趟处理即可计算出这两个集合。对于每个程序块，分析程序将两个集合都初始化为 \\(\\emptyset\\)。接下来按从上到下顺序遍历，并适当地更新 UEVar 和 VarKill 集合，以反映程序块的每个操作的影响。 // x = y op z for each block b do Init(b) end Init(b) UEVar(b) = \\(\\emptyset\\) VarKill(b) = \\(\\emptyset\\) for i in range(1, k) do if y \\(\\notin\\) VarKill(b) then add y to UEVar(b) end if z \\(\\notin\\) VarKill(b) then add z to UEVar(b) end add x to VarKill(b) end end 求解方程式，为每个程序块生成 LiveOut 集合 求解过程需要反复进行，直到所有 LiveOut 集合不再改变为止 for i in range(0, N - 1), LiveOut(i) = \\(\\emptyset\\) do changed = true while changed do changed = false for i in range(0, N - 1) do recompute LiveOut(i) if LiveOut(i) changed then changed = true end end end end 比如对于一个控制流图 ;; IR b0: %i = i32 1 b1: br i32 %i, label %b3, label %b2 b2: %s = i32 0 b3: %s = add nsw i32 %s, %i %i = add nsw i32 %i, 1 br i32 %i, label %b1, label %b4 b4: print i32 %s 根据控制流信息可以轻松计算出 VarKill 和 UEVar UEVar VarKill b0 \\(\\emptyset\\) {i} b1 {i} \\(\\emptyset\\) b2 \\(\\emptyset\\) {s} b3 {s, i} {s, i} b4 {s} \\(\\emptyset\\) 在这个例子中，开始计算每个程序块的 LiveOut。 迭代次数 LiveOut(b0) LiveOut(b1) LiveOut(b2) LiveOut(b3) LiveOut(b4) 初始 \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) 1 {i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 2 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 3 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 查找未初始化的变量计算出 CFG 的每个结点的 LiveOut 集合后，查找未初始化变量的使用就变得简单了。如果有一个变量 v，且 \\(v \\in LiveOut(n_{0})\\)，\\(n_{0}\\) 为 CFG 的入口结点，那么一定存在一条从 \\(n_{0}\\) 到 v 的某个使用之处的路径，v 在该路径上未被定义。因此编译器可以识别处其中未被初始化的变量。但也有几种可能导致编译器错误的识别。 v 通过另一个名字初始化 v 在当前过程被调用之前就已存在 v 在路径上没有被初始化，但实际上该路径总是不会出现 如果过程包含对另一过程的调用，且 v 通过允许修改的方式传递给后者，那么分析程序必须考虑调用可能带来的副效应。在缺少被调用者的具体信息时，就需要假定其总是被修改。 对活动变量的其他使用除了查找未初始化变量外，编译器还可以在许多上下文中使用活动变量 全局寄存器分配中，活动变量会发挥关键作用，除非值是活动的，否则寄存器分配器不必将其保持在寄存器中；当值从活动转变为不活动时，分配器可以因其他用途重用该寄存器。 活动变量可以用于改进 SSA 构建：对一个值来说，它不活动的任何程序块中都不需要 \\(\\phi\\) 函数。用活动变量信息可以显著减少编译器构建程序 SSA 时必须插入 \\(\\phi\\) 函数的数目。 编译器可以使用活动变量信息发现无用的 store 操作。如果一个操作将 v 存在内从中，如果 v 是不活动的，那么该 store 操作是无用的。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:5:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#定义数据流问题"},{"categories":["CompilerPrinciple"],"content":" 利用活动信息查找未初始化变量如果过程 p 在为某个变量 v 分配一个值之前能够使用 v 的值，那么就说 v 在这次使用时是为初始化的。通过计算活动情况的信息，可以找到对未初始化变量的潜在使用。当且仅当 CFG 中存在一条从 p 到使用 v 的某个位置之间的路径，且 v 在该路径中没有被重新定义，变量 v 在位置 p 处是活动的。通过计算将过程中的每个 BB 对应的活动信息编码到集合 LiveOut(bb) 中，该集合包含在 BB 退出时的所有活动的变量。 给定 CFG 入口结点 \\(n_{0}\\) 的 LiveOut 集合，\\(LiveOut(n_{0})\\) 中的每个变量都有一次潜在的未初始化使用。 定义数据流问题为了计算 CFG 中结点 n 的 LiveOut，需要使用其后继结点的 LiveOut，以及另外两个集合 UEVar 和 VarKill。定义 LiveOut 的方程如下： \\[LiveOut(n) = \\cup_{m\\in{}succ(n)} \\left( UEVar(m) \\cup \\left( LiveOut(m) \\cap \\overline{VarKill(m)} \\right) \\right).\\] UEVar(m) 包含了 m 中向上展现的变量，即那些在 m 中重新定义之前就开始使用的变量。VarKill(m) 包含了 m 中定义的所有变量，\\(\\overline{VarKill(m)}\\) 则是其补集，即未在 m 中定义的变量的集合。由于 LiveOut(n) 是利用 n 的后继结点来定义的，因此该方程描述了一个反向数据流问题。 解决这个数据流问题对一个过程及其 CFG 计算各个结点的 LiveOut 集合，编译器可以使用一个三步算法 构建 CFG，这个步骤在概念上很简单 收集初始信息，分析程序在一趟简单的遍历中分别为每个程序块 b 计算一个 UEVar 和 VarKill 集合 为了计算 LiveOut 集合，分析程序需要每个程序块的 UEVar 和 VarKill 集合。一趟处理即可计算出这两个集合。对于每个程序块，分析程序将两个集合都初始化为 \\(\\emptyset\\)。接下来按从上到下顺序遍历，并适当地更新 UEVar 和 VarKill 集合，以反映程序块的每个操作的影响。 // x = y op z for each block b do Init(b) end Init(b) UEVar(b) = \\(\\emptyset\\) VarKill(b) = \\(\\emptyset\\) for i in range(1, k) do if y \\(\\notin\\) VarKill(b) then add y to UEVar(b) end if z \\(\\notin\\) VarKill(b) then add z to UEVar(b) end add x to VarKill(b) end end 求解方程式，为每个程序块生成 LiveOut 集合 求解过程需要反复进行，直到所有 LiveOut 集合不再改变为止 for i in range(0, N - 1), LiveOut(i) = \\(\\emptyset\\) do changed = true while changed do changed = false for i in range(0, N - 1) do recompute LiveOut(i) if LiveOut(i) changed then changed = true end end end end 比如对于一个控制流图 ;; IR b0: %i = i32 1 b1: br i32 %i, label %b3, label %b2 b2: %s = i32 0 b3: %s = add nsw i32 %s, %i %i = add nsw i32 %i, 1 br i32 %i, label %b1, label %b4 b4: print i32 %s 根据控制流信息可以轻松计算出 VarKill 和 UEVar UEVar VarKill b0 \\(\\emptyset\\) {i} b1 {i} \\(\\emptyset\\) b2 \\(\\emptyset\\) {s} b3 {s, i} {s, i} b4 {s} \\(\\emptyset\\) 在这个例子中，开始计算每个程序块的 LiveOut。 迭代次数 LiveOut(b0) LiveOut(b1) LiveOut(b2) LiveOut(b3) LiveOut(b4) 初始 \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) 1 {i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 2 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 3 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 查找未初始化的变量计算出 CFG 的每个结点的 LiveOut 集合后，查找未初始化变量的使用就变得简单了。如果有一个变量 v，且 \\(v \\in LiveOut(n_{0})\\)，\\(n_{0}\\) 为 CFG 的入口结点，那么一定存在一条从 \\(n_{0}\\) 到 v 的某个使用之处的路径，v 在该路径上未被定义。因此编译器可以识别处其中未被初始化的变量。但也有几种可能导致编译器错误的识别。 v 通过另一个名字初始化 v 在当前过程被调用之前就已存在 v 在路径上没有被初始化，但实际上该路径总是不会出现 如果过程包含对另一过程的调用，且 v 通过允许修改的方式传递给后者，那么分析程序必须考虑调用可能带来的副效应。在缺少被调用者的具体信息时，就需要假定其总是被修改。 对活动变量的其他使用除了查找未初始化变量外，编译器还可以在许多上下文中使用活动变量 全局寄存器分配中，活动变量会发挥关键作用，除非值是活动的，否则寄存器分配器不必将其保持在寄存器中；当值从活动转变为不活动时，分配器可以因其他用途重用该寄存器。 活动变量可以用于改进 SSA 构建：对一个值来说，它不活动的任何程序块中都不需要 \\(\\phi\\) 函数。用活动变量信息可以显著减少编译器构建程序 SSA 时必须插入 \\(\\phi\\) 函数的数目。 编译器可以使用活动变量信息发现无用的 store 操作。如果一个操作将 v 存在内从中，如果 v 是不活动的，那么该 store 操作是无用的。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:5:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#解决这个数据流问题"},{"categories":["CompilerPrinciple"],"content":" 利用活动信息查找未初始化变量如果过程 p 在为某个变量 v 分配一个值之前能够使用 v 的值，那么就说 v 在这次使用时是为初始化的。通过计算活动情况的信息，可以找到对未初始化变量的潜在使用。当且仅当 CFG 中存在一条从 p 到使用 v 的某个位置之间的路径，且 v 在该路径中没有被重新定义，变量 v 在位置 p 处是活动的。通过计算将过程中的每个 BB 对应的活动信息编码到集合 LiveOut(bb) 中，该集合包含在 BB 退出时的所有活动的变量。 给定 CFG 入口结点 \\(n_{0}\\) 的 LiveOut 集合，\\(LiveOut(n_{0})\\) 中的每个变量都有一次潜在的未初始化使用。 定义数据流问题为了计算 CFG 中结点 n 的 LiveOut，需要使用其后继结点的 LiveOut，以及另外两个集合 UEVar 和 VarKill。定义 LiveOut 的方程如下： \\[LiveOut(n) = \\cup_{m\\in{}succ(n)} \\left( UEVar(m) \\cup \\left( LiveOut(m) \\cap \\overline{VarKill(m)} \\right) \\right).\\] UEVar(m) 包含了 m 中向上展现的变量，即那些在 m 中重新定义之前就开始使用的变量。VarKill(m) 包含了 m 中定义的所有变量，\\(\\overline{VarKill(m)}\\) 则是其补集，即未在 m 中定义的变量的集合。由于 LiveOut(n) 是利用 n 的后继结点来定义的，因此该方程描述了一个反向数据流问题。 解决这个数据流问题对一个过程及其 CFG 计算各个结点的 LiveOut 集合，编译器可以使用一个三步算法 构建 CFG，这个步骤在概念上很简单 收集初始信息，分析程序在一趟简单的遍历中分别为每个程序块 b 计算一个 UEVar 和 VarKill 集合 为了计算 LiveOut 集合，分析程序需要每个程序块的 UEVar 和 VarKill 集合。一趟处理即可计算出这两个集合。对于每个程序块，分析程序将两个集合都初始化为 \\(\\emptyset\\)。接下来按从上到下顺序遍历，并适当地更新 UEVar 和 VarKill 集合，以反映程序块的每个操作的影响。 // x = y op z for each block b do Init(b) end Init(b) UEVar(b) = \\(\\emptyset\\) VarKill(b) = \\(\\emptyset\\) for i in range(1, k) do if y \\(\\notin\\) VarKill(b) then add y to UEVar(b) end if z \\(\\notin\\) VarKill(b) then add z to UEVar(b) end add x to VarKill(b) end end 求解方程式，为每个程序块生成 LiveOut 集合 求解过程需要反复进行，直到所有 LiveOut 集合不再改变为止 for i in range(0, N - 1), LiveOut(i) = \\(\\emptyset\\) do changed = true while changed do changed = false for i in range(0, N - 1) do recompute LiveOut(i) if LiveOut(i) changed then changed = true end end end end 比如对于一个控制流图 ;; IR b0: %i = i32 1 b1: br i32 %i, label %b3, label %b2 b2: %s = i32 0 b3: %s = add nsw i32 %s, %i %i = add nsw i32 %i, 1 br i32 %i, label %b1, label %b4 b4: print i32 %s 根据控制流信息可以轻松计算出 VarKill 和 UEVar UEVar VarKill b0 \\(\\emptyset\\) {i} b1 {i} \\(\\emptyset\\) b2 \\(\\emptyset\\) {s} b3 {s, i} {s, i} b4 {s} \\(\\emptyset\\) 在这个例子中，开始计算每个程序块的 LiveOut。 迭代次数 LiveOut(b0) LiveOut(b1) LiveOut(b2) LiveOut(b3) LiveOut(b4) 初始 \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) 1 {i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 2 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 3 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 查找未初始化的变量计算出 CFG 的每个结点的 LiveOut 集合后，查找未初始化变量的使用就变得简单了。如果有一个变量 v，且 \\(v \\in LiveOut(n_{0})\\)，\\(n_{0}\\) 为 CFG 的入口结点，那么一定存在一条从 \\(n_{0}\\) 到 v 的某个使用之处的路径，v 在该路径上未被定义。因此编译器可以识别处其中未被初始化的变量。但也有几种可能导致编译器错误的识别。 v 通过另一个名字初始化 v 在当前过程被调用之前就已存在 v 在路径上没有被初始化，但实际上该路径总是不会出现 如果过程包含对另一过程的调用，且 v 通过允许修改的方式传递给后者，那么分析程序必须考虑调用可能带来的副效应。在缺少被调用者的具体信息时，就需要假定其总是被修改。 对活动变量的其他使用除了查找未初始化变量外，编译器还可以在许多上下文中使用活动变量 全局寄存器分配中，活动变量会发挥关键作用，除非值是活动的，否则寄存器分配器不必将其保持在寄存器中；当值从活动转变为不活动时，分配器可以因其他用途重用该寄存器。 活动变量可以用于改进 SSA 构建：对一个值来说，它不活动的任何程序块中都不需要 \\(\\phi\\) 函数。用活动变量信息可以显著减少编译器构建程序 SSA 时必须插入 \\(\\phi\\) 函数的数目。 编译器可以使用活动变量信息发现无用的 store 操作。如果一个操作将 v 存在内从中，如果 v 是不活动的，那么该 store 操作是无用的。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:5:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#查找未初始化的变量"},{"categories":["CompilerPrinciple"],"content":" 利用活动信息查找未初始化变量如果过程 p 在为某个变量 v 分配一个值之前能够使用 v 的值，那么就说 v 在这次使用时是为初始化的。通过计算活动情况的信息，可以找到对未初始化变量的潜在使用。当且仅当 CFG 中存在一条从 p 到使用 v 的某个位置之间的路径，且 v 在该路径中没有被重新定义，变量 v 在位置 p 处是活动的。通过计算将过程中的每个 BB 对应的活动信息编码到集合 LiveOut(bb) 中，该集合包含在 BB 退出时的所有活动的变量。 给定 CFG 入口结点 \\(n_{0}\\) 的 LiveOut 集合，\\(LiveOut(n_{0})\\) 中的每个变量都有一次潜在的未初始化使用。 定义数据流问题为了计算 CFG 中结点 n 的 LiveOut，需要使用其后继结点的 LiveOut，以及另外两个集合 UEVar 和 VarKill。定义 LiveOut 的方程如下： \\[LiveOut(n) = \\cup_{m\\in{}succ(n)} \\left( UEVar(m) \\cup \\left( LiveOut(m) \\cap \\overline{VarKill(m)} \\right) \\right).\\] UEVar(m) 包含了 m 中向上展现的变量，即那些在 m 中重新定义之前就开始使用的变量。VarKill(m) 包含了 m 中定义的所有变量，\\(\\overline{VarKill(m)}\\) 则是其补集，即未在 m 中定义的变量的集合。由于 LiveOut(n) 是利用 n 的后继结点来定义的，因此该方程描述了一个反向数据流问题。 解决这个数据流问题对一个过程及其 CFG 计算各个结点的 LiveOut 集合，编译器可以使用一个三步算法 构建 CFG，这个步骤在概念上很简单 收集初始信息，分析程序在一趟简单的遍历中分别为每个程序块 b 计算一个 UEVar 和 VarKill 集合 为了计算 LiveOut 集合，分析程序需要每个程序块的 UEVar 和 VarKill 集合。一趟处理即可计算出这两个集合。对于每个程序块，分析程序将两个集合都初始化为 \\(\\emptyset\\)。接下来按从上到下顺序遍历，并适当地更新 UEVar 和 VarKill 集合，以反映程序块的每个操作的影响。 // x = y op z for each block b do Init(b) end Init(b) UEVar(b) = \\(\\emptyset\\) VarKill(b) = \\(\\emptyset\\) for i in range(1, k) do if y \\(\\notin\\) VarKill(b) then add y to UEVar(b) end if z \\(\\notin\\) VarKill(b) then add z to UEVar(b) end add x to VarKill(b) end end 求解方程式，为每个程序块生成 LiveOut 集合 求解过程需要反复进行，直到所有 LiveOut 集合不再改变为止 for i in range(0, N - 1), LiveOut(i) = \\(\\emptyset\\) do changed = true while changed do changed = false for i in range(0, N - 1) do recompute LiveOut(i) if LiveOut(i) changed then changed = true end end end end 比如对于一个控制流图 ;; IR b0: %i = i32 1 b1: br i32 %i, label %b3, label %b2 b2: %s = i32 0 b3: %s = add nsw i32 %s, %i %i = add nsw i32 %i, 1 br i32 %i, label %b1, label %b4 b4: print i32 %s 根据控制流信息可以轻松计算出 VarKill 和 UEVar UEVar VarKill b0 \\(\\emptyset\\) {i} b1 {i} \\(\\emptyset\\) b2 \\(\\emptyset\\) {s} b3 {s, i} {s, i} b4 {s} \\(\\emptyset\\) 在这个例子中，开始计算每个程序块的 LiveOut。 迭代次数 LiveOut(b0) LiveOut(b1) LiveOut(b2) LiveOut(b3) LiveOut(b4) 初始 \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) 1 {i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 2 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 3 {s, i} {s, i} {s, i} {s, i} \\(\\emptyset\\) 查找未初始化的变量计算出 CFG 的每个结点的 LiveOut 集合后，查找未初始化变量的使用就变得简单了。如果有一个变量 v，且 \\(v \\in LiveOut(n_{0})\\)，\\(n_{0}\\) 为 CFG 的入口结点，那么一定存在一条从 \\(n_{0}\\) 到 v 的某个使用之处的路径，v 在该路径上未被定义。因此编译器可以识别处其中未被初始化的变量。但也有几种可能导致编译器错误的识别。 v 通过另一个名字初始化 v 在当前过程被调用之前就已存在 v 在路径上没有被初始化，但实际上该路径总是不会出现 如果过程包含对另一过程的调用，且 v 通过允许修改的方式传递给后者，那么分析程序必须考虑调用可能带来的副效应。在缺少被调用者的具体信息时，就需要假定其总是被修改。 对活动变量的其他使用除了查找未初始化变量外，编译器还可以在许多上下文中使用活动变量 全局寄存器分配中，活动变量会发挥关键作用，除非值是活动的，否则寄存器分配器不必将其保持在寄存器中；当值从活动转变为不活动时，分配器可以因其他用途重用该寄存器。 活动变量可以用于改进 SSA 构建：对一个值来说，它不活动的任何程序块中都不需要 \\(\\phi\\) 函数。用活动变量信息可以显著减少编译器构建程序 SSA 时必须插入 \\(\\phi\\) 函数的数目。 编译器可以使用活动变量信息发现无用的 store 操作。如果一个操作将 v 存在内从中，如果 v 是不活动的，那么该 store 操作是无用的。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:5:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#对活动变量的其他使用"},{"categories":["CompilerPrinciple"],"content":" 全局代码置放很多处理器对分支处理的代价是不对称的：落空分支 (fall-through branch) 的代价要小于采纳分支 (taken branch)。比如一个条件跳转语句，其真条件分支执行频率比假条件分支高得多，那将真条件分支设置为落空分支性能更高。 为了全局代码置放优化，编译器应该将可能性最高的执行路径置放在落空分支上。其次编译器应该将执行得较不频繁的代码移动到过程末尾。这样可以尽可能生成更长的代码序列。 获取路径剖析数据 对于全局代码置放优化，编译器需要预估 CFG 中各条边的相对执行频度。从代码的剖析运行 (profiling run) 获取所需的信息。简单地说，就是统计 CFG 中各条边的执行次数，从而获得剖析数据。 以链的形式在 CFG 中构建热路径 编译器为判断如何设置代码布局而构建一个执行最频繁的边的集合，即热路径 (hot path)。编译器可以使用贪心算法查找热路径。 首先为每个程序块创建一条退化的链，其中只包含块本身。接下来遍历 CFG 的各个边，按执行频度的顺序采用各边，使得最频繁的边优先。对于边 \\(\u003cx, y\u003e\\)，只有当 x 是所在链的最后一个结点，而 y 是所在链的第一个结点时，才会合并这两条链。 E = edges for each block b do make a degenerate chain, d, for b priority(d) = E end p = 0 for each CFG edge \u003cx, y\u003e, x != y, in decreasing frequency order do if x is the tail of chain a and y is the head of chain b then t = priority(a) append b onto a priority(a) = min(t, priority(b), p++) end end 进行代码布局 为生成最终的汇编代码，编译器必须将所有 BB 按一个固定的线性顺序置放。可以根据链集合计算出一个线性布局： 一个链内部的各 BB 按顺序置放，使链中的边能够通过落空分支实现 在多个链之间，根据链的优先级选择 t = chain headed by the CFG entry node WorkList = {(t, priority(t))} while WorkList != \\(\\emptyset\\) do remove a chain c of lowest priority from WorkList for each block x in c in chain order do place x at the end of the executable code end for each block x in c do for each edge \u003cx, y\u003e where y is unplaced do t = chain containing \u003cx, y\u003e if (t, priority(t)) \\(\\notin\\) WorkList then WorkList = WorkList \\(\\cup\\) { (t, priority(t)) } end end end end ","date":"09-23","objectID":"/2022/introduction_to_optimization/:5:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#全局代码置放"},{"categories":["CompilerPrinciple"],"content":" 过程间优化将一个程序划分为多个过程，可以有效抽象出基础功能的代码，得以复用和抽象功能。但是从负面来看，程序划分限制了编译器理解调用过程内部行为的能力，比如编译器不能假定一个引用传递不会产生副作用。而另一点，过程调用需要转存当前上下文，这是代价巨大的。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:6:0","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#过程间优化"},{"categories":["CompilerPrinciple"],"content":" 内联替换那编译器可以通过将被调用过程的副本替换到调用位置上，并根据调用位置的上下文调整代码，这种变换称为内联替换 (inline subsitution)。 在每个调用位置上，编译器必须决定是否内联该调用。一个调用位置上所做的决策可能会影响到其他调用位置上的决策。如 a 调用 b、b 调用 c 的过程，如果内联过程 c，可能会改变内联到 a 中的特征。因此在内联替换时需要一些准则来考察。 被调用者的规模，如果被调用者的代码长度小于进行上下文保存、恢复的长度，那么内联替换可以减少代码长度 调用者的规模，如果希望生成的代码足够的小 动态调用计数，对频繁调用位置上的改进可以提供更大的收益 常数值实参，调用时使用常数值实参，可能产生潜在的代码改进 静态调用计数，编译器跟踪一个过程在不同位置的调用次数 参数计数，参数的数目可以充当过程链接代价的一种表示 过程中的调用，检查调用图的叶结点，通常这是良好的候选内联对象 编译器会根据一些准则，然后应用一条或一组相应的启发式规则，来决定一个调用是否被内联替换。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:6:1","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#内联替换"},{"categories":["CompilerPrinciple"],"content":" 过程置放与全局代码置放类似，根据调用图试图将有调用关系的过程尽可能置放在相邻的位置。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:6:2","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#过程置放"},{"categories":["CompilerPrinciple"],"content":" 针对过程间优化的编译器组织结构对于传统编译器来说，编译单元可能是单个过程、单个类或单个代码文件，编译器生成的目标代码完全取决于编译单元的内容。到达编译单元边界时，编译器无法链接另一个编译单元中的行为，通常只能使用最坏的结果进行假设优化。 人们提出了针对过程间优化的不同编译器组织结构： 扩大编译单元，这是最简单的一种解决方法 链接时优化 (Link-Time Optimization)，直接地将过程间优化移动到链接器中，其中可以访问所有的静态链接代码。 ","date":"09-23","objectID":"/2022/introduction_to_optimization/:6:3","series":null,"tags":["Note","Optimization"],"title":"优化简介","uri":"/2022/introduction_to_optimization/#针对过程间优化的编译器组织结构"},{"categories":["CompilerPrinciple"],"content":"GinShio | 橡树第 5 章：中间表示学习笔记","date":"09-19","objectID":"/2022/intermediate_representation/","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/"},{"categories":["CompilerPrinciple"],"content":"编译器通常组织为一连串的处理 pass，在每两个 pass 之间需要将已知的所有信息进行传递，因此编译器需要中间表示 (IR, Intermediate Representation) 表达信息。IR 在编译器中可能是唯一的，也可能有多种。在转换期间，编译器不会回头查看源代码，而是只观察 IR，因此 IR 的性质对编译器对代码的处理由决定性影响。 除了词法分析器，大多数 pass 的输入都是 IR；除了代码生成器，大多数 pass 的输出都是 IR。大多数源代码并不足以支持编译器必须的信息，比如变量与常量的地址，或参数使用的寄存器等，为了记录所有信息，大多数编译器还会添加表和集合来记录额外信息，通常认为这也是 IR 的一部分。 IR 的实现会迫使编译器的开发人员专注于实际问题，用廉价的方式来执行需要频繁进行的操作，并且简洁的表示编译器间可能出现的所有结构。除了这些，还需要可读的 IR 表示。当然，对于使用 IR 的编译器来说，总会在 IR 上进行多次处理，一个 pass 处理中收集信息，另一个 pass 处理中优化代码。 ","date":"09-19","objectID":"/2022/intermediate_representation/:0:0","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#"},{"categories":["CompilerPrinciple"],"content":" IR 的分类编译器使用过许多种 IR，主要是三个方面：结构性的组织、抽象层次和命名方案。一般来说，这三个属性是独立的。对于 IR 从结构上可以分为： 图 IR 将编译器信息编码在图中。算法通过途中的对象来表述：结点、边、列表、树。 线性 IR 类似某些抽象机上的伪代码，相应算法将迭代遍历简单的线性操作序列。 混合 IR 混合前两种以获取优势，比如现代编译器中常见的 BasicBlock (BB) 内线性表示，而使用控制流图来表示 bb 之间的关系。 IR 的结构性组织对编译器的分析、优化、代码生成等有极大影响，比如树形 IR 得出的处理 pass 在结构上很自然的设计为某种形式的树遍历，而线性 IR 得出的处理 pass 一般顺序迭代遍历各个操作。 IR 所处的抽象层次，如果接近源代码，可能只需要一个结点就可以表示数组访问或过程调用，而较底层的表示中，可能需要合并几个 IR 操作。 uint[100][100] arr; arr[50][50] = 32; ;; Spir-V %_var_arr = OpVariable %_ptr_arr_100_100 Function %elem = OpAccessChain %_ptr_int %_var_arr %uint_50 %uint_50 OpStore %elem %uint_32 ;; LLVM IR %1 = alloca [100 x [100 x i32]], align 4 %2 = getelementptr inbounds [100 x [100 x i32]], [100 x [100 x i32]]* %1, i64 0, i64 50 %3 = getelementptr inbounds [100 x i32], [100 x i32]* %2, i64 0, i64 50 store i32 32, i32* %3, align 4 很明显，上面这三个不同层次的代码，对于简单的构造数组，并进行访问元素赋值，它们是不一样的。低层次的 IR 可以展现出更多源代码中所隐藏的细节，从而为编译器优化提供更多可能。 在 IR 分类中的第三个方向即命名方案，比如表达式 \\(a-2*b\\)，编译器可以产生如下 IR ;; LLVM IR %b = load i32, i32* %bstack, align 4 ; _1 = 2 * b %1 = mul nsw i32 2, %b %a = load i32, i32* %astack, align 4 ; _2 = a - _1 %2 = sub nsw i32 %a, %1 加载进寄存器、进行运算，一共分配了 4 个名字。如果将 %1 和 %2 这两个名字都替换为 %b，那么分配的名字数目将会缩减一半。因此名字的分配对编译器有很大影响。比如子表达式 \\(2 - b\\) 拥有唯一的名字，如果之后编译器发现有冗余的该表达式求值，并且期间并没有产生对变量 b 的副作用，可以直接用此处产生的值替换，而不用再次计算。另外，命名方案的选择还会影响到编译时的数据结构大小以及编译所用的时间。 ","date":"09-19","objectID":"/2022/intermediate_representation/:1:0","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#ir-的分类"},{"categories":["CompilerPrinciple"],"content":" 图 IR虽然很多编译器使用图 IR 作为中间表示，但是其中的抽象层次、图的结构等方面可能有巨大不同。 ","date":"09-19","objectID":"/2022/intermediate_representation/:2:0","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#图-ir"},{"categories":["CompilerPrinciple"],"content":" 与语法相关的树语法分析树是一种树形 IR，表示程序中的源代码形式的图。大多数的树形 IR，其结构对应于源代码的语法。 语法分析树相较于源代码，语法分析树更为庞大，其表示了完整的语法推导过程，树中的每个结点都表示推导过程中的语法符号。编译器必须为每个结点和边分配内存，并在编译器间遍历所有结点与边。 为了缩减语法分析树的规模，对语法分析树做以变换，消除推导过程中的一部分步骤和对应的语法结点，这种变换产生了抽象语法树。 抽象语法树抽象语法树 (AST, Abstract Syntax Tree) 保留了语法分析树的基本结构，单剔除了非必要结点。源代码到源代码的转换系统 (语法制导编辑器和自动并行化工具) 通常使用 AST，而 Lisp 系中的 S-expression (symbolic expression) 本质上也是 AST。 (define factorial (if (= x 0) 1 (* x (factorial (- x 1))))) AST 的表示选择也会影响编译器的行为，比如一个复数常数，实部和虚部分别为树形结构的子结点，这种结构的 AST 适用于语法制导编辑器，可以方便的分别修改实部与虚部的值。由于其他常数都是单结点的，而这种结构的常数需要编译器在处理常数添加额外的代码。如果将复数常数设计为单结点的，那么编译器在编辑或加载到寄存器时操作会变得复杂，但可以简化其他操作。 有向非循环图虽然 AST 相比语法分析树已经很简洁了，但它依然保留了所有源代码的结构，即使表达式中存在完全相同的两个子树。有向非循环图 (DAG, Directed Acyclic Graph) 是 AST 避免这种复制的简化。相同的子树可以被重复利用，也就意味着结点可以有多个父结点。 DAG 可以显示展示出冗余的表达式，减少冗余的求值，降低求值过程中的代价。但明显，编译器必须证明两个相同的表达式之间没有副作用产生，副作用产生时 DAG 需要重新计算子树。 编译器使用 DAG 主要基于两点原因，能够减少内存占用，其次能够暴露出冗余之处，从而优化代码。通常在优化时，DAG 作为衍生的 IR 使用，建立 DAG 以获取相关的冗余信息，之后将其丢弃。 ","date":"09-19","objectID":"/2022/intermediate_representation/:2:1","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#与语法相关的树"},{"categories":["CompilerPrinciple"],"content":" 与语法相关的树语法分析树是一种树形 IR，表示程序中的源代码形式的图。大多数的树形 IR，其结构对应于源代码的语法。 语法分析树相较于源代码，语法分析树更为庞大，其表示了完整的语法推导过程，树中的每个结点都表示推导过程中的语法符号。编译器必须为每个结点和边分配内存，并在编译器间遍历所有结点与边。 为了缩减语法分析树的规模，对语法分析树做以变换，消除推导过程中的一部分步骤和对应的语法结点，这种变换产生了抽象语法树。 抽象语法树抽象语法树 (AST, Abstract Syntax Tree) 保留了语法分析树的基本结构，单剔除了非必要结点。源代码到源代码的转换系统 (语法制导编辑器和自动并行化工具) 通常使用 AST，而 Lisp 系中的 S-expression (symbolic expression) 本质上也是 AST。 (define factorial (if (= x 0) 1 (* x (factorial (- x 1))))) AST 的表示选择也会影响编译器的行为，比如一个复数常数，实部和虚部分别为树形结构的子结点，这种结构的 AST 适用于语法制导编辑器，可以方便的分别修改实部与虚部的值。由于其他常数都是单结点的，而这种结构的常数需要编译器在处理常数添加额外的代码。如果将复数常数设计为单结点的，那么编译器在编辑或加载到寄存器时操作会变得复杂，但可以简化其他操作。 有向非循环图虽然 AST 相比语法分析树已经很简洁了，但它依然保留了所有源代码的结构，即使表达式中存在完全相同的两个子树。有向非循环图 (DAG, Directed Acyclic Graph) 是 AST 避免这种复制的简化。相同的子树可以被重复利用，也就意味着结点可以有多个父结点。 DAG 可以显示展示出冗余的表达式，减少冗余的求值，降低求值过程中的代价。但明显，编译器必须证明两个相同的表达式之间没有副作用产生，副作用产生时 DAG 需要重新计算子树。 编译器使用 DAG 主要基于两点原因，能够减少内存占用，其次能够暴露出冗余之处，从而优化代码。通常在优化时，DAG 作为衍生的 IR 使用，建立 DAG 以获取相关的冗余信息，之后将其丢弃。 ","date":"09-19","objectID":"/2022/intermediate_representation/:2:1","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#语法分析树"},{"categories":["CompilerPrinciple"],"content":" 与语法相关的树语法分析树是一种树形 IR，表示程序中的源代码形式的图。大多数的树形 IR，其结构对应于源代码的语法。 语法分析树相较于源代码，语法分析树更为庞大，其表示了完整的语法推导过程，树中的每个结点都表示推导过程中的语法符号。编译器必须为每个结点和边分配内存，并在编译器间遍历所有结点与边。 为了缩减语法分析树的规模，对语法分析树做以变换，消除推导过程中的一部分步骤和对应的语法结点，这种变换产生了抽象语法树。 抽象语法树抽象语法树 (AST, Abstract Syntax Tree) 保留了语法分析树的基本结构，单剔除了非必要结点。源代码到源代码的转换系统 (语法制导编辑器和自动并行化工具) 通常使用 AST，而 Lisp 系中的 S-expression (symbolic expression) 本质上也是 AST。 (define factorial (if (= x 0) 1 (* x (factorial (- x 1))))) AST 的表示选择也会影响编译器的行为，比如一个复数常数，实部和虚部分别为树形结构的子结点，这种结构的 AST 适用于语法制导编辑器，可以方便的分别修改实部与虚部的值。由于其他常数都是单结点的，而这种结构的常数需要编译器在处理常数添加额外的代码。如果将复数常数设计为单结点的，那么编译器在编辑或加载到寄存器时操作会变得复杂，但可以简化其他操作。 有向非循环图虽然 AST 相比语法分析树已经很简洁了，但它依然保留了所有源代码的结构，即使表达式中存在完全相同的两个子树。有向非循环图 (DAG, Directed Acyclic Graph) 是 AST 避免这种复制的简化。相同的子树可以被重复利用，也就意味着结点可以有多个父结点。 DAG 可以显示展示出冗余的表达式，减少冗余的求值，降低求值过程中的代价。但明显，编译器必须证明两个相同的表达式之间没有副作用产生，副作用产生时 DAG 需要重新计算子树。 编译器使用 DAG 主要基于两点原因，能够减少内存占用，其次能够暴露出冗余之处，从而优化代码。通常在优化时，DAG 作为衍生的 IR 使用，建立 DAG 以获取相关的冗余信息，之后将其丢弃。 ","date":"09-19","objectID":"/2022/intermediate_representation/:2:1","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#抽象语法树"},{"categories":["CompilerPrinciple"],"content":" 与语法相关的树语法分析树是一种树形 IR，表示程序中的源代码形式的图。大多数的树形 IR，其结构对应于源代码的语法。 语法分析树相较于源代码，语法分析树更为庞大，其表示了完整的语法推导过程，树中的每个结点都表示推导过程中的语法符号。编译器必须为每个结点和边分配内存，并在编译器间遍历所有结点与边。 为了缩减语法分析树的规模，对语法分析树做以变换，消除推导过程中的一部分步骤和对应的语法结点，这种变换产生了抽象语法树。 抽象语法树抽象语法树 (AST, Abstract Syntax Tree) 保留了语法分析树的基本结构，单剔除了非必要结点。源代码到源代码的转换系统 (语法制导编辑器和自动并行化工具) 通常使用 AST，而 Lisp 系中的 S-expression (symbolic expression) 本质上也是 AST。 (define factorial (if (= x 0) 1 (* x (factorial (- x 1))))) AST 的表示选择也会影响编译器的行为，比如一个复数常数，实部和虚部分别为树形结构的子结点，这种结构的 AST 适用于语法制导编辑器，可以方便的分别修改实部与虚部的值。由于其他常数都是单结点的，而这种结构的常数需要编译器在处理常数添加额外的代码。如果将复数常数设计为单结点的，那么编译器在编辑或加载到寄存器时操作会变得复杂，但可以简化其他操作。 有向非循环图虽然 AST 相比语法分析树已经很简洁了，但它依然保留了所有源代码的结构，即使表达式中存在完全相同的两个子树。有向非循环图 (DAG, Directed Acyclic Graph) 是 AST 避免这种复制的简化。相同的子树可以被重复利用，也就意味着结点可以有多个父结点。 DAG 可以显示展示出冗余的表达式，减少冗余的求值，降低求值过程中的代价。但明显，编译器必须证明两个相同的表达式之间没有副作用产生，副作用产生时 DAG 需要重新计算子树。 编译器使用 DAG 主要基于两点原因，能够减少内存占用，其次能够暴露出冗余之处，从而优化代码。通常在优化时，DAG 作为衍生的 IR 使用，建立 DAG 以获取相关的冗余信息，之后将其丢弃。 ","date":"09-19","objectID":"/2022/intermediate_representation/:2:1","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#有向非循环图"},{"categories":["CompilerPrinciple"],"content":" 图 控制流图程序中最简单的控制流单位是基本单位块 (BB, Basic Block)，也就是最大长度的无分支代码序列。BB 中的所有指令总是按顺序全部执行，结束于一个分支、跳转或条件跳转指令。 控制流图 (CFG, Control Flow Graph) 对程序中各 BB 之间的控制流建立了模型。CFG 是一个有向图 \\(G=(N, E)\\)，每个结点 \\(n \\in N\\) 对应于一个 BB，每条边 \\(e=(n_{i}, n_{j}) \\in E\\) 对应于一个可能的从块 \\(n_{i}\\) 到 \\(n_{j}\\) 的一个可能的控制转移。 CFG 不同于面向语法的 IR，而是提供一种语法结构的表示。 假定每个 CFG 都有唯一的入口结点 \\(n_{0}\\)，和一个唯一的出口结点 \\(n_{f}\\)。对于有多个入口的一个过程，编译器可以添加 \\(n_{0}\\) 到各个实际入口结点的边，对于更为常见的多出口，编译器将添加各个实际出口结点到 \\(n_{f}\\) 的边。 while (condition) do stmt1 end stmt2 if (condition) then stmt1 else stmt2 end stmt3 编译器通常将 CFG 与另一种 IR 联用，从而组合成一种混合 IR。用 CFG 表示块之间的关系，而块内部采用表达式层次上的 AST、DAG 或某种线性 IR。 另外构造 CFG 时，代码块也可以不采用 BB 构造，而是使用对应于源代码层次上的单一语句的单语句块，单语句块可以简化用于分析和优化的算法。BB 与单语句块的选择实际上是时间与空间的权衡，单语句块需要更多的结点与边，同时遍历块结点时也需要更多的时间。 依赖关系图编译器还是用图来编码表示值从创建之外 (定义) 到使用处的流动，数据依赖图 (DDG, Data Dependence Graph) 就是用于表示这种关系的。 ;; LLVM IR %1 = load of c[i] %2 = load of b[i - 1] %3 = fadd %1, %2 b[i] = %3 DDG 对操作序列进行了实际约束：一个值不能在定义前进行使用。因此编译器可以根据 DDG 对指令进行重排，这正是乱序重排的基础。DDG 通常作为衍生 IR使用，在指令调度中发挥着重要作用。另外，对数组元素的引用，其值取决于之前定义的数组变量结点，因此可以通过 DDG 将所有对数组的引用关联起来。 调用图为解决跨过程边界的效率低下问题，编译器会进行过程间分析和优化。调用图 (Call Graph) 用结点表示过程，边表示每个不同的过程的调用位置。比如，过程 p 中有三处不同位置对过程 q 的调用，因此调用图中有三条 (p, q) 的边。 但是软件工程方面的惯用法和语言特性都会使调用图变得复杂 分离编译，即分别独立编译程序的若干小子集的惯例，典型代表为 C / C++ 的编译单元为一个源文件。该行为限制了编译器建立调用图并进行过程间分析和优化的能力。 过程参数，即将过程作为参数或返回值的高阶函数，都会引入具有二义性的调用位置，使得调用图的构建复杂化。例如一个函数对象，同一个调用位置可能调用不同的过程。 OO中的 override ","date":"09-19","objectID":"/2022/intermediate_representation/:2:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#图"},{"categories":["CompilerPrinciple"],"content":" 图 控制流图程序中最简单的控制流单位是基本单位块 (BB, Basic Block)，也就是最大长度的无分支代码序列。BB 中的所有指令总是按顺序全部执行，结束于一个分支、跳转或条件跳转指令。 控制流图 (CFG, Control Flow Graph) 对程序中各 BB 之间的控制流建立了模型。CFG 是一个有向图 \\(G=(N, E)\\)，每个结点 \\(n \\in N\\) 对应于一个 BB，每条边 \\(e=(n_{i}, n_{j}) \\in E\\) 对应于一个可能的从块 \\(n_{i}\\) 到 \\(n_{j}\\) 的一个可能的控制转移。 CFG 不同于面向语法的 IR，而是提供一种语法结构的表示。 假定每个 CFG 都有唯一的入口结点 \\(n_{0}\\)，和一个唯一的出口结点 \\(n_{f}\\)。对于有多个入口的一个过程，编译器可以添加 \\(n_{0}\\) 到各个实际入口结点的边，对于更为常见的多出口，编译器将添加各个实际出口结点到 \\(n_{f}\\) 的边。 while (condition) do stmt1 end stmt2 if (condition) then stmt1 else stmt2 end stmt3 编译器通常将 CFG 与另一种 IR 联用，从而组合成一种混合 IR。用 CFG 表示块之间的关系，而块内部采用表达式层次上的 AST、DAG 或某种线性 IR。 另外构造 CFG 时，代码块也可以不采用 BB 构造，而是使用对应于源代码层次上的单一语句的单语句块，单语句块可以简化用于分析和优化的算法。BB 与单语句块的选择实际上是时间与空间的权衡，单语句块需要更多的结点与边，同时遍历块结点时也需要更多的时间。 依赖关系图编译器还是用图来编码表示值从创建之外 (定义) 到使用处的流动，数据依赖图 (DDG, Data Dependence Graph) 就是用于表示这种关系的。 ;; LLVM IR %1 = load of c[i] %2 = load of b[i - 1] %3 = fadd %1, %2 b[i] = %3 DDG 对操作序列进行了实际约束：一个值不能在定义前进行使用。因此编译器可以根据 DDG 对指令进行重排，这正是乱序重排的基础。DDG 通常作为衍生 IR使用，在指令调度中发挥着重要作用。另外，对数组元素的引用，其值取决于之前定义的数组变量结点，因此可以通过 DDG 将所有对数组的引用关联起来。 调用图为解决跨过程边界的效率低下问题，编译器会进行过程间分析和优化。调用图 (Call Graph) 用结点表示过程，边表示每个不同的过程的调用位置。比如，过程 p 中有三处不同位置对过程 q 的调用，因此调用图中有三条 (p, q) 的边。 但是软件工程方面的惯用法和语言特性都会使调用图变得复杂 分离编译，即分别独立编译程序的若干小子集的惯例，典型代表为 C / C++ 的编译单元为一个源文件。该行为限制了编译器建立调用图并进行过程间分析和优化的能力。 过程参数，即将过程作为参数或返回值的高阶函数，都会引入具有二义性的调用位置，使得调用图的构建复杂化。例如一个函数对象，同一个调用位置可能调用不同的过程。 OO中的 override ","date":"09-19","objectID":"/2022/intermediate_representation/:2:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#控制流图"},{"categories":["CompilerPrinciple"],"content":" 图 控制流图程序中最简单的控制流单位是基本单位块 (BB, Basic Block)，也就是最大长度的无分支代码序列。BB 中的所有指令总是按顺序全部执行，结束于一个分支、跳转或条件跳转指令。 控制流图 (CFG, Control Flow Graph) 对程序中各 BB 之间的控制流建立了模型。CFG 是一个有向图 \\(G=(N, E)\\)，每个结点 \\(n \\in N\\) 对应于一个 BB，每条边 \\(e=(n_{i}, n_{j}) \\in E\\) 对应于一个可能的从块 \\(n_{i}\\) 到 \\(n_{j}\\) 的一个可能的控制转移。 CFG 不同于面向语法的 IR，而是提供一种语法结构的表示。 假定每个 CFG 都有唯一的入口结点 \\(n_{0}\\)，和一个唯一的出口结点 \\(n_{f}\\)。对于有多个入口的一个过程，编译器可以添加 \\(n_{0}\\) 到各个实际入口结点的边，对于更为常见的多出口，编译器将添加各个实际出口结点到 \\(n_{f}\\) 的边。 while (condition) do stmt1 end stmt2 if (condition) then stmt1 else stmt2 end stmt3 编译器通常将 CFG 与另一种 IR 联用，从而组合成一种混合 IR。用 CFG 表示块之间的关系，而块内部采用表达式层次上的 AST、DAG 或某种线性 IR。 另外构造 CFG 时，代码块也可以不采用 BB 构造，而是使用对应于源代码层次上的单一语句的单语句块，单语句块可以简化用于分析和优化的算法。BB 与单语句块的选择实际上是时间与空间的权衡，单语句块需要更多的结点与边，同时遍历块结点时也需要更多的时间。 依赖关系图编译器还是用图来编码表示值从创建之外 (定义) 到使用处的流动，数据依赖图 (DDG, Data Dependence Graph) 就是用于表示这种关系的。 ;; LLVM IR %1 = load of c[i] %2 = load of b[i - 1] %3 = fadd %1, %2 b[i] = %3 DDG 对操作序列进行了实际约束：一个值不能在定义前进行使用。因此编译器可以根据 DDG 对指令进行重排，这正是乱序重排的基础。DDG 通常作为衍生 IR使用，在指令调度中发挥着重要作用。另外，对数组元素的引用，其值取决于之前定义的数组变量结点，因此可以通过 DDG 将所有对数组的引用关联起来。 调用图为解决跨过程边界的效率低下问题，编译器会进行过程间分析和优化。调用图 (Call Graph) 用结点表示过程，边表示每个不同的过程的调用位置。比如，过程 p 中有三处不同位置对过程 q 的调用，因此调用图中有三条 (p, q) 的边。 但是软件工程方面的惯用法和语言特性都会使调用图变得复杂 分离编译，即分别独立编译程序的若干小子集的惯例，典型代表为 C / C++ 的编译单元为一个源文件。该行为限制了编译器建立调用图并进行过程间分析和优化的能力。 过程参数，即将过程作为参数或返回值的高阶函数，都会引入具有二义性的调用位置，使得调用图的构建复杂化。例如一个函数对象，同一个调用位置可能调用不同的过程。 OO中的 override ","date":"09-19","objectID":"/2022/intermediate_representation/:2:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#依赖关系图"},{"categories":["CompilerPrinciple"],"content":" 图 控制流图程序中最简单的控制流单位是基本单位块 (BB, Basic Block)，也就是最大长度的无分支代码序列。BB 中的所有指令总是按顺序全部执行，结束于一个分支、跳转或条件跳转指令。 控制流图 (CFG, Control Flow Graph) 对程序中各 BB 之间的控制流建立了模型。CFG 是一个有向图 \\(G=(N, E)\\)，每个结点 \\(n \\in N\\) 对应于一个 BB，每条边 \\(e=(n_{i}, n_{j}) \\in E\\) 对应于一个可能的从块 \\(n_{i}\\) 到 \\(n_{j}\\) 的一个可能的控制转移。 CFG 不同于面向语法的 IR，而是提供一种语法结构的表示。 假定每个 CFG 都有唯一的入口结点 \\(n_{0}\\)，和一个唯一的出口结点 \\(n_{f}\\)。对于有多个入口的一个过程，编译器可以添加 \\(n_{0}\\) 到各个实际入口结点的边，对于更为常见的多出口，编译器将添加各个实际出口结点到 \\(n_{f}\\) 的边。 while (condition) do stmt1 end stmt2 if (condition) then stmt1 else stmt2 end stmt3 编译器通常将 CFG 与另一种 IR 联用，从而组合成一种混合 IR。用 CFG 表示块之间的关系，而块内部采用表达式层次上的 AST、DAG 或某种线性 IR。 另外构造 CFG 时，代码块也可以不采用 BB 构造，而是使用对应于源代码层次上的单一语句的单语句块，单语句块可以简化用于分析和优化的算法。BB 与单语句块的选择实际上是时间与空间的权衡，单语句块需要更多的结点与边，同时遍历块结点时也需要更多的时间。 依赖关系图编译器还是用图来编码表示值从创建之外 (定义) 到使用处的流动，数据依赖图 (DDG, Data Dependence Graph) 就是用于表示这种关系的。 ;; LLVM IR %1 = load of c[i] %2 = load of b[i - 1] %3 = fadd %1, %2 b[i] = %3 DDG 对操作序列进行了实际约束：一个值不能在定义前进行使用。因此编译器可以根据 DDG 对指令进行重排，这正是乱序重排的基础。DDG 通常作为衍生 IR使用，在指令调度中发挥着重要作用。另外，对数组元素的引用，其值取决于之前定义的数组变量结点，因此可以通过 DDG 将所有对数组的引用关联起来。 调用图为解决跨过程边界的效率低下问题，编译器会进行过程间分析和优化。调用图 (Call Graph) 用结点表示过程，边表示每个不同的过程的调用位置。比如，过程 p 中有三处不同位置对过程 q 的调用，因此调用图中有三条 (p, q) 的边。 但是软件工程方面的惯用法和语言特性都会使调用图变得复杂 分离编译，即分别独立编译程序的若干小子集的惯例，典型代表为 C / C++ 的编译单元为一个源文件。该行为限制了编译器建立调用图并进行过程间分析和优化的能力。 过程参数，即将过程作为参数或返回值的高阶函数，都会引入具有二义性的调用位置，使得调用图的构建复杂化。例如一个函数对象，同一个调用位置可能调用不同的过程。 OO中的 override ","date":"09-19","objectID":"/2022/intermediate_representation/:2:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#调用图"},{"categories":["CompilerPrinciple"],"content":" 线性 IR线性 IR 可以说是图 IR 的一种备选方案，当然汇编代码可以理解为一种线性代码。线性代码是一个指令序列，对序列进行顺序执行。线性 IR 用编码表达程序中各个位置间的控制转移，控制流通常模拟目标机上的实现。控制流将线性 IR 的 BB 划分开，块结束于分支、跳转或有标号的操作前。 线性 IR 有很多种类 单地址码模拟了累加器机器和堆栈机的行为，暴露了机器对隐式名字的使用，因此编译器能够相应地调整代码，由该 IR 得出的代码相当紧凑 二地址码模拟了具有破坏性操作的机器，随着内存逐渐富裕，这种代码逐渐废除 三地址码可以表示两个操作数和一个结果的操作，随着 RISC 指令集的崛起，这种与 RISC 指令十分相似的 IR 被广泛使用 信息 破坏性操作是一种用操作的结果重新定义其中一个操作数的操作 ","date":"09-19","objectID":"/2022/intermediate_representation/:3:0","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#线性-ir"},{"categories":["CompilerPrinciple"],"content":" 堆栈机码堆栈机码是一种单地址码，假定操作数存在一个栈中，大多数操作从栈中获取操作数，并将其结果推入栈中。例如，整数减法操作会从栈顶移出两个元素并计算其值，将结果入栈。 ;; 计算 a - 2 * b push 2 push b multiply push a subtract 栈的存在产生了对某些新操作的需求，栈 IR 通常包括一个 swap 操作，用于交换栈顶两个元素的值。堆栈机码比较紧凑，由于栈本身建立了一个隐式的名称空间，从而消除了 IR 中的许多名字，极大缩减了 IR 形式下的程序大小。但是换来的是，所有的结果和参数都是暂态的，除非显示的将其移入内存。 堆栈机代码非常简单，且易于生成和使用，因此 Smalltalk80 和 Java 都采用了类似堆栈机码的字节码。 ","date":"09-19","objectID":"/2022/intermediate_representation/:3:1","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#堆栈机码"},{"categories":["CompilerPrinciple"],"content":" 三地址码三地址码是什么可以看这里。三地址码比较有吸引力的原因是 代码相当紧凑，操作通常占用 1 或 2 个字节，各个名字通常由整数或表索引表示，通常 4 个字节就够了，一条指令只需要占用很少的内存空间 操作数与结果可以分别指定名字，这给编译器提供了相当的自由度，以控制名字和值的重用，没有破坏性操作。谨慎选择的名称空间表示存在改进代码的新机会 许多现代处理器实现了三地址操作，三地址码可以有效模拟这一性质 由于抽象层次的不同，三地址码可以由极大的差异，通常会包含大部分底层操作 (如跳转、分支、内存操作等)，也有内部封装了控制流的高级指令 (如 max、min 等)。 例如 mvcl (move characters long) 指令 (功能与 C 函数 memcpy 一致)，在实现了该指令的架构上，编译器可以直接使用该指令表示复杂的操作，在不关注指令内部工作机制的情况下进行分析、优化、移动等操作。 实际编译器的中间表示可能有多种。 GCC 长期使用被称为寄存器传输语言 (RTL, Register Transfer Language) 的底层 IR。近年 GCC 转移到了语法分析器产生特定接口的语法分析树，这些接口可以从语言相关的语法分析树转化为语言无关的类似于树的 IR – GIMPLE。GIMPLE 使用表达式和赋值的三地址码对树结构进行了注解，并表示控制流，基于 GIMPLE 构建 SSA。之后 GCC 将 GIMPLE 转化为 RTL 进行最后的优化和代码生成。 LLVM 使用单一的类型化的线性三地址 SSA IR，LLVM IR 对数组和结构地址提供了显示支持，还有对向量或 SIMD 数据和操作支持。 Open64 使用了一组被称为 WHIRL 的相关 IR (共 5 个)，WHIRL 提供了不同抽象层次的 IR，保存了足够多的信息用于提高优化效果。 ","date":"09-19","objectID":"/2022/intermediate_representation/:3:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#三地址码"},{"categories":["CompilerPrinciple"],"content":" 将值映射到名字对特定 IR 和抽象层次的选择，有助于确定编译器能够操控和优化的操作。类似的，编译器对计算出的各种值分配内部名字的规则，也会对优化产生影响，可能会揭示优化的机会，也可能使优化的机会变得模糊。必须为程序执行过程中产生的许多中间结果进行命名，与名字相关的选择很大程度上决定了哪些计算过程是可以分析和优化的。 ","date":"09-19","objectID":"/2022/intermediate_representation/:4:0","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#将值映射到名字"},{"categories":["CompilerPrinciple"],"content":" 临时值的命名IR 通常比源代码包含更多的细节，这些细节有的是源代码中隐含的，而有一些是在转换过程中的选择。比如一个简单的计算块 a = b + c b = a - d c = b + c d = a - d 其中源代码并没有提供关于值的命名，由此可能误导他人第一行 \\(a = b + c\\) 与第三行 \\(c = b + c\\) 的值是相同的。而转换为 IR 后，由此多出了更多临时变量 ;; LLVM IR ;; a = b + c %1 = load i32, i32* %bp, align 4 %2 = load i32, i32* %cp, align 4 %3 = add nsw i32 %1, %2 store i32 %3, i32* %ap, align 4 ;; b = a - d %4 = load i32, i32* %dp, align 4 %5 = sub nsw i32 %3, %4 store i32 %5, i32* %bp, align 4 ;; c = b + c %6 = add nsw i32 %5, %2 store i32 %6, i32* %cp, align 4 ;; d = a - d ;; %5 = sub nsw i32 %3, %4 store i32 %5, i32* %dp, align 4 很明显，源代码第三行的表达式 b+c 与第一行的表达式 b+c 表示的是完全不同的含义，而第二行却与第四行是完全相同的。 在底层 IR 中，各个中间结果都有自身的名字，使用不同的名字会将这些结果暴露给分析和变换的过程。编译器实现的大多数改进都来自对上下文的利用，因此 IR 必须暴露上下文信息，命名可能隐藏上下文信息，因为其中可能将一个名字用于表示多个不同的值；命名可能暴露上下文信息，只要能够在名字和值之间建立对应关系。 ","date":"09-19","objectID":"/2022/intermediate_representation/:4:1","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#临时值的命名"},{"categories":["CompilerPrinciple"],"content":" 静态单赋值形式静态单赋值形式 (SSA, Static Single-Assignment Form) 是一种命名规范，名字唯一地对应到代码特定的定义位置，每个名字都是通过单个操作定义的。因此每个名字都编码了对应值的来源地信息；文本化的名字实际上指向了一个特定的定义位置。为了使这种唯一性的命名规范与控制流效应相一致，SSA 会在控制流路径满足相应条件的位置上插入一些特殊操作，即 \\(\\phi\\) (phi) 函数。 while (x \u003c 100) do x = x + 1 y = x + y end return y \\(\\phi\\) 是一种 IR 层面的伪指令，用于合并来自不同控制流的名字，并定义出一个新的名字。当然 \\(\\phi\\) 不限于三地址模型，它可以添加任意数量的操作数。 ;; LLVM IR entry: %1 = load i32, i32* %xp, align 4 %2 = load i32, i32* %yp, align 4 br label %cond cond: ; preds = %entry, %loop %3 = phi i32 [ %1, %entry ], [ %6, %loop ] %4 = phi i32 [ %2, %entry ], [ %7, %loop ] %5 = icmp slt i32 %3, 100 br i1 %5, label %loop, label %next loop: ; preds = %cond %6 = add nsw i32 %3, 1 %7 = add nsw i32 %6, %4 br label %cond next: ; preds = %cond ret i32 %4 \\(\\phi\\) 的行为取决于上下文，它选择其中一个控制流中的名字来确定新定义的名字的值。当该块由 entry 块跳转而来时，名字 %3 使用 %1 作为其值；相反由 loop 块跳转而来时，其绑定的是 %6 的值。在 BB 的入口处，该块的所有 \\(\\phi\\) 都将在任何其他语句之前并发执行，该行为允许操作 SSA 的算法在处理 BB 顶部时忽略 \\(\\phi\\) 的顺序。 SSA 的 \\(\\phi\\) 包含了值的产生与引用两个方面的信息，单赋值的特性使编译器可以规避许多与值的生命周期相关的问题。当然 SSA 会有一些问题，比如上面 LLVM IR 的代码，从 entry 块进入 cond 块时，实际上 %6 还没有被定义，因而 \\(\\phi\\) 不可能读取该未定义值。 ","date":"09-19","objectID":"/2022/intermediate_representation/:4:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#静态单赋值形式"},{"categories":["CompilerPrinciple"],"content":" 内存模型如命名会影响 IR 能够表示的信息一样，编译器对于每个值存储的位置也有类似的影响。对于代码中计算的每个值，编译器必须确定该值将驻留在何处。一般来说编译器使用以下两种内存模型： 寄存器到寄存器的模型 (Register-to-Register Model) 编译器可以激进的将值保存在寄存器中，忽略物理寄存器集合规定的任何限制。编译器可在其生命周期内合法的将值保存在寄存器中，仅程序显式的要求将值存储在内存中，编译器才采取相应的操作。在过程间调用时，如果任何局部变量作为参数传地给被调用过程，这些局部变量必须存储在内存中。而有些值无法存储在寄存器中，也将被存储在内存。编译器将会生成相应的代码，每次计算出值时将其存储到内存，使用时从内存加载进来。 内存到内存的模型 (Memory-to-Memory Model) 编译器假定所有值都在内存中，只有使用时加载进内存，使用完毕后将值立即写回内存。这种模型下的 IR 引用的寄存器数目会小一些，不过可能需要设计者添加内存到内存的操作 (如内存到内存的加法)。 内存模型的选择会影响到其余部分。比如 R2R Model 编译器通常假设虚拟寄存器是无限多个，因而寄存器分配必须将 IR 中使用的虚拟寄存器集合映射到物理寄存器上。但是 R2R Model 同样反映出该值在寄存器中是安全的。 ","date":"09-19","objectID":"/2022/intermediate_representation/:4:3","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#内存模型"},{"categories":["CompilerPrinciple"],"content":" 符号表作为转换过程的一部分，编译器需要推导与被转换程序操控的各种实体有关的信息。它必须发现并存储许多不同种类的信息，如遇到的各种各样的名字：变量、常数、过程、函数、标号、结构和文件。 当然还有很多额外信息。比如对一个变量，需要包括数据类型、存储类别、声明变量的过程名、语法层次、在内存中的存储位置等。而对于一个数组，编译器还需要知道数组的维数和各个维度上索引的上下界。对于记录或结构，编译器需要了解成员字段和每个字段的相关信息。而对于函数和过程，编译器需要知道参数的数目，以及各参数的类型，可能还有返回值类型。 编译器需要在 IR 中记录这些信息，或者按需推导。但是大部分编译器都会直接存储这些信息，如将其记录在变量声明的结点中。遍历查找声明是需要代价的，当然也可以将 IR 线索化，每次引用都有一个指向对应声明的链接。 另一种选择是建立一个中央存储库，以提供相关信息的高效访问，这就是现代编译器中不可或缺的符号表。当然一个编译器可能包含几个不同的、专门化的符号表。 ","date":"09-19","objectID":"/2022/intermediate_representation/:5:0","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#符号表"},{"categories":["CompilerPrinciple"],"content":" 处理嵌套的作用域大多数编程语言允许程序在多个层次上声明名称，每个层次都在程序的文本中有一个作用域，声明的名字将在作用域中使用。而作用域在运行时对应着生命周期，即其中的变量在运行时会保存的时间。如果源语言允许嵌套的作用域，那么前端需要一种机制将特定的引用转换为正确的作用域和生命周期。编译器进行这种转换的主要机制是一种作用域化的符号表。 (let ((x 3) (y 4)) (* x y)) ; =\u003e 12 (* x y) ; ERROR: unbound variable: x 为编译包含嵌套作用域的程序，编译器必须将每个变量引用映射到与之对应的特定声明。这个过程称为名字解析 (name resolution)，将各次引用映射到其声明所在的词法层次，完成这一工作的就是作用域化的符号表。 在管理嵌套作用域时，语法分析器必须稍微改变一下其管理符号表的方法。语法分析器每次进入一个新的词法作用域时，它将为该作用域建立一个新的符号表。这种方法将创建一束符号表，按词法作用域的层次嵌套关系连接在一起，在当前作用域中遇到声明时，就将相应的信息输入到当前作用域的符号表中。在遇到引用时，需要检查当前作用域的符号表，如果当前符号表并不包含该名字的声明，那么就依次向外一层检查，直到遇到该名字的声明或所有可见的作用域都没有该名字的声明。 ","date":"09-19","objectID":"/2022/intermediate_representation/:5:1","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#处理嵌套的作用域"},{"categories":["CompilerPrinciple"],"content":" 符号表的用途编译器可能会建立许多不同的符号表来用于不同的目标。 结构表用于指定结构或记录中字段的文本串，存在与变量和过程不同的名称空间。对于结构中的每个字段，编译器必须记录其类型、大小、结构内偏移量等信息，它还需要确定结构的总长度。管理字段名的命名空间由几种方式： 独立表， 编译器为每个记录定义维护一个对应的符号表。概念上这种方式最干净纯粹。 选择符表，编译器可以为所有字段名维护一个独立的表。为避免不同结构中同名字段间的冲突，编译器必须使用修饰名，即为同一个结构中的字段添加同样的但全局唯一的前缀。该前缀可以是结构名，也可以是结构名在符号表中的索引值。这种方法编译器必须想办法将同一个结构的字段关联起来。 统一表，编译器通过修饰名将字段同样存储在主符号表中，这样可以减少表的数目，但也意味着增加主符号表中的表项与字段。 独立表的好处在于，任何与作用域相关的问题，都可以自然而然地匹配到主符号表的作用域管理框架中，结构本身可见时，其内部的符号表可以通过结构在主符号表中的表项访问。 使用链接表解决 OO 范式的名字解析问题OO 范式中的名字的作用域规则同时取决于数据的结构与代码的结构，这也导致出现了一组更复杂的符号表。比如 Java 中对于正在被编译的代码、代码中已知和引用的任何外部类、包含代码的类之上的继承层次，都分别需要相应的符号表。 相对简单的实现方式是，对每个类附加一个符号表，其中涉及两个嵌套的层次结构：用于类中各个方法内部的词法作用域，和追踪类的继承层次结构。 编译器对每个方法都需要一个词法作用域化的符号表，对每个类都需要一个包含指向继承层次中父类的链接符号表。当然还需要指向其他类的符号表的链接，以及指向包层次变量符号表的链接。 ","date":"09-19","objectID":"/2022/intermediate_representation/:5:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#符号表的用途"},{"categories":["CompilerPrinciple"],"content":" 符号表的用途编译器可能会建立许多不同的符号表来用于不同的目标。 结构表用于指定结构或记录中字段的文本串，存在与变量和过程不同的名称空间。对于结构中的每个字段，编译器必须记录其类型、大小、结构内偏移量等信息，它还需要确定结构的总长度。管理字段名的命名空间由几种方式： 独立表， 编译器为每个记录定义维护一个对应的符号表。概念上这种方式最干净纯粹。 选择符表，编译器可以为所有字段名维护一个独立的表。为避免不同结构中同名字段间的冲突，编译器必须使用修饰名，即为同一个结构中的字段添加同样的但全局唯一的前缀。该前缀可以是结构名，也可以是结构名在符号表中的索引值。这种方法编译器必须想办法将同一个结构的字段关联起来。 统一表，编译器通过修饰名将字段同样存储在主符号表中，这样可以减少表的数目，但也意味着增加主符号表中的表项与字段。 独立表的好处在于，任何与作用域相关的问题，都可以自然而然地匹配到主符号表的作用域管理框架中，结构本身可见时，其内部的符号表可以通过结构在主符号表中的表项访问。 使用链接表解决 OO 范式的名字解析问题OO 范式中的名字的作用域规则同时取决于数据的结构与代码的结构，这也导致出现了一组更复杂的符号表。比如 Java 中对于正在被编译的代码、代码中已知和引用的任何外部类、包含代码的类之上的继承层次，都分别需要相应的符号表。 相对简单的实现方式是，对每个类附加一个符号表，其中涉及两个嵌套的层次结构：用于类中各个方法内部的词法作用域，和追踪类的继承层次结构。 编译器对每个方法都需要一个词法作用域化的符号表，对每个类都需要一个包含指向继承层次中父类的链接符号表。当然还需要指向其他类的符号表的链接，以及指向包层次变量符号表的链接。 ","date":"09-19","objectID":"/2022/intermediate_representation/:5:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#结构表"},{"categories":["CompilerPrinciple"],"content":" 符号表的用途编译器可能会建立许多不同的符号表来用于不同的目标。 结构表用于指定结构或记录中字段的文本串，存在与变量和过程不同的名称空间。对于结构中的每个字段，编译器必须记录其类型、大小、结构内偏移量等信息，它还需要确定结构的总长度。管理字段名的命名空间由几种方式： 独立表， 编译器为每个记录定义维护一个对应的符号表。概念上这种方式最干净纯粹。 选择符表，编译器可以为所有字段名维护一个独立的表。为避免不同结构中同名字段间的冲突，编译器必须使用修饰名，即为同一个结构中的字段添加同样的但全局唯一的前缀。该前缀可以是结构名，也可以是结构名在符号表中的索引值。这种方法编译器必须想办法将同一个结构的字段关联起来。 统一表，编译器通过修饰名将字段同样存储在主符号表中，这样可以减少表的数目，但也意味着增加主符号表中的表项与字段。 独立表的好处在于，任何与作用域相关的问题，都可以自然而然地匹配到主符号表的作用域管理框架中，结构本身可见时，其内部的符号表可以通过结构在主符号表中的表项访问。 使用链接表解决 OO 范式的名字解析问题OO 范式中的名字的作用域规则同时取决于数据的结构与代码的结构，这也导致出现了一组更复杂的符号表。比如 Java 中对于正在被编译的代码、代码中已知和引用的任何外部类、包含代码的类之上的继承层次，都分别需要相应的符号表。 相对简单的实现方式是，对每个类附加一个符号表，其中涉及两个嵌套的层次结构：用于类中各个方法内部的词法作用域，和追踪类的继承层次结构。 编译器对每个方法都需要一个词法作用域化的符号表，对每个类都需要一个包含指向继承层次中父类的链接符号表。当然还需要指向其他类的符号表的链接，以及指向包层次变量符号表的链接。 ","date":"09-19","objectID":"/2022/intermediate_representation/:5:2","series":null,"tags":["Note","IR"],"title":"中间表示","uri":"/2022/intermediate_representation/#使用链接表解决-oo-范式的名字解析问题"},{"categories":["Programming"],"content":"GinShio | CS61A Study Notes - Getting started","date":"09-18","objectID":"/2022/cs61a_01_getting_started/","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/"},{"categories":["Programming"],"content":"虽然 CS61A 使用 Python 进行教学，但我希望好好学一下 Erlang 和 Scheme。如果想查看更多关于 CS61A 的信息，请访问 课程主页，当然我也会将一部分内容和实现放在自己的 repo 中。 scheme 有很多不同的实现，而大多实现不兼容。因此我使用的是 MIT/GNU Scheme。 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:0:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#"},{"categories":["Programming"],"content":" Lab00: Getting Startted (入门)首先搭建一个环境，CS61A 中指 Python3 环境。 Setup: 以下是本课程所用到的基础软件，也是重要的组件。 终端 (Terminal)：安装一个终端可以让你运行本课程的 OK 命令 编程环境 (Environment)：编程环境是必须的，当然课程需要的是 Python3.7，这样你才可以运行 OK 命令 文本编辑器 (Text Editor)：VSCode、Atom 什么都行，只要能用来写代码 练习使用终端，并组织你的项目文件 学习 Python 基础 做一个练习 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#lab00-getting-startted--入门"},{"categories":["Programming"],"content":" 安装 安装终端在 MacOS 和 Linux 中本身就自带了终端软件 (Terminal)，如果是 KDE，终端软件被称作 Konsole。而 Windows 中，直接在 Store 中下载 Windows Terminal 即可。当然 Windows 下推荐使用 WSL，但是系统不是关注的重点。 安装语言环境最低要求 Python3.7，因为这是运行 OK 命令的必要条件。当然你也可以使用别的编程语言。 Windows 下，你可以在这里下载 Python3 或 erlang。安装之后将路径添加到 PATH 系统环境变量中。如果你用 WSL 那和 Linux 下没什么区别。 对于 Linux 用户，就很方便了。使用包管理器进行安装，apt install erlang python3 (Debian / Ubuntu)，或者 zypper in erlang python3 (openSUSE)，有或者 pacman -S erlang python3 (Arch)，当然其他发行版也可以用相关的命令进行安装。 安装文本编辑器说实话，文本编辑器的选择很自由。当然 CS61A 还是推荐了一些流行的文本编辑器 Visual Studio Code (VSCode)：一个全功能桌面编辑器，有很多支持不同语言的扩展可用 Atom：轻量级桌面编辑器。不过我不建议，MS 接手 GitHub 后，Atom 就死了 Vim、Emacs：命令行编辑器 PyCharm： JetBrains 推出的 Python IDE Sublime Text CS61A 贴心的提供了在线编辑器 https://code.cs61a.org/ ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#安装"},{"categories":["Programming"],"content":" 安装 安装终端在 MacOS 和 Linux 中本身就自带了终端软件 (Terminal)，如果是 KDE，终端软件被称作 Konsole。而 Windows 中，直接在 Store 中下载 Windows Terminal 即可。当然 Windows 下推荐使用 WSL，但是系统不是关注的重点。 安装语言环境最低要求 Python3.7，因为这是运行 OK 命令的必要条件。当然你也可以使用别的编程语言。 Windows 下，你可以在这里下载 Python3 或 erlang。安装之后将路径添加到 PATH 系统环境变量中。如果你用 WSL 那和 Linux 下没什么区别。 对于 Linux 用户，就很方便了。使用包管理器进行安装，apt install erlang python3 (Debian / Ubuntu)，或者 zypper in erlang python3 (openSUSE)，有或者 pacman -S erlang python3 (Arch)，当然其他发行版也可以用相关的命令进行安装。 安装文本编辑器说实话，文本编辑器的选择很自由。当然 CS61A 还是推荐了一些流行的文本编辑器 Visual Studio Code (VSCode)：一个全功能桌面编辑器，有很多支持不同语言的扩展可用 Atom：轻量级桌面编辑器。不过我不建议，MS 接手 GitHub 后，Atom 就死了 Vim、Emacs：命令行编辑器 PyCharm： JetBrains 推出的 Python IDE Sublime Text CS61A 贴心的提供了在线编辑器 https://code.cs61a.org/ ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#安装终端"},{"categories":["Programming"],"content":" 安装 安装终端在 MacOS 和 Linux 中本身就自带了终端软件 (Terminal)，如果是 KDE，终端软件被称作 Konsole。而 Windows 中，直接在 Store 中下载 Windows Terminal 即可。当然 Windows 下推荐使用 WSL，但是系统不是关注的重点。 安装语言环境最低要求 Python3.7，因为这是运行 OK 命令的必要条件。当然你也可以使用别的编程语言。 Windows 下，你可以在这里下载 Python3 或 erlang。安装之后将路径添加到 PATH 系统环境变量中。如果你用 WSL 那和 Linux 下没什么区别。 对于 Linux 用户，就很方便了。使用包管理器进行安装，apt install erlang python3 (Debian / Ubuntu)，或者 zypper in erlang python3 (openSUSE)，有或者 pacman -S erlang python3 (Arch)，当然其他发行版也可以用相关的命令进行安装。 安装文本编辑器说实话，文本编辑器的选择很自由。当然 CS61A 还是推荐了一些流行的文本编辑器 Visual Studio Code (VSCode)：一个全功能桌面编辑器，有很多支持不同语言的扩展可用 Atom：轻量级桌面编辑器。不过我不建议，MS 接手 GitHub 后，Atom 就死了 Vim、Emacs：命令行编辑器 PyCharm： JetBrains 推出的 Python IDE Sublime Text CS61A 贴心的提供了在线编辑器 https://code.cs61a.org/ ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#安装语言环境"},{"categories":["Programming"],"content":" 安装 安装终端在 MacOS 和 Linux 中本身就自带了终端软件 (Terminal)，如果是 KDE，终端软件被称作 Konsole。而 Windows 中，直接在 Store 中下载 Windows Terminal 即可。当然 Windows 下推荐使用 WSL，但是系统不是关注的重点。 安装语言环境最低要求 Python3.7，因为这是运行 OK 命令的必要条件。当然你也可以使用别的编程语言。 Windows 下，你可以在这里下载 Python3 或 erlang。安装之后将路径添加到 PATH 系统环境变量中。如果你用 WSL 那和 Linux 下没什么区别。 对于 Linux 用户，就很方便了。使用包管理器进行安装，apt install erlang python3 (Debian / Ubuntu)，或者 zypper in erlang python3 (openSUSE)，有或者 pacman -S erlang python3 (Arch)，当然其他发行版也可以用相关的命令进行安装。 安装文本编辑器说实话，文本编辑器的选择很自由。当然 CS61A 还是推荐了一些流行的文本编辑器 Visual Studio Code (VSCode)：一个全功能桌面编辑器，有很多支持不同语言的扩展可用 Atom：轻量级桌面编辑器。不过我不建议，MS 接手 GitHub 后，Atom 就死了 Vim、Emacs：命令行编辑器 PyCharm： JetBrains 推出的 Python IDE Sublime Text CS61A 贴心的提供了在线编辑器 https://code.cs61a.org/ ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#安装文本编辑器"},{"categories":["Programming"],"content":" 编程语言基础 算术运算符在大部分编程语言中，算术运算所包含的东西都差不多。比如 + (加)、- (减)、* (乘)、/ (除) 等，erlang 和 python 都不例外 3 + 4. % 7 3 - 4. % -1 3 * 4. % 12 3 / 4. % 0.75 3 + 4 + 5 + 6. % 18 3 + 4 # 7 3 - 4 # -1 3 * 4 # 12 3 / 4 # 0.75 3 + 4 + 5 + 6 # 18 scheme 是 Lisp 的方言，因此和 Lisp 一样，使用前缀运算，即表达式的第一个为方法调用 (+ 3 4) ; 7 (- 3 4) ; -1 (* 3 4) ; 12 (/ 3 4) ; 7.5e-1 (+ 3 4 5 6) ; 18 (+) ; 0 (*) ; 1 (/ 3) ; 1/3 当然还有一些额外的运算，比如 python 不止提供了 // (整数除法) 和 % (取模)，还提供了 ** (幂运算)；erlang 提供了 div (整数除法) 和 rem (余数)；scheme 则是有 quotient (整数除法)、remainder (余数运算) 和 modulo (模运算)。很遗憾的就是 erlang 中并没有取模运算。 3 div 4. % 0 5 div 2. % 2 3 rem 4. % 3 -3 rem 4. % -3 3 // 4 # 0 5 // 2 # 2 3 % 4 # 3 -3 % 4 # 1 3 ** 4 # 81 (quotient 3 4) ; 0 (quotient 5 2) ; 2 (remainder 3 4) ; 3 (remainder -3 4) ; -3 (modulo 3 4) ; 3 (modulo -3 4) ; 1 字符串在 Python 中字符串还是比较友好的，默认以 UTF-8 编码 \"this is a string.\" 'this also is a string' \"\"\" STRING! \"\"\" \"string\"[3] # 'i' len(\"string\") # 6 len(\"中文\") # 2 scheme 中字符串也是很正常的样子，不过并不是 UTF-8 编码的字符串，有点类似 C 语言的字符数组。关于 scheme 的 string 更多详情请查阅文档。 (string? \"str\") ;; #t (make-string 5 #\\x30) ;; \"00000\" (string-length \"str\") ;; 3 (string-length \"中文\") ;; 6 与 Python 一样，erlang 的字符串也是 UTF-8 编码的字符数组。 length(\"teste\"). % 5 length(\"中文\"). % 2, [20013,25991] 不过 erlang 中还有一种常用的字符串，即字节序列形式的字符串，这就与 C 语言中的字符串有点像了。 \u003c\u003c\"中文\"/utf8\u003e\u003e. % valid string \u003c\u003c\"中文\"\u003e\u003e. % not valid string is_bitstring(\"test\"). % false % byte_size(\"test\"). % exception error: bad argument % bit_size(\"test\"). % exception error: bad argument is_bitstring(\u003c\u003c\"test\"\u003e\u003e). % true byte_size(\u003c\u003c\"test\"\u003e\u003e). % 4 bit_size(\u003c\u003c\"test\"\u003e\u003e). % 32 赋值运算如果学过 C 语言或者 Python，一定对赋值不陌生，即将一个值与一个名字进行绑定。 a = (100 + 50) // 2 a # 75 a = \"str\" a # \"str\" 当然 scheme 中可以使用 let 定义一个有作用域的变量，用 define 则会定义一个全局的变量。而赋值语句则是 set!，与 Python 一样，scheme 也是一门动态强类型语言。 (let ((x 3) (y 4)) (* x y)) ;; 12 ;; (* x y) ;ERROR: unbound variable: y (define x 3) (+ x) ;; 3 (set! x \"str\") ; \"str\" (let ((x 0) (y 1)) (let* ((x y) (y x)) (list x y)) ;; (1 1) (let ((x y) (y x)) (list x y))) ;; (1 0) 不过对于 erlang 情况就有点特殊了。在 erlang 中 = 表示将值与名字进行匹配，如果是没有定义的名字，也会进行绑定。这是由于 erlang 是纯 (pure) 函数式语言，等号的语义和 Python / C 中是不一样的。 X = 10. % 10 % X = 5. % exception error: no match of right hand side value 5 % 6 = X. % exception error: no match of right hand side value 10 % 6 = Y. % variable 'Y' is unbound X = 10. % 10 10 = X. % 10 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#编程语言基础"},{"categories":["Programming"],"content":" 编程语言基础 算术运算符在大部分编程语言中，算术运算所包含的东西都差不多。比如 + (加)、- (减)、* (乘)、/ (除) 等，erlang 和 python 都不例外 3 + 4. % 7 3 - 4. % -1 3 * 4. % 12 3 / 4. % 0.75 3 + 4 + 5 + 6. % 18 3 + 4 # 7 3 - 4 # -1 3 * 4 # 12 3 / 4 # 0.75 3 + 4 + 5 + 6 # 18 scheme 是 Lisp 的方言，因此和 Lisp 一样，使用前缀运算，即表达式的第一个为方法调用 (+ 3 4) ; 7 (- 3 4) ; -1 (* 3 4) ; 12 (/ 3 4) ; 7.5e-1 (+ 3 4 5 6) ; 18 (+) ; 0 (*) ; 1 (/ 3) ; 1/3 当然还有一些额外的运算，比如 python 不止提供了 // (整数除法) 和 % (取模)，还提供了 ** (幂运算)；erlang 提供了 div (整数除法) 和 rem (余数)；scheme 则是有 quotient (整数除法)、remainder (余数运算) 和 modulo (模运算)。很遗憾的就是 erlang 中并没有取模运算。 3 div 4. % 0 5 div 2. % 2 3 rem 4. % 3 -3 rem 4. % -3 3 // 4 # 0 5 // 2 # 2 3 % 4 # 3 -3 % 4 # 1 3 ** 4 # 81 (quotient 3 4) ; 0 (quotient 5 2) ; 2 (remainder 3 4) ; 3 (remainder -3 4) ; -3 (modulo 3 4) ; 3 (modulo -3 4) ; 1 字符串在 Python 中字符串还是比较友好的，默认以 UTF-8 编码 \"this is a string.\" 'this also is a string' \"\"\" STRING! \"\"\" \"string\"[3] # 'i' len(\"string\") # 6 len(\"中文\") # 2 scheme 中字符串也是很正常的样子，不过并不是 UTF-8 编码的字符串，有点类似 C 语言的字符数组。关于 scheme 的 string 更多详情请查阅文档。 (string? \"str\") ;; #t (make-string 5 #\\x30) ;; \"00000\" (string-length \"str\") ;; 3 (string-length \"中文\") ;; 6 与 Python 一样，erlang 的字符串也是 UTF-8 编码的字符数组。 length(\"teste\"). % 5 length(\"中文\"). % 2, [20013,25991] 不过 erlang 中还有一种常用的字符串，即字节序列形式的字符串，这就与 C 语言中的字符串有点像了。 \u003c\u003c\"中文\"/utf8\u003e\u003e. % valid string \u003c\u003c\"中文\"\u003e\u003e. % not valid string is_bitstring(\"test\"). % false % byte_size(\"test\"). % exception error: bad argument % bit_size(\"test\"). % exception error: bad argument is_bitstring(\u003c\u003c\"test\"\u003e\u003e). % true byte_size(\u003c\u003c\"test\"\u003e\u003e). % 4 bit_size(\u003c\u003c\"test\"\u003e\u003e). % 32 赋值运算如果学过 C 语言或者 Python，一定对赋值不陌生，即将一个值与一个名字进行绑定。 a = (100 + 50) // 2 a # 75 a = \"str\" a # \"str\" 当然 scheme 中可以使用 let 定义一个有作用域的变量，用 define 则会定义一个全局的变量。而赋值语句则是 set!，与 Python 一样，scheme 也是一门动态强类型语言。 (let ((x 3) (y 4)) (* x y)) ;; 12 ;; (* x y) ;ERROR: unbound variable: y (define x 3) (+ x) ;; 3 (set! x \"str\") ; \"str\" (let ((x 0) (y 1)) (let* ((x y) (y x)) (list x y)) ;; (1 1) (let ((x y) (y x)) (list x y))) ;; (1 0) 不过对于 erlang 情况就有点特殊了。在 erlang 中 = 表示将值与名字进行匹配，如果是没有定义的名字，也会进行绑定。这是由于 erlang 是纯 (pure) 函数式语言，等号的语义和 Python / C 中是不一样的。 X = 10. % 10 % X = 5. % exception error: no match of right hand side value 5 % 6 = X. % exception error: no match of right hand side value 10 % 6 = Y. % variable 'Y' is unbound X = 10. % 10 10 = X. % 10 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#算术运算符"},{"categories":["Programming"],"content":" 编程语言基础 算术运算符在大部分编程语言中，算术运算所包含的东西都差不多。比如 + (加)、- (减)、* (乘)、/ (除) 等，erlang 和 python 都不例外 3 + 4. % 7 3 - 4. % -1 3 * 4. % 12 3 / 4. % 0.75 3 + 4 + 5 + 6. % 18 3 + 4 # 7 3 - 4 # -1 3 * 4 # 12 3 / 4 # 0.75 3 + 4 + 5 + 6 # 18 scheme 是 Lisp 的方言，因此和 Lisp 一样，使用前缀运算，即表达式的第一个为方法调用 (+ 3 4) ; 7 (- 3 4) ; -1 (* 3 4) ; 12 (/ 3 4) ; 7.5e-1 (+ 3 4 5 6) ; 18 (+) ; 0 (*) ; 1 (/ 3) ; 1/3 当然还有一些额外的运算，比如 python 不止提供了 // (整数除法) 和 % (取模)，还提供了 ** (幂运算)；erlang 提供了 div (整数除法) 和 rem (余数)；scheme 则是有 quotient (整数除法)、remainder (余数运算) 和 modulo (模运算)。很遗憾的就是 erlang 中并没有取模运算。 3 div 4. % 0 5 div 2. % 2 3 rem 4. % 3 -3 rem 4. % -3 3 // 4 # 0 5 // 2 # 2 3 % 4 # 3 -3 % 4 # 1 3 ** 4 # 81 (quotient 3 4) ; 0 (quotient 5 2) ; 2 (remainder 3 4) ; 3 (remainder -3 4) ; -3 (modulo 3 4) ; 3 (modulo -3 4) ; 1 字符串在 Python 中字符串还是比较友好的，默认以 UTF-8 编码 \"this is a string.\" 'this also is a string' \"\"\" STRING! \"\"\" \"string\"[3] # 'i' len(\"string\") # 6 len(\"中文\") # 2 scheme 中字符串也是很正常的样子，不过并不是 UTF-8 编码的字符串，有点类似 C 语言的字符数组。关于 scheme 的 string 更多详情请查阅文档。 (string? \"str\") ;; #t (make-string 5 #\\x30) ;; \"00000\" (string-length \"str\") ;; 3 (string-length \"中文\") ;; 6 与 Python 一样，erlang 的字符串也是 UTF-8 编码的字符数组。 length(\"teste\"). % 5 length(\"中文\"). % 2, [20013,25991] 不过 erlang 中还有一种常用的字符串，即字节序列形式的字符串，这就与 C 语言中的字符串有点像了。 \u003c\u003c\"中文\"/utf8\u003e\u003e. % valid string \u003c\u003c\"中文\"\u003e\u003e. % not valid string is_bitstring(\"test\"). % false % byte_size(\"test\"). % exception error: bad argument % bit_size(\"test\"). % exception error: bad argument is_bitstring(\u003c\u003c\"test\"\u003e\u003e). % true byte_size(\u003c\u003c\"test\"\u003e\u003e). % 4 bit_size(\u003c\u003c\"test\"\u003e\u003e). % 32 赋值运算如果学过 C 语言或者 Python，一定对赋值不陌生，即将一个值与一个名字进行绑定。 a = (100 + 50) // 2 a # 75 a = \"str\" a # \"str\" 当然 scheme 中可以使用 let 定义一个有作用域的变量，用 define 则会定义一个全局的变量。而赋值语句则是 set!，与 Python 一样，scheme 也是一门动态强类型语言。 (let ((x 3) (y 4)) (* x y)) ;; 12 ;; (* x y) ;ERROR: unbound variable: y (define x 3) (+ x) ;; 3 (set! x \"str\") ; \"str\" (let ((x 0) (y 1)) (let* ((x y) (y x)) (list x y)) ;; (1 1) (let ((x y) (y x)) (list x y))) ;; (1 0) 不过对于 erlang 情况就有点特殊了。在 erlang 中 = 表示将值与名字进行匹配，如果是没有定义的名字，也会进行绑定。这是由于 erlang 是纯 (pure) 函数式语言，等号的语义和 Python / C 中是不一样的。 X = 10. % 10 % X = 5. % exception error: no match of right hand side value 5 % 6 = X. % exception error: no match of right hand side value 10 % 6 = Y. % variable 'Y' is unbound X = 10. % 10 10 = X. % 10 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#字符串"},{"categories":["Programming"],"content":" 编程语言基础 算术运算符在大部分编程语言中，算术运算所包含的东西都差不多。比如 + (加)、- (减)、* (乘)、/ (除) 等，erlang 和 python 都不例外 3 + 4. % 7 3 - 4. % -1 3 * 4. % 12 3 / 4. % 0.75 3 + 4 + 5 + 6. % 18 3 + 4 # 7 3 - 4 # -1 3 * 4 # 12 3 / 4 # 0.75 3 + 4 + 5 + 6 # 18 scheme 是 Lisp 的方言，因此和 Lisp 一样，使用前缀运算，即表达式的第一个为方法调用 (+ 3 4) ; 7 (- 3 4) ; -1 (* 3 4) ; 12 (/ 3 4) ; 7.5e-1 (+ 3 4 5 6) ; 18 (+) ; 0 (*) ; 1 (/ 3) ; 1/3 当然还有一些额外的运算，比如 python 不止提供了 // (整数除法) 和 % (取模)，还提供了 ** (幂运算)；erlang 提供了 div (整数除法) 和 rem (余数)；scheme 则是有 quotient (整数除法)、remainder (余数运算) 和 modulo (模运算)。很遗憾的就是 erlang 中并没有取模运算。 3 div 4. % 0 5 div 2. % 2 3 rem 4. % 3 -3 rem 4. % -3 3 // 4 # 0 5 // 2 # 2 3 % 4 # 3 -3 % 4 # 1 3 ** 4 # 81 (quotient 3 4) ; 0 (quotient 5 2) ; 2 (remainder 3 4) ; 3 (remainder -3 4) ; -3 (modulo 3 4) ; 3 (modulo -3 4) ; 1 字符串在 Python 中字符串还是比较友好的，默认以 UTF-8 编码 \"this is a string.\" 'this also is a string' \"\"\" STRING! \"\"\" \"string\"[3] # 'i' len(\"string\") # 6 len(\"中文\") # 2 scheme 中字符串也是很正常的样子，不过并不是 UTF-8 编码的字符串，有点类似 C 语言的字符数组。关于 scheme 的 string 更多详情请查阅文档。 (string? \"str\") ;; #t (make-string 5 #\\x30) ;; \"00000\" (string-length \"str\") ;; 3 (string-length \"中文\") ;; 6 与 Python 一样，erlang 的字符串也是 UTF-8 编码的字符数组。 length(\"teste\"). % 5 length(\"中文\"). % 2, [20013,25991] 不过 erlang 中还有一种常用的字符串，即字节序列形式的字符串，这就与 C 语言中的字符串有点像了。 \u003c\u003c\"中文\"/utf8\u003e\u003e. % valid string \u003c\u003c\"中文\"\u003e\u003e. % not valid string is_bitstring(\"test\"). % false % byte_size(\"test\"). % exception error: bad argument % bit_size(\"test\"). % exception error: bad argument is_bitstring(\u003c\u003c\"test\"\u003e\u003e). % true byte_size(\u003c\u003c\"test\"\u003e\u003e). % 4 bit_size(\u003c\u003c\"test\"\u003e\u003e). % 32 赋值运算如果学过 C 语言或者 Python，一定对赋值不陌生，即将一个值与一个名字进行绑定。 a = (100 + 50) // 2 a # 75 a = \"str\" a # \"str\" 当然 scheme 中可以使用 let 定义一个有作用域的变量，用 define 则会定义一个全局的变量。而赋值语句则是 set!，与 Python 一样，scheme 也是一门动态强类型语言。 (let ((x 3) (y 4)) (* x y)) ;; 12 ;; (* x y) ;ERROR: unbound variable: y (define x 3) (+ x) ;; 3 (set! x \"str\") ; \"str\" (let ((x 0) (y 1)) (let* ((x y) (y x)) (list x y)) ;; (1 1) (let ((x y) (y x)) (list x y))) ;; (1 0) 不过对于 erlang 情况就有点特殊了。在 erlang 中 = 表示将值与名字进行匹配，如果是没有定义的名字，也会进行绑定。这是由于 erlang 是纯 (pure) 函数式语言，等号的语义和 Python / C 中是不一样的。 X = 10. % 10 % X = 5. % exception error: no match of right hand side value 5 % 6 = X. % exception error: no match of right hand side value 10 % 6 = Y. % variable 'Y' is unbound X = 10. % 10 10 = X. % 10 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:1:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#赋值运算"},{"categories":["Programming"],"content":" Lec02: Functions (函数)","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:2:0","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#lec02-functions--函数"},{"categories":["Programming"],"content":" 表达式表达式表述了一个计算或过程，并会产生一个值。比如 \\(2^{100}\\)、\\(\\sin \\pi\\) 等。在表达式中存在运算符 (operator) 和运算数 (operand) (+ 1 2 3 4 5) ;; 15 当然名字也可以作为一个表达式使用 (let ((x 3)) x) ;; 3 但是表达式有一定的执行顺序，简单说先执行子表达式，将子表达式的值代入表达式中继续执行。直到计算出整个表达式的值。 ;; 3 + 5 * 3 - 6 / 3 * 4 - 7 (- (+ 3 (* 5 3)) (* (/ 6 3) 4) 7) 有时候这种嵌套的表达式表示实在让人不爽，反正先计算子表达式，再用子表达式的值计算随后的表达式。因此有些编程语言中引入了管道运算符，将嵌套表达式简单化。 # 3 + 5 * 3 - 6 / 3 * 4 - 7 5 |\u003e Kernel.*(3) |\u003e Kernel.+(3) |\u003e Kernel.-(6 |\u003e Kernel./(3) |\u003e Kernel.*(4)) |\u003e Kernel.-(7) # 3 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:2:1","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#表达式"},{"categories":["Programming"],"content":" 函数赋值可以理解为一个名称绑定了一个值，而很多语言中将函数也作为一个值。因此函数也可以被绑定在名称之上。 (let ((add +)) (add 1 2)) ;; 3 在编程中，有时会说 纯函数 (pure function)，也就是没有 副作用 (side effect) 的函数。即在面对相同的输入时，函数无论如何调用，其输出是相同的。也就是说，纯函数并不会依赖外部状态，也不会对外部状态进行改变。 (expt 10 3) ;; pure function (print \"Hello world\") ;; non-pure function 定义一个自己的函数，当然也可以定义一个匿名函数 def name(param): # function body return param name_lambda = lambda param: param (define (name param) ;; function body param) (define name-lambda (lambda (param) param)) name(Param) -\u003e Param. NameLambda = fun (Param) -\u003e Param end. 函数既然作为一等公民，自然而然的可以开始在函数的返回值或参数中出现，这样返回函数或参数是函数的函数被称为高等函数。 lists:foldl(fun(X, Sum) -\u003e X + Sum end, 0, [1,2,3,4,5]). % 15 当函数调用时，会进行以下操作： 添加函数栈帧，初始化新的环境 在栈帧中绑定实际参数 (arguments) 和形式参数 (parameters) 在新环境中执行函数体 在 Python 中函数如果不返回，则将默认直接返回 None，而很多函数式编程语言中默认都会返回块的最后一个表达式 def square_not_return(x): x * x # return None (define (square x) (* x x)) ;; return x * x fun (X) -\u003e X * X end. % return x * x 当定义函数时，就会将一个名字与该函数绑定在当前的环境中。调用相当于在当前环境中查找并调用该函数。 在新的环境中，各个函数是从全局作用域中派生出来的，因此新环境中除了访问自己环境内的变量，还可以访问父作用域的变量。 (define x 10) ;; global x: 10 (define y 20) ;; global y: 20 (define (multi y) (* x y)) ;; global x * local y (multi 5) ;; 50 (multi y) ;; 200 ","date":"09-18","objectID":"/2022/cs61a_01_getting_started/:2:2","series":["CS61A Note"],"tags":["Note","CS61A","SICP"],"title":"CS61A 入门","uri":"/2022/cs61a_01_getting_started/#函数"},{"categories":["Programming"],"content":"GinShio | memory_alignment: CPU alignment and GPU alignment","date":"09-03","objectID":"/2022/memory_alignment/","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/"},{"categories":["Programming"],"content":"众所周知，运行的程序是需要内存占用的，在编码时假定栈上的空间是连续的，且定义的所有变量都连续分布在栈上。 实际上，虽然变量是连续分布在栈上的，但编译器会根据不同类型与对齐方式，将变量重新排列，达到最优情况。 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define __print_position(type, CNT) \\ type VAR##CNT; \\ printf(\"VAR\"#CNT \" (\" #type \")\\t: %p\\n\", \u0026VAR##CNT); #define _print_position(type, CNT) __print_position(type, CNT) #define print_position(type) _print_position(type, __COUNTER__) int main(void) { print_position(int); // VAR0 (int) : 0x7ffe84765470 print_position(double); // VAR1 (double): 0x7ffe84765478 print_position(char); // VAR2 (char) : 0x7ffe8476546f print_position(float); // VAR3 (float) : 0x7ffe84765474 print_position(div_t); // VAR4 (div_t) : 0x7ffe84765480 } 本文主要集中在结构体的对齐。 ","date":"09-03","objectID":"/2022/memory_alignment/:0:0","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#"},{"categories":["Programming"],"content":" 为什么需要内存对齐","date":"09-03","objectID":"/2022/memory_alignment/:1:0","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#为什么需要内存对齐"},{"categories":["Programming"],"content":" 性能现代处理器拥有多级缓存，而数据必须通过这些缓存；支持单字节读取会将内存吞吐量和执行执行单元吞吐量紧密绑定 (称为 cpu-bound，CPU 绑定)。这与 PIO 被 DMA 超越 在硬件驱动上有很多相似的原因。 CPU 总是读取一个字长的大小 (32bit 处理器为 4 bytes)，当访问未对齐的地址时 – 如果 CPU 支持的话，处理器会读入多个字。CPU 将跨字读取程序请求的地址，这将产生 2 倍于请求数据大小的内存读写。因此很容易出现读取 2 字节比读取 4 字节慢的情况。 如果一个两字节的数据在字内没有对齐，那处理器只需要读取一次，并执行一次偏移量计算，这通常只需要一个周期。 另外对齐可以更好的确定是否在同一个 cache line 上，某些类型的应用会针对 cache line 进行优化，从而取得更好的性能。 ","date":"09-03","objectID":"/2022/memory_alignment/:1:1","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#性能"},{"categories":["Programming"],"content":" 范围给定任意的地址空间，如果架构认为 2 个最低有效位 (LSB) 总是 0 (如 32bit 机器)，那么它可以访问四倍大小的内存 (2 bit 可以表示 4 个不同的状态)，或者相同大小的内存但有额外两个标志位。2 个最低有效位意味着 4 字节对齐，地址在增加时只会从第 2 位开始变动，最低两位始终是 00。 这可以影响处理器的物理结构，这意味着地址总线可以少两位，或者 CPU 少两个针脚，亦或者电路板上少走 2 根线。 ","date":"09-03","objectID":"/2022/memory_alignment/:1:2","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#范围"},{"categories":["Programming"],"content":" 原子性CPU 可以原子地操作对齐的字内存，这意味着没有指令可以中断这次操作。这对许多无锁数据结构和其他并发范式的正确操作有着至关重要的作用。 ","date":"09-03","objectID":"/2022/memory_alignment/:1:3","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#原子性"},{"categories":["Programming"],"content":" 结论处理器的内存系统比这里描述的复杂得多，这里有一个关于 x86 处理器如何实际寻址 的讨论，对这方面的理解会有些帮助 (许多处理器的工作方式差不多)。 坚持内存对齐还有很多好处，可以在这篇文章中阅读。计算机的主要用途是传输数据，现代内存架构和技术经过数十年的优化，以一种高度可靠的方式促进在更多、更快的执行单元间处理更多数据的输入、输出。 ","date":"09-03","objectID":"/2022/memory_alignment/:1:4","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#结论"},{"categories":["Programming"],"content":" 数据模型C 与其派生语言在很多时候，类型的大小是和平台相关的，因此用数据模型来定义不同平台下的数据大小。 Model short int long long long ptr RunTime LP32 16 16 32 64 32 MSVC (16bit) ILP32 16 32 32 64 32 MSVC (32bit), Linux (32bit) LLP64 16 32 32 64 64 MSVC, MinGW LP64 16 32 64 64 64 *nix, Cygwin, z/OS ILP64 16 64 64 64 64 Solaris SILP64 64 64 64 64 64 UNICOS 虽然数据模型定义很清楚，但在处理跨平台代码时，数据类型大小的处理是很头疼的。 好在 C / C++ 在 stdint.h 中还提供了更多种类的定长整型，长度主要是 8、16、32 和 64 bit，且提供了不同需求的定长整型 fast 和 least。 定长整型，e.g. uint8_t、int16_t。定长整型是编译器可选项，因此可能不存在这个指定的类型。定长整型指定的位长度不可多也不可少，即强制要求位长度匹配。 最接近的定长整型，e.g. int_least16_t、uint_least16_t。最接近的定长整型指可以比指定的位长度可以多但不可以少。比如使用 uint_least8_t，但平台不支持 uint16_t 但支持 uint32_t，因此该类型是 uint32_t。 最快的定长整型，e.g. int_fast32_t、uint_fast32_t。最快的定长整型指可以比指定的位多但不能少，且在满足指定位长的情况下使用执行最快的整型。比如使用 uint_fast8_t，平台支持 uint32_t 和 uint16_t，但最快的是 uint32_t，因此该类型使用前者。 最后再说一下，由于指针在不同平台上的大小是不一样的，因此在转换指针位整型时，为了跨平台性，可以选择标准库可选的 intptr_t 和 uintptr_t。 ","date":"09-03","objectID":"/2022/memory_alignment/:2:0","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#数据模型"},{"categories":["Programming"],"content":" C++ 的内存对齐 信息 本章节数据模型为 LP64 data model ","date":"09-03","objectID":"/2022/memory_alignment/:3:0","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#c-plus-plus-的内存对齐"},{"categories":["Programming"],"content":" 具名要求 平凡类首先，可平凡复制类型 满足以下所有条件 至少有一个未被弃置的复制构造函数、移动构造函数、复制赋值运算符或移动赋值运算符 每个复制构造函数都是平凡的或被弃置的 每个移动构造函数都是平凡的或被弃置的 每个复制赋值运算符都是平凡的或被弃置的 每个移动赋值运算符都是平凡的或被弃置的 有一个未被弃置的平凡析构函数 一个 平凡类，满足以下所有条件 是一个可平凡复制类型 有一个或多个默认构造函数，它们全部都是平凡的或被弃置的，而且其中至少有一个未被弃置 struct A {}; // is trivial struct B { B(B const\u0026) = delete; }; // is trivial struct C { C() {} }; // is non-trivial struct D { ~D() {} }; // is non-trivial struct E { ~E() = delete; }; // is non-trivial struct F { private: ~F() = default; } // is non-trivial struct G { virtual ~G() = default; } // is non-trivial struct H { H() = default; H(const H \u0026) = delete; H(H \u0026\u0026) noexcept = delete; H \u0026operator=(H const \u0026) = delete; H \u0026operator=(H \u0026\u0026) noexcept = delete; ~H() = default; }; // is non-trivial struct I { I() = default; I(int) {} }; // is trivial struct J { J() = default; J(const J \u0026) {} }; // is non-trivial struct K { int x; }; // is trivial struct L { int x{0}; }; // is non-trivial 如果你用 gcc 或 clang 编译，会发现编译器显示 E、F 和 H 是平凡类，按照标准，实际上应该不是平凡类，可以在 bugzilla 查看 gcc 和 clang 的 bug 报告。 另外，可平凡复制类可以用 ::memcpy 或 ::memmove 在两个不存在潜在重叠的对象之间互相拷贝。 struct A { int x; }; A a = { .x = 10 }; // C++20 A b = { .x = 20 }; ::memcpy(\u0026b, \u0026a, sizeof(A)); // b.x = 10 平凡类可以认为不持有资源，因此可以直接覆盖或丢弃对象，不会造成资源的泄漏。 template \u003ctypename T, size_t N\u003e void destroy_array_element( typename ::std::enable_if\u003c::std::is_trivial\u003cT\u003e::value\u003e::type (\u0026/* arr */)[N]) {} template \u003ctypename T, size_t N\u003e void destroy_array_element(T (\u0026arr)[N]) { for (size_t i = 0; i \u003c N; ++i) { arr[i].~T(); } } 标准布局类满足以下所有条件是标准布局类 所有非静态数据成员都是标准布局类类型或它们的引用 没有虚函数和虚基类 所有非静态数据成员都具有相同的可访问性 没有非标准布局的基类 该类和它的所有基类中的非静态数据成员和位域都在相同的类中首次声明 给定该类为 S，且作为基类时集合 M(S) 没有元素，其中 M(X) 对于类型 X 定义如下： 如果 X 是没有 (可能继承来的) 非静态数据成员的非联合体类类型，那么集合 M(X) 为空。 如果 X 是首个非静态数据成员 (可能是匿名联合体) 具有 X0 类型的非联合体类类型，那么集合 M(X) 包含 X0 和 M(X0) 中的元素。 如果 X 是联合体类型，集合 M(X) 是包含所有 \\(U_{i}\\) 的集合与每个 M(\\(U_{i}\\)) 集合的并集，其中每个 \\(U_{i}\\) 是 X 的第 i 个非静态数据成员的类型。 如果 X 是元素类型是 \\(X_{e}\\) 的数组类型，集合 M(X) 包含 \\(X_{e}\\) 和 M(\\(Xe\\)) 中的元素。 如果 X 不是类类型或数组类型，那么集合 M(X) 为空。 struct A { int a; }; // is standard layout struct B : public A { double b; }; // isn't standard layout struct C { A a; double b; }; // is standard layout struct D { int a; double b; }; // is standard layout struct E { public: int a; private: double b; }; // isn't standard layout struct F { public: int fun() { return 0; } private: double a; }; // is standard layout 而标准布局拥有一些特性 指向标准布局类类型的指针可以被 reinterpret_cast 成指向它的首个非静态非位域数据成员的指针，或指向它的任何基类子对象的指针，反之亦然。简单地说即不允许标准布局类型的首个数据成员前有填充 宏 offsetof 可以用于确定任何成员距标准布局类起始的偏移量 平凡类与标准布局类总结很明显 C 语言中的所有类型都是标准布局的，但是 C++ 引入了 POD (plain old data) 的概念来表示 C 中这些类型 (C++20 移除了这一概念)，即满足以下所有条件的类： 平凡类 标准布局类 所有非静态数据成员都是 POD 类类型 可以这样理解，平凡类规定了一个类型无关心任何资源，即最基础的构造、析构方式；标准布局类规定了一个类型如何布局每个字段的。只要是标准布局类就可以和 C 程序无痛操作，但这个类型可能不是平凡类型，因此将 POD 拆分为两个概念。 最好理解的就是 ::std::vector，它采用 RAII 的方式自己管理资源，有复杂的构造、析构函数，它不是一个平凡类，但它是一个标准布局类，因此完全其完全遵循内存对齐方式，也可以用 memcpy 将其内部的值拷贝下来。 // #include \u003cstdint.h\u003e // #include \u003cstdlib.h\u003e // #include \u003cstring.h\u003e // #include \u003ciostream\u003e // #include \u003cvector\u003e ::std::vector\u003cchar\u003e v{'a', 'b', 'c'}; uintptr_t *copy = reinterpret_cast\u003cuintptr_t *\u003e(::alloca(sizeof v)); ::memcpy(copy, \u0026v, sizeof v); for (size_t i = 0, e = sizeof(v) / sizeof(uintptr_t); i \u003c e; ++i) { ::std::cout \u003c\u003c copy[i] \u003c\u003c ::std::endl; } // maybe output: // 94066226852544 // 94066226852547 // 94066226852547 ","date":"09-03","objectID":"/2022/memory_alignment/:3:1","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#具名要求"},{"categories":["Programming"],"content":" 具名要求 平凡类首先，可平凡复制类型 满足以下所有条件 至少有一个未被弃置的复制构造函数、移动构造函数、复制赋值运算符或移动赋值运算符 每个复制构造函数都是平凡的或被弃置的 每个移动构造函数都是平凡的或被弃置的 每个复制赋值运算符都是平凡的或被弃置的 每个移动赋值运算符都是平凡的或被弃置的 有一个未被弃置的平凡析构函数 一个 平凡类，满足以下所有条件 是一个可平凡复制类型 有一个或多个默认构造函数，它们全部都是平凡的或被弃置的，而且其中至少有一个未被弃置 struct A {}; // is trivial struct B { B(B const\u0026) = delete; }; // is trivial struct C { C() {} }; // is non-trivial struct D { ~D() {} }; // is non-trivial struct E { ~E() = delete; }; // is non-trivial struct F { private: ~F() = default; } // is non-trivial struct G { virtual ~G() = default; } // is non-trivial struct H { H() = default; H(const H \u0026) = delete; H(H \u0026\u0026) noexcept = delete; H \u0026operator=(H const \u0026) = delete; H \u0026operator=(H \u0026\u0026) noexcept = delete; ~H() = default; }; // is non-trivial struct I { I() = default; I(int) {} }; // is trivial struct J { J() = default; J(const J \u0026) {} }; // is non-trivial struct K { int x; }; // is trivial struct L { int x{0}; }; // is non-trivial 如果你用 gcc 或 clang 编译，会发现编译器显示 E、F 和 H 是平凡类，按照标准，实际上应该不是平凡类，可以在 bugzilla 查看 gcc 和 clang 的 bug 报告。 另外，可平凡复制类可以用 ::memcpy 或 ::memmove 在两个不存在潜在重叠的对象之间互相拷贝。 struct A { int x; }; A a = { .x = 10 }; // C++20 A b = { .x = 20 }; ::memcpy(\u0026b, \u0026a, sizeof(A)); // b.x = 10 平凡类可以认为不持有资源，因此可以直接覆盖或丢弃对象，不会造成资源的泄漏。 template void destroy_array_element( typename ::std::enable_if\u003c::std::is_trivial::value\u003e::type (\u0026/* arr */)[N]) {} template void destroy_array_element(T (\u0026arr)[N]) { for (size_t i = 0; i \u003c N; ++i) { arr[i].~T(); } } 标准布局类满足以下所有条件是标准布局类 所有非静态数据成员都是标准布局类类型或它们的引用 没有虚函数和虚基类 所有非静态数据成员都具有相同的可访问性 没有非标准布局的基类 该类和它的所有基类中的非静态数据成员和位域都在相同的类中首次声明 给定该类为 S，且作为基类时集合 M(S) 没有元素，其中 M(X) 对于类型 X 定义如下： 如果 X 是没有 (可能继承来的) 非静态数据成员的非联合体类类型，那么集合 M(X) 为空。 如果 X 是首个非静态数据成员 (可能是匿名联合体) 具有 X0 类型的非联合体类类型，那么集合 M(X) 包含 X0 和 M(X0) 中的元素。 如果 X 是联合体类型，集合 M(X) 是包含所有 \\(U_{i}\\) 的集合与每个 M(\\(U_{i}\\)) 集合的并集，其中每个 \\(U_{i}\\) 是 X 的第 i 个非静态数据成员的类型。 如果 X 是元素类型是 \\(X_{e}\\) 的数组类型，集合 M(X) 包含 \\(X_{e}\\) 和 M(\\(Xe\\)) 中的元素。 如果 X 不是类类型或数组类型，那么集合 M(X) 为空。 struct A { int a; }; // is standard layout struct B : public A { double b; }; // isn't standard layout struct C { A a; double b; }; // is standard layout struct D { int a; double b; }; // is standard layout struct E { public: int a; private: double b; }; // isn't standard layout struct F { public: int fun() { return 0; } private: double a; }; // is standard layout 而标准布局拥有一些特性 指向标准布局类类型的指针可以被 reinterpret_cast 成指向它的首个非静态非位域数据成员的指针，或指向它的任何基类子对象的指针，反之亦然。简单地说即不允许标准布局类型的首个数据成员前有填充 宏 offsetof 可以用于确定任何成员距标准布局类起始的偏移量 平凡类与标准布局类总结很明显 C 语言中的所有类型都是标准布局的，但是 C++ 引入了 POD (plain old data) 的概念来表示 C 中这些类型 (C++20 移除了这一概念)，即满足以下所有条件的类： 平凡类 标准布局类 所有非静态数据成员都是 POD 类类型 可以这样理解，平凡类规定了一个类型无关心任何资源，即最基础的构造、析构方式；标准布局类规定了一个类型如何布局每个字段的。只要是标准布局类就可以和 C 程序无痛操作，但这个类型可能不是平凡类型，因此将 POD 拆分为两个概念。 最好理解的就是 ::std::vector，它采用 RAII 的方式自己管理资源，有复杂的构造、析构函数，它不是一个平凡类，但它是一个标准布局类，因此完全其完全遵循内存对齐方式，也可以用 memcpy 将其内部的值拷贝下来。 // #include // #include // #include // #include // #include ::std::vector v{'a', 'b', 'c'}; uintptr_t *copy = reinterpret_cast(::alloca(sizeof v)); ::memcpy(copy, \u0026v, sizeof v); for (size_t i = 0, e = sizeof(v) / sizeof(uintptr_t); i \u003c e; ++i) { ::std::cout \u003c\u003c copy[i] \u003c\u003c ::std::endl; } // maybe output: // 94066226852544 // 94066226852547 // 94066226852547 ","date":"09-03","objectID":"/2022/memory_alignment/:3:1","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#平凡类"},{"categories":["Programming"],"content":" 具名要求 平凡类首先，可平凡复制类型 满足以下所有条件 至少有一个未被弃置的复制构造函数、移动构造函数、复制赋值运算符或移动赋值运算符 每个复制构造函数都是平凡的或被弃置的 每个移动构造函数都是平凡的或被弃置的 每个复制赋值运算符都是平凡的或被弃置的 每个移动赋值运算符都是平凡的或被弃置的 有一个未被弃置的平凡析构函数 一个 平凡类，满足以下所有条件 是一个可平凡复制类型 有一个或多个默认构造函数，它们全部都是平凡的或被弃置的，而且其中至少有一个未被弃置 struct A {}; // is trivial struct B { B(B const\u0026) = delete; }; // is trivial struct C { C() {} }; // is non-trivial struct D { ~D() {} }; // is non-trivial struct E { ~E() = delete; }; // is non-trivial struct F { private: ~F() = default; } // is non-trivial struct G { virtual ~G() = default; } // is non-trivial struct H { H() = default; H(const H \u0026) = delete; H(H \u0026\u0026) noexcept = delete; H \u0026operator=(H const \u0026) = delete; H \u0026operator=(H \u0026\u0026) noexcept = delete; ~H() = default; }; // is non-trivial struct I { I() = default; I(int) {} }; // is trivial struct J { J() = default; J(const J \u0026) {} }; // is non-trivial struct K { int x; }; // is trivial struct L { int x{0}; }; // is non-trivial 如果你用 gcc 或 clang 编译，会发现编译器显示 E、F 和 H 是平凡类，按照标准，实际上应该不是平凡类，可以在 bugzilla 查看 gcc 和 clang 的 bug 报告。 另外，可平凡复制类可以用 ::memcpy 或 ::memmove 在两个不存在潜在重叠的对象之间互相拷贝。 struct A { int x; }; A a = { .x = 10 }; // C++20 A b = { .x = 20 }; ::memcpy(\u0026b, \u0026a, sizeof(A)); // b.x = 10 平凡类可以认为不持有资源，因此可以直接覆盖或丢弃对象，不会造成资源的泄漏。 template void destroy_array_element( typename ::std::enable_if\u003c::std::is_trivial::value\u003e::type (\u0026/* arr */)[N]) {} template void destroy_array_element(T (\u0026arr)[N]) { for (size_t i = 0; i \u003c N; ++i) { arr[i].~T(); } } 标准布局类满足以下所有条件是标准布局类 所有非静态数据成员都是标准布局类类型或它们的引用 没有虚函数和虚基类 所有非静态数据成员都具有相同的可访问性 没有非标准布局的基类 该类和它的所有基类中的非静态数据成员和位域都在相同的类中首次声明 给定该类为 S，且作为基类时集合 M(S) 没有元素，其中 M(X) 对于类型 X 定义如下： 如果 X 是没有 (可能继承来的) 非静态数据成员的非联合体类类型，那么集合 M(X) 为空。 如果 X 是首个非静态数据成员 (可能是匿名联合体) 具有 X0 类型的非联合体类类型，那么集合 M(X) 包含 X0 和 M(X0) 中的元素。 如果 X 是联合体类型，集合 M(X) 是包含所有 \\(U_{i}\\) 的集合与每个 M(\\(U_{i}\\)) 集合的并集，其中每个 \\(U_{i}\\) 是 X 的第 i 个非静态数据成员的类型。 如果 X 是元素类型是 \\(X_{e}\\) 的数组类型，集合 M(X) 包含 \\(X_{e}\\) 和 M(\\(Xe\\)) 中的元素。 如果 X 不是类类型或数组类型，那么集合 M(X) 为空。 struct A { int a; }; // is standard layout struct B : public A { double b; }; // isn't standard layout struct C { A a; double b; }; // is standard layout struct D { int a; double b; }; // is standard layout struct E { public: int a; private: double b; }; // isn't standard layout struct F { public: int fun() { return 0; } private: double a; }; // is standard layout 而标准布局拥有一些特性 指向标准布局类类型的指针可以被 reinterpret_cast 成指向它的首个非静态非位域数据成员的指针，或指向它的任何基类子对象的指针，反之亦然。简单地说即不允许标准布局类型的首个数据成员前有填充 宏 offsetof 可以用于确定任何成员距标准布局类起始的偏移量 平凡类与标准布局类总结很明显 C 语言中的所有类型都是标准布局的，但是 C++ 引入了 POD (plain old data) 的概念来表示 C 中这些类型 (C++20 移除了这一概念)，即满足以下所有条件的类： 平凡类 标准布局类 所有非静态数据成员都是 POD 类类型 可以这样理解，平凡类规定了一个类型无关心任何资源，即最基础的构造、析构方式；标准布局类规定了一个类型如何布局每个字段的。只要是标准布局类就可以和 C 程序无痛操作，但这个类型可能不是平凡类型，因此将 POD 拆分为两个概念。 最好理解的就是 ::std::vector，它采用 RAII 的方式自己管理资源，有复杂的构造、析构函数，它不是一个平凡类，但它是一个标准布局类，因此完全其完全遵循内存对齐方式，也可以用 memcpy 将其内部的值拷贝下来。 // #include // #include // #include // #include // #include ::std::vector v{'a', 'b', 'c'}; uintptr_t *copy = reinterpret_cast(::alloca(sizeof v)); ::memcpy(copy, \u0026v, sizeof v); for (size_t i = 0, e = sizeof(v) / sizeof(uintptr_t); i \u003c e; ++i) { ::std::cout \u003c\u003c copy[i] \u003c\u003c ::std::endl; } // maybe output: // 94066226852544 // 94066226852547 // 94066226852547 ","date":"09-03","objectID":"/2022/memory_alignment/:3:1","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#标准布局类"},{"categories":["Programming"],"content":" 具名要求 平凡类首先，可平凡复制类型 满足以下所有条件 至少有一个未被弃置的复制构造函数、移动构造函数、复制赋值运算符或移动赋值运算符 每个复制构造函数都是平凡的或被弃置的 每个移动构造函数都是平凡的或被弃置的 每个复制赋值运算符都是平凡的或被弃置的 每个移动赋值运算符都是平凡的或被弃置的 有一个未被弃置的平凡析构函数 一个 平凡类，满足以下所有条件 是一个可平凡复制类型 有一个或多个默认构造函数，它们全部都是平凡的或被弃置的，而且其中至少有一个未被弃置 struct A {}; // is trivial struct B { B(B const\u0026) = delete; }; // is trivial struct C { C() {} }; // is non-trivial struct D { ~D() {} }; // is non-trivial struct E { ~E() = delete; }; // is non-trivial struct F { private: ~F() = default; } // is non-trivial struct G { virtual ~G() = default; } // is non-trivial struct H { H() = default; H(const H \u0026) = delete; H(H \u0026\u0026) noexcept = delete; H \u0026operator=(H const \u0026) = delete; H \u0026operator=(H \u0026\u0026) noexcept = delete; ~H() = default; }; // is non-trivial struct I { I() = default; I(int) {} }; // is trivial struct J { J() = default; J(const J \u0026) {} }; // is non-trivial struct K { int x; }; // is trivial struct L { int x{0}; }; // is non-trivial 如果你用 gcc 或 clang 编译，会发现编译器显示 E、F 和 H 是平凡类，按照标准，实际上应该不是平凡类，可以在 bugzilla 查看 gcc 和 clang 的 bug 报告。 另外，可平凡复制类可以用 ::memcpy 或 ::memmove 在两个不存在潜在重叠的对象之间互相拷贝。 struct A { int x; }; A a = { .x = 10 }; // C++20 A b = { .x = 20 }; ::memcpy(\u0026b, \u0026a, sizeof(A)); // b.x = 10 平凡类可以认为不持有资源，因此可以直接覆盖或丢弃对象，不会造成资源的泄漏。 template void destroy_array_element( typename ::std::enable_if\u003c::std::is_trivial::value\u003e::type (\u0026/* arr */)[N]) {} template void destroy_array_element(T (\u0026arr)[N]) { for (size_t i = 0; i \u003c N; ++i) { arr[i].~T(); } } 标准布局类满足以下所有条件是标准布局类 所有非静态数据成员都是标准布局类类型或它们的引用 没有虚函数和虚基类 所有非静态数据成员都具有相同的可访问性 没有非标准布局的基类 该类和它的所有基类中的非静态数据成员和位域都在相同的类中首次声明 给定该类为 S，且作为基类时集合 M(S) 没有元素，其中 M(X) 对于类型 X 定义如下： 如果 X 是没有 (可能继承来的) 非静态数据成员的非联合体类类型，那么集合 M(X) 为空。 如果 X 是首个非静态数据成员 (可能是匿名联合体) 具有 X0 类型的非联合体类类型，那么集合 M(X) 包含 X0 和 M(X0) 中的元素。 如果 X 是联合体类型，集合 M(X) 是包含所有 \\(U_{i}\\) 的集合与每个 M(\\(U_{i}\\)) 集合的并集，其中每个 \\(U_{i}\\) 是 X 的第 i 个非静态数据成员的类型。 如果 X 是元素类型是 \\(X_{e}\\) 的数组类型，集合 M(X) 包含 \\(X_{e}\\) 和 M(\\(Xe\\)) 中的元素。 如果 X 不是类类型或数组类型，那么集合 M(X) 为空。 struct A { int a; }; // is standard layout struct B : public A { double b; }; // isn't standard layout struct C { A a; double b; }; // is standard layout struct D { int a; double b; }; // is standard layout struct E { public: int a; private: double b; }; // isn't standard layout struct F { public: int fun() { return 0; } private: double a; }; // is standard layout 而标准布局拥有一些特性 指向标准布局类类型的指针可以被 reinterpret_cast 成指向它的首个非静态非位域数据成员的指针，或指向它的任何基类子对象的指针，反之亦然。简单地说即不允许标准布局类型的首个数据成员前有填充 宏 offsetof 可以用于确定任何成员距标准布局类起始的偏移量 平凡类与标准布局类总结很明显 C 语言中的所有类型都是标准布局的，但是 C++ 引入了 POD (plain old data) 的概念来表示 C 中这些类型 (C++20 移除了这一概念)，即满足以下所有条件的类： 平凡类 标准布局类 所有非静态数据成员都是 POD 类类型 可以这样理解，平凡类规定了一个类型无关心任何资源，即最基础的构造、析构方式；标准布局类规定了一个类型如何布局每个字段的。只要是标准布局类就可以和 C 程序无痛操作，但这个类型可能不是平凡类型，因此将 POD 拆分为两个概念。 最好理解的就是 ::std::vector，它采用 RAII 的方式自己管理资源，有复杂的构造、析构函数，它不是一个平凡类，但它是一个标准布局类，因此完全其完全遵循内存对齐方式，也可以用 memcpy 将其内部的值拷贝下来。 // #include // #include // #include // #include // #include ::std::vector v{'a', 'b', 'c'}; uintptr_t *copy = reinterpret_cast(::alloca(sizeof v)); ::memcpy(copy, \u0026v, sizeof v); for (size_t i = 0, e = sizeof(v) / sizeof(uintptr_t); i \u003c e; ++i) { ::std::cout \u003c\u003c copy[i] \u003c\u003c ::std::endl; } // maybe output: // 94066226852544 // 94066226852547 // 94066226852547 ","date":"09-03","objectID":"/2022/memory_alignment/:3:1","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#平凡类与标准布局类总结"},{"categories":["Programming"],"content":" 标准布局类的内存对齐内存对齐有些规律可循： 对象的起始地址能够被其对齐大小整除 成员相对于起始地址的偏移量能够被自身的对齐大小整除，否则在前一个成员后面填充字节 类的大小能够被其对齐大小整除，否则在最后填充字节 如果是空类，按照标准该类的对象必须占有一个字节 (除非 空基类优化)，在C中空类的大小是 0 字节 默认条件下，类型的对齐大小与其所有字段的对齐大小最大值相同 普通的标准布局类对于任何标准布局类，都可以轻松用上面的规律判断出类型的大小 struct S {}; // sizeof = 1, alignof = 1 struct T : public S { char x; }; // sizeof = 1, alignof = 1 struct U { int x; // offsetof = 0 char y; // offsetof = 4 char z; // offsetof = 5 }; // sizeof = 8, alignof = 4 struct V { int a; // offsetof = 0 T b; // offsetof = 4 U c; // offsetof = 8 double d; // offsetof = 16 }; // sizeof = 24, alignof = 8 struct W { int val; // offset = 0 W *left; // offset = 8 W *right; // offset = 16 }; // sizeof = 24, alignof = 8 最后要说明一下数组，数组就像是你在这个位置引入了数组长度个该类型的变量。 struct S { int x[4]; }; // sizeof = 16, alignof = 4 struct T { int a; // offsetof = 0 char b[9]; // offsetof = 4 short c[2]; // offsetof = 14 double *d; // offsetof = 24 }; // sizeof = 32, alignof = 8 struct U { char x; // offsetof = 0 char y[1]; // offsetof = 1 short z; // offsetof = 2 }; // sizeof = 4, alignof = 2 你以为这就完了吗？当然不是，C 语言中有个很有意思的用法，即 C99 中出现的 柔性数组声明。将最后一个字段定义为数组，且长度为 0，此时数组底层数据类型将影响类型的对齐大小，但不会影响整个类型的大小。当然对于 C++ 标准并没有支持，全靠编译器自己去扩展。 struct S { int i; // offset = 0 double d[]; // offset = 8 }; // sizeof = 8, alignof = 8 struct T { int i; // offset = 0 char c[0]; // offset = 4 }; // sizeof = 4, alignof = 4 带有柔性数组成员的类，需要使用动态分配的方式，因为柔性数组成员无法被初始化。实际上编译器不能确定数组的长度，因此即使给定的额外的空间不足以存放底层类型数据，也由程序员保证访问的正确性，访问溢出的范围将是 UB。 S s1; // sizeof(s1) = 8, length(d) = 1, accessing d is a UB // S s2 = {1, {3.14}}; // error: initialization of flexible array member is not allowed S* s3 = reinterpret_cast\u003cS*\u003e(alloca(sizeof(S))); // equivalent to s1 // s4: sizeof(*s4) = 8, length(d) = 6 S *s4 = reinterpret_cast\u003cS *\u003e(alloca(sizeof(S) + 6 * sizeof(S::d[0]))); // s5: sizeof(*s5) = 8, length(d) = 1, accessing d[1] is a UB S *s5 = reinterpret_cast\u003cS *\u003e(alloca(sizeof(S) + 10)); *s4 = *s5; // copy size = sizeof(S) 带有位域的标准布局类对于带有位域的标准布局类，也很简单，位域不会跨底层数据存储，也就是说当剩余位不够时，下一个位域字段会存储在下一个底层数据中。而无名位域字段可以起到占位的作用。另外声明位域后，实际会用一个底层数据填充到类里，类的大小与对齐会收到该底层数据的影响。 struct S { // offsetof = 0 unsigned char b1 : 3, : 2; // offsetof = 1 unsigned char b2 : 6, b3 : 2; }; // sizeof = 2, alignof = 1 位域字段的大小可以指定为 0，意味着下一个位域将声明在下一个底层数据中。但实际 0 长度的位域字段并不会为类引入一个底层数据。 struct S { int : 0; }; // sizeof = 1, alignof = 1 struct T { uint64_t : 0; uint32_t x; // offsetof = 0 }; // sizeof = 4, alignof = 4 struct U { // offsetof = 0 unsigned char b1 : 3, : 0; // offsetof = 1 unsigned char b2 : 2; }; // sizeof = 2, alignof = 1 手动指定对齐大小的标准布局类回到本章开始的 5 条规律，实际上自己手动指定对齐时，也是适用的。 #pragma pack(N) 和 gnu::packed 指定排布字段时以打包方式进行，即每个字段都连续排布，字段与字段之间不会产生额外的内存空洞，这样可以减少不必要内存的浪费。 struct [[gnu::packed]] S { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 3, alignof = 1 struct [[gnu::packed]] T { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 2, alignof = 1 struct [[gnu::packed]] alignas(4) U { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 struct [[gnu::packed]] alignas(4) V { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 但是今天的重点是 C++11 引入的 alignas 声明符。实际上它不止可以指定结构体时如何对齐的，还可以指定一个对象是怎么对齐的。指定的对齐大小都必须是 2 的正整数幂，如果指定的对齐方式弱于默认的对齐方式，编译器可能会忽略或报错。 最简单的先从指定结构体的声明说起。 struct alignas(4) S {}; // sizeof = 4, alignof = 4 struct SS { S s; // offsetof = 0 S *t; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(SS) T { S s; // offsetof = 0 char t; // offsetof = 4 short u; // offsetof = 6 short v; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(1) U : public S {}; // error or ignore // struct alignas(5) V : public S {}; // error struct alignas(4) W : public S {}; 对于变量指定对齐大小，对齐大小并不意味着实际占用，下一个字段依然会根据自身对齐方式紧凑排列。 struct S { int16_t i; // offsetof = 0 char c1; // offsetof = 2 char a[11]; // offsetof = 3 char c2; // offsetof = 14 }; // sizeof = 16, offsetof = 2 struct T { alignas(4) int16_t i; // offsetof = 0 char c1; // offsetof = 2 alignas(8) char a[11]; // offsetof = 8 char c2; // offsetof = 19 }; // siz","date":"09-03","objectID":"/2022/memory_alignment/:3:2","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#标准布局类的内存对齐"},{"categories":["Programming"],"content":" 标准布局类的内存对齐内存对齐有些规律可循： 对象的起始地址能够被其对齐大小整除 成员相对于起始地址的偏移量能够被自身的对齐大小整除，否则在前一个成员后面填充字节 类的大小能够被其对齐大小整除，否则在最后填充字节 如果是空类，按照标准该类的对象必须占有一个字节 (除非 空基类优化)，在C中空类的大小是 0 字节 默认条件下，类型的对齐大小与其所有字段的对齐大小最大值相同 普通的标准布局类对于任何标准布局类，都可以轻松用上面的规律判断出类型的大小 struct S {}; // sizeof = 1, alignof = 1 struct T : public S { char x; }; // sizeof = 1, alignof = 1 struct U { int x; // offsetof = 0 char y; // offsetof = 4 char z; // offsetof = 5 }; // sizeof = 8, alignof = 4 struct V { int a; // offsetof = 0 T b; // offsetof = 4 U c; // offsetof = 8 double d; // offsetof = 16 }; // sizeof = 24, alignof = 8 struct W { int val; // offset = 0 W *left; // offset = 8 W *right; // offset = 16 }; // sizeof = 24, alignof = 8 最后要说明一下数组，数组就像是你在这个位置引入了数组长度个该类型的变量。 struct S { int x[4]; }; // sizeof = 16, alignof = 4 struct T { int a; // offsetof = 0 char b[9]; // offsetof = 4 short c[2]; // offsetof = 14 double *d; // offsetof = 24 }; // sizeof = 32, alignof = 8 struct U { char x; // offsetof = 0 char y[1]; // offsetof = 1 short z; // offsetof = 2 }; // sizeof = 4, alignof = 2 你以为这就完了吗？当然不是，C 语言中有个很有意思的用法，即 C99 中出现的 柔性数组声明。将最后一个字段定义为数组，且长度为 0，此时数组底层数据类型将影响类型的对齐大小，但不会影响整个类型的大小。当然对于 C++ 标准并没有支持，全靠编译器自己去扩展。 struct S { int i; // offset = 0 double d[]; // offset = 8 }; // sizeof = 8, alignof = 8 struct T { int i; // offset = 0 char c[0]; // offset = 4 }; // sizeof = 4, alignof = 4 带有柔性数组成员的类，需要使用动态分配的方式，因为柔性数组成员无法被初始化。实际上编译器不能确定数组的长度，因此即使给定的额外的空间不足以存放底层类型数据，也由程序员保证访问的正确性，访问溢出的范围将是 UB。 S s1; // sizeof(s1) = 8, length(d) = 1, accessing d is a UB // S s2 = {1, {3.14}}; // error: initialization of flexible array member is not allowed S* s3 = reinterpret_cast(alloca(sizeof(S))); // equivalent to s1 // s4: sizeof(*s4) = 8, length(d) = 6 S *s4 = reinterpret_cast(alloca(sizeof(S) + 6 * sizeof(S::d[0]))); // s5: sizeof(*s5) = 8, length(d) = 1, accessing d[1] is a UB S *s5 = reinterpret_cast(alloca(sizeof(S) + 10)); *s4 = *s5; // copy size = sizeof(S) 带有位域的标准布局类对于带有位域的标准布局类，也很简单，位域不会跨底层数据存储，也就是说当剩余位不够时，下一个位域字段会存储在下一个底层数据中。而无名位域字段可以起到占位的作用。另外声明位域后，实际会用一个底层数据填充到类里，类的大小与对齐会收到该底层数据的影响。 struct S { // offsetof = 0 unsigned char b1 : 3, : 2; // offsetof = 1 unsigned char b2 : 6, b3 : 2; }; // sizeof = 2, alignof = 1 位域字段的大小可以指定为 0，意味着下一个位域将声明在下一个底层数据中。但实际 0 长度的位域字段并不会为类引入一个底层数据。 struct S { int : 0; }; // sizeof = 1, alignof = 1 struct T { uint64_t : 0; uint32_t x; // offsetof = 0 }; // sizeof = 4, alignof = 4 struct U { // offsetof = 0 unsigned char b1 : 3, : 0; // offsetof = 1 unsigned char b2 : 2; }; // sizeof = 2, alignof = 1 手动指定对齐大小的标准布局类回到本章开始的 5 条规律，实际上自己手动指定对齐时，也是适用的。 #pragma pack(N) 和 gnu::packed 指定排布字段时以打包方式进行，即每个字段都连续排布，字段与字段之间不会产生额外的内存空洞，这样可以减少不必要内存的浪费。 struct [[gnu::packed]] S { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 3, alignof = 1 struct [[gnu::packed]] T { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 2, alignof = 1 struct [[gnu::packed]] alignas(4) U { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 struct [[gnu::packed]] alignas(4) V { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 但是今天的重点是 C++11 引入的 alignas 声明符。实际上它不止可以指定结构体时如何对齐的，还可以指定一个对象是怎么对齐的。指定的对齐大小都必须是 2 的正整数幂，如果指定的对齐方式弱于默认的对齐方式，编译器可能会忽略或报错。 最简单的先从指定结构体的声明说起。 struct alignas(4) S {}; // sizeof = 4, alignof = 4 struct SS { S s; // offsetof = 0 S *t; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(SS) T { S s; // offsetof = 0 char t; // offsetof = 4 short u; // offsetof = 6 short v; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(1) U : public S {}; // error or ignore // struct alignas(5) V : public S {}; // error struct alignas(4) W : public S {}; 对于变量指定对齐大小，对齐大小并不意味着实际占用，下一个字段依然会根据自身对齐方式紧凑排列。 struct S { int16_t i; // offsetof = 0 char c1; // offsetof = 2 char a[11]; // offsetof = 3 char c2; // offsetof = 14 }; // sizeof = 16, offsetof = 2 struct T { alignas(4) int16_t i; // offsetof = 0 char c1; // offsetof = 2 alignas(8) char a[11]; // offsetof = 8 char c2; // offsetof = 19 }; // siz","date":"09-03","objectID":"/2022/memory_alignment/:3:2","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#普通的标准布局类"},{"categories":["Programming"],"content":" 标准布局类的内存对齐内存对齐有些规律可循： 对象的起始地址能够被其对齐大小整除 成员相对于起始地址的偏移量能够被自身的对齐大小整除，否则在前一个成员后面填充字节 类的大小能够被其对齐大小整除，否则在最后填充字节 如果是空类，按照标准该类的对象必须占有一个字节 (除非 空基类优化)，在C中空类的大小是 0 字节 默认条件下，类型的对齐大小与其所有字段的对齐大小最大值相同 普通的标准布局类对于任何标准布局类，都可以轻松用上面的规律判断出类型的大小 struct S {}; // sizeof = 1, alignof = 1 struct T : public S { char x; }; // sizeof = 1, alignof = 1 struct U { int x; // offsetof = 0 char y; // offsetof = 4 char z; // offsetof = 5 }; // sizeof = 8, alignof = 4 struct V { int a; // offsetof = 0 T b; // offsetof = 4 U c; // offsetof = 8 double d; // offsetof = 16 }; // sizeof = 24, alignof = 8 struct W { int val; // offset = 0 W *left; // offset = 8 W *right; // offset = 16 }; // sizeof = 24, alignof = 8 最后要说明一下数组，数组就像是你在这个位置引入了数组长度个该类型的变量。 struct S { int x[4]; }; // sizeof = 16, alignof = 4 struct T { int a; // offsetof = 0 char b[9]; // offsetof = 4 short c[2]; // offsetof = 14 double *d; // offsetof = 24 }; // sizeof = 32, alignof = 8 struct U { char x; // offsetof = 0 char y[1]; // offsetof = 1 short z; // offsetof = 2 }; // sizeof = 4, alignof = 2 你以为这就完了吗？当然不是，C 语言中有个很有意思的用法，即 C99 中出现的 柔性数组声明。将最后一个字段定义为数组，且长度为 0，此时数组底层数据类型将影响类型的对齐大小，但不会影响整个类型的大小。当然对于 C++ 标准并没有支持，全靠编译器自己去扩展。 struct S { int i; // offset = 0 double d[]; // offset = 8 }; // sizeof = 8, alignof = 8 struct T { int i; // offset = 0 char c[0]; // offset = 4 }; // sizeof = 4, alignof = 4 带有柔性数组成员的类，需要使用动态分配的方式，因为柔性数组成员无法被初始化。实际上编译器不能确定数组的长度，因此即使给定的额外的空间不足以存放底层类型数据，也由程序员保证访问的正确性，访问溢出的范围将是 UB。 S s1; // sizeof(s1) = 8, length(d) = 1, accessing d is a UB // S s2 = {1, {3.14}}; // error: initialization of flexible array member is not allowed S* s3 = reinterpret_cast(alloca(sizeof(S))); // equivalent to s1 // s4: sizeof(*s4) = 8, length(d) = 6 S *s4 = reinterpret_cast(alloca(sizeof(S) + 6 * sizeof(S::d[0]))); // s5: sizeof(*s5) = 8, length(d) = 1, accessing d[1] is a UB S *s5 = reinterpret_cast(alloca(sizeof(S) + 10)); *s4 = *s5; // copy size = sizeof(S) 带有位域的标准布局类对于带有位域的标准布局类，也很简单，位域不会跨底层数据存储，也就是说当剩余位不够时，下一个位域字段会存储在下一个底层数据中。而无名位域字段可以起到占位的作用。另外声明位域后，实际会用一个底层数据填充到类里，类的大小与对齐会收到该底层数据的影响。 struct S { // offsetof = 0 unsigned char b1 : 3, : 2; // offsetof = 1 unsigned char b2 : 6, b3 : 2; }; // sizeof = 2, alignof = 1 位域字段的大小可以指定为 0，意味着下一个位域将声明在下一个底层数据中。但实际 0 长度的位域字段并不会为类引入一个底层数据。 struct S { int : 0; }; // sizeof = 1, alignof = 1 struct T { uint64_t : 0; uint32_t x; // offsetof = 0 }; // sizeof = 4, alignof = 4 struct U { // offsetof = 0 unsigned char b1 : 3, : 0; // offsetof = 1 unsigned char b2 : 2; }; // sizeof = 2, alignof = 1 手动指定对齐大小的标准布局类回到本章开始的 5 条规律，实际上自己手动指定对齐时，也是适用的。 #pragma pack(N) 和 gnu::packed 指定排布字段时以打包方式进行，即每个字段都连续排布，字段与字段之间不会产生额外的内存空洞，这样可以减少不必要内存的浪费。 struct [[gnu::packed]] S { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 3, alignof = 1 struct [[gnu::packed]] T { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 2, alignof = 1 struct [[gnu::packed]] alignas(4) U { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 struct [[gnu::packed]] alignas(4) V { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 但是今天的重点是 C++11 引入的 alignas 声明符。实际上它不止可以指定结构体时如何对齐的，还可以指定一个对象是怎么对齐的。指定的对齐大小都必须是 2 的正整数幂，如果指定的对齐方式弱于默认的对齐方式，编译器可能会忽略或报错。 最简单的先从指定结构体的声明说起。 struct alignas(4) S {}; // sizeof = 4, alignof = 4 struct SS { S s; // offsetof = 0 S *t; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(SS) T { S s; // offsetof = 0 char t; // offsetof = 4 short u; // offsetof = 6 short v; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(1) U : public S {}; // error or ignore // struct alignas(5) V : public S {}; // error struct alignas(4) W : public S {}; 对于变量指定对齐大小，对齐大小并不意味着实际占用，下一个字段依然会根据自身对齐方式紧凑排列。 struct S { int16_t i; // offsetof = 0 char c1; // offsetof = 2 char a[11]; // offsetof = 3 char c2; // offsetof = 14 }; // sizeof = 16, offsetof = 2 struct T { alignas(4) int16_t i; // offsetof = 0 char c1; // offsetof = 2 alignas(8) char a[11]; // offsetof = 8 char c2; // offsetof = 19 }; // siz","date":"09-03","objectID":"/2022/memory_alignment/:3:2","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#带有位域的标准布局类"},{"categories":["Programming"],"content":" 标准布局类的内存对齐内存对齐有些规律可循： 对象的起始地址能够被其对齐大小整除 成员相对于起始地址的偏移量能够被自身的对齐大小整除，否则在前一个成员后面填充字节 类的大小能够被其对齐大小整除，否则在最后填充字节 如果是空类，按照标准该类的对象必须占有一个字节 (除非 空基类优化)，在C中空类的大小是 0 字节 默认条件下，类型的对齐大小与其所有字段的对齐大小最大值相同 普通的标准布局类对于任何标准布局类，都可以轻松用上面的规律判断出类型的大小 struct S {}; // sizeof = 1, alignof = 1 struct T : public S { char x; }; // sizeof = 1, alignof = 1 struct U { int x; // offsetof = 0 char y; // offsetof = 4 char z; // offsetof = 5 }; // sizeof = 8, alignof = 4 struct V { int a; // offsetof = 0 T b; // offsetof = 4 U c; // offsetof = 8 double d; // offsetof = 16 }; // sizeof = 24, alignof = 8 struct W { int val; // offset = 0 W *left; // offset = 8 W *right; // offset = 16 }; // sizeof = 24, alignof = 8 最后要说明一下数组，数组就像是你在这个位置引入了数组长度个该类型的变量。 struct S { int x[4]; }; // sizeof = 16, alignof = 4 struct T { int a; // offsetof = 0 char b[9]; // offsetof = 4 short c[2]; // offsetof = 14 double *d; // offsetof = 24 }; // sizeof = 32, alignof = 8 struct U { char x; // offsetof = 0 char y[1]; // offsetof = 1 short z; // offsetof = 2 }; // sizeof = 4, alignof = 2 你以为这就完了吗？当然不是，C 语言中有个很有意思的用法，即 C99 中出现的 柔性数组声明。将最后一个字段定义为数组，且长度为 0，此时数组底层数据类型将影响类型的对齐大小，但不会影响整个类型的大小。当然对于 C++ 标准并没有支持，全靠编译器自己去扩展。 struct S { int i; // offset = 0 double d[]; // offset = 8 }; // sizeof = 8, alignof = 8 struct T { int i; // offset = 0 char c[0]; // offset = 4 }; // sizeof = 4, alignof = 4 带有柔性数组成员的类，需要使用动态分配的方式，因为柔性数组成员无法被初始化。实际上编译器不能确定数组的长度，因此即使给定的额外的空间不足以存放底层类型数据，也由程序员保证访问的正确性，访问溢出的范围将是 UB。 S s1; // sizeof(s1) = 8, length(d) = 1, accessing d is a UB // S s2 = {1, {3.14}}; // error: initialization of flexible array member is not allowed S* s3 = reinterpret_cast(alloca(sizeof(S))); // equivalent to s1 // s4: sizeof(*s4) = 8, length(d) = 6 S *s4 = reinterpret_cast(alloca(sizeof(S) + 6 * sizeof(S::d[0]))); // s5: sizeof(*s5) = 8, length(d) = 1, accessing d[1] is a UB S *s5 = reinterpret_cast(alloca(sizeof(S) + 10)); *s4 = *s5; // copy size = sizeof(S) 带有位域的标准布局类对于带有位域的标准布局类，也很简单，位域不会跨底层数据存储，也就是说当剩余位不够时，下一个位域字段会存储在下一个底层数据中。而无名位域字段可以起到占位的作用。另外声明位域后，实际会用一个底层数据填充到类里，类的大小与对齐会收到该底层数据的影响。 struct S { // offsetof = 0 unsigned char b1 : 3, : 2; // offsetof = 1 unsigned char b2 : 6, b3 : 2; }; // sizeof = 2, alignof = 1 位域字段的大小可以指定为 0，意味着下一个位域将声明在下一个底层数据中。但实际 0 长度的位域字段并不会为类引入一个底层数据。 struct S { int : 0; }; // sizeof = 1, alignof = 1 struct T { uint64_t : 0; uint32_t x; // offsetof = 0 }; // sizeof = 4, alignof = 4 struct U { // offsetof = 0 unsigned char b1 : 3, : 0; // offsetof = 1 unsigned char b2 : 2; }; // sizeof = 2, alignof = 1 手动指定对齐大小的标准布局类回到本章开始的 5 条规律，实际上自己手动指定对齐时，也是适用的。 #pragma pack(N) 和 gnu::packed 指定排布字段时以打包方式进行，即每个字段都连续排布，字段与字段之间不会产生额外的内存空洞，这样可以减少不必要内存的浪费。 struct [[gnu::packed]] S { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 3, alignof = 1 struct [[gnu::packed]] T { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 2, alignof = 1 struct [[gnu::packed]] alignas(4) U { uint8_t x; // offsetof = 0 uint16_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 struct [[gnu::packed]] alignas(4) V { uint16_t x : 4; uint8_t y; // offsetof = 1 }; // sizeof = 4, alignof = 4 但是今天的重点是 C++11 引入的 alignas 声明符。实际上它不止可以指定结构体时如何对齐的，还可以指定一个对象是怎么对齐的。指定的对齐大小都必须是 2 的正整数幂，如果指定的对齐方式弱于默认的对齐方式，编译器可能会忽略或报错。 最简单的先从指定结构体的声明说起。 struct alignas(4) S {}; // sizeof = 4, alignof = 4 struct SS { S s; // offsetof = 0 S *t; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(SS) T { S s; // offsetof = 0 char t; // offsetof = 4 short u; // offsetof = 6 short v; // offsetof = 8 }; // sizeof = 16, alignof = 8 struct alignas(1) U : public S {}; // error or ignore // struct alignas(5) V : public S {}; // error struct alignas(4) W : public S {}; 对于变量指定对齐大小，对齐大小并不意味着实际占用，下一个字段依然会根据自身对齐方式紧凑排列。 struct S { int16_t i; // offsetof = 0 char c1; // offsetof = 2 char a[11]; // offsetof = 3 char c2; // offsetof = 14 }; // sizeof = 16, offsetof = 2 struct T { alignas(4) int16_t i; // offsetof = 0 char c1; // offsetof = 2 alignas(8) char a[11]; // offsetof = 8 char c2; // offsetof = 19 }; // siz","date":"09-03","objectID":"/2022/memory_alignment/:3:2","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#手动指定对齐大小的标准布局类"},{"categories":["Programming"],"content":" 非标准布局类的内存对齐对于访问限定造成的非标准布局类，我们不能假定其按照标准布局进行布局，其行为依赖于编译器。在 C++11 标准中，只保证了在同一访问性的变量按声明顺序排布，但不保证不同访问性的变量的排布顺序。 struct S { public: int s; int t; private: int u; public: int v; }; 也就是说，上面这个示例中，只保证了 \u0026S::s \u003c \u0026S::t \u003c \u0026S::v，但不会保证 \u0026S::s \u003c \u0026S::u。或者说，在内存中，可能出现 s, t, u, v 的顺序，也可能出现 u, s, t, v 的顺序。 当然不止访问性导致的顺序问题，在不同类中声明的字段也会造成顺序问题。也就是说，我们不能假定基类中声明的变量，其位置一定先于派生类中声明的变量。 struct S { int s; }; struct T { int t; }; struct U : public S, T { int u; }; 也就是说，上面这个示例中，不能保证 \u0026U::s \u003c \u0026U::u。但是标准保证，在派生类指针转换到基类指针时，会自动计算基类字对象的偏移量。但不保证 U 的对象首地址就是 S 的字对象首地址。 U *up = reinterpret_cast\u003cU *\u003e(alloca(sizeof(U))); S *ssp = static_cast\u003cS *\u003e(up); // offset adjustment T *stp = static_cast\u003cT *\u003e(up); // offset adjustment S *rsp = reinterpret_cast\u003cS *\u003e(up); // no offset adjustment T *rtp = reinterpret_cast\u003cT *\u003e(up); // no offset adjustment 最后再来说一下虚类的内存对齐，这是很有意思的一个问题。标准并没有规定如何实现虚函数，但大部分的编译器都采用虚表的方式实现，即在对象中插入一个虚函数表的指针。但是需要注意的是，虚表一个对象中仅存在，基类子对象中不会有虚表。 struct S { bool s; // offsetof = 0 }; // sizeof = 1, alignof = 1 struct T { virtual ~T() = default; int t; }; struct U : public S, T { virtual ~U() = default; int u; }; 在编译器的实现中，很可能先排布虚基类，再排布非虚基类，因此在不同的排布方式其类大小与布局是无法确定的。 ","date":"09-03","objectID":"/2022/memory_alignment/:3:3","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#非标准布局类的内存对齐"},{"categories":["Programming"],"content":" GLSLang 的内存对齐 信息 GLSL 4.60, Vulkan binding 在 GLSLang 中，一个字长为 4 bytes。而 GLSLang 中的对齐，也和 C / C++ 中很相似，因此在标准布局类的内存对齐中介绍的对齐方式，和这里是基本一致的。另外 GLSLang 中基础类型的大小都是字长的倍数，因此之后 sizeof 的结果单位默认为 word。 type sizeof alignof type sizeof alignof type sizeof alignof void bool 1 int 1 1 uint 1 1 float 1 1 double 2 2 vec2 2 2 vec3 3 4 vec4 4 4 dvec2 4 4 dvec3 6 8 dvec4 8 8 ivec2 2 2 ivec3 3 4 ivec4 4 4 uvec2 2 2 uvec3 3 4 uvec4 4 4 ","date":"09-03","objectID":"/2022/memory_alignment/:4:0","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#glslang-的内存对齐"},{"categories":["Programming"],"content":" buffer 的布局修饰buffer 作为可读可写的全局对象，其布局由实现定义，除非手动指定布局。uniform 是一种特殊的全局 buffer，只可读，默认 std140 布局且无法修改；push_constant 是一种特殊的 uniform，其存储在寄存器，大小约为 16 words，实现可以使用 uniform 代替实现，当超出大小时同样将超出部分存储在 uniform buffer 中，默认布局为 std430，可以修改布局。 在 buffer 中，默认矩阵都是列主矩阵 (column_major)，可以在布局中对其进行修改 layout(binding = 0, column_major) buffer CMTest { // matrix stride = 16 mat2x3 cm; // is equalent to 2-elements array of vec3 }; layout(binding = 1, row_major) buffer RMTest { // matrix stride = 8 mat2x3 rm; // is equalent to 3-elements array of vec2 }; packed 与 CPU 上的概念是一致的，尽可能紧凑的排布字段，节省内存，而不考虑对齐。但 SPIRV 禁止使用 packed 与 shared 的布局方式。 在 GLSLang 的布局中，其偏移量同样是对齐大小的整数倍。std140 布局有以下规律 标量类型其对齐大小与自身大小相同 二元或四元向量，其基础类型大小若为 N，则向量大小与对齐大小相同，对齐大小为 \\(2N\\) 或 \\(4N\\)。特别地，三元向量的大小为 \\(3N\\)，但对齐大小为 \\(4N\\) 数组中的每个元素填充到 4 words 的倍数 结构体变量的对齐大小填充到 4 words 的倍数 C 列 R 行的列主矩阵，等价于一个有 C 个 R 元向量的数组；类似的，有 N 个元素的列主矩阵的数组，等价于一个有 \\(N \\times C\\) 个 R 元向量的数组 C 列 R 行的行主矩阵，等价于一个有 R 个 C 元向量的数组；类似的，有 N 个元素的行主矩阵的数组，等价于一个有 \\(N \\times R\\) 个 C 元向量的数组 struct S { vec2 v; }; layout(binding = 0, std140) buffer BufferObject { mat2x3 m; // offsetof = 0 bool b[2]; // offsetof = 8 vec3 v1; // offsetof = 16 uint u; // offsetof = 19 S s; // offsetof = 20 float f2; // offsetof = 24 vec2 v2; // offsetof = 26 dvec3 dv; // offsetof = 32 } bo; // sizeof = 40, alignof = 8 对于 std430 布局，不再有 std140 中的将数组和结构体元素对齐填充到 4 words 的要求，也就是说，std430 更为紧凑，且更接近我们在 CPU 中的布局。 struct S { vec2 v; }; layout(binding = 0, std430) buffer BufferObject { mat2x3 m; // offsetof = 0 bool b[2]; // offsetof = 8 vec3 v1; // offsetof = 12 uint u; // offsetof = 15 S s; // offsetof = 16 float f2; // offsetof = 18 vec2 v2; // offsetof = 20 dvec3 dv; // offsetof = 24 } bo; // sizeof = 32, alignof = 8 虽然默认的布局方式已经很好了，不过有时也可能会手动的修改以下字段的偏移量。这时候需要使用 offset。但是编译器不会检查手动设置的偏移量是否与其他字段存在重叠。 layout(binding = 0, std430) buffer BufferObject { mat2x3 m; // offsetof = 0 bool b[2]; // offsetof = 8 layout(offset = 48) uint u; // offsetof = 12 vec2 v; // offsetof = 14 layout(offset = 0) int i; // offset = 0 } bo; align 的使用也和前面说的 CPU 上的用法差不多 layout(binding = 0, std430) buffer BufferObject { vec2 a; // offsetof = 0 layout(align = 16) float b; // offsetof = 4 } bo; // sizeof = 8, alignof = 4 ","date":"09-03","objectID":"/2022/memory_alignment/:4:1","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#buffer-的布局修饰"},{"categories":["Programming"],"content":" locationlocation 相当于每个 shader 数据传输的一个存储点，location 根据编号进行匹配，其匹配上一个 shader 的 in 与下一个 shader 的 out。同一个 location 不能在 shader 中声明多次，in 与 out 是完全不同的 location。 layout(location = 0) in vec2 i; // layout(location = 0) in vec2 i2; // error layout(location = 0) out vec2 o; // okay location 大小为 4 words。声明的每个变量占据一个 location，当变量大小大于 4 words 时，将顺延占据下一个 location。 layout(location = 0) in dvec4 dv; // location = 1, occupied by dv // layout(location = 1) in vec4 v; // error layout(location = 2) in vec4 v; 而数组每个元素占据一个 location，并且元素占据的 location 值是依次递增的，因此 layout(location = 0) in float a[2]; // location = 1, occupied by a[1] layout(location = 2) in float f1; layout(location = 3) in mat2 m[2]; // cxr matrix is equialent to c-elements array of r-vector // location = 4, occupied by m[0] // location = 5, occupied by m[1] // location = 6, occupied by m[1] layout(location = 7) in float f2; 一个一个指定 location 实在是太麻烦了，因此可以使用 block 来指定第一个变量的初始 location 值，然后让其他变量的 location 值自动递增。 layout(location = 3) in block { float a[2]; // location = 3 mat2 m; // location = 5 vec2 v; // location = 7 layout(location = 0) mat2 m2; // location = 0 bool b; // location = 2 // vec3 v3; // error layout(location = 8) vec3 v3; // location = 8 }; 也可以用 struct 来递增 location，但区别是无法在 struct 中指定 location。 layout(locaton = 3) in struct { vec3 a; // location = 3 mat2 b; // location = 4, 5 // layout(location = 6) vec2 c; // error }; 之前说过 location 的大小是 4 words，如果一个 location 只用其中的一部分存储变量显然是低效的，component 可以指定变量在 location 的偏移量。但是需要注意的是， component 偏移后剩余部分必须能存储该变量。 layout(location = 0, component = 0) in float x; // l = 0, c = 0 layout(location = 0, component = 1) in float y; // l = 0, c = 1 layout(location = 0, component = 2) in float z; // l = 0, c = 2 layout(location = 1) in vec2 a; // l = 1, c = 0 // layout(location = 1, component = 2) in dvec3 b; // error layout(location = 2, component = 0) in float b; // l = 2, c = 0 layout(location = 2, component = 1) in vec3 c; // l = 2, c = 1 如果指定了数组的 component，则数组的每个元素依然顺序递增占据每个 location，但每个 location 的起始位置都是指定的 component。 layout(location = 0, component = 2) in float f[6]; // every element c = 2 // layout(location = 2, component = 0) in vec4 v; // error layout(location = 1, component = 0) in vec2 v; // l = 1, c = 0 // f[1] at location 1, component 2 ","date":"09-03","objectID":"/2022/memory_alignment/:4:2","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#location"},{"categories":["Programming"],"content":" 使用 GLM 与 GLSLang 传递数据写这篇文章的起因完全是因为在 host 和 device 之间传递数据时，遇到了一个对齐相关的 bug。 struct PCO { uint32_t time; // offsetof = 0 ::glm::vec2 extent; // offsetof = 4 }; // sizeof = 12, alignof = 4 layout(push_constant) uniform PCO { int time; // offsetof = 0 vec2 extent; // offsetof = 2 }; // sizeof = 4, alignof = 2 在反复检查代码没有问题后，尝试交换 time 字段与 extent 字段，结果程序能正常运行。很明显 host 的对齐与 device 是不一致的。由于 SPIRV 无法使用 packed 来压缩内存大小，因此只能手动实现对齐。 通过之前的学习，在此列出几种比较优雅的解决这个问题的方法。 利用位域产生空洞，强迫结构体与 glsl 中布局一致 struct PCO { uint32_t time; // offsetof = 0 uint32_t : 1, : 0; ::glm::vec2 extent; // offsetof = 8 }; // sizeof = 16, alignof = 4 指定字段与 glsl 中的对齐大小一致 struct PCO { uint32_t time; // offsetof = 0 alignas(8)::glm::vec2 extent; // offsetof = 8 }; // sizeof = 16, alignof = 8 ","date":"09-03","objectID":"/2022/memory_alignment/:5:0","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#使用-glm-与-glslang-传递数据"},{"categories":["Programming"],"content":" Useful link Purpose of memory alignment C++中 long long 和 int64_t 哪个应用场景更广？ 整理一下 C++ POD 类型的细节（一） 和 整理一下 C++ POD 类型的细节（二） #pragma pack effect Alignment and Packing History of non-standard-layout class layouts C++ Disambiguation: subobject and subclass object Simple features I’m missing in GLSL: enums and sizeof OpenGL Wiki ","date":"09-03","objectID":"/2022/memory_alignment/:6:0","series":null,"tags":["C++","GLSLang","Memory"],"title":"内存对齐","uri":"/2022/memory_alignment/#useful-link"},{"categories":["Applications"],"content":"GinShio | use package manager with Git-Bash on Windows","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/"},{"categories":["Applications"],"content":"Windows 中，在安装 Git Bash 时，会安装一个最小化的 Msys 环境，用于提供 Uinx 兼容层。单独安装一个 msys 不如直接使用 Git 引入的来的爽。 另外还有些好处，比如安装依赖只需要从包管理器安装，而无需到处找官网安装配环境。 ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:0:0","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#"},{"categories":["Applications"],"content":" 安装 GitGit 的安装应该是都会的，但还是应该说以下，在 Windows 上安装 git 时，实际上是有很多细节需要注意的。 选择 git 使用的默认的编辑器 实际上，git 已经在这里说的很明白了，默认 vim 是一个历史原因，推建我们使用更现代的 GUI 编辑器。实际上，你可以使用 core.editor 来修改你想使用的编辑器。当然，如果你不设置这个值，git 会用环境变量中的 EDITOR 作为默认编辑器使用，而 Unix 世界中，EDITOR 往往是 Vi 或 Vim。 最后说一下我的习惯，我并不喜欢 Vim，但是配置了的 Emacs 打开太慢了，由其是简单的写一个 message (VSCode 人称小 emacs)，所以我更偏向于终端编辑器 GNU Nano，图形编辑器则更喜欢用 Kate。 初始化新仓库时的默认分值名称 你可以使用 init.defaultbranch 来更改默认的分支名称。 环境变量的作用域 我更推建第一种使用方式，我们只会在 Git-Bash 中使用 Unix tools。这样现得我们的环境变量更为干净。其实在 Powershell 中还好，在 CMD Prompt 中使用 [ 也太精分了。 换行符转换 这是重中之重，也是经常出问题的地方。个人推建设置为 git 不管换行符 (checkout as-is, commit as-is)，由自己根据项目要求手动管理换行符。可以用 core.autocrlf=false 来指定这种方式。 其中，as-is 的意思是原本是什么样就是什么样，Git 不会转换换行符。以下这两种方式是最容易出问题的，当原本的换行符被替换时，整个文件将发生冲突。 Checkout Windows-style, commit Unix-style: 拉取时转换为 Windows 换行符，和用户本地一致，提交时自动转换为 Unix 风格换行符。可以用 core.autocrlf=true 来指定这种方式。 Checkout as-is, commit Unix-style: 拉取时保持不变，提交时全部转换为 Unix 风格换行符。可以用 core.autocrlf=input 来指定这种方式。 用哪种终端模拟器配合 Git-Bash 使用 只推荐使用 MinTTY。 选择 git pull 的默认行为 pull 的行为主要有 rebase、merge 和 fast-forward，主要由变量 pull.rebase 和 pull.ff 控制。rebase 的行为可以理解为每次都将自己的提交放在 remote 提交之后；merge 的行为是将生成一个新的节点；fast-forward 则会在一个提交树上类似于 rebase，当出现分叉时行为类似于 merge，如果是 ff-only 时只会产生 rebase 行为，出现分叉则会导致命令失败。 pull.ff=false git pull --merge # merge pull.ff=true git pull --merge # merge --ff pull.ff=only git pull --merge # merge --ff-only pull.rebase=true git pull # rebase git pull --merge # merge pull.rebase=false git pull # merge git pull --merge # merge ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:1:0","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#安装-git"},{"categories":["Applications"],"content":" MSYS 设置","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:2:0","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#msys-设置"},{"categories":["Applications"],"content":" msys2 环境 环境 目录 工具链 架构 C 库 C++ 库 msys /usr gcc x86_64 cygwin libstdc++ mingw64 /mingw64 gcc x86_64 msvcrt libstdc++ ucrt64 /ucrt64 gcc x86_64 ucrt libstdc++ clang64 /clang64 llvm x86_64 ucrt libc++ mingw32 /mingw32 gcc i686 msvcrt libstdc++ clang32 /clang32 llvm i686 ucrt libc++ clangarm64 /clangarm64 llvm aarch64 ucrt libc++ gcc 工具链有着更广泛的测试，有 Fortran 的支持，并且也有构建的 clang 工具链可用。 llvm 工具链仅有 llvm 支持，不支持 gcc 工具链。并且提供了 ASAN 和 TLS (Thread Local Storage) 支持。 对比 C 库，msvcrt (Microsoft Visual C++ Runtime) 与 ucrt (Universal C Runtime) 都依赖于 MSVC，但前者对于 C99 兼容性不好，ucrt 有着更好的 MSVC 兼容性。 ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:2:1","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#msys2-环境"},{"categories":["Applications"],"content":" Windows Terminal 支持Git 安装时可选 Windows Terminal 支持。当然还需要对打开 GitBash 的命令进行修改 C:/Program Files/Git/msys2_shell.cmd -defterm -here -no-start -mingw64 如果你想更改默认的 shell，只要将命令改为 C:/Program Files/Git/msys2_shell.cmd -defterm -here -no-start -mingw64 -shell shell ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:2:2","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#windows-terminal-支持"},{"categories":["Applications"],"content":" 环境变量 MSYS2_PATH_TYPE 控制系统设置的环境变量是否出现在 msys 中 mode comment strict 不包含 Windows PATH minimal 仅系统 PATH inherit 全部 PATH ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:2:3","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#环境变量"},{"categories":["Applications"],"content":" 安装包管理器","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:3:0","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#安装包管理器"},{"categories":["Applications"],"content":" 安装 pacman 及其依赖构建 Unix 环境的第一步就是有一个包管理器，我们直接使用 MSYS 的 pacman 包管理器。下载完成后，将其解压到 Git 的根目录下，在本章中，我们将用 / 表示 Git 安装的根目录。 这时的 pacman 还是无法使用的阶段，毕竟 Git 携带的是最小化的环境，并没有 pacman 需要的依赖。不过 msys package 中已经为我们详细列出了其所需的依赖。如果你想知道 Git 安装了哪些软件，可以查看 /etc/package-versions.txt。 bash \u003e= 4.2.045 bzip2 curl gettext gnupg msys2-keyring pacman-mirrors which xz zstd 实际上，我们并不需要安装列出的所有依赖，因为 Git 已经帮我们安装了一部分了。我们只需要安装 msys2-keyring 和 pacman-mirrors。 ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:3:1","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#安装-pacman-及其依赖"},{"categories":["Applications"],"content":" 更新 pacman现在更新 pacman 的 repo，在更新之前别忘了初始化 msys key。涉及到 pacman 等需要权限的命令需要用管理员权限打开 Git-Bash。 # init msys keyring pacman-db-upgrade pacman-key --init pacman-key --populate msys2 # update package source yes |pacman -Syuu 如果你用 pacman 搜索自己安装的 pacman 或者 Git 安装的 curl，会发现包管理器并没有把它们记录为已安装的状态。将刚刚自己安装的 pacman、msys2-keyring 和 pacman-mirrors 及其版本号，写入 /etc/package-versions.txt。并用 pacman 将其写入数据库中。如果其间遇到未找到目标的错误，把对应行删去即可。 yes |pacman -S $(cut -d ' ' -f 1 /etc/package-versions.txt) \\ man git git-extras mingw-w64-x86_64-git-lfs curl 修改 /etc/pacman.conf，关闭除 mingw64 和 msys 以外的所有软件源。下面是关闭了 mingw32 的示例。 #[mingw32] #Include = /etc/pacman.d/mirrorlist.mingw32 ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:3:2","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#更新-pacman"},{"categories":["Applications"],"content":" pacman 的使用在使用之前，安装一个好用、称手的 shell 是头等大事。我可没有暗示 bash 不好用，我只是更喜欢 fish shell 而已。当然，我还是推建以下几种 shell bash 最通用的 shell，一般也是默认的 shell tcsh 继承自 csh 的 shell，也是 bsd 世界的默认 shell，语法不兼容 bash zsh 语法兼容 bash 的 shell，扩展性强，近年很流行与 Oh-my-zsh 使用 fish 语法不兼容 bash 的 shell，扩展性不如 zsh，单交互模式体验很好 通常我使用 shell 的配置文件添加包管理器的命令别命，方便使用。 缩写 操作 备注 ref refresh 刷新所有软件源 in install 安装包 arm autoremove 移除不再需要的包 rm remove 移除包 dup dist-upgrade 发行版升级 lu list-updates 列出需要升级的包 up update 升级包 if info 获取包信息 se search 查询包 cln clean 清除本地缓存 ve verify 验证包完整性 alias Pref=\"pacman -Sy\" alias Pin=\"pacman -S\" alias Parm=\"pacman -Qdtq |pacman -Rs -\" alias Prm=\"pacman -Rs\" alias Pdup=\"pacman -Syuu\" alias Plu=\"pacman -Qu\" alias Pup=\"pacman -Su\" alias Pif=\"pacman -Si\" alias Pse=\"pacman -Ss\" alias Pcln=\"pacman -Scc\" alias Pve=\"pacman -Dk\" abbr -a Pref \"pacman -Sy\" abbr -a Pin \"pacman -S\" abbr -a Parm \"pacman -Qdtq |pacman -Rs -\" abbr -a Prm \"pacman -Rs\" abbr -a Pdup \"pacman -Syuu\" abbr -a Plu \"pacman -Qu\" abbr -a Pup \"pacman -Su\" abbr -a Pif \"pacman -Si\" abbr -a Pse \"pacman -Ss\" abbr -a Pcln \"pacman -Scc\" abbr -a Pve \"pacman -Dk\" ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:3:3","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#pacman-的使用"},{"categories":["Applications"],"content":" 已知的问题 curl: (77) error setting certificate verify locations 这个问题是因为 mingw-w64-x86_64-ca-certificates 包调用 p11-kit 时发生错误，Git 的默认安装路径为 /c/Program Files/Git，而 p11-kit 执行时将 Program 作为一个命令执行。因此在安装 Git 时，应避免使用带空格的路径或中文路径。 ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:4:0","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#已知的问题"},{"categories":["Applications"],"content":" Useful Link Git Pro MSYS2 MSYS2 Packages pacman She sells seashells bash tcsh Zsh fish ","date":"08-27","objectID":"/2022/git_bash_with_pacman_on_windows/:5:0","series":null,"tags":["Windows","Git","Msys","Tool"],"title":"在 Windows 的 Git Bash 中使用包管理器","uri":"/2022/git_bash_with_pacman_on_windows/#useful-link"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第六章读书笔记","date":"05-28","objectID":"/2022/compilerprinciple_008/","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/"},{"categories":["CompilerPrinciple"],"content":"在将给定源语言的一个程序翻译成特定机器代码的过程中，一个编译器可能构造出一系列中间表示。高层的中间表示接近源语言，而底层的表示接近目标语言。语法树是高层表示，它刻画了源程序的自然层次性结构，且适用于静态类型检查。低层表示适用于机器相关处理，如寄存器分配、指令选择等。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:0:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#"},{"categories":["CompilerPrinciple"],"content":" 语法树的变体语法树中的各个结点代表了源程序的构造，一个结点的所有子结点反映了该结点对应构造的有意义的组成成分。为表达式构建的有向无环图 (Directed Acyclic Graph, DAG) 指出了表达式中的公共子表达式。 与语法分析树有些不同的是，DAG 的结点可能有多个父结点，也就是说这个结点是个公共子结点。比如表达式 \\(a + a * (b - c) + (b - c) * d\\) SDD 既可以构造语法树，也可以构造 DAG，在构造 DAG 结点时每次构造之前都会检查是否已存在这样的结点。如果已存在结点，就返回已有结点，否则构建新结点。 编号 产生式 语义规则 1 \\(E\\rightarrow{}E_{1}+T\\) \\(E.node=\\textbf{new}\\ Node(’+’,E_{1}.node,T.node)\\) 2 \\(E\\rightarrow{}E_{1}-T\\) \\(E.node=\\textbf{new}\\ Node(’-’,E_{1}.node,T.node)\\) 3 \\(\\begin{aligned}E\u0026\\rightarrow{}T\\\\T\u0026\\rightarrow{}T_{1}*F\\end{aligned}\\) \\(\\begin{aligned}E.node\u0026=T.node\\\\T.node\u0026=\\textbf{new}\\ Node(’*’,T_{1}.node,F.node)\\end{aligned}\\) 4 \\(T\\rightarrow{}(E)\\) \\(T.node=E.node\\) 5 \\(T\\rightarrow{}\\textbf{id}\\) \\(T.node=\\textbf{new}\\ Leaf(\\textbf{id},\\textbf{id}.entry)\\) 6 \\(T\\rightarrow{}\\textbf{num}\\) \\(T.node=\\textbf{new}\\ Leaf(\\textbf{num},\\textbf{num}.entry)\\) 通常语法树或 DAG 的结点存放在记录数组中，每个记录第一个字段是运算符代码，也是该结点的标号；叶结点可能有一个存放词法值的字段，而内部结点可能有两个指向其左右运算数的字段。 在这样的一个数组中，我们只需要给定结点对应的整数下标就可以引用该结点了。而这个下标被称为表达式的值编码 (value number)。通常为了防止结点太多所造成的巨大的搜索开销，可以用 Hash 的方法实现，加快创建结点时的搜索。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#语法树的变体"},{"categories":["CompilerPrinciple"],"content":" 三地址码三地址码中一条指令的右侧最多有一个运算符，因此 \\(x+y*z\\) 这样的代码可能被翻译成 \\[\\begin{aligned} t_{1} \u0026= y * z\\\\ t_{2} \u0026= x + t_{1} \\end{aligned}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#三地址码"},{"categories":["CompilerPrinciple"],"content":" 地址和指令三地址码基于两个基本概念：地址和指令。地址通常具有以下形式之一： 名字，为方便起见，允许源程序名字作为三地址码的地址。实现中通常是指向符号表的指针 常量 编译器生成的临时变量 还有其他一些三地址指令的形式： 形如 x = y op z 的赋值指令，其中 op 是双目运算符，x/y/z 是地址 形如 x = op y 的赋值指令，其中 op 是单目运算符，x/y 是地址 形如 x = y 的复制指令，将 y 的值赋给 x 无条件转移指令 goto L，下一步执行的指令为标号为 L 的三地址码 形如 if x goto L 或 if False x goto L 的条件转移指令，当条件 x 为假时转移到标号为 L 的三地址码，否则顺序执行下一指令 形如 if x relop y goto L 条件转移指令，如果 x 和 y 满足 relop 关系则跳转，否则顺序执行 过程调用 call p,n，参数传递 param x，返回指令 return y 带下标的指令 x[i] = y 和 x = y[i] 形如 x = \u0026y、x = *y 或 *x = y 的地址及指针赋值指令 比方说语句 do i = i + 1; while (a[i] \u003c v); 可以翻译为三地址码 L: t1 = i + 1 i = t1 t2 = i * 8 t3 = a [ t2 ] if t3 \u003c v goto L 当然也可以 100: t1 = i + 1 101: i = t1 102: t2 = i * 8 103: t3 = a [ t2 ] 104: if t3 \u003c v goto 100 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:2:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#地址和指令"},{"categories":["CompilerPrinciple"],"content":" 四元式表示地址和指令 一节中介绍了三地址码的具体组成，但在数据结构上的表示方法，常用四元式、三元式和间接三元式这三种描述方式。 四元式 (quadruple) 有四个字段，分别是 op、arg1、arg2 和 result，其中 op 是运算符的内部编码。比如三地址码 x=y+z 可以用四元式 (=, y, z, x) 表示。但有一些特例： 形如 x = -y 这样的单目运算符和赋值运算符 x = y，不使用 arg2 形如 param 这样的运算既不使用 arg2 也不使用 result 条件或非条件转移指令将目标标号存入 result 字段 比如说语句 a = b * -c + b * -c，可以得到三地址码 t1 = minus c t2 = b * t1 t3 = minus c t4 = b * t3 t5 = t2 + t4 a = t5 得到的四元式 No op arg1 arg2 result 1 minus c t1 2 * b t1 t2 3 minus c t3 4 * b t3 t4 5 + t2 t4 t5 6 = t5 a ","date":"05-28","objectID":"/2022/compilerprinciple_008/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#四元式表示"},{"categories":["CompilerPrinciple"],"content":" 三元式表示三元式 (triple) 只有 op、arg1、arg2 三个字段，四元式中 result 字段主要保留的是临时变量，因此三元式中用运算的位置表示其结果。但是像 x[i] = y 这样的三元式，需要两个条目，我们可以把 x 和 i 置于同一个三元式，y 置于另一个三元式。 作为语句 a = b * -c + b * -c，可以得到三元式 No op arg1 arg2 1 minus c 2 * b (1) 3 minus c 4 * b (2) 5 + (1) (3) 6 = a (4) 对于优化器来说，指令的位置会经常发生变化，此时四元式用临时变量引用结果，不会有任何变化，但三元式中使用位置引用结果的，这将导致需要频繁修改指令。 间接三元式 (indirect triple) 包含了一个指向三元式的指针列表，而不是指向序列本身。在优化重排三元式时，可以直接对间接的指针列表进行重排，不会影响三元式本身。 当然间接引用列表和三元式表不在一起，只是为了方便展示，才放在一起。 instruction No op arg1 arg2 (1) 1 minus c (2) 2 * b (1) (3) 3 minus c (4) 4 * b (2) (5) 5 + (1) (3) (6) 6 = a (4) ","date":"05-28","objectID":"/2022/compilerprinciple_008/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#三元式表示"},{"categories":["CompilerPrinciple"],"content":" 静态单赋值形式静态单赋值形式 (static single assignment, SSA) 是另一种中间表示，它有利于实现某种类型的代码优化。SSA 中所有的赋值都是针对具有不同名字的变量。 三地址码 静态单赋值形式 p = a + b p1 = a + b q = p - c q1 = p1 - c p = q * d p2 = q1 * d p = e - p p3 = e - p2 q = p + q q2 = p3 + q1 在一个程序中，同一个变量可能在两个不同的控制流中被赋值，比如 \\[\\begin{aligned} \u0026 \\textbf{if}\\ (flag)\\ x\\,=\\,-1;\\ \\textbf{else}\\ x\\,=\\,1;\\\\ \u0026 y\\,=\\,x\\,*\\,a; \\end{aligned}\\] 但是最终 y 的取值，应该由哪个 x 变量决定。SSA 给出的解决方案是 \\(\\phi\\) 函数，将 x 的两处赋值合并起来，最终得到的是 \\[\\begin{aligned} \u0026 \\textbf{if}\\ (flag)\\ x_{1}\\,=\\,-1;\\ \\textbf{else}\\ x_{2}\\,=\\,1;\\\\ \u0026 x_{3}\\,=\\,\\phi(x_{1},\\,x_{2}); \\end{aligned}\\] 如果控制流经过这个条件语句为真，那么 \\(\\phi(x_{1},x_{2})\\) 的值为 \\(x_{1}\\)，否则值为 \\(x_{2}\\)。也就是说，根据到达包含\\(\\phi\\)函数的赋值语句的不同控制流路经，\\(\\phi\\)函数返回不同的数值。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:2:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#静态单赋值形式"},{"categories":["CompilerPrinciple"],"content":" 类型和声明可以将类型的应用划为类型检查和翻译： 类型检查 (type checking) 用一组逻辑规则来推理一个程序在运行时的行为。保证运算分量的类型和运算符的预期相匹配。 翻译时的应用 (translation application) 根据一个名字的类型，编译器可以确定这个名字在运行时需要多大的存储空间，或者其他需要类型信息的地方。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:3:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#类型和声明"},{"categories":["CompilerPrinciple"],"content":" 类型表达式类型也有自己的结构，也就是我们说的类型表达式 (type expression)。类型表达式可能是基本类型，也可能通过将类型构造算子 (运算符) 作用于类型表达式。基本类型的集合和类型构造算子根据被检查的具体语言而定。就像 int[2][3] 被解释为 array(2, array(2, integer)) 基本类型是一个类型表达式 类名是一个类型表达式 将类型构造算子 array 作用一个数字和一个类型表达式，可以得到一个类型表达式 一个记录是包含有名字段的数据结构，将 record 类型构造算子应用于字段名和相应的类型可以构造得到一个类型表达式 使用类型构造算子 \\(\\rightarrow\\) 可以构造得到函数类型的类型表达式 如果 s 和 t 是类型表达式，那么可以使用笛卡尔积 \\(s \\times t\\) 描述类型的列表或元组，且假定 \\(\\times\\) 是左结合的，且享有最高优先级 类型表达式的值可以为该类型的变量 图是表示类型表达式的方便方法，内部结点表示类型构造算子，而叶结点可以是基本类型、类型名或类型变量。这与构造 DAG 的结点的值编码方式类似。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:3:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#类型表达式"},{"categories":["CompilerPrinciple"],"content":" 类型等价如果两个类型表达式相等，那么这两个类型等价。不过在给一个类型表达式起别名时，在类型表达式中的别名代表一个类型，还是另一个类型表达式的缩写。 当用图表示类型表达式时，只有以下的某个条件成立时，两种类型之间结构等价 (structurally equivalent)。如果类型名仅表示自身，那么前两个定义了类型表达式的名等价 (name equivalence) 关系 它们是相同的基本类型 它们是将相同的类型构造算子应用于结构等价的类型而构造得到 一个类型是另一个类型表达式的别名 如果使用与 DAG 结点的编码方式，那么名等价表达式将被赋予相同的值编码。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:3:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#类型等价"},{"categories":["CompilerPrinciple"],"content":" 声明在研究类型及其声明时，将使用一个经过化简的文法，一次只声明一个名字。 \\[\\begin{aligned} D \u0026\\rightarrow{} T \\, \\textbf{id};\\ D\\ |\\ \\varepsilon\\\\ T \u0026\\rightarrow{} B C \\ |\\ \\textbf{record}\\, ‘\\{’ \\, D \\, ‘\\}’\\\\ B \u0026\\rightarrow{} \\textbf{int}\\ |\\ \\textbf{float}\\\\ C \u0026\\rightarrow{} \\varepsilon\\ |\\ [ \\, \\textbf{num} \\, ] \\, C \\end{aligned}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:3:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#声明"},{"categories":["CompilerPrinciple"],"content":" 局部变量名的存储布局从变量的类型可以得知类型信息或所需的内存占用。在编译时，可以使用这些数量为每个名字分配一个相对地址。名字的类型和相对地址信息保存在相应的符号表条目中。对于字符串这样的变长数据，以及动态数组这样的只有运行时才能确定大小的数据，处理方法是为这些数据的指针保留一个已知的固定大小的存储区域。 假设存储区域是连续的字节块，其中字节是可寻址的最小内存单位。一个字节通常有 8 bit，若干字节组成一个机器字。多字节数据对像往往被存储在一段连续的字节中，并以初始字节的地址作为该数据对象的地址。 类型的宽度 (width) 是指该类型的每个对象需要多少存储单元。一个基本类型需要整数个字节，为方便访问，数组和类这样的组合类型数据分配的内存是一个连续存储的字节块。 因此我们可以给出声明中给出的示例的 SDT，在这个 SDT 中，每个非终结符使用综合属性 type 和 width 记录类型信息，并有继承属性 t 和 w 传递类型信息。 \\[\\begin{array}{lllll} T \u0026\\rightarrow \u0026B \u0026\\{\u0026t=B.type;\\,w=B.width;\\ \\}\\\\ \u0026 \u0026C \u0026\\{\u0026T.type=C.type;\\,T.width=C.width;\\ \\}\\\\ B \u0026\\rightarrow \u0026\\textbf{int} \u0026\\{\u0026B.type=integer;\\,B.width=4;\\ \\}\\\\ B \u0026\\rightarrow \u0026\\textbf{float} \u0026\\{\u0026B.type=float;\\,B.width=8;\\ \\}\\\\ C \u0026\\rightarrow \u0026\\varepsilon \u0026\\{\u0026C.type=t;\\,C.width=w;\\ \\}\\\\ C \u0026\\rightarrow \u0026[\\,\\textbf{num}\\,]\\,C_{1} \u0026\\{\u0026C.type=array(\\textbf{num}.value,C_{1}.type);\\\\ \u0026 \u0026 \u0026 \u0026C.width=\\textbf{num}.value*C_{1}.width;\\ \\} \\end{array}\\] 现在还不足以支持我们为及其相关的特性进行优化，比如地址与机器字对齐。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:3:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#局部变量名的存储布局"},{"categories":["CompilerPrinciple"],"content":" 声明的序列像 C 和 Java 这样的语言支持将单个过程中的所有声明作为一个组进行处理。这些声明可能分布在一个 Java 过程中，但仍然能够在分析过程处理它们。比如使用 offset 作为跟踪下一个可用的相对地址的变量。 在声明序列之前，将 offset 设置为 0，每处理一个变量时，将其加入符号表，并将相对地址设置为当前的 offset，并为 offset 累加上当前变量的 width。 \\[\\begin{array}{lllll} P \u0026\\rightarrow{} \u0026 \u0026 \\{ \u0026 offset=0;\\ \\ \\}\\\\ \u0026 \u0026 D \u0026 \u0026 \\\\ D \u0026\\rightarrow{} \u0026 T\\ \\textbf{id}; \u0026 \\{ \u0026 top.put(\\textbf{id}.lexeme, T.type, offset);\\\\ \u0026 \u0026 \u0026 \u0026 offset=offset + T.width;\\ \\ \\}\\\\ \u0026 \u0026 D_{1} \u0026 \u0026 \\\\ D \u0026\\rightarrow{} \u0026 \\varepsilon \u0026 \u0026 \\\\ \\end{array}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:3:5","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#声明的序列"},{"categories":["CompilerPrinciple"],"content":" 记录和类中的字段声明的序列中介绍的方案还可以用于记录和类中的字段。但需要注意 记录或类中的各个字段名称必须不同 字段的偏移量 (相对地址) 是相对于该记录或类的数据区地址而言的 由于外部可能和内部名称相同，但它们属于不同的作用域，其地址也各不相同。因此方便起见，记录或类类型可以使用一个专有的符号表，对它们的各个字段的类型和相对地址进行编码。 \\[\\begin{array}{llll} T \\ \\rightarrow \u0026 \\textbf{record} ‘\\{’ \u0026 \\{ \u0026 Env.push(top);\\ top=\\textbf{new}\\,Env();\\\\ \u0026 \u0026 \u0026 Stack.push(offset);\\ offset=0;\\ \\ \\}\\\\ \u0026 D\\ ‘\\}’ \u0026 \\{ \u0026 T.type=recode(top);\\ T.width=offset;\\\\ \u0026 \u0026 \u0026 top=Env.top();\\ offset=Stack.pop();\\ \\ \\} \\end{array}\\] 关于记录类型的存储方式还可以推广到类中，因为无需为类中的方法保留存储空间。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:3:6","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#记录和类中的字段"},{"categories":["CompilerPrinciple"],"content":" 表达式的翻译","date":"05-28","objectID":"/2022/compilerprinciple_008/:4:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#表达式的翻译"},{"categories":["CompilerPrinciple"],"content":" 表达式中的运算考虑一个赋值语句，用 SDD 为其生成三地址码。属性 S.code 和 E.code 分别表示语句和表达式的三地址码，属性 E.addr 存放 E 的值的地址。 \\[\\begin{array}{llllll} S \u0026\\rightarrow{} \u0026 \\textbf{id} = E\\ ; \u0026 \\{ \u0026 S.code = \u0026E.code\\ ||\\\\ \u0026 \u0026 \u0026 \u0026 \u0026gen(top.get(\\textbf{id}.lexeme)\\ ‘=’\\ E.addr);\\ \\ \\}\\\\ E \u0026\\rightarrow{} \u0026 E_{1} + E_{2} \u0026 \\{ \u0026 E.addr = \u0026\\textbf{new}\\,Temp();\\\\ \u0026 \u0026 \u0026 \u0026 E.code = \u0026E_{1}.code\\ ||\\ E_{2}.code\\ ||\\\\ \u0026 \u0026 \u0026 \u0026 \u0026gen(E.addr\\ ‘=’\\ E_{1}.addr\\ ‘+’\\ E_{2}.addr);\\ \\ \\}\\\\ \u0026|{} \u0026 -\\,E_{1} \u0026 \\{ \u0026 E.addr = \u0026\\textbf{new}\\,Temp();\\\\ \u0026 \u0026 \u0026 \u0026 E.code = \u0026E_{1}.code\\ ||\\\\ \u0026 \u0026 \u0026 \u0026 \u0026gen(E.addr\\ ‘=’\\ ‘\\textbf{minus}’\\ E_{1}.addr);\\ \\ \\}\\\\ \u0026|{} \u0026 (\\,E_{1}\\,) \u0026 \\{ \u0026 E.addr = \u0026E_{1}.addr;\\\\ \u0026 \u0026 \u0026 \u0026 E.code = \u0026E_{1}.code\\ \\ \\}\\\\ \u0026|{} \u0026 \\textbf{id} \u0026 \\{ \u0026 E.addr = \u0026top.get(\\textbf{id}.lexeme);\\\\ \u0026 \u0026 \u0026 \u0026 E.code = \u0026’’\\ \\ \\} \\end{array}\\] 可以将 new Temp() 理解为产生一个完全不同的临时变量，对应三地址码中的一个临时变量。而 gen() 可以理解为产生一个三地址码。 因此根据上面这个 SDD，可以将表达式 a = b + -c 表示为三地址码序列 \\[\\begin{aligned} t_{1} \u0026= \\texttt{minus}\\ c\\\\ t_{2} \u0026= b + t_{1}\\\\ a \u0026= t_{2} \\end{aligned}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:4:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#表达式中的运算"},{"categories":["CompilerPrinciple"],"content":" 数组元素的寻址将数组元素存储在一块连续的空间里就可以快速地访问它们。假设数组元素的宽度为 w，那么数组的第 i 个元素的开始地址为 \\(base + i * w\\)，base 是数组的内存开始的相对地址，也就是 array[0] 的相对地址。对于二维数组，假设一行的宽度是 \\(w_{1}\\)，同一行中每个元素的宽度是 \\(w_{2}\\)，那么 \\(array[i_{1}][i_{2}]\\) 的相对地址的计算公式为 \\[base + i_{1} * w_{1} + i_{2} * w_{2}\\] 对 k 维数组，根据 \\(w_{1}\\) 和 \\(w_{2}\\) 的推广，可以得到 \\[base + i_{1} * w_{1} + i_{2} * w_{2} + \\cdots + i_{k} * w_{k}\\] 如果数组第 j 维上有 \\(n_{j}\\) 个元素，该数组的每个元素的宽度 \\(w=w_{k}\\)，在二维数组中 (即 \\(k=2,\\,w=w_{2}\\)) \\(array[i_{1}][i_{2}]\\) 的相对地址为 \\[base + (i_{1} * n_{2} + i_{2}) * w\\] 推广的 k 维数组，可以得到 \\[base + ((\\cdots ((i_{1} * n_{2} + i_{2}) * n_{3} + i_{3}) \\cdots) * n_{k} + i_{k}) * w\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:4:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#数组元素的寻址"},{"categories":["CompilerPrinciple"],"content":" 数组引用的翻译为数组引用生成代码时要解决的首要问题是将地址计算公式与引用文法关联起来，首先文法可以由 \\(L\\ \\rightarrow\\ {}L\\,[\\,E\\,]\\ |\\ \\textbf{id}\\,[\\,E\\,]\\) 给出。 \\[\\begin{array}{lllll} S \u0026\\rightarrow{} \u0026\\textbf{id}\\,=\\,E\\ ; \u0026 \\{ \u0026 gen(top.get(\\textbf{id}.lexeme)\\,’=’\\,E.addr);\\ \\ \\}\\\\ \u0026|{} \u0026L\\,=\\,E\\ ; \u0026 \u0026 gen(L.array.base\\,’[’\\,L.addr\\,’]’\\,’=’\\,E.addr);\\ \\ \\}\\\\ E \u0026\\rightarrow{} \u0026E_{1}\\,+\\,E_{2} \u0026 \\{ \u0026 E.addr\\,=\\,\\textbf{new}\\ Temp();\\\\ \u0026 \u0026 \u0026 \u0026 gen(E.addr\\,’=’\\,E_{1}.addr\\,’+’\\,E_{2}.addr);\\ \\ \\}\\\\ \u0026|{} \u0026\\textbf{id} \u0026 \\{ \u0026 E.addr\\,=\\,top.get(\\textbf{id}.lexeme);\\ \\ \\}\\\\ \u0026|{} \u0026L \u0026 \\{ \u0026 E.addr\\,=\\,\\textbf{new}\\ Temp();\\\\ \u0026 \u0026 \u0026 \u0026 gen(E.addr\\,’=’\\,L.array.base\\,’[’\\,L.addr\\,’]’);\\ \\ \\}\\\\ L \u0026\\rightarrow{} \u0026\\textbf{id}\\,[\\,E\\,] \u0026 \\{ \u0026 L.array\\,=\\,top.get(\\textbf{id}.lexeme);\\\\ \u0026 \u0026 \u0026 \u0026 L.type\\,=\\,L.array.type.elem;\\\\ \u0026 \u0026 \u0026 \u0026 L.addr\\,=\\,\\textbf{new}\\ Temp();\\\\ \u0026 \u0026 \u0026 \u0026 gen(E.addr\\,’=’\\,E.addr\\,’*’\\,L.type.width);\\ \\ \\}\\\\ \u0026|{} \u0026L_{1}\\,[\\,E\\,] \u0026 \\{ \u0026 L.array\\,=\\,L_{1}.array\\\\ \u0026 \u0026 \u0026 \u0026 L.type\\,=\\,L_{1}.type.elem\\\\ \u0026 \u0026 \u0026 \u0026 t\\,=\\,\\textbf{new}\\ Temp();\\\\ \u0026 \u0026 \u0026 \u0026 L.addr\\,=\\,\\textbf{new}\\ Temp();\\\\ \u0026 \u0026 \u0026 \u0026 gen(t\\,’=’\\,E.addr\\,’*’\\,L.type.width);\\\\ \u0026 \u0026 \u0026 \u0026 gen(L.addr\\,’=’\\,L_{1}.addr\\,’+’\\,t);\\ \\ \\} \\end{array}\\] 非终结符 L 有三个综合属性 L.addr 表示一个临时变量，被用于累加计算地址的 \\(i_{j} * w_{j}\\) 项，计算数组的偏移量 L.array 指向数组名称对应的符号表指针，L.array.base 是分析完所有下标后，数组的起始地址 L.type 是 L 生成的子数组类型，其宽度为 t.width，数组的元素类型由 t.elem 给出 如果 a 表示一个 \\(2\\times3\\) 的整数数组，c、i、j 都是整数，那么 a 的类型是 \\(array(2, array(3, integer))\\)。假设整数宽度为 4，那么 a 的宽度为 24。a[i] 的类型为 \\(array(3, integer)\\)，宽度 \\(w_{1}\\) 为 12。 表达式 \\(c+a[i][j]\\) 的三地址码可以表示为： \\[\\begin{aligned} t_{1} \u0026= i*12\\\\ t_{2} \u0026= j*4\\\\ t_{3} \u0026= t_{1}+t_{2}\\\\ t_{4} \u0026= a\\,[\\,t_{3}\\,]\\\\ t_{5} \u0026= c + t_{4} \\end{aligned}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:4:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#数组引用的翻译"},{"categories":["CompilerPrinciple"],"content":" 类型检查为了进行类型检查，编译器需要给源程序的每个组成部分赋予一个类型表达式，编译器确定这些类型表达式是否满足一组逻辑规则，这些规则称为源语言的类型系统 (type system)。 如果目标代码在保存元素值的同时保存了类型信息，那么任何检查都可以动态地进行。一个 健全 (sound) 的类型系统可以消除对动态类型错误检查的需要，因为它可以静态地确定这些错误不会在目标程序运行时发生。如果编译器可以保证它接受的程序在运行时不会发生类型错误，那么该语言的实现被称为强类型的。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:5:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#类型检查"},{"categories":["CompilerPrinciple"],"content":" 类型检查规则类型检查由两种形式，综合和推导。类型综合 (type synthesis) 根据子表达式的类型构造该表达式的类型。它要求名字先声明再使用。表达式 \\(E_{1}+E_{2}\\) 的类型是根据 \\(E_{1}\\) 和 \\(E_{2}\\) 定义的。还有一个经典例子：如果 f 的类型是 \\(s\\rightarrow{}t\\) 且 x 的类型是 s，那么 f(x) 的类型是 t。 类型推导 (type inference) 根据一个语言结构的使用方式来确定该结构的类型，比如说 null(x) 检测一个列表是否为空，x 必须是列表类型，但内部元素类型是未知的 (往往用 \\(\\alpha\\)、\\(\\beta\\) 等希腊字母作为类型变量)。 同样地，对于语句我们也可以由类似的检查，比如条件语句 \\(\\textbf{if}\\,(E)\\,S\\)，可以看作接收 E 为布尔类型，而语句结果为 void 类型。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:5:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#类型检查规则"},{"categories":["CompilerPrinciple"],"content":" 类型转换考虑类似于 \\(x+i\\) 的表达式，如果 x 是浮点类型且 i 是整型，它们需要不同的指令来完成运算。编译器需要把 \\(+\\) 的某个运算分量进行转换，以保证进行运算时，两个运算分量具有相同的类型。 大概可以用近似实现 if (E1.type == integer \u0026\u0026 E2.type == integer) E.type = integer; else if (E1.type == float \u0026\u0026 E2.type == integer) E.type = float; ... 但是类型的增多将需要处理的工作量也急剧增长。因此，在处理大量类型时，精心组织用于类型转换的语义动作就变得十分重要。 不过不同语言的类型转换规则是不同的，Java 的转换规则分为拓宽 (widening) 和窄化 (narrowing)，前者可以保持原有信息，而后者则可能丢失信息。 如果类型转换由编译器完成，那么称作隐式类型转换，或者自动类型转换 (conversion)，有些语言中只允许拓宽进行隐式转换。如果由程序员写出代码完成的类型转换称为显示类型转换，或者说强制类型转换 (cast)。 检查 \\(E\\rightarrow{}E_{1}+E_{2}\\) 的语义动作可以使用两个函数 max(t1, t2) 接受两个类型参数，并返回扩展层次结构中的较大者。如果两个类型不在这个层次结构中，返回错误 如果需要类型 t 的地址 a 中的内容转换成 w 类型的值，则函数 widen(a, t, w) 将生成转换代码。如果 t 和 w 相同，则返回 a 本身；否则生成一条指令来进行转换，并返回临时结果对象。 现在我们可以很轻松地处理加法 \\[\\begin{array}{llll} E \u0026\\rightarrow{} E_{1} + E_{2} \u0026 \\{ \u0026 E.type = max(E_{1}.type, E_{2}.type);\\\\ \u0026 \u0026 \u0026 a_{1} = widen(E_{1}.addr, E_{1}.type, E.type);\\\\ \u0026 \u0026 \u0026 a_{2} = widen(E_{2}.addr, E_{2}.type, E.type);\\\\ \u0026 \u0026 \u0026 E.addr = \\textbf{new}\\ Temp();\\\\ \u0026 \u0026 \u0026 gen(E.addr\\,’=’\\,a_{1}\\,’+’\\,a_{2});\\ \\ \\} \\end{array}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:5:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#类型转换"},{"categories":["CompilerPrinciple"],"content":" 控制流布尔表达式通常被用来： 改变控制流 计算逻辑值 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:6:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#控制流"},{"categories":["CompilerPrinciple"],"content":" 布尔表达式与短路代码布尔表达式由作用于布尔变量或关系表达式的布尔运算符构成，文法通常如 (其中 comp 是比较运算符)： \\[\\begin{array}{lll} B \u0026 \\rightarrow{} \u0026 B\\ ||\\ B \\\\ \u0026 |{} \u0026 B\\ \\\u0026\\\u0026\\ B \\\\ \u0026 |{} \u0026 !B \\\\ \u0026 |{} \u0026 (\\,B\\,) \\\\ \u0026 |{} \u0026 E\\ \\textbf{comp}\\ E \\\\ \u0026 |{} \u0026 \\textbf{true}\\\\ \u0026 |{} \u0026 \\textbf{false} \\end{array}\\] 程序设计语言的语义决定了是否需要对一个布尔表达式进行完整求值，如果允许部分求值足以确定整个表达式的值时不再执行完全求值，这被称为短路运算。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:6:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#布尔表达式与短路代码"},{"categories":["CompilerPrinciple"],"content":" 控制流语句常见的控制流语句如下 \\[\\begin{array}{lll} S \u0026\\rightarrow{} \u0026 \\textbf{if}\\ (\\,B\\,)\\ S_{1}\\\\ \u0026|{} \u0026 \\textbf{if}\\ (\\,B\\,)\\ S_{1}\\ \\textbf{else}\\ S_{2}\\\\ \u0026|{} \u0026 \\textbf{while}\\ (\\,B\\,)\\ S_{1}\\\\ \u0026|{} \u0026 S_{1}\\ S_{2} \\end{array}\\] 控制流会出现类似以下的效果 以这种结构实现控制流语句的 SDD \\[\\begin{array}{llll} P \u0026\\rightarrow{} S \u0026 S.next \u0026= newlabel()\\\\ \u0026 \u0026 P.code \u0026= S.code\\ ||\\ label(S.next)\\\\ S \u0026\\rightarrow{} \\textbf{assign} \u0026 S.code \u0026= \\textbf{assign}.code\\\\ S \u0026\\rightarrow{} \\textbf{if}\\ (\\,B\\,)\\ S_{1} \u0026 B.true \u0026= newlabel()\\\\ \u0026 \u0026 B.false \u0026= S_{1}.next = S.next\\\\ \u0026 \u0026 S.code \u0026= B.code\\ ||\\ label(B.true)\\ ||\\ S_{1}.code\\\\ S \u0026\\rightarrow{} \\textbf{if}\\ (\\,B\\,)\\ S_{1}\\ \\textbf{else}\\ S_{2} \u0026 B.true \u0026= newlabel()\\\\ \u0026 \u0026 B.false \u0026= newlabel()\\\\ \u0026 \u0026 S_{1}.next \u0026= S_{2}.next = S.next\\\\ \u0026 \u0026 S.code \u0026= B.code\\\\ \u0026 \u0026 \u0026||\\ label(B.true)\\ ||\\ S_{1}.code\\\\ \u0026 \u0026 \u0026||\\ gen(‘goto’\\ S.next)\\\\ \u0026 \u0026 \u0026||\\ label(B.false)\\ ||\\ S_{2}.code\\\\ S \u0026\\rightarrow{} \\textbf{while}\\ (\\,B\\,)\\ S_{1} \u0026 begin \u0026= newlabel()\\\\ \u0026 \u0026 B.true \u0026= newlabel()\\\\ \u0026 \u0026 B.false \u0026= S.next\\\\ \u0026 \u0026 S_{1}.next \u0026= begin\\\\ \u0026 \u0026 S.code \u0026= label(begin)\\ ||\\ B.code\\\\ \u0026 \u0026 \u0026||\\ label(B.true)\\ ||\\ S_{1}.code\\\\ \u0026 \u0026 \u0026||\\ gen(‘goto’\\ begin)\\\\ S \u0026\\rightarrow{} S_{1}\\ S_{2} \u0026 S_{1}.next \u0026= newlabel()\\\\ \u0026 \u0026 S_{2}.next \u0026= S.next\\\\ \u0026 \u0026 S.code \u0026= S_{1}.code\\ ||\\ label(S_{1}.next)\\ ||\\ S_{2}.code \\end{array}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:6:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#控制流语句"},{"categories":["CompilerPrinciple"],"content":" 布尔表达式的控制流翻译我们需要针对布尔表达式生成相应的 SDD，将其翻译为三地址码， \\[\\begin{array}{llll} B \u0026\\rightarrow{} B_{1}\\ ||\\ B_{2} \u0026 B_{1}.true \u0026= B.true\\\\ \u0026 \u0026 B_{1}.false \u0026= newlabel()\\\\ \u0026 \u0026 B_{2}.true \u0026= B.true\\\\ \u0026 \u0026 B_{2}.false \u0026= B_{1}.false\\\\ \u0026 \u0026 B.code \u0026= B_{1}.code\\ ||\\ label(B_{1}.false)\\ ||\\ B_{2}.code\\\\ B \u0026\\rightarrow{} B_{1}\\ \\\u0026\\\u0026\\ B_{2} \u0026 B_{1}.true \u0026= newlabel()\\\\ \u0026 \u0026 B_{1}.false \u0026= B.false\\\\ \u0026 \u0026 B_{2}.true \u0026= B.true\\\\ \u0026 \u0026 B_{2}.false \u0026= B.false\\\\ \u0026 \u0026 B.code \u0026= B_{1}.code\\ ||\\ label(B_{1}.true)\\ ||\\ B_{2}.code\\\\ B \u0026\\rightarrow{} !\\,B_{1} \u0026 B_{1}.true \u0026= B.false\\\\ \u0026 \u0026 B_{1}.false \u0026= B.true\\\\ \u0026 \u0026 B.code \u0026= B_{1}.code\\\\ B \u0026\\rightarrow{} E_{1} \\textbf{comp} E_{2} \u0026 B.code \u0026= E_{1}.code\\ ||\\ E_{2}.code\\\\ \u0026 \u0026 \u0026||\\ gen(‘if’\\ E_{1}.addr\\ \\textbf{comp}.op\\ E_{2}.addr\\ ‘goto’\\ B.true)\\\\ \u0026 \u0026 \u0026||\\ gen(‘goto’\\ B.false)\\\\ B \u0026\\rightarrow{} \\textbf{true} \u0026 B.code \u0026= gen(‘goto’\\ B.true)\\\\ B \u0026\\rightarrow{} \\textbf{false} \u0026 B.code \u0026= gen(‘goto’\\ B.false) \\end{array}\\] B 的其余产生式按照下面的方法翻译： 假定 B 形如 \\(B_{1}\\ ||\\ B_{2}\\)，如果 \\(B_{1}\\) 为真，那么 B 本身为真，因此 \\(B_{1}.true\\) 和 \\(B.true\\) 相同；如果 \\(B_{1}\\) 为假，那么就要对 \\(B_{2}\\) 求值，因此将 \\(B_{1}.false\\) 设置为 \\(B_{2}\\) 的代码标号。此时 \\(B_{2}\\) 的出口等于 B 的出口 \\(B_{1}\\ \\\u0026\\\u0026\\ B_{2}\\) 类似于上一项 不需要为 \\(B\\rightarrow{}!\\,B_{1}\\) 产生新代码，只需要将 B 的真假出口对换 将常量 true 和 false 分别翻译为 \\(B.true\\) 和 \\(B.false\\) 的跳转指令 考虑以下语句 if (x\u003c100 || x\u003e200 \u0026\u0026 x != y) x=0;，可以生成得到如下语法 \\[\\begin{aligned} \u0026 if\\ x\\ \u003c\\ 100\\ \\texttt{goto}\\ L_{2}\\\\ \u0026 \\textbf{goto}\\ L_{3}\\\\ L_{3}:\\quad \u0026 if\\ x\\ \u003e\\ 200\\ \\texttt{goto}\\ L_{4}\\\\ \u0026 \\textbf{goto}\\ L_{1}\\\\ L_{4}:\\quad \u0026 if\\ x\\ !=\\ y\\ \\texttt{goto}\\ L_{2}\\\\ \u0026 \\textbf{goto}\\ L_{1}\\\\ L_{2}:\\quad \u0026 x\\ =\\ 0\\\\ L_{1}:\\quad \u0026 \\end{aligned}\\] 但是在这个生成的语句中，goto L3 是冗余的，下一条语句的标号就是 L3。另外，如果将 L3 和 L4 的 if 换为 ifFalse，那么还可以省去两条 goto，因此生成的最佳代码为 \\[\\begin{aligned} \u0026 if\\ x\\ \u003c\\ 100\\ \\texttt{goto}\\ L_{2}\\\\ \u0026 ifFalse\\ x\\ \u003e\\ 200\\ \\texttt{goto}\\ L_{1}\\\\ \u0026 ifFalse\\ x\\ !=\\ y\\ \\texttt{goto}\\ L_{1}\\\\ L_{2}:\\quad \u0026 x\\ =\\ 0\\\\ L_{1}:\\quad \u0026 \\end{aligned}\\] 有点 lisp 里 when (ifTrue) 和 unless (ifFalse) 那味了。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:6:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#布尔表达式的控制流翻译"},{"categories":["CompilerPrinciple"],"content":" 避免生成冗余的 goto 指令在布尔表达式的控制流翻译中，展示了用 ifFalse 之后，指令自然流向下一个指令，从而减少了一个跳转指令。 通常代码表达式紧跟在布尔表达式之后，通过使用一个特殊标志 fallthrough (直落)，修改控制流语句和布尔表达式的控制流翻译中介绍的 SDD，就可以使控制流从 B 直接流向 S，而不需要跳转。比如将 \\(S\\rightarrow{}\\textbf{if}\\ (\\,B\\,)\\ S_{1}\\) 新的语义规则： \\[\\begin{aligned} B.true \u0026= \\texttt{fallthrough}\\\\ B.false \u0026= S_{1}.next\\ =\\ S.next\\\\ S.code \u0026= B.code\\ ||\\ S_{1}.code \\end{aligned}\\] 现在尝试修改布尔表达式的语义规则，使其尽可能允许控制流直落。在 B.true 和 B.false 都是显示的标号时，也就是说都不是 fallthrough 时，\\(B\\rightarrow{}E_{1}\\ \\textbf{comp}\\ E_{2}\\) 将产生新的语义规则。如果 B.true 是显示的标号而 B.false 是 fallthrough，将产生一条 if 指令确保条件为假时控制流可以直落；反之产生一条 ifFalse 指令。如果都是 fallthrough 将不产生任何跳转指令。新语义规则如下： test = E1.addr comp.op E2.addr s = if B.true != fallthrough and B.false != fallthrough then gen(‘if’ test ‘goto’ B.true) || gen(‘goto’ B.false) else if B.true != fallthrough then gen(‘if’ test ‘goto’ B.true) else if B.false != fallthrough then gen(‘ifFalse’ test ‘goto’ B.false) else ‘’ B.code = E1.code || E2.code || s 但是在短路运算中，会稍微不同。比如 \\(B\\rightarrow{}B_{1}\\ ||\\ B_{2}\\)，如果 \\(B.true\\) 为 fallthrough 那么 B 为真是会直落到之后的语句。但是 \\(B_{1}\\) 为真时会短路该表达式，必须进行转跳而跳过 \\(B_{2}\\)，直接到达 B 的下一条指令。而 \\(B_{1}\\) 为假时，需要由 \\(B_{2}\\) 来决定表达式的值，因此需要保证 \\(B_{1}.false\\) 可以正确由 \\(B_{1}\\) 直落到 \\(B_{2}\\)。新的语义规则如下 B1.true = if B.true != fallthrough then B.true else newlabel() B1.false = fallthrough B2.true = B.true B2.false = B.false B.code = if B.true != fallthrough then B1.code || B2.code else B1.code || B2.code || label(B1.true) ","date":"05-28","objectID":"/2022/compilerprinciple_008/:6:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#避免生成冗余的-goto-指令"},{"categories":["CompilerPrinciple"],"content":" 布尔值与转跳代码对于一个只是求值布尔表达式，而非控制流中的布尔表达式，就不能像控制流中如此实现。因此可以 使用两趟处理法。在构造出完整的抽象语法树后，进行深度优先遍历，依据语义计算得到相应的翻译结果 对语句进行一趟处理，对表达式进行两趟处理。 在布尔表达式上，可以为它们生成转跳代码，并在出口处将 true 和 false 赋值给临时变量。比如 \\(x = a \u003c b \\\u0026\\\u0026 c \u003c d\\) 可以实现为 \\[\\begin{aligned} \u0026 ifFalse\\ a\\ \u003c\\ b\\ \\texttt{goto}\\ L_{1}\\\\ \u0026 ifFalse\\ c\\ \u003c\\ d\\ \\texttt{goto}\\ L_{1}\\\\ \u0026 t\\ =\\ true\\\\ \u0026 \\texttt{goto}\\ L_{2}\\\\ L_{1}:\\quad \u0026 t\\ =\\ false\\\\ L_{2}:\\quad \u0026 x\\ =\\ t \\end{aligned}\\] ","date":"05-28","objectID":"/2022/compilerprinciple_008/:6:5","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#布尔值与转跳代码"},{"categories":["CompilerPrinciple"],"content":" 回填回填 (backpatching) 使用转跳指令组成的列表作为综合属性，在生成转跳指令时暂时不指定转跳指令的目标，而是在确定目标标号时填充这些目标标号。同一个列表中的所有转跳指令具有相同的标号。 回填技术可以用在一趟式扫描中完成对布尔表达式或控制流语句的目标代码生成。虽然目标代码的形式与前文介绍的相同，但处理标号的方式不同。回填可以使用 B.truelist 和 B.falselist 来管理布尔表达式的转跳代码的标号。控制语句中的 S.nextlist 是用来管理下一跳的代码标号。在生成这些代码时，标号字段是尚未填写的，将这些不完整的转跳指令保存在指令列表中。 在实现上，主要由三个函数完成 makelist(label) 生成只包含转跳到 label 的指令列表 merge(l1, l2) 将两个列表合并 backpatch(l, label) 将 label 作为目标标号插入列表中的各个指令中 为布尔表达式构造自底向上分析的文法。 \\[\\begin{aligned} B \u0026\\rightarrow{} B_{1}\\,||\\,M\\,B_{2}\\ |\\ B_{1}\\,\\\u0026\\\u0026\\,M\\,B_{2}\\ |\\ !\\,B_{1}\\ |\\ (\\,B_{1}\\,)\\ |\\ E_{1}\\,\\textbf{comp}\\,E_{2}\\ |\\ \\textbf{true}\\ |\\ \\textbf{false}\\\\ M \u0026\\rightarrow{} \\varepsilon \\end{aligned}\\] 针对该文法，重新设计 SDT。 \\[\\begin{array}{llll} B \u0026\\rightarrow{} B_{1}\\ ||\\ M\\,B_{2} \u0026 \\{ \u0026 backpatch(B_{1}.falselist,\\,M.instr);\\\\ \u0026 \u0026 \u0026 B.truelist\\ =\\ merge(B_{1}.truelist,\\,B_{2}.truelist);\\\\ \u0026 \u0026 \u0026 B.falselist\\ =\\ B_{2}.falselist;\\ \\ \\}\\\\ B \u0026\\rightarrow{} B_{1}\\ \\\u0026\\\u0026\\ M\\,B_{2} \u0026 \\{ \u0026 backpatch(B_{1}.truelist,\\,M.instr);\\\\ \u0026 \u0026 \u0026 B.truelist\\ =\\ B_{2}.truelist;\\\\ \u0026 \u0026 \u0026 B.falselist\\ =\\ merge(B_{1}.falselist,\\,B_{2}.falselist);\\ \\ \\}\\\\ B \u0026\\rightarrow{} !\\,B_{1} \u0026 \\{ \u0026 B.truelist\\ =\\ B_{1}.falselist;\\\\ \u0026 \u0026 \u0026 B.falselist\\ =\\ B_{1}.truelist;\\ \\ \\}\\\\ B \u0026\\rightarrow{} (\\,B_{1}\\,) \u0026 \\{ \u0026 B.truelist\\ =\\ B_{1}.truelist;\\\\ \u0026 \u0026 \u0026 B.falselist\\ =\\ B_{1}.falselist;\\ \\ \\}\\\\ B \u0026\\rightarrow{} E_{1}\\ \\textbf{comp}\\ E_{2} \u0026 \\{ \u0026 B.truelist\\ =\\ makelist(nextinstr);\\\\ \u0026 \u0026 \u0026 B.falselist\\ =\\ makelist(nextinstr + 1);\\\\ \u0026 \u0026 \u0026 gen(‘if’\\ E_{1}.addr\\ \\textbf{comp}.op\\ E_{2}.addr\\ ‘goto\\ \\_’);\\\\ \u0026 \u0026 \u0026 gen(‘goto\\ \\_’);\\ \\ \\}\\\\ B \u0026\\rightarrow{} \\textbf{true} \u0026 \\{ \u0026 B.truelist\\ =\\ makelist(nextinstr);\\\\ \u0026 \u0026 \u0026 gen(‘goto\\ \\_’);\\ \\ \\}\\\\ B \u0026\\rightarrow{} \\textbf{false} \u0026 \\{ \u0026 B.falselist\\ =\\ makelist(nextinstr);\\\\ \u0026 \u0026 \u0026 gen(‘goto\\ \\_’);\\ \\ \\}\\\\ M \u0026\\rightarrow{} \\varepsilon \u0026 \\{ \u0026 M.instr\\ =\\ nextinstr;\\ \\ \\} \\end{array}\\] 控制转移语句也可以用类似的方法实现。 ","date":"05-28","objectID":"/2022/compilerprinciple_008/:7:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"中间代码生成","uri":"/2022/compilerprinciple_008/#回填"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第五章读书笔记","date":"05-19","objectID":"/2022/compilerprinciple_007/","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/"},{"categories":["CompilerPrinciple"],"content":"最通用的语法制导翻译的方法是先通过构造一棵语法分析树，然后通过访问这棵树的各个结点来计算结点的属性值。在很多情况下，翻译可以在语法扫描分析期间完成，不需要构造出明确的语法分析树。语法制导翻译主要有两类： L 属性翻译 (从左到右)，可以在语法分析过程中完成的翻译方案 S 属性翻译 (综合)，可以很容易与自底向上语法分析过程联系起来 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:0:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#"},{"categories":["CompilerPrinciple"],"content":" 语法制导定义语法制导定义 (Syntax-Directed Definition, SDD) 是一个上下文无关文法 (Context-Free Grammar, CFG) 和属性及规则的结合，属性和文法符号相关联，而规则和产生式相关联。如果 X 是一个符号而 a 是 X 的一个属性，那么我们用 X.a 表示 a 在某个标号为 X 的分析树结点上的值。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#语法制导定义"},{"categories":["CompilerPrinciple"],"content":" 继承属性与综合属性处理非终结符的两种属性： 综合属性 (synthesized attribute) 在分析树结点 N 上的非终结符 A 的综合属性是由 N 上的产生式所关联的语义规则来定义的。这个产生式的头部一定是 A，结点 N 上的综合属性只能通过 N 的子结点或 N 本身的属性值来定义。 继承属性 (inherited attribute) 在分析树结点 N 上的非终结符 B 的继承属性是由 N 的父结点上的产生式所关联的语义规则来定义的。这个产生式的体中必然包含符号 B，结点 N 上的继承属性只能通过 N 的父结点、N 本身和 N 的兄弟结点上的属性来定义。 虽然不允许结点 N 的继承属性通过子结点上的属性来定义，但可以通过结点本身的继承属性定义综合属性。另外，终结符可以由综合属性，但不能有继承属性，它的属性值是由词法分析器提供的词法值，SDD 中没有计算终结符的属性值的语义规则。 比如有一个简单的加乘计算器。 编号 产生式 语义规则 1 \\(L\\rightarrow{}E\\textbf{n}\\) \\(L.val = E.val\\) 2 \\(E\\rightarrow{}E_{1}+T\\) \\(E.val = E_{1}.val + T.val\\) 3 \\(E\\rightarrow{}T\\) \\(E.val = T.val\\) 4 \\(T\\rightarrow{}T_{1}*F\\) \\(T.val = T_{1}.val \\times F.val\\) 5 \\(T\\rightarrow{}F\\) \\(T.val = F.val\\) 6 \\(F\\rightarrow(E)\\) \\(F.val = E.val\\) 7 \\(F\\rightarrow{}\\textbf{digit}\\) \\(F.val = \\textbf{digit}.lexval\\) 产生式 1 设置了整个表达式的值，而产生式 2 的值由它的两个子结点的值求和得来，类似的产生式 4 的值由它的两个子结点的值求积得来。而产生式 7 的值由词法单元返回的数值得来。这个只包含综合属性的 SDD 被称作 S 属性 (S-attribute) SDD，它的头部的非终结符的值都由产生式的体的属性值计算得来。 一个没有副作用的 SDD 也被称为属性文法 (attribute grammar)，一个属性文法的规则仅仅通过其他属性值和常量值来定义一个属性值。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:1:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#继承属性与综合属性"},{"categories":["CompilerPrinciple"],"content":" 在语法分析树的结点上对 SDD 求值在语法分析器上进行求值有助于将 SDD 翻译方案可视化，虽然实际上不需要构建语法分析树。在应用一个 SDD 规则之前首先构造出一棵语法分析树，并用这些规则对树上的各个结点上的所有属性进行求值。一个显示了各个属性值的语法分析树称作注释语法分析树 (annotated parse tree)。 根据上节的产生式和规则，可以构造出一棵语法分析树，对应的非终结符的每个结点都按照自底向上的顺序计算。 在对某个结点的属性进行求值之前，需要先求出这个属性所依赖的所有属性值。比如上一节提到的计算器示例 S-attribute SSD，在求值结点的 val 属性之前就必须求值那个结点的所有子结点的属性值。对于综合属性，可以按照任何自底向上的书序计算属性值 (如 postorder)。对于由综合属性和继承属性的 SDD，不能保证有一个顺序来对各个结点上的属性进行求值。比如产生式 \\(A\\rightarrow{}B\\)，语义规则为 \\(A.s = B.i;\\ {}B.i=A.s+1;\\)，这个规则是循环的，不可能先求出结点的 A.s 或子结点的 B.i 中的一个值。 从计算的角度上看，给定一个 SDD，很难确定是否存在属性值上的循环依赖关系，但存在 SDD 的一个有用子类，它能保证对每棵语法分析树都存在一个求值顺序。 当一棵语法分析树的结构和源代码的抽象语法不匹配时，继承属性是很有用的。因为文法不是为了翻译而定义的，是为了语法分析而进行定义的，因此可能产生不匹配的情况。 现在再实现一个无左递归的 SDD 文法，用于计算 3*5 这样的项。 编号 产生式 语义规则 1 \\(T\\rightarrow{}FT^{’}\\) \\(T^{’}.inh = F.val; T.val = T^{’}.syn\\) 2 \\(T^{’}\\rightarrow{}*FT^{’}_1\\) \\(T^{’}_{1}.inh = T^{’}.inh \\times{} F.val; T^{’}.syn = T^{’}_{1}.syn\\) 3 \\(T^{’}\\rightarrow{}\\varepsilon\\) \\(T^{’}.syn=T^{’}.inh\\) 4 \\(F\\rightarrow{}\\textbf{digit}\\) \\(F.val = \\textbf{digit}.lexval\\) 自顶向下的过程中，第一个输入 3 将与 \\(\\times\\) 不在同一棵子树下，我们需要使用继承属性将运算分量传递给运算符 *。非终结符 T 和 F 各自有一个综合属性 val，终结符 digit 有一个综合属性 lexval，非终结符 \\(T^{’}\\) 有一个继承属性 inh 和一个综合属性 syn。 这些语义规则基于如下思想：运算符 * 的做运算分量是通过继承得到的。也就是说， \\(T^{’}\\rightarrow{}*FT^{’}\\) 的头部继承了产生体中的 * 左运算分量。当递归地处理完毕后，这个结果就通过综合属性传递到树的根部。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:1:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#在语法分析树的结点上对-sdd-求值"},{"categories":["CompilerPrinciple"],"content":" SDD 求值顺序依赖图 (dependency graph) 是一个有用的工具，它可以确定一棵给定的语法分析树中各个属性实例的求值顺序。注释语法分析树显示了各个属性的值，依赖图可以帮我们确定如何计算这些值。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#sdd-求值顺序"},{"categories":["CompilerPrinciple"],"content":" 依赖图依赖图描述了某个语法分析树中的属性实例之间的信息流。从一个属性实例到另一个属性实例的边表示计算第二个属性实例时需要第一个属性实例的值。图中的边表示语义规则所蕴含的约束。详细说明： 对于语法分析树的结点，比如一个标号为文法符号 X 的结点，和 X 关联的每个属性都在依赖图中有一个结点。 假设和产生式 p 关联的语义规则通过 X.c 的值定义了综合属性 A.b 的值 (还可能用到其他属性值)。那么相应依赖图中有一条从 X.c 到 A.b 的边。 假设和产生式 p 关联的一个语义规则通过 X.a 的值定义了继承属性 B.c 的值。那么在相应依赖图中有一条从 X.a 到 B.c 的边。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:2:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#依赖图"},{"categories":["CompilerPrinciple"],"content":" 属性求值顺序依赖图刻画了对一棵语法分析树中不同结点上的属性求值时可能采取的顺序。如果依赖图中有一条从结点 M 到结点 N 的边，那么先对 M 对应的属性求值，再对 N 对应的属性求值。因此，所有的棵性求值顺序就是满足下列条件的结点顺序 \\(N_{1}, N_{2}, \\cdots, N_{k}\\)；如果有一条从结点 \\(N_{i}\\) 到 \\(N_{j}\\) 的依赖图的边，那么 \\(i \u003c j\\)。这样的排序将一个有向图变成了线性排序，即图的拓扑排序 (topological sort)。 如果依赖图中存在任意的环，将不存在拓扑排序，即没办法对相应的 SDD 求值；如果图中没有没有环，那么总能找到至少一个拓扑排序。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#属性求值顺序"},{"categories":["CompilerPrinciple"],"content":" S 属性的定义有些特定类型的 SDD 不允许产生带环的依赖图，且有两类还可以和自顶向下以及自底向上语法分析过程一起高效实现的 SSD，即之前提到的 S-attribute 和 L-attribute。 如果一个 SDD 的每个属性都是综合属性，那么这个 SDD 就是 S-attribute Definition。 S-attribute SDD 可以按照分析树的结点，以任何自底向上的顺序计算各个属性值。最简单的方式即后序遍历语法分析树。因此 S-attribute SDD 可以在自底向上语法分析的过程中实现。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#s-属性的定义"},{"categories":["CompilerPrinciple"],"content":" L 属性的定义L-attribute SSD 的思想是在一个产生式体所关联的各个属性之间，依赖图的边总是从左到右。也就是说每个属性必须： 要么是一个综合属性； 要么是一个继承属性。但这个继承属性有如下规则限制。假设存在一个产生式 \\(A\\rightarrow{}X_{1}X_{2}\\cdots{}X_{n}\\)，且有一个通过这个产生式关联的规则计算得到的继承属性 \\(X_{i}.a\\)，那么这个规则只能使用 和产生式的头部 A 关联的继承属性 位于 \\(X_{i}\\) 左边的文法符号实例 \\(X_{1}, X_{2}, \\cdots, X_{i-1}\\) 相关的继承属性或综合属性 和这个 \\(X_{i}\\) 实例本身相关的继承属性或综合属性，但是在由这个 \\(X_{i}\\) 的全部属性组成的依赖图中不存在环 因此可以确定，之前乘法文法的规则，是一个 L-attribute Definition SSD。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:2:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#l-属性的定义"},{"categories":["CompilerPrinciple"],"content":" 具有受控副作用的语义规则有时一个语义规则可能出现副作用，比如打印一个结果，或将一个标识符类型加入到符号表中。 对于 SDD，需要在属性文法与翻译方案之间找到一个平衡点。属性文法没有副作用，并支持任何与依赖图一致的求值顺序。翻译方案要求从左向右顺序求值，并允许语义行为包含任何代码片段。那么我们可以用以下方法之一控制 SDD 中的副作用： 支持那些不会对属性求值产生约束的附带副作用。即如果按照依赖图的任何拓扑排序进行属性求值，最终都可以产生正确的翻译结果，那么就允许副作用存在。 对允许的求值顺序添加约束，使得以任何允许的顺序求值都会产生相同的翻译结果。这个约束可以看作隐含加入到依赖图的边。 简单实现一个声明 D，可以声明基本类型 T (int 或 float)，后跟一个标识符列表。假设每个标识符录入符号表条目中。假设一个标识符的类型不会影响其他标识符对应的符号表条目，条目可以按照任意顺序进行更新。另外，SDD 也不检查标识符是否被声明了多次。 编号 产生式 语义规则 1 \\(D\\rightarrow{}TL\\) \\(L.inh=T.type\\) 2 \\(T\\rightarrow{}\\textbf{int}\\) \\(T.type=integer\\) 3 \\(T\\rightarrow{}\\textbf{float}\\) \\(T.type=float\\) 4 \\(L\\rightarrow{}L_{1}\\,\\textit{,}\\,\\textbf{id}\\) \\(L_{1}.inh=L.inh; addType(\\textbf{id}.entry, L.inh)\\) 5 \\(L\\rightarrow{}\\textbf{id}\\) \\(addType(\\textbf{id}.entry, L.inh)\\) 需要注意的是 addType 的参数 id.entry: 在词法分析中得到的一个指向某个符号表对象的值 L.inh: 标识符的类型值 因此 addType 可以正确的将 id 所代表的标识符类型设置为 L.inh。也可以认为调用 addType 是设置该结点的哑属性。比如输入串 float f1, f2, f3，我们依据此输入构建依赖图。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:2:5","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#具有受控副作用的语义规则"},{"categories":["CompilerPrinciple"],"content":" 语法制导翻译的应用语法制导的翻译技术通常用于类型检查与中间代码生成。本节主要应用与抽象语法树的构造。为了完成到中间代码的翻译，编译器接下来可能使用一组规则来编译这棵语法树 (实际建立在 SSD 上)。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:3:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#语法制导翻译的应用"},{"categories":["CompilerPrinciple"],"content":" 抽象语法树的构建一棵语法树中的每个结点代表一个程序构造，这个结点的子结点代表这个构造有意义的组成部分。比如表达式 \\(E_{1}+E_{2}\\) 的语法树结点的标号为 \\(+\\)，两个子结点分别代表子表达式 \\(E_{1}\\) 和 \\(E_{2}\\)。 我们将使用具有适当数量的字段的对象来实现一棵语法分析树的各个结点。每个对象将有一个 op 字段，即这个结点的标号。这些对象将具有如下所述的其他字段： 如果结点是叶结点，那么对象将有一个附加字段来存放这些叶节点的词法值。构造函数 Leaf(op, val) 创建一个叶对象。也可以把结点看作记录，那么 Leaf 可以返回指向叶结点对应的新记录的指针。 如果结点是内部结点，那么它的附加字段的个数与该结点在语法分析树中的子结点个数相同。构造函数 Node 带有两个或多个参数 Node(op, c1, c2, ..., ck)。 以下示例为 S 属性定义为一个简单的表达式文法构造出语法树，这个文法只有二元运算符 \\(+\\) 和 \\(-\\)，通常这两个运算符具有相同优先级且都是左结合。所有非终结符都有一个综合属性 node，表示相应抽象语法树结点 编号 产生式 语义规则 1 \\(E\\rightarrow{}E_{1}+T\\) \\(E.node=\\textbf{new}\\ Node(’+’, E_{1}.node, T.node)\\) 2 \\(E\\rightarrow{}E_{1}-T\\) \\(E.node=\\textbf{new}\\ Node(’-’, E_{1}.node,T.node)\\) 3 \\(E\\rightarrow{}T\\) \\(E.node=T.node\\) 4 \\(T\\rightarrow(E)\\) \\(T.node=E.node\\) 5 \\(T\\rightarrow{}\\textbf{id}\\) \\(T.node=\\textbf{new}\\ Leaf(\\textbf{id}, \\textbf{id}.entry)\\) 6 \\(T\\rightarrow{}\\textbf{num}\\) \\(T.node=\\textbf{new}\\ Leaf(\\textbf{num}, \\textbf{num}.val)\\) 比如说输入 \\(a-4+c\\) 构造一棵抽象语法树，这棵抽象语法树的结点被显示为记录，这些记录的第一个字段是 op。现在抽象语法树的边用实线表示，基础的语法分析树的边用点状虚线表示 (并没有真的生成它)，最后一种线状虚线表示 \\(E.node\\) 和 \\(T.node\\) 的值，每条线都指向适当的抽象语法树的结点。 在自底向上分析过程中，我们可以得到如下的抽象语法树构造步骤。 \\[\\begin{aligned} p_{1}\u0026={}\\textbf{new}\\ Leaf(\\textbf{id}, entry-a);\\\\ p_{2}\u0026={}\\textbf{new}\\ Leaf(\\textbf{num}, 4);\\\\ p_{3}\u0026={}\\textbf{new}\\ Node(’-’, p_{1}, p_{2});\\\\ p_{4}\u0026={}\\textbf{new}\\ Leaf(\\textbf{id}, entry-c);\\\\ p_{5}\u0026={}\\textbf{new}\\ Node(’+’, p_{3}, p_{4}); \\end{aligned}\\] 如果改用自顶向下语法分析，得到的抽象语法树是相同的，其构造步骤也相同，但语法分析树的构造与抽象语法树的构造有极大不同。 编号 产生式 语义规则 1 \\(E\\rightarrow{}TE^{’}\\) \\(E.node=E^{’}.syn;\\ E^{’}.inh=T.node\\) 2 \\(E^{’}\\rightarrow{}+TE^{’}_{1}\\) \\(E^{’}_{1}.inh={}\\textbf{new}\\ Node(’+’,E^{’}.inh,T.node); E^{’}.syn=E^{’}_{1}.syn\\) 3 \\(E^{’}\\rightarrow{}-TE^{’}_{1}\\) \\(E^{’}_{1}.inh={}\\textbf{new}\\ Node(’-’,E^{’}.inh,T.node); E^{’}.syn=E^{’}_{1}.syn\\) 4 \\(E^{’}\\rightarrow{}\\varepsilon\\) \\(E^{’}.syn={}E^{’}.inh\\) 5 \\(T\\rightarrow{}(E)\\) \\(T.node=E.node\\) 6 \\(T\\rightarrow{}\\textbf{id}\\) \\(T.node={}\\textbf{new}\\ Leaf(\\textbf{id},\\textbf{id}.entry)\\) 7 \\(T\\rightarrow{}\\textbf{num}\\) \\(T.node={}\\textbf{new}\\ Leaf(\\textbf{num},\\textbf{num}.val)\\) 这个在语法分析树的结点上对 SDD 求值提到的简易乘加计算器类似，通过继承属性将左边的计算结果进行传递。对于同样的表达式 a-4+c 将有不一样的依赖图。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:3:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#抽象语法树的构建"},{"categories":["CompilerPrinciple"],"content":" 类型的结构当语法分析树的结构与输入的抽象语法树结构不同时，继承属性是很有用的。这种情况下，继承属性可以用来将信息从语法分析树的一部分传递到另一部分。 比如 C 语言中的 int[2][3]，相应的表达式可以是 array(2, array(3, integer))。因此我们可以尝试用以下的 SDD 来构造语法分析树。 产生式 语义规则 \\(T\\rightarrow{}BC\\) \\(T.t=C.t;\\ C.b=B.t\\) \\(B\\rightarrow{}\\textbf{int}\\) \\(B.t=integer\\) \\(B\\rightarrow{}\\textbf{float}\\) \\(B.t=float\\) \\(C\\rightarrow{}[\\textbf{num}]C_{1}\\) \\(C.t=array(\\textbf{num}.val, C_{1}.t);\\ C_{1}.b=C.b\\) \\(C\\rightarrow{}\\varepsilon\\) \\(C.t=C.b\\) 非终结符 B 和 T 有一个表示类型的综合属性 t，非终结符 C 有两个属性：继承属性 b 和综合属性 t。继承属性 b 将一个基本类型沿树向下传播，而综合属性 t 则收集最终得到的结果。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:3:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#类型的结构"},{"categories":["CompilerPrinciple"],"content":" 语法制导的翻译方案语法制导的翻译方案 (Syntax-Directed Translation Scheme, SDT) 是语法制导定义的一种补充，是在其产生式体中嵌入了程序片段的一个上下文无关文法，这些片段称为语义动作，它们可以出现在产生式体的任何地方。任何 SDT 都可以通过下面的方式实现：首先构造一棵语法分析树，然后按照从左到右的深度优先顺序来执行这些动作，也就是说在一个前序遍历过程中执行。 通常 SDT 在语法分析的过程中实现，不会真的构造一棵语法分析树。但着重点我们放在两类 SDD 上： 基本文法可以用 LR 技术分析，且是 S-attribute SDD 基本文法可以用 LL 技术分析，且是 L-attribute SDD 在语法分析过程中实现的 SDT 可以按照如下的方式识别：将每个内嵌的语义动作替换为一个独有的非终结符 (marker nonterminal)。每个标记非终结符 M 只有一个产生式 \\(M\\rightarrow{}\\varepsilon\\)。如果带有标记非终结符的文法可以使用某个方法进行语法分析，那么这个 SDT 就可以在语法分析过程中实现。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:4:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#语法制导的翻译方案"},{"categories":["CompilerPrinciple"],"content":" 后缀翻译方案最简单的实现 SDD 的情况是第一种文法，即可以用 LR 技术分析，且是 S-attribute SDD。这种情况下我们可以构造出一个 SDT，其中的每个动作都放在产生式的最后，并在归约为头部的时候执行这个动作。所有动作都在产生式最右端的 SDT 称为后缀翻译方案。 后缀 SDT 当归约发生时执行相应的语义动作。各个文法符号的属性值可以放到栈中的某个位置，使得执行归约的时候可以找到它们，最好的方法就是将属性和文法符号一起放入栈的记录里。 如果所有属性都是综合属性，且所有动作都位于产生式某位，那么我们可以在把产生式体归约成产生式头的时候计算各个属性的值。 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:4:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#后缀翻译方案"},{"categories":["CompilerPrinciple"],"content":" 产生式内部带有语义动作的 SDT动作可以放在产生式体中的任何位置上。当一个动作左边的所有符号都被处理后，该动作立即执行。因此，如果有一个产生式 \\(B\\rightarrow{}X\\{a\\}Y\\)，那么我们识别的 X 或者所有从 X 推导出的终结符之后，动作 a 就会执行。即： 如果语法分析过程是自底向上的，那么我们在 X 的此次出现位于语法分析栈的栈顶时，立即执行动作 a 如果语法分析过程是自顶向下的，那么在试图展开 Y 的本次出现或输入中检测 Y 之前执行动作语义 a 可以在语法分析过程中实现的 SDT 包括后缀 SDT 和 实现 L 属性的 SDD 介绍的 L-attribute 定义 SDT。但不是所有的 SDT 都可以在语法分析过程中实现。 作为 SDT 的极端例子，不能在自顶向下或自底向上的语法分析过程中实现这个 SDT。因此语法分析程序必须在它还不知道出现在输入中的运算符是 \\(*\\) 还是 \\(+\\) 的时候，就执行打印这些操作。 编号 产生式 1 \\(L\\rightarrow{}E\\textbf{n}\\) 2 \\(E\\rightarrow{}\\texttt{\\{\\,print(’+’);\\,\\}}\\ E_{1}+T\\) 3 \\(E\\rightarrow{}T\\) 4 \\(T\\rightarrow{}\\texttt{\\{\\,print(’-’);\\,\\}}\\ T_{1}*F\\) 5 \\(T\\rightarrow{}F\\) 6 \\(F\\rightarrow{}(E)\\) 7 \\(F\\rightarrow{}\\textbf{digit}\\ \\texttt{\\{\\,print(}\\textbf{digit}\\texttt{.lexval);\\,\\}}\\) 任何 SDT 都可以按照下列方法实现： 忽略语义动作，对输入进行语法分析，并产生一棵语法分析树 检查每个内部结点 N，假设它的产生式为 \\(A\\rightarrow{}\\alpha\\)，将 \\(\\alpha\\) 中的各个动作当作 N 的附加结点加入，使得 N 的子结点从左到右和 \\(\\alpha\\) 中的符号及动作完全一致 对这棵语法分析树进行前序遍历，且当访问到一个以某个动作为标号的结点时立刻执行这个动作 现在构造表达式 \\(3*5+4\\) 的语法分析树，可以按照构造的语法分析树得到这个前缀形式 \\(+\\,*\\,3\\,5\\,4\\) ","date":"05-19","objectID":"/2022/compilerprinciple_007/:4:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#产生式内部带有语义动作的-sdt"},{"categories":["CompilerPrinciple"],"content":" 从 SDT 中消除左递归由于左递归文法不能在自顶向下的语法分析中进行，因此有了消除左递归的算法。当文法是 SDT 的一部分时，还需要考虑如何处理其中的动作。 最简单的情况下，只关心一个 SDT 的动作的执行顺序的情况。如果每个动作只打印一个字符串，那就关心的是打印字符串的顺序。 当转换文法的时候，将动作当成终结符好处理。基于这个思路，文法转换保存了由文法生成的符号串中终结符的顺序，因此动作在任何从左到右分析过程中都按照相同的顺序执行 (无论是 LR 还是 LL)。 消除左递归在 LL 文法中讲过。比如将 \\(A\\rightarrow{}A\\alpha\\,|\\,\\beta\\) 转换为 \\[\\begin{aligned} A \u0026\\rightarrow{} \\beta{}R\\\\ R \u0026\\rightarrow{} \\alpha{}R\\,|\\,\\varepsilon \\end{aligned}\\] 如过将其应用到一个带有动作的产生式 \\(E \\rightarrow{} E_{1} + T\\ \\{\\,\\texttt{print}(’+’);\\,\\} \\ |\\ T\\)，消除左递归后得到 \\[\\begin{aligned} E \u0026\\rightarrow{} TR\\\\ R \u0026\\rightarrow{} + T\\ \\{\\,\\texttt{print}(’+’);\\,\\}\\ R\\\\ R \u0026\\rightarrow{} \\varepsilon \\end{aligned}\\] 但是这种方式在计算 S-attribute SDD 时没有什么问题，但计算 L-attribute SDD 时需要非常小心。好消息是，可以实现一个通用的，解决单个递归产生式、单个非递归产生式并该左递归非终结符只有单个属性的方案。可以将此方案推广到多个递归 / 非递归产生式，但实现起来非常麻烦。 假设 \\[\\begin{aligned} A \u0026\\rightarrow{} A_{1}Y\\ \\{\\,A.a\\,=\\,g(A_{1}.a,Y.y)\\,\\}\\\\ A \u0026\\rightarrow{} X\\ \\{\\,A.a\\,=\\,f(X.x)\\,\\} \\end{aligned}\\] 基础文法可以消除左递归改为 \\[\\begin{aligned} A \u0026\\rightarrow{} XR\\\\ R \u0026\\rightarrow{} YR\\,|\\,\\varepsilon \\end{aligned}\\] 可以看出无论是在原文法上应用后缀 SDT 还是消除左递归后应用 SDT，其结果都是相同的。只不过消除左递归后，还需要一个综合属性 R.s 沿树向上拷贝。 最终可以得到 SDT \\[\\begin{aligned} A \u0026\\rightarrow{} X\\ \\{\\,R.i\\,=\\,f(X.x);\\,\\}\\ R\\ \\{\\,A.a\\,=\\,R.s;\\,\\}\\\\ R \u0026\\rightarrow{} Y\\ \\{\\,R_{1}.i\\,=\\,g(R.i,Y.y);\\,\\}\\ R_{1}\\ \\{\\,R.s\\,=\\,R_{1}.s;\\,\\}\\\\ R \u0026\\rightarrow{} \\varepsilon\\ \\{\\,R.s\\,=\\,R.i;\\,\\} \\end{aligned}\\] ","date":"05-19","objectID":"/2022/compilerprinciple_007/:4:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#从-sdt-中消除左递归"},{"categories":["CompilerPrinciple"],"content":" L 属性定义的 SDT只要文法是 LR 的，就能保证 S-attribute SDD 转换成后缀 SDT，后缀 SDT 可以正确的按照自底向上的方式进行语法分析和翻译。 现在我们考虑更加一般化的情况，L-attribute SDD。基础文法假设采用自顶向下的方式进行语法分析，只需要将动作附加到一棵语法分析树中，并对其进行前序遍历时完成动作。因此我们可以用以下规则将 L-attribute SDD 转换到 SDT： 把计算某个非终结符 A 的继承属性的动作插入到产生式体中紧靠在 A 的本次出现之前的位置上。 将计算一个产生式头的综合属性动作放在最右端。 比如 C 语言的 while 语句 \\(S \\rightarrow{} \\textbf{while}\\, ( C)\\, S_{1}\\)， 继承属性 \\(S.next\\) 是必须在 S 执行结束之后执行的代码的开始处标号 综合属性 \\(S.code\\) 是中间代码序列，实现了语句 S 并在最后转跳到 \\(S.next\\) 继承属性 \\(C.true\\) 是必须在 C 为真时执行的代码的开始处标号 继承属性 \\(C.false\\) 是必须在 C 为假时执行的代码的开始处标号 综合属性 \\(C.code\\) 是一个中间代码序列，实现了表达式 C，并根据 C 的值转跳到 \\(C.true\\) 或 \\(C.false\\) 实现的 SDD 类似 \\[\\begin{aligned} S\\rightarrow{} \\textbf{while}\\,( C)\\,S_{1} \u0026\\qquad L_{1} = new();\\\\ \u0026\\qquad L_{2} = new();\\\\ \u0026\\qquad S_{1}.next = L_{1};\\\\ \u0026\\qquad C.false = S.next;\\\\ \u0026\\qquad C.true = L_{2};\\\\ \u0026\\qquad S.code = \\textbf{label}\\ ||\\ L_{1} \\ ||\\ C.code\\ ||\\ \\textbf{label} \\ ||\\ L_{2}\\ ||\\ S_{1}.code \\end{aligned}\\] 当然最后的 \\(||\\) 表示连接各代码片段的符号。这个 SDD 是 L 属性的，因此转换为 SDT 时还需要考虑变量 \\(L_{1}\\) 和 \\(L_{2}\\)。如果将语义动作当作哑非终结符来处理，那么变量可以当作其综合属性处理。由于不依赖于其他属性，因此可以分配到表达式的第一个语义动作中。 \\[\\begin{aligned} S\\rightarrow{} \u0026\\textbf{while}\\,( \u0026 \\{\\,L_{1}=new();\\ L_{2}=new();\\ C.false=S.next;\\ C.true=L_{2};\\,\\}\\\\ \u0026 C\\,) \u0026 \\{\\,S_{1}.next=L_{1};\\,\\}\\\\ \u0026 S_{1} \u0026 \\{\\,S.code=\\textbf{label}\\ ||\\ L_{1} \\ ||\\ C.code\\ ||\\ \\textbf{label} \\ ||\\ L_{2}\\ ||\\ S_{1}.code\\,\\} \\end{aligned}\\] ","date":"05-19","objectID":"/2022/compilerprinciple_007/:4:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#l-属性定义的-sdt"},{"categories":["CompilerPrinciple"],"content":" 实现 L 属性的 SDD","date":"05-19","objectID":"/2022/compilerprinciple_007/:5:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#实现-l-属性的-sdd"},{"categories":["CompilerPrinciple"],"content":" 在递归下降语法分析中进行翻译可以按照如下方法将一个语法分析器扩展成一个翻译器 函数 A 的参数是非终结符 A 的继承属性 函数 A 的返回值是非终结符 A 的综合属性集合。在函数内要进行语法分析并处理属性 决定用哪个产生式展开 A 需要读入终结符时，在输入中检查这些符号是否出现 在局部变量中保存所有必要的属性值 调用对应于被选定非终结符的函数，并提供正确的参数 类似的，可以写出关于上一个示例 while 的伪代码 string S(label next): string Scode, Ccode; label L1, L2; if curr_input is while: read curr_input; check next punctuation is ‘(’, and read it; L1 = new(); L2 = new(); Ccode = C(next, L2); check next punctuation is ‘)’, and read it; Scode = S(L1); return “label” || L1 || Ccode || “label” || L2 || Scode; else: pass ","date":"05-19","objectID":"/2022/compilerprinciple_007/:5:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#在递归下降语法分析中进行翻译"},{"categories":["CompilerPrinciple"],"content":" 边扫描边生成代码使用属性来构造代码并构造出很长的串，代价是很大的。通常在代码生成的时，执行一个 SDT 语义动作，逐步将各个代码片段加入到缓冲区或文件中。为了实现这个功能，下列要素必不可少： 存在一个 (一个或多个非终结符的) 主属性 主属性是综合属性 对主属性求值规则保证： a. 主属性是将相关产生式体中的非终结符的主属性值连接起来得到的 b. 各个非终结符的主属性值在连接运算中出现的顺序，和这些非终结符在产生式体中出现的顺序相同 现在用 print 将各个元素打印出来，修改为边扫描边生成的函数 现在我们可以实现相应的 SDT \\[\\begin{aligned} S\\rightarrow{} \u0026\\textbf{while}\\,( \u0026 \\{\\,L_{1}=new();\\ L_{2}=new();\\ C.false=S.next;\\ C.true=L_{2};\\ print(“label”, L_{1});\\,\\}\\\\ \u0026 C\\,) \u0026 \\{\\,S_{1}.next=L_{1};\\ print(“label”, L_{2});\\,\\}\\\\ \u0026 S_{1} \u0026 \\end{aligned}\\] ","date":"05-19","objectID":"/2022/compilerprinciple_007/:5:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#边扫描边生成代码"},{"categories":["CompilerPrinciple"],"content":" L 属性的 SDD 和 LL 语法分析假设一个 L 属性的 SDD 的基础文法是 LL 文法，且按照 L 属性定义的 SDT 一节将其转换为一个 SDT，其语义动作被嵌入到各个产生式中。可以 LL 语法分析中完成翻译过程，需要扩展语法分析栈来存放语义动作和属性求值所需的某些数据项。额外保存动作记录 (action-record) 和综合记录 (synthesize-record)，其中前者是即将执行的语义动作，而后者保存的是非终结符的综合属性。 非终结符 A 的继承属性放在表示这个非终结符的栈记录中，对这些属性求值的代码通常在仅靠 A 的上面。 非终结符 A 的综合属性单独的存放在仅靠 A 的下面。 还是用 while 的例子来说明 对于 while 我们可以回到计算综合属性 S.next，来构建新的语法分析栈 ","date":"05-19","objectID":"/2022/compilerprinciple_007/:5:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#l-属性的-sdd-和-ll-语法分析"},{"categories":["CompilerPrinciple"],"content":" L 属性的 SDD 的自底向上语法分析使用自底向上的方法来完成任何可以用自顶向下的方式完成翻译过程。更准确地说，给定一个 LL 文法为基础的 L 属性 SDD，可以修改为 LR 语法分析为基础的 SDD： 按照 L 属性定义的 SDT 方法构造的 SDT 为起点，在各个非终结符之前计算其继承属性，并在产生式后端动作中计算综合属性 对每个内嵌的语义动作，向这个文法中引入一个标记非终结符来替换它。每个这样的位置都有一个不同的标记，任意标记 M 都有一个产生式 \\(M\\rightarrow{}\\varepsilon\\) 如果标记非终结符在某个产生式 \\(A\\rightarrow{}\\alpha{}\\,\\{a\\}\\,\\beta\\) 中替换了语义动作 \\(a\\)，对 \\(a\\) 进行修改得到 \\(a^{’}\\)，且将 \\(a^{’}\\) 关联到 \\(M\\rightarrow{}\\varepsilon\\) 上。这个动作 \\(a^{’}\\) a. 将动作 a 需要的 A 或 \\(\\alpha\\) 中符号的任何属性作为 M 的继承属性进行拷贝 b. 按照 a 中的方法计算各个属性，但计算得到的属性作为 M 的综合属性 简单地说，假设有产生式 \\(A\\rightarrow{}BC\\)，继承属性 \\(B.i\\) 由 \\(A.i\\) 计算得到。则有 SDT 片段 \\(A\\rightarrow{}\\{\\,B.i=f(A.i);\\,\\}\\ B\\ C\\)。用上述规则修改 SDT，将其修改为 \\[\\begin{aligned} A \u0026\\rightarrow{} M B C\\\\ M \u0026\\rightarrow{} \\{\\,M.i=A.i;\\ M.s=f(M.i);\\,\\} \\end{aligned}\\] 那么对于之前 C 语言的 while 示例，可以改写为 \\[\\begin{aligned} S \u0026\\rightarrow{} \\textbf{while} (\\,M\\,C\\,) N S_{1}\\\\ M \u0026\\rightarrow{} \\varepsilon\\\\ N \u0026\\rightarrow{} \\varepsilon \\end{aligned}\\] ","date":"05-19","objectID":"/2022/compilerprinciple_007/:5:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","IR"],"title":"语法制导翻译","uri":"/2022/compilerprinciple_007/#l-属性的-sdd-的自底向上语法分析"},{"categories":["CompilerPrinciple"],"content":"GinShio | 词法分析软件 Flex 及语法分析软件 Bison 的用法","date":"05-01","objectID":"/2022/flex_and_bison/","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/"},{"categories":["CompilerPrinciple"],"content":"正正规规地开始写实现一个编译器感觉压力还是蛮大的。初步选型是 基础工具 Git CMake gcc 编译器生成工具 Flex GNU Bison 其他工具 Clang Format CppLint EditConfig C++ 支持 17 简直太棒了！！！还不知道毕昇杯能不能用 CMake 去构建。 由于是多人合作项目，代码风格暂时定的是 Google，估计啥都不知道。想想到时候 review 代码就头大。 这篇主要是记录下从 info (Emacs 看 info 真方便) 中学习的 Flex 和 GNU Bison 相关用法。 TL;DR. main 函数示例 或 北大编译实践 ","date":"05-01","objectID":"/2022/flex_and_bison/:0:0","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#"},{"categories":["CompilerPrinciple"],"content":" FlexFlex 可以理解为词法分析器生成工具 Lex 的开源版本，意为 fast lexical analyzer generator。根据描述的正则表达式与 C 代码 (这些被称为 规则)，来生成对应的分析器代码 (文件名默认为 lex.yy.c)，其中定义了接口 yylex() 用来启动分析器，函数原型如下 int yylex(void); Flex 默认会生成标准的 C99 代码，而非 K\u0026R 风格代码。在调用 yylex 时，它会持续从全局输入文件 yyin 中扫描 token，直到遇到 EOF 或 action 执行返回语句。如果 yylex 因 return 停止扫描，可以再次调用扫描器，从中断处继续扫描。当扫描到 EOF 时，只有 yywrap() 返回 0 才继续读取其他文件，返回非零时扫描器会终止并返回 0。如果你不实现 yywrap(),需要使用 %option noyywrap 或链接 -lfl 使用总返回 1 的默认版本。 话说回来，学习词法分析最大的收获应该是知道了正则是怎么运作的，自己哪天心血来潮了说不定就手撕一个。 ","date":"05-01","objectID":"/2022/flex_and_bison/:1:0","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex"},{"categories":["CompilerPrinciple"],"content":" Flex 输入文件的格式输入文件被分为三个部分，两个部分之间使用 %% 分隔。 definitions %% rules %% user code Flex’s definitions 格式定义部分会声明一些 name 方便后续使用，或者声明开始条件。 name 是以字母或下划线 (_) 开头，并在后面跟随零或多个字母、数字、下划线、短线 (-)。声明通常是一个正则表达式，可以看作是 name 的值。语法如下 name definition 有点难懂？那就看一个示例 DIGIT [0-9] ID [a-z][a-z0-9]* 这里定义了两个 name，当然我们可以在其他 name 中使用已声明的 name，比如声明 FLOAT FLOAT {DIGIT}+\".\"{DIGIT}* FLOAT2 ([0-9])+\".\"([0-9])* 这个例子中 FLOAT 与 FLOAT2 的声明是完全等价的。 注释与 C 的多行注释相同。lex 中可以直接写 C 代码，当然要用 %{ 和 %} 包裹起来，这个块将在生成代码时被完全复制。另外 top 块是很重要的一个语法，它会被生成在文件的顶端，在所有的代码之前，因此你可以在 top 块中定义一些感兴趣的宏或文件。top 块示例如下： %top{ /* This code goes at the \"top\" of the generated file. */ #include \u003cstdint.h\u003e #include \u003cinttypes.h\u003e } Flex’s rules 格式规则部分每个规则一行，同样地你可以在 %{ 和 %} 之间的块中写 C 语言代码，Flex 会帮你完全一致的复制到生成文件中。语法如下 pattern action pattern 相当于给定的正则表达式，当输入与 pattern 匹配时将执行 action 所对应的动作。详细说明一下支持的 pattern 用法 x 字符 x . 除换行外的任意字符 [xyz] 一个字符集，可以匹配其中一个字符，示例可以匹配 x 或 y 或 z [abj-oZ] 一个带范围的字符集，匹配其中一个字符。示例可以匹配 a 或 b 或 j 到 o 之间的字符或 Z [^A-Z] 一个否定字符集，匹配其中没有出现的字符。示例可以匹配除了大写字母外的任意字符 [a-z]{-}[aeiou] 一个带除外集的字符集，匹配除了后面集合外的所有出现在集合中的字符。示例可以匹配小写辅音字母 r* 匹配零或多个 r r+ 匹配一或多个 r r? 匹配零或一个 r r{2,5} 匹配二到五个 r r{2,} 匹配二或更多 r r{4} 正好四个 r {name} 匹配第一节中定义的 name [xyz]\\[foo 转义字符 \\123 字符的八进制表示。示例为大写字母 S \\x53 字符的十六进制表示。示例为大写字母 S (r) 利用 () 优先级修改匹配优先级 (?r-s:pattern) 对 pattern 应用 r 模式但不应用 s 模式。其中可选模式如下 i: 不区分大小写 s: 修改 . 语义，匹配任意字符 (包括换行) x: 忽略 pattern 中的注释与空白符。空白符依旧可以出现在 \"\" 中、转义字符、或字符集中。 示例 示例 等价表示 \\((?:foo)\\) \\((foo)\\) \\((?i:ab7)\\) \\(([aA][bB]7)\\) \\((?-i:ab)\\) \\((ab)\\) \\((?s:.)\\) \\(([\\backslash{}x00-\\backslash{}xFF])\\) \\((?ix-s:\\ a\\ .\\ b)\\) \\(([Aa][\\^\\backslash{}n][bB])\\) \\((?x:a\\ b)\\) \\((“ab”)\\) \\((?x:a\\backslash\\ b)\\) \\((“a\\ b”)\\) \\((?x:a\"\\ “b)\\) \\((“a\\ b”)\\) \\((?x:a[\\ ]b)\\) \\((“a\\ b”)\\) \\(\\begin{aligned}(?x: \u0026 a\\\\ \u0026 /* comment */\\\\ \u0026 b\\\\ \u0026 c)\\end{aligned}\\) \\((abc)\\) (?# comment) 忽略 () 中的所有内容，且可以跨多行，但是注释不能有 ) 字符 rs cat r|s or r/s 尾随上下文 (trailing context)，只有在 s 之前的情况下匹配 r，但只进行 r 的行为 ^r 仅在行首匹配 r r$ 仅在行尾匹配 r \u003cs\u003er 仅以 s 开始的 r \u003cs1,s2,s3\u003er 仅以 s1 或 s2 或 s3 开始的 r \u003c*\u003er 以任意条件开始的 r \u003c\u003cEOF\u003e\u003e 文件末尾 \u003cs1,s2\u003e\u003c\u003cEOF\u003e\u003e 以条件 s1 或 s2 开始的文件末尾 在字符串中，除了转义符 (\\)、字符集运算符 (- 和 ]]) 以及行首标记 (^)，其他特殊字符均失去了其特殊意义。 另外上面的 pattern 是根据优先级排列的，最上面的优先级最高。比如表达式 \\(foo|bar*\\) 与表达式 \\((foo)|(ba(r*))\\) 相同。如果想要改为重复 foo 或 bar 数次，可以写为表达式 \\((foo|bar)*\\)。 Flex 支持 POSIX Bracket Expressions，但是没找到相应的 Shorthand 支持。不过从 info 上看，支持了，但没全支持。 POSIX Description ASCII Shorthand [:alnum:] 字母和数字 \\([a-zA-Z0-9]\\) [:alpha:] 字母 \\([a-zA-Z]\\) [:ascii:] ASCII 字符 \\([\\backslash{}x00-\\backslash{}xFF]\\) [:blank:] 空格及 Tab \\([ \\backslash{}t]\\) \\h [:cntrl:] 控制字符 \\([\\backslash{}x00-\\backslash{}x1F\\backslash{}x7F]\\) [:digit:] 数字 \\([0-9]\\) \\d [:graph:] 可见字符 \\([\\backslash{}x21-\\backslash{}x7E]\\) [:lower:] 小写字符 \\([a-z]\\) \\l [:print:] 可见字符及空格 \\([\\backslash{}x20-\\backslash{}x7E]\\) [:punct:] 标点字符 \\([!\"\\backslash\\#\\$\\%\\\u0026’()*+,\\backslash-./:;\u003c=\u003e?@\\backslash[\\backslash\\backslash\\backslash]\\verb!^!\\_\\verb!`!\\{\\mid\\}\\verb!~!]\\) [:space:] 空白字符 \\([\\ \\backslash{}t\\backslash{}r\\backslash{}n\\backslash{}v\\backslash{}f]\\) \\s [:upper:] 大写字符 \\([A-Z]\\) \\u [:word:] 单词字符 \\([a-zA-Z0-9\\_]\\) \\w [:xdigit:] 十六进制字符 \\([a-fA-F0-9]\\) 其中 Flex 不支持 [:ascii:] 和 [:word:] 两个字符集。 在 Flex 配置文件中，字符集将被立即展开，这意味着 flex 环境对字符集感兴趣。 如果添加了大小写不敏感标记，那么 [:upper:] 和 [:lower:] 与 [:alpha:] 等价 有范围的字符类在跨越大小写字符时 (比如 [a-Z])，在不区分大小写的扫描器上应该慎用。此时 Flex 并不知道你是想将所有的大小写字符折叠在一起，还是要指定 ASCII 数值范围。在出现此类问题时，Flex 优先指定数值范围，并发出一个警告。 范围 结果 数值范围 二义性范围 [a-t] ok \\([a-tA-T]\\) [A-T] ok \\([a-tA-T]\\) [A-t] 二义性 \\([A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!a-t]\\) \\([a-tA-T]\\) [_-{] 二义性 \\(\\verb!_!\\verb!`!a-z\\{\\) \\(\\verb!_!\\verb!`!a-zA-Z\\{\\) [@-C] 二义性 \\(@ABC\\) \\(@A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!abc\\) Flex 允许否定 POSIX 字符集，只需要在字符类名称前加上 ^，但是大小写不敏感的扫描器上 [:^upper:] 和 [:^lower:] 会被跳过，它们的含义不清。 操作符 {-} 可以为两个字符集做差，但小心生成一个永远不会被匹配的空集。不过做差的两个集合不必是包含关系，比如 [a-c]{-}[b-z] 的结果为 [a]。 操作符 {+} 求两个字符集的并集。比如 [[:alpha:]]{-}[[:lower:]]{+}[q] 的结果为 [A-Zq]。 一个 pattern 最多有一个尾随上下文 (/ 或 $)，开始条件、^ 和 \u003c\u003cEOF\u003e\u003e 只能出现在 pattern 的开头。这些内容均不能出现在 () 的优先级分组中。另外 ^ 与 $ 没有出现在相应位置的情况下，将被视为普通字符。 Flex’s code 格式最后一部分即代码段，用来写 C 语言代码，并","date":"05-01","objectID":"/2022/flex_and_bison/:1:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-输入文件的格式"},{"categories":["CompilerPrinciple"],"content":" Flex 输入文件的格式输入文件被分为三个部分，两个部分之间使用 %% 分隔。 definitions %% rules %% user code Flex’s definitions 格式定义部分会声明一些 name 方便后续使用，或者声明开始条件。 name 是以字母或下划线 (_) 开头，并在后面跟随零或多个字母、数字、下划线、短线 (-)。声明通常是一个正则表达式，可以看作是 name 的值。语法如下 name definition 有点难懂？那就看一个示例 DIGIT [0-9] ID [a-z][a-z0-9]* 这里定义了两个 name，当然我们可以在其他 name 中使用已声明的 name，比如声明 FLOAT FLOAT {DIGIT}+\".\"{DIGIT}* FLOAT2 ([0-9])+\".\"([0-9])* 这个例子中 FLOAT 与 FLOAT2 的声明是完全等价的。 注释与 C 的多行注释相同。lex 中可以直接写 C 代码，当然要用 %{ 和 %} 包裹起来，这个块将在生成代码时被完全复制。另外 top 块是很重要的一个语法，它会被生成在文件的顶端，在所有的代码之前，因此你可以在 top 块中定义一些感兴趣的宏或文件。top 块示例如下： %top{ /* This code goes at the \"top\" of the generated file. */ #include #include } Flex’s rules 格式规则部分每个规则一行，同样地你可以在 %{ 和 %} 之间的块中写 C 语言代码，Flex 会帮你完全一致的复制到生成文件中。语法如下 pattern action pattern 相当于给定的正则表达式，当输入与 pattern 匹配时将执行 action 所对应的动作。详细说明一下支持的 pattern 用法 x 字符 x . 除换行外的任意字符 [xyz] 一个字符集，可以匹配其中一个字符，示例可以匹配 x 或 y 或 z [abj-oZ] 一个带范围的字符集，匹配其中一个字符。示例可以匹配 a 或 b 或 j 到 o 之间的字符或 Z [^A-Z] 一个否定字符集，匹配其中没有出现的字符。示例可以匹配除了大写字母外的任意字符 [a-z]{-}[aeiou] 一个带除外集的字符集，匹配除了后面集合外的所有出现在集合中的字符。示例可以匹配小写辅音字母 r* 匹配零或多个 r r+ 匹配一或多个 r r? 匹配零或一个 r r{2,5} 匹配二到五个 r r{2,} 匹配二或更多 r r{4} 正好四个 r {name} 匹配第一节中定义的 name [xyz]\\[foo 转义字符 \\123 字符的八进制表示。示例为大写字母 S \\x53 字符的十六进制表示。示例为大写字母 S (r) 利用 () 优先级修改匹配优先级 (?r-s:pattern) 对 pattern 应用 r 模式但不应用 s 模式。其中可选模式如下 i: 不区分大小写 s: 修改 . 语义，匹配任意字符 (包括换行) x: 忽略 pattern 中的注释与空白符。空白符依旧可以出现在 \"\" 中、转义字符、或字符集中。 示例 示例 等价表示 \\((?:foo)\\) \\((foo)\\) \\((?i:ab7)\\) \\(([aA][bB]7)\\) \\((?-i:ab)\\) \\((ab)\\) \\((?s:.)\\) \\(([\\backslash{}x00-\\backslash{}xFF])\\) \\((?ix-s:\\ a\\ .\\ b)\\) \\(([Aa][\\^\\backslash{}n][bB])\\) \\((?x:a\\ b)\\) \\((“ab”)\\) \\((?x:a\\backslash\\ b)\\) \\((“a\\ b”)\\) \\((?x:a\"\\ “b)\\) \\((“a\\ b”)\\) \\((?x:a[\\ ]b)\\) \\((“a\\ b”)\\) \\(\\begin{aligned}(?x: \u0026 a\\\\ \u0026 /* comment */\\\\ \u0026 b\\\\ \u0026 c)\\end{aligned}\\) \\((abc)\\) (?# comment) 忽略 () 中的所有内容，且可以跨多行，但是注释不能有 ) 字符 rs cat r|s or r/s 尾随上下文 (trailing context)，只有在 s 之前的情况下匹配 r，但只进行 r 的行为 ^r 仅在行首匹配 r r$ 仅在行尾匹配 r r 仅以 s 开始的 r r 仅以 s1 或 s2 或 s3 开始的 r \u003c*\u003er 以任意条件开始的 r \u003c\u003e 文件末尾 \u003c\u003e 以条件 s1 或 s2 开始的文件末尾 在字符串中，除了转义符 (\\)、字符集运算符 (- 和 ]]) 以及行首标记 (^)，其他特殊字符均失去了其特殊意义。 另外上面的 pattern 是根据优先级排列的，最上面的优先级最高。比如表达式 \\(foo|bar*\\) 与表达式 \\((foo)|(ba(r*))\\) 相同。如果想要改为重复 foo 或 bar 数次，可以写为表达式 \\((foo|bar)*\\)。 Flex 支持 POSIX Bracket Expressions，但是没找到相应的 Shorthand 支持。不过从 info 上看，支持了，但没全支持。 POSIX Description ASCII Shorthand [:alnum:] 字母和数字 \\([a-zA-Z0-9]\\) [:alpha:] 字母 \\([a-zA-Z]\\) [:ascii:] ASCII 字符 \\([\\backslash{}x00-\\backslash{}xFF]\\) [:blank:] 空格及 Tab \\([ \\backslash{}t]\\) \\h [:cntrl:] 控制字符 \\([\\backslash{}x00-\\backslash{}x1F\\backslash{}x7F]\\) [:digit:] 数字 \\([0-9]\\) \\d [:graph:] 可见字符 \\([\\backslash{}x21-\\backslash{}x7E]\\) [:lower:] 小写字符 \\([a-z]\\) \\l [:print:] 可见字符及空格 \\([\\backslash{}x20-\\backslash{}x7E]\\) [:punct:] 标点字符 \\([!\"\\backslash\\#\\$\\%\\\u0026’()*+,\\backslash-./:;\u003c=\u003e?@\\backslash[\\backslash\\backslash\\backslash]\\verb!^!\\_\\verb!`!\\{\\mid\\}\\verb!~!]\\) [:space:] 空白字符 \\([\\ \\backslash{}t\\backslash{}r\\backslash{}n\\backslash{}v\\backslash{}f]\\) \\s [:upper:] 大写字符 \\([A-Z]\\) \\u [:word:] 单词字符 \\([a-zA-Z0-9\\_]\\) \\w [:xdigit:] 十六进制字符 \\([a-fA-F0-9]\\) 其中 Flex 不支持 [:ascii:] 和 [:word:] 两个字符集。 在 Flex 配置文件中，字符集将被立即展开，这意味着 flex 环境对字符集感兴趣。 如果添加了大小写不敏感标记，那么 [:upper:] 和 [:lower:] 与 [:alpha:] 等价 有范围的字符类在跨越大小写字符时 (比如 [a-Z])，在不区分大小写的扫描器上应该慎用。此时 Flex 并不知道你是想将所有的大小写字符折叠在一起，还是要指定 ASCII 数值范围。在出现此类问题时，Flex 优先指定数值范围，并发出一个警告。 范围 结果 数值范围 二义性范围 [a-t] ok \\([a-tA-T]\\) [A-T] ok \\([a-tA-T]\\) [A-t] 二义性 \\([A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!a-t]\\) \\([a-tA-T]\\) [_-{] 二义性 \\(\\verb!_!\\verb!`!a-z\\{\\) \\(\\verb!_!\\verb!`!a-zA-Z\\{\\) [@-C] 二义性 \\(@ABC\\) \\(@A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!abc\\) Flex 允许否定 POSIX 字符集，只需要在字符类名称前加上 ^，但是大小写不敏感的扫描器上 [:^upper:] 和 [:^lower:] 会被跳过，它们的含义不清。 操作符 {-} 可以为两个字符集做差，但小心生成一个永远不会被匹配的空集。不过做差的两个集合不必是包含关系，比如 [a-c]{-}[b-z] 的结果为 [a]。 操作符 {+} 求两个字符集的并集。比如 [[:alpha:]]{-}[[:lower:]]{+}[q] 的结果为 [A-Zq]。 一个 pattern 最多有一个尾随上下文 (/ 或 $)，开始条件、^ 和 \u003c\u003e 只能出现在 pattern 的开头。这些内容均不能出现在 () 的优先级分组中。另外 ^ 与 $ 没有出现在相应位置的情况下，将被视为普通字符。 Flex’s code 格式最后一部分即代码段，用来写 C 语言代码，并","date":"05-01","objectID":"/2022/flex_and_bison/:1:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-s-definitions-格式"},{"categories":["CompilerPrinciple"],"content":" Flex 输入文件的格式输入文件被分为三个部分，两个部分之间使用 %% 分隔。 definitions %% rules %% user code Flex’s definitions 格式定义部分会声明一些 name 方便后续使用，或者声明开始条件。 name 是以字母或下划线 (_) 开头，并在后面跟随零或多个字母、数字、下划线、短线 (-)。声明通常是一个正则表达式，可以看作是 name 的值。语法如下 name definition 有点难懂？那就看一个示例 DIGIT [0-9] ID [a-z][a-z0-9]* 这里定义了两个 name，当然我们可以在其他 name 中使用已声明的 name，比如声明 FLOAT FLOAT {DIGIT}+\".\"{DIGIT}* FLOAT2 ([0-9])+\".\"([0-9])* 这个例子中 FLOAT 与 FLOAT2 的声明是完全等价的。 注释与 C 的多行注释相同。lex 中可以直接写 C 代码，当然要用 %{ 和 %} 包裹起来，这个块将在生成代码时被完全复制。另外 top 块是很重要的一个语法，它会被生成在文件的顶端，在所有的代码之前，因此你可以在 top 块中定义一些感兴趣的宏或文件。top 块示例如下： %top{ /* This code goes at the \"top\" of the generated file. */ #include #include } Flex’s rules 格式规则部分每个规则一行，同样地你可以在 %{ 和 %} 之间的块中写 C 语言代码，Flex 会帮你完全一致的复制到生成文件中。语法如下 pattern action pattern 相当于给定的正则表达式，当输入与 pattern 匹配时将执行 action 所对应的动作。详细说明一下支持的 pattern 用法 x 字符 x . 除换行外的任意字符 [xyz] 一个字符集，可以匹配其中一个字符，示例可以匹配 x 或 y 或 z [abj-oZ] 一个带范围的字符集，匹配其中一个字符。示例可以匹配 a 或 b 或 j 到 o 之间的字符或 Z [^A-Z] 一个否定字符集，匹配其中没有出现的字符。示例可以匹配除了大写字母外的任意字符 [a-z]{-}[aeiou] 一个带除外集的字符集，匹配除了后面集合外的所有出现在集合中的字符。示例可以匹配小写辅音字母 r* 匹配零或多个 r r+ 匹配一或多个 r r? 匹配零或一个 r r{2,5} 匹配二到五个 r r{2,} 匹配二或更多 r r{4} 正好四个 r {name} 匹配第一节中定义的 name [xyz]\\[foo 转义字符 \\123 字符的八进制表示。示例为大写字母 S \\x53 字符的十六进制表示。示例为大写字母 S (r) 利用 () 优先级修改匹配优先级 (?r-s:pattern) 对 pattern 应用 r 模式但不应用 s 模式。其中可选模式如下 i: 不区分大小写 s: 修改 . 语义，匹配任意字符 (包括换行) x: 忽略 pattern 中的注释与空白符。空白符依旧可以出现在 \"\" 中、转义字符、或字符集中。 示例 示例 等价表示 \\((?:foo)\\) \\((foo)\\) \\((?i:ab7)\\) \\(([aA][bB]7)\\) \\((?-i:ab)\\) \\((ab)\\) \\((?s:.)\\) \\(([\\backslash{}x00-\\backslash{}xFF])\\) \\((?ix-s:\\ a\\ .\\ b)\\) \\(([Aa][\\^\\backslash{}n][bB])\\) \\((?x:a\\ b)\\) \\((“ab”)\\) \\((?x:a\\backslash\\ b)\\) \\((“a\\ b”)\\) \\((?x:a\"\\ “b)\\) \\((“a\\ b”)\\) \\((?x:a[\\ ]b)\\) \\((“a\\ b”)\\) \\(\\begin{aligned}(?x: \u0026 a\\\\ \u0026 /* comment */\\\\ \u0026 b\\\\ \u0026 c)\\end{aligned}\\) \\((abc)\\) (?# comment) 忽略 () 中的所有内容，且可以跨多行，但是注释不能有 ) 字符 rs cat r|s or r/s 尾随上下文 (trailing context)，只有在 s 之前的情况下匹配 r，但只进行 r 的行为 ^r 仅在行首匹配 r r$ 仅在行尾匹配 r r 仅以 s 开始的 r r 仅以 s1 或 s2 或 s3 开始的 r \u003c*\u003er 以任意条件开始的 r \u003c\u003e 文件末尾 \u003c\u003e 以条件 s1 或 s2 开始的文件末尾 在字符串中，除了转义符 (\\)、字符集运算符 (- 和 ]]) 以及行首标记 (^)，其他特殊字符均失去了其特殊意义。 另外上面的 pattern 是根据优先级排列的，最上面的优先级最高。比如表达式 \\(foo|bar*\\) 与表达式 \\((foo)|(ba(r*))\\) 相同。如果想要改为重复 foo 或 bar 数次，可以写为表达式 \\((foo|bar)*\\)。 Flex 支持 POSIX Bracket Expressions，但是没找到相应的 Shorthand 支持。不过从 info 上看，支持了，但没全支持。 POSIX Description ASCII Shorthand [:alnum:] 字母和数字 \\([a-zA-Z0-9]\\) [:alpha:] 字母 \\([a-zA-Z]\\) [:ascii:] ASCII 字符 \\([\\backslash{}x00-\\backslash{}xFF]\\) [:blank:] 空格及 Tab \\([ \\backslash{}t]\\) \\h [:cntrl:] 控制字符 \\([\\backslash{}x00-\\backslash{}x1F\\backslash{}x7F]\\) [:digit:] 数字 \\([0-9]\\) \\d [:graph:] 可见字符 \\([\\backslash{}x21-\\backslash{}x7E]\\) [:lower:] 小写字符 \\([a-z]\\) \\l [:print:] 可见字符及空格 \\([\\backslash{}x20-\\backslash{}x7E]\\) [:punct:] 标点字符 \\([!\"\\backslash\\#\\$\\%\\\u0026’()*+,\\backslash-./:;\u003c=\u003e?@\\backslash[\\backslash\\backslash\\backslash]\\verb!^!\\_\\verb!`!\\{\\mid\\}\\verb!~!]\\) [:space:] 空白字符 \\([\\ \\backslash{}t\\backslash{}r\\backslash{}n\\backslash{}v\\backslash{}f]\\) \\s [:upper:] 大写字符 \\([A-Z]\\) \\u [:word:] 单词字符 \\([a-zA-Z0-9\\_]\\) \\w [:xdigit:] 十六进制字符 \\([a-fA-F0-9]\\) 其中 Flex 不支持 [:ascii:] 和 [:word:] 两个字符集。 在 Flex 配置文件中，字符集将被立即展开，这意味着 flex 环境对字符集感兴趣。 如果添加了大小写不敏感标记，那么 [:upper:] 和 [:lower:] 与 [:alpha:] 等价 有范围的字符类在跨越大小写字符时 (比如 [a-Z])，在不区分大小写的扫描器上应该慎用。此时 Flex 并不知道你是想将所有的大小写字符折叠在一起，还是要指定 ASCII 数值范围。在出现此类问题时，Flex 优先指定数值范围，并发出一个警告。 范围 结果 数值范围 二义性范围 [a-t] ok \\([a-tA-T]\\) [A-T] ok \\([a-tA-T]\\) [A-t] 二义性 \\([A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!a-t]\\) \\([a-tA-T]\\) [_-{] 二义性 \\(\\verb!_!\\verb!`!a-z\\{\\) \\(\\verb!_!\\verb!`!a-zA-Z\\{\\) [@-C] 二义性 \\(@ABC\\) \\(@A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!abc\\) Flex 允许否定 POSIX 字符集，只需要在字符类名称前加上 ^，但是大小写不敏感的扫描器上 [:^upper:] 和 [:^lower:] 会被跳过，它们的含义不清。 操作符 {-} 可以为两个字符集做差，但小心生成一个永远不会被匹配的空集。不过做差的两个集合不必是包含关系，比如 [a-c]{-}[b-z] 的结果为 [a]。 操作符 {+} 求两个字符集的并集。比如 [[:alpha:]]{-}[[:lower:]]{+}[q] 的结果为 [A-Zq]。 一个 pattern 最多有一个尾随上下文 (/ 或 $)，开始条件、^ 和 \u003c\u003e 只能出现在 pattern 的开头。这些内容均不能出现在 () 的优先级分组中。另外 ^ 与 $ 没有出现在相应位置的情况下，将被视为普通字符。 Flex’s code 格式最后一部分即代码段，用来写 C 语言代码，并","date":"05-01","objectID":"/2022/flex_and_bison/:1:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-s-rules-格式"},{"categories":["CompilerPrinciple"],"content":" Flex 输入文件的格式输入文件被分为三个部分，两个部分之间使用 %% 分隔。 definitions %% rules %% user code Flex’s definitions 格式定义部分会声明一些 name 方便后续使用，或者声明开始条件。 name 是以字母或下划线 (_) 开头，并在后面跟随零或多个字母、数字、下划线、短线 (-)。声明通常是一个正则表达式，可以看作是 name 的值。语法如下 name definition 有点难懂？那就看一个示例 DIGIT [0-9] ID [a-z][a-z0-9]* 这里定义了两个 name，当然我们可以在其他 name 中使用已声明的 name，比如声明 FLOAT FLOAT {DIGIT}+\".\"{DIGIT}* FLOAT2 ([0-9])+\".\"([0-9])* 这个例子中 FLOAT 与 FLOAT2 的声明是完全等价的。 注释与 C 的多行注释相同。lex 中可以直接写 C 代码，当然要用 %{ 和 %} 包裹起来，这个块将在生成代码时被完全复制。另外 top 块是很重要的一个语法，它会被生成在文件的顶端，在所有的代码之前，因此你可以在 top 块中定义一些感兴趣的宏或文件。top 块示例如下： %top{ /* This code goes at the \"top\" of the generated file. */ #include #include } Flex’s rules 格式规则部分每个规则一行，同样地你可以在 %{ 和 %} 之间的块中写 C 语言代码，Flex 会帮你完全一致的复制到生成文件中。语法如下 pattern action pattern 相当于给定的正则表达式，当输入与 pattern 匹配时将执行 action 所对应的动作。详细说明一下支持的 pattern 用法 x 字符 x . 除换行外的任意字符 [xyz] 一个字符集，可以匹配其中一个字符，示例可以匹配 x 或 y 或 z [abj-oZ] 一个带范围的字符集，匹配其中一个字符。示例可以匹配 a 或 b 或 j 到 o 之间的字符或 Z [^A-Z] 一个否定字符集，匹配其中没有出现的字符。示例可以匹配除了大写字母外的任意字符 [a-z]{-}[aeiou] 一个带除外集的字符集，匹配除了后面集合外的所有出现在集合中的字符。示例可以匹配小写辅音字母 r* 匹配零或多个 r r+ 匹配一或多个 r r? 匹配零或一个 r r{2,5} 匹配二到五个 r r{2,} 匹配二或更多 r r{4} 正好四个 r {name} 匹配第一节中定义的 name [xyz]\\[foo 转义字符 \\123 字符的八进制表示。示例为大写字母 S \\x53 字符的十六进制表示。示例为大写字母 S (r) 利用 () 优先级修改匹配优先级 (?r-s:pattern) 对 pattern 应用 r 模式但不应用 s 模式。其中可选模式如下 i: 不区分大小写 s: 修改 . 语义，匹配任意字符 (包括换行) x: 忽略 pattern 中的注释与空白符。空白符依旧可以出现在 \"\" 中、转义字符、或字符集中。 示例 示例 等价表示 \\((?:foo)\\) \\((foo)\\) \\((?i:ab7)\\) \\(([aA][bB]7)\\) \\((?-i:ab)\\) \\((ab)\\) \\((?s:.)\\) \\(([\\backslash{}x00-\\backslash{}xFF])\\) \\((?ix-s:\\ a\\ .\\ b)\\) \\(([Aa][\\^\\backslash{}n][bB])\\) \\((?x:a\\ b)\\) \\((“ab”)\\) \\((?x:a\\backslash\\ b)\\) \\((“a\\ b”)\\) \\((?x:a\"\\ “b)\\) \\((“a\\ b”)\\) \\((?x:a[\\ ]b)\\) \\((“a\\ b”)\\) \\(\\begin{aligned}(?x: \u0026 a\\\\ \u0026 /* comment */\\\\ \u0026 b\\\\ \u0026 c)\\end{aligned}\\) \\((abc)\\) (?# comment) 忽略 () 中的所有内容，且可以跨多行，但是注释不能有 ) 字符 rs cat r|s or r/s 尾随上下文 (trailing context)，只有在 s 之前的情况下匹配 r，但只进行 r 的行为 ^r 仅在行首匹配 r r$ 仅在行尾匹配 r r 仅以 s 开始的 r r 仅以 s1 或 s2 或 s3 开始的 r \u003c*\u003er 以任意条件开始的 r \u003c\u003e 文件末尾 \u003c\u003e 以条件 s1 或 s2 开始的文件末尾 在字符串中，除了转义符 (\\)、字符集运算符 (- 和 ]]) 以及行首标记 (^)，其他特殊字符均失去了其特殊意义。 另外上面的 pattern 是根据优先级排列的，最上面的优先级最高。比如表达式 \\(foo|bar*\\) 与表达式 \\((foo)|(ba(r*))\\) 相同。如果想要改为重复 foo 或 bar 数次，可以写为表达式 \\((foo|bar)*\\)。 Flex 支持 POSIX Bracket Expressions，但是没找到相应的 Shorthand 支持。不过从 info 上看，支持了，但没全支持。 POSIX Description ASCII Shorthand [:alnum:] 字母和数字 \\([a-zA-Z0-9]\\) [:alpha:] 字母 \\([a-zA-Z]\\) [:ascii:] ASCII 字符 \\([\\backslash{}x00-\\backslash{}xFF]\\) [:blank:] 空格及 Tab \\([ \\backslash{}t]\\) \\h [:cntrl:] 控制字符 \\([\\backslash{}x00-\\backslash{}x1F\\backslash{}x7F]\\) [:digit:] 数字 \\([0-9]\\) \\d [:graph:] 可见字符 \\([\\backslash{}x21-\\backslash{}x7E]\\) [:lower:] 小写字符 \\([a-z]\\) \\l [:print:] 可见字符及空格 \\([\\backslash{}x20-\\backslash{}x7E]\\) [:punct:] 标点字符 \\([!\"\\backslash\\#\\$\\%\\\u0026’()*+,\\backslash-./:;\u003c=\u003e?@\\backslash[\\backslash\\backslash\\backslash]\\verb!^!\\_\\verb!`!\\{\\mid\\}\\verb!~!]\\) [:space:] 空白字符 \\([\\ \\backslash{}t\\backslash{}r\\backslash{}n\\backslash{}v\\backslash{}f]\\) \\s [:upper:] 大写字符 \\([A-Z]\\) \\u [:word:] 单词字符 \\([a-zA-Z0-9\\_]\\) \\w [:xdigit:] 十六进制字符 \\([a-fA-F0-9]\\) 其中 Flex 不支持 [:ascii:] 和 [:word:] 两个字符集。 在 Flex 配置文件中，字符集将被立即展开，这意味着 flex 环境对字符集感兴趣。 如果添加了大小写不敏感标记，那么 [:upper:] 和 [:lower:] 与 [:alpha:] 等价 有范围的字符类在跨越大小写字符时 (比如 [a-Z])，在不区分大小写的扫描器上应该慎用。此时 Flex 并不知道你是想将所有的大小写字符折叠在一起，还是要指定 ASCII 数值范围。在出现此类问题时，Flex 优先指定数值范围，并发出一个警告。 范围 结果 数值范围 二义性范围 [a-t] ok \\([a-tA-T]\\) [A-T] ok \\([a-tA-T]\\) [A-t] 二义性 \\([A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!a-t]\\) \\([a-tA-T]\\) [_-{] 二义性 \\(\\verb!_!\\verb!`!a-z\\{\\) \\(\\verb!_!\\verb!`!a-zA-Z\\{\\) [@-C] 二义性 \\(@ABC\\) \\(@A-Z\\backslash[\\backslash\\backslash\\backslash]\\verb!_!\\verb!`!abc\\) Flex 允许否定 POSIX 字符集，只需要在字符类名称前加上 ^，但是大小写不敏感的扫描器上 [:^upper:] 和 [:^lower:] 会被跳过，它们的含义不清。 操作符 {-} 可以为两个字符集做差，但小心生成一个永远不会被匹配的空集。不过做差的两个集合不必是包含关系，比如 [a-c]{-}[b-z] 的结果为 [a]。 操作符 {+} 求两个字符集的并集。比如 [[:alpha:]]{-}[[:lower:]]{+}[q] 的结果为 [A-Zq]。 一个 pattern 最多有一个尾随上下文 (/ 或 $)，开始条件、^ 和 \u003c\u003e 只能出现在 pattern 的开头。这些内容均不能出现在 () 的优先级分组中。另外 ^ 与 $ 没有出现在相应位置的情况下，将被视为普通字符。 Flex’s code 格式最后一部分即代码段，用来写 C 语言代码，并","date":"05-01","objectID":"/2022/flex_and_bison/:1:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-s-code-格式"},{"categories":["CompilerPrinciple"],"content":" Flex 匹配 Flex 匹配规则Flex 的匹配原则是最长匹配，且按优先级匹配。即多个同时匹配的规则，采用匹配到文本最长的规则；多个同时匹配且长度相同的规则，采用第一个列出的规则 (最上面的优先级最高)。长度包含尾随上下文的尾随部分。 当匹配成功时，匹配的文本将可以使用全局指针 yytext 获取，长度可以通过全局整型 yyleng 获取。之后开始执行匹配模式相应的 action，然后再扫描剩余的输入。但是没有匹配时，将执行默认规则：输入的下一个字符将被当作匹配并复制到标准输出。 需要注意，yytext 可以用两种不同的方式定义：字符指针或字符数组。在定义部分，你可以用 %pointer 或 %array 来指定用何种方式，当然默认使用指针的方式，如果有 lex 兼容选项默认使用数组。使用指针带来的劣势是：修改 yytext 将会受限，且调用 unput() 会破坏其中的内容，也可能出现 lex 的移植性问题。 遗憾的是，生成 C++ 扫描器类是不能使用 %pointer 方式的。 Flex 匹配动作每一个匹配都有一个想对应的动作，在匹配成功时将执行这个动作。pattern 在第一个非转义空白处结尾，这行剩下的则是 action 部分，action 由任意的 C 代码片段组成。 比如下面这个程序，压缩空白符到一个空格，并丢弃所有行末空白符。 [ \\t]+ putchar(' '); [ \\t]+$ /* ignore this token */ action 可以用 { 和 } 包括起来，在里面写多行 C 语言代码，类似于 C 的代码块。Flex 会甄别字符串和注释中括号，更好的方式是用 %{ 和 %} 来定义代码块。 如果 action 是一个 \\(\\mid\\)，意思是同下一个规则的 action 相同。 之前说了可以包含 return 语句，它将返回值给名为 yylex 的函数，这个函数每次在上次停止的地方继续向下处理，直到到达文件末尾或执行返回。 action 可以自由修改 yytext，除了延长它 (末尾添加字符将覆盖之后的字符，在使用 array 方式时不能修改)。action 还可以自由修改 yyleng，除非 action 还包括使用 yymore()。 还为 action 定义了一些预设指令 ECHO 复制 yytext 到扫描器输出 BEGIN 将扫描器重定位在开始条件末尾 REJECT 拒绝当前的最优匹配，采用次优匹配。如果你的 action 由其他动作，需要放在 REJECT 之前，否则不会执行。以下代码，在匹配 ‘abcd’ 时会输出 ‘abcdabcaba’ a | ab | abc | abcd ECHO; REJECT; .|\\n /* */ yymore() 告诉扫描器下次匹配到规则时，将当前的 token 放在 yytext 的首部。比如以下代码，在匹配 ‘mega-kludge’ 时会输出 ‘mega-mega-kludge’ mega- ECHO; yymore(); kludge ECHO; 使用 yymore 的时候需要注意两点 yymore 依赖于 yyleng，因此不要在使用时修改 yyleng yymore 会导致扫描器的匹配速度略微下降 yyless(n) 当前 token 将使用开始的 n 个字符，扫描会从匹配的 n 个字符之后继续扫描。在使用 yyless 之后，yytext 与 yyleng 将会被调整 (yyleng 将等于 n)。比如以下代码，匹配 ‘foobar’ 时会输出 ‘foobarbar’ foobar ECHO; yyless(3); [a-z]+ ECHO; 要十分注意 yyless() 的使用，如果参数为 0 时，扫描器将陷入无限循环中。 unput(c) 将字符 c 重新放入输入中，它会是下一个带扫描的字符。如果使用 %pointer 情况下会导致 yytext 被破坏，从最右端开始每次吞一个字符。如果需要调用 unput()，需要使用 %array 构建或者先将 yytext 复制到其他地方 { /* Copy yytext because unput() trashes yytext */ char* yycopy = strdup(yytext); unput(')'); for (int i = yyleng - 1; i \u003e= 0; --i) { unput(yycopy[i]); } unput('('); free(yycopy); } input() 从输入流中读取下一个字符。比如 C 风格注释，将全部注释丢弃。 \"/*\" { int c; while (1) { while ((c = input()) != '*' \u0026\u0026 c != EOF) { continue; } /* eat up text of comment */ if (c == '*') { while ((c = input()) == '*') { continue; } if (c == '/') { break; } /* found the end */ } if (c == EOF) { error( \"EOF in comment\" ); break; } } /* end of while */ } 需要注意一点，如果用 C++ 编译，则需要使用 yyinput() 防止 input 和 C++ 的 stream 冲突。 YY_FLUSH_BUFFER 清空内部缓冲区，在下次匹配是先使用 YY_INPUT() 重填缓冲区。这个方法会在之后解释。 yyterminate() 替代 action 中的返回语句并返回 0 表示全部完成。通常在遇到 EOF 时使用该函数。 EOF 规则\u003c\u003cEOF\u003e\u003e 是个特殊规则，表示遇到文件末尾且 yywrap 返回非零值时 (表示没有其他文件要处理) 要执行的操作，好像并不是每个文件的 EOF 规则。该 action 通常执行以下操作之一： 将 yyin 分配给新的输入文件； 执行返回语句； 或，使用 yy_switch_to_buffer 切换到新的缓冲区 (见 多输入缓冲区) EOF 只能与开始条件一起使用，不合格的 EOF 将适用于所有没有 EOF 的开始动作。 %x quote %% ...other rules for dealing with quotes... \u003cquote\u003e\u003c\u003cEOF\u003e\u003e { error( \"unterminated quote\" ); yyterminate(); } \u003c\u003cEOF\u003e\u003e { if ( *++filelist ) yyin = fopen( *filelist, \"r\" ); else yyterminate(); } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-匹配"},{"categories":["CompilerPrinciple"],"content":" Flex 匹配 Flex 匹配规则Flex 的匹配原则是最长匹配，且按优先级匹配。即多个同时匹配的规则，采用匹配到文本最长的规则；多个同时匹配且长度相同的规则，采用第一个列出的规则 (最上面的优先级最高)。长度包含尾随上下文的尾随部分。 当匹配成功时，匹配的文本将可以使用全局指针 yytext 获取，长度可以通过全局整型 yyleng 获取。之后开始执行匹配模式相应的 action，然后再扫描剩余的输入。但是没有匹配时，将执行默认规则：输入的下一个字符将被当作匹配并复制到标准输出。 需要注意，yytext 可以用两种不同的方式定义：字符指针或字符数组。在定义部分，你可以用 %pointer 或 %array 来指定用何种方式，当然默认使用指针的方式，如果有 lex 兼容选项默认使用数组。使用指针带来的劣势是：修改 yytext 将会受限，且调用 unput() 会破坏其中的内容，也可能出现 lex 的移植性问题。 遗憾的是，生成 C++ 扫描器类是不能使用 %pointer 方式的。 Flex 匹配动作每一个匹配都有一个想对应的动作，在匹配成功时将执行这个动作。pattern 在第一个非转义空白处结尾，这行剩下的则是 action 部分，action 由任意的 C 代码片段组成。 比如下面这个程序，压缩空白符到一个空格，并丢弃所有行末空白符。 [ \\t]+ putchar(' '); [ \\t]+$ /* ignore this token */ action 可以用 { 和 } 包括起来，在里面写多行 C 语言代码，类似于 C 的代码块。Flex 会甄别字符串和注释中括号，更好的方式是用 %{ 和 %} 来定义代码块。 如果 action 是一个 \\(\\mid\\)，意思是同下一个规则的 action 相同。 之前说了可以包含 return 语句，它将返回值给名为 yylex 的函数，这个函数每次在上次停止的地方继续向下处理，直到到达文件末尾或执行返回。 action 可以自由修改 yytext，除了延长它 (末尾添加字符将覆盖之后的字符，在使用 array 方式时不能修改)。action 还可以自由修改 yyleng，除非 action 还包括使用 yymore()。 还为 action 定义了一些预设指令 ECHO 复制 yytext 到扫描器输出 BEGIN 将扫描器重定位在开始条件末尾 REJECT 拒绝当前的最优匹配，采用次优匹配。如果你的 action 由其他动作，需要放在 REJECT 之前，否则不会执行。以下代码，在匹配 ‘abcd’ 时会输出 ‘abcdabcaba’ a | ab | abc | abcd ECHO; REJECT; .|\\n /* */ yymore() 告诉扫描器下次匹配到规则时，将当前的 token 放在 yytext 的首部。比如以下代码，在匹配 ‘mega-kludge’ 时会输出 ‘mega-mega-kludge’ mega- ECHO; yymore(); kludge ECHO; 使用 yymore 的时候需要注意两点 yymore 依赖于 yyleng，因此不要在使用时修改 yyleng yymore 会导致扫描器的匹配速度略微下降 yyless(n) 当前 token 将使用开始的 n 个字符，扫描会从匹配的 n 个字符之后继续扫描。在使用 yyless 之后，yytext 与 yyleng 将会被调整 (yyleng 将等于 n)。比如以下代码，匹配 ‘foobar’ 时会输出 ‘foobarbar’ foobar ECHO; yyless(3); [a-z]+ ECHO; 要十分注意 yyless() 的使用，如果参数为 0 时，扫描器将陷入无限循环中。 unput(c) 将字符 c 重新放入输入中，它会是下一个带扫描的字符。如果使用 %pointer 情况下会导致 yytext 被破坏，从最右端开始每次吞一个字符。如果需要调用 unput()，需要使用 %array 构建或者先将 yytext 复制到其他地方 { /* Copy yytext because unput() trashes yytext */ char* yycopy = strdup(yytext); unput(')'); for (int i = yyleng - 1; i \u003e= 0; --i) { unput(yycopy[i]); } unput('('); free(yycopy); } input() 从输入流中读取下一个字符。比如 C 风格注释，将全部注释丢弃。 \"/*\" { int c; while (1) { while ((c = input()) != '*' \u0026\u0026 c != EOF) { continue; } /* eat up text of comment */ if (c == '*') { while ((c = input()) == '*') { continue; } if (c == '/') { break; } /* found the end */ } if (c == EOF) { error( \"EOF in comment\" ); break; } } /* end of while */ } 需要注意一点，如果用 C++ 编译，则需要使用 yyinput() 防止 input 和 C++ 的 stream 冲突。 YY_FLUSH_BUFFER 清空内部缓冲区，在下次匹配是先使用 YY_INPUT() 重填缓冲区。这个方法会在之后解释。 yyterminate() 替代 action 中的返回语句并返回 0 表示全部完成。通常在遇到 EOF 时使用该函数。 EOF 规则\u003c\u003e 是个特殊规则，表示遇到文件末尾且 yywrap 返回非零值时 (表示没有其他文件要处理) 要执行的操作，好像并不是每个文件的 EOF 规则。该 action 通常执行以下操作之一： 将 yyin 分配给新的输入文件； 执行返回语句； 或，使用 yy_switch_to_buffer 切换到新的缓冲区 (见 多输入缓冲区) EOF 只能与开始条件一起使用，不合格的 EOF 将适用于所有没有 EOF 的开始动作。 %x quote %% ...other rules for dealing with quotes... \u003c\u003e { error( \"unterminated quote\" ); yyterminate(); } \u003c\u003e { if ( *++filelist ) yyin = fopen( *filelist, \"r\" ); else yyterminate(); } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-匹配规则"},{"categories":["CompilerPrinciple"],"content":" Flex 匹配 Flex 匹配规则Flex 的匹配原则是最长匹配，且按优先级匹配。即多个同时匹配的规则，采用匹配到文本最长的规则；多个同时匹配且长度相同的规则，采用第一个列出的规则 (最上面的优先级最高)。长度包含尾随上下文的尾随部分。 当匹配成功时，匹配的文本将可以使用全局指针 yytext 获取，长度可以通过全局整型 yyleng 获取。之后开始执行匹配模式相应的 action，然后再扫描剩余的输入。但是没有匹配时，将执行默认规则：输入的下一个字符将被当作匹配并复制到标准输出。 需要注意，yytext 可以用两种不同的方式定义：字符指针或字符数组。在定义部分，你可以用 %pointer 或 %array 来指定用何种方式，当然默认使用指针的方式，如果有 lex 兼容选项默认使用数组。使用指针带来的劣势是：修改 yytext 将会受限，且调用 unput() 会破坏其中的内容，也可能出现 lex 的移植性问题。 遗憾的是，生成 C++ 扫描器类是不能使用 %pointer 方式的。 Flex 匹配动作每一个匹配都有一个想对应的动作，在匹配成功时将执行这个动作。pattern 在第一个非转义空白处结尾，这行剩下的则是 action 部分，action 由任意的 C 代码片段组成。 比如下面这个程序，压缩空白符到一个空格，并丢弃所有行末空白符。 [ \\t]+ putchar(' '); [ \\t]+$ /* ignore this token */ action 可以用 { 和 } 包括起来，在里面写多行 C 语言代码，类似于 C 的代码块。Flex 会甄别字符串和注释中括号，更好的方式是用 %{ 和 %} 来定义代码块。 如果 action 是一个 \\(\\mid\\)，意思是同下一个规则的 action 相同。 之前说了可以包含 return 语句，它将返回值给名为 yylex 的函数，这个函数每次在上次停止的地方继续向下处理，直到到达文件末尾或执行返回。 action 可以自由修改 yytext，除了延长它 (末尾添加字符将覆盖之后的字符，在使用 array 方式时不能修改)。action 还可以自由修改 yyleng，除非 action 还包括使用 yymore()。 还为 action 定义了一些预设指令 ECHO 复制 yytext 到扫描器输出 BEGIN 将扫描器重定位在开始条件末尾 REJECT 拒绝当前的最优匹配，采用次优匹配。如果你的 action 由其他动作，需要放在 REJECT 之前，否则不会执行。以下代码，在匹配 ‘abcd’ 时会输出 ‘abcdabcaba’ a | ab | abc | abcd ECHO; REJECT; .|\\n /* */ yymore() 告诉扫描器下次匹配到规则时，将当前的 token 放在 yytext 的首部。比如以下代码，在匹配 ‘mega-kludge’ 时会输出 ‘mega-mega-kludge’ mega- ECHO; yymore(); kludge ECHO; 使用 yymore 的时候需要注意两点 yymore 依赖于 yyleng，因此不要在使用时修改 yyleng yymore 会导致扫描器的匹配速度略微下降 yyless(n) 当前 token 将使用开始的 n 个字符，扫描会从匹配的 n 个字符之后继续扫描。在使用 yyless 之后，yytext 与 yyleng 将会被调整 (yyleng 将等于 n)。比如以下代码，匹配 ‘foobar’ 时会输出 ‘foobarbar’ foobar ECHO; yyless(3); [a-z]+ ECHO; 要十分注意 yyless() 的使用，如果参数为 0 时，扫描器将陷入无限循环中。 unput(c) 将字符 c 重新放入输入中，它会是下一个带扫描的字符。如果使用 %pointer 情况下会导致 yytext 被破坏，从最右端开始每次吞一个字符。如果需要调用 unput()，需要使用 %array 构建或者先将 yytext 复制到其他地方 { /* Copy yytext because unput() trashes yytext */ char* yycopy = strdup(yytext); unput(')'); for (int i = yyleng - 1; i \u003e= 0; --i) { unput(yycopy[i]); } unput('('); free(yycopy); } input() 从输入流中读取下一个字符。比如 C 风格注释，将全部注释丢弃。 \"/*\" { int c; while (1) { while ((c = input()) != '*' \u0026\u0026 c != EOF) { continue; } /* eat up text of comment */ if (c == '*') { while ((c = input()) == '*') { continue; } if (c == '/') { break; } /* found the end */ } if (c == EOF) { error( \"EOF in comment\" ); break; } } /* end of while */ } 需要注意一点，如果用 C++ 编译，则需要使用 yyinput() 防止 input 和 C++ 的 stream 冲突。 YY_FLUSH_BUFFER 清空内部缓冲区，在下次匹配是先使用 YY_INPUT() 重填缓冲区。这个方法会在之后解释。 yyterminate() 替代 action 中的返回语句并返回 0 表示全部完成。通常在遇到 EOF 时使用该函数。 EOF 规则\u003c\u003e 是个特殊规则，表示遇到文件末尾且 yywrap 返回非零值时 (表示没有其他文件要处理) 要执行的操作，好像并不是每个文件的 EOF 规则。该 action 通常执行以下操作之一： 将 yyin 分配给新的输入文件； 执行返回语句； 或，使用 yy_switch_to_buffer 切换到新的缓冲区 (见 多输入缓冲区) EOF 只能与开始条件一起使用，不合格的 EOF 将适用于所有没有 EOF 的开始动作。 %x quote %% ...other rules for dealing with quotes... \u003c\u003e { error( \"unterminated quote\" ); yyterminate(); } \u003c\u003e { if ( *++filelist ) yyin = fopen( *filelist, \"r\" ); else yyterminate(); } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-匹配动作"},{"categories":["CompilerPrinciple"],"content":" Flex 匹配 Flex 匹配规则Flex 的匹配原则是最长匹配，且按优先级匹配。即多个同时匹配的规则，采用匹配到文本最长的规则；多个同时匹配且长度相同的规则，采用第一个列出的规则 (最上面的优先级最高)。长度包含尾随上下文的尾随部分。 当匹配成功时，匹配的文本将可以使用全局指针 yytext 获取，长度可以通过全局整型 yyleng 获取。之后开始执行匹配模式相应的 action，然后再扫描剩余的输入。但是没有匹配时，将执行默认规则：输入的下一个字符将被当作匹配并复制到标准输出。 需要注意，yytext 可以用两种不同的方式定义：字符指针或字符数组。在定义部分，你可以用 %pointer 或 %array 来指定用何种方式，当然默认使用指针的方式，如果有 lex 兼容选项默认使用数组。使用指针带来的劣势是：修改 yytext 将会受限，且调用 unput() 会破坏其中的内容，也可能出现 lex 的移植性问题。 遗憾的是，生成 C++ 扫描器类是不能使用 %pointer 方式的。 Flex 匹配动作每一个匹配都有一个想对应的动作，在匹配成功时将执行这个动作。pattern 在第一个非转义空白处结尾，这行剩下的则是 action 部分，action 由任意的 C 代码片段组成。 比如下面这个程序，压缩空白符到一个空格，并丢弃所有行末空白符。 [ \\t]+ putchar(' '); [ \\t]+$ /* ignore this token */ action 可以用 { 和 } 包括起来，在里面写多行 C 语言代码，类似于 C 的代码块。Flex 会甄别字符串和注释中括号，更好的方式是用 %{ 和 %} 来定义代码块。 如果 action 是一个 \\(\\mid\\)，意思是同下一个规则的 action 相同。 之前说了可以包含 return 语句，它将返回值给名为 yylex 的函数，这个函数每次在上次停止的地方继续向下处理，直到到达文件末尾或执行返回。 action 可以自由修改 yytext，除了延长它 (末尾添加字符将覆盖之后的字符，在使用 array 方式时不能修改)。action 还可以自由修改 yyleng，除非 action 还包括使用 yymore()。 还为 action 定义了一些预设指令 ECHO 复制 yytext 到扫描器输出 BEGIN 将扫描器重定位在开始条件末尾 REJECT 拒绝当前的最优匹配，采用次优匹配。如果你的 action 由其他动作，需要放在 REJECT 之前，否则不会执行。以下代码，在匹配 ‘abcd’ 时会输出 ‘abcdabcaba’ a | ab | abc | abcd ECHO; REJECT; .|\\n /* */ yymore() 告诉扫描器下次匹配到规则时，将当前的 token 放在 yytext 的首部。比如以下代码，在匹配 ‘mega-kludge’ 时会输出 ‘mega-mega-kludge’ mega- ECHO; yymore(); kludge ECHO; 使用 yymore 的时候需要注意两点 yymore 依赖于 yyleng，因此不要在使用时修改 yyleng yymore 会导致扫描器的匹配速度略微下降 yyless(n) 当前 token 将使用开始的 n 个字符，扫描会从匹配的 n 个字符之后继续扫描。在使用 yyless 之后，yytext 与 yyleng 将会被调整 (yyleng 将等于 n)。比如以下代码，匹配 ‘foobar’ 时会输出 ‘foobarbar’ foobar ECHO; yyless(3); [a-z]+ ECHO; 要十分注意 yyless() 的使用，如果参数为 0 时，扫描器将陷入无限循环中。 unput(c) 将字符 c 重新放入输入中，它会是下一个带扫描的字符。如果使用 %pointer 情况下会导致 yytext 被破坏，从最右端开始每次吞一个字符。如果需要调用 unput()，需要使用 %array 构建或者先将 yytext 复制到其他地方 { /* Copy yytext because unput() trashes yytext */ char* yycopy = strdup(yytext); unput(')'); for (int i = yyleng - 1; i \u003e= 0; --i) { unput(yycopy[i]); } unput('('); free(yycopy); } input() 从输入流中读取下一个字符。比如 C 风格注释，将全部注释丢弃。 \"/*\" { int c; while (1) { while ((c = input()) != '*' \u0026\u0026 c != EOF) { continue; } /* eat up text of comment */ if (c == '*') { while ((c = input()) == '*') { continue; } if (c == '/') { break; } /* found the end */ } if (c == EOF) { error( \"EOF in comment\" ); break; } } /* end of while */ } 需要注意一点，如果用 C++ 编译，则需要使用 yyinput() 防止 input 和 C++ 的 stream 冲突。 YY_FLUSH_BUFFER 清空内部缓冲区，在下次匹配是先使用 YY_INPUT() 重填缓冲区。这个方法会在之后解释。 yyterminate() 替代 action 中的返回语句并返回 0 表示全部完成。通常在遇到 EOF 时使用该函数。 EOF 规则\u003c\u003e 是个特殊规则，表示遇到文件末尾且 yywrap 返回非零值时 (表示没有其他文件要处理) 要执行的操作，好像并不是每个文件的 EOF 规则。该 action 通常执行以下操作之一： 将 yyin 分配给新的输入文件； 执行返回语句； 或，使用 yy_switch_to_buffer 切换到新的缓冲区 (见 多输入缓冲区) EOF 只能与开始条件一起使用，不合格的 EOF 将适用于所有没有 EOF 的开始动作。 %x quote %% ...other rules for dealing with quotes... \u003c\u003e { error( \"unterminated quote\" ); yyterminate(); } \u003c\u003e { if ( *++filelist ) yyin = fopen( *filelist, \"r\" ); else yyterminate(); } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#eof-规则"},{"categories":["CompilerPrinciple"],"content":" 开始条件开始条件 (sc) 相当于一种激活机制，用 BEGIN 触发 sc，在下次 BEGIN 前，给定 sc 都是激活状态，而其他 sc 都是屏蔽状态。 在定义部分声明，有 %s (兼容性) 或 %x (排他性) 两种 sc，后跟随名称列表。兼容性 sc 激活时不会屏蔽非 sc 规则，但排他性 sc 激活时不但屏蔽其他 sc 规则，还屏蔽非 sc 规则。 %s example %% \u003cexample\u003efoo do_something(); bar something_else(); 等价于 %x example %% \u003cexample\u003efoo do_something(); \u003cINITIAL,example\u003e bar something_else(); 对于下面这个代码，如果没有 \u003cINITIAL,example\u003e 那么 bar 会被屏蔽；而如果只使用 \u003cexample\u003e sc，只有在 example sc 被激活时才能匹配 bar。 另外有一个特殊的sc \u003c*\u003e 可以在任何 sc 激活时被匹配。使用 BEGIN(0) 或 BEGIN(INITIAL) 可以回到初始状态，即非 sc 状态。如果你希望 BEGIN 动作也可以在 rules 开头作为 action 给出，直接进入指定的 sc 激活状态。 int enter_special; %x SPECIAL %% if ( enter_special ) BEGIN(SPECIAL); \u003cSPECIAL\u003eblahblahblah ...... 给出一个示例，这个示例将检测整数和浮点数，但是浮点数可能以整数开头被认为是一个整数，因此，这个例子采用 sc 来扫描整数或浮点数。一行一个数字，如果是浮点数将以 expect-floats 开头。 %{ #include \u003cmath.h\u003e %} %s expect %% expect-floats BEGIN(expect); \u003cexpect\u003e[0-9]+.[0-9]+ { printf( \"found a float, = %f\\n\", atof( yytext ) ); } \u003cexpect\u003e\\n { /* that's the end of the line, so * we need another \"expect-number\" * before we'll recognize any more * numbers */ BEGIN(INITIAL); } [0-9]+ { printf( \"found an integer, = %d\\n\", atoi( yytext ) ); } \".\" printf( \"found a dot\\n\" ); 当然用 sc 来扫描注释是更加简便的一种方式，相比之前介绍的 input() 方式，这很简单。 %x comment %% int line_num = 1; \"/*\" BEGIN(comment); \u003ccomment\u003e[^*\\n]* /* eat anything that's not a '*' */ \u003ccomment\u003e\"*\"+[^*/\\n]* /* eat up '*'s not followed by '/'s */ \u003ccomment\u003e\\n ++line_num; \u003ccomment\u003e\"*\"+\"/\" BEGIN(INITIAL); 如果你希望得到一个高性能的扫描器，那就需要每个规则尽可能多的匹配文本。 另外，sc 的存储方式实际是 int，因此你可以采用如此方式记录上一个状态。 %x comment foo %% int line_num = 1; int comment_caller; \"/*\" { comment_caller = INITIAL; BEGIN(comment); } ... \u003cfoo\u003e\"/*\" { comment_caller = foo; BEGIN(comment); } \u003ccomment\u003e[^*\\n]* /* eat anything that's not a '*' */ \u003ccomment\u003e\"*\"+[^*/\\n]* /* eat up '*'s not followed by '/'s */ \u003ccomment\u003e\\n ++line_num; \u003ccomment\u003e\"*\"+\"/\" BEGIN(comment_caller); 一个更好的方式是，YY_START 可以记录当前的 sc 状态，因此写成 comment_caller = YY_START; 是更好的方式。如果你想兼容 lex 则可以用它的别名 YYSTATE。 最后，看一份 C 风格的字符串示例 %x str %% char string_buf[MAX_STR_CONST]; char *string_buf_ptr; \\\" string_buf_ptr = string_buf; BEGIN(str); \u003cstr\u003e\\\" { /* saw closing quote - all done */ BEGIN(INITIAL); *string_buf_ptr = '\\0'; /* return string constant token type and * value to parser */ } \u003cstr\u003e\\n { /* error - unterminated string constant */ /* generate error message */ } \u003cstr\u003e\\\\[0-7]{1,3} { /* octal escape sequence */ int result; (void) sscanf( yytext + 1, \"%o\", \u0026result ); if ( result \u003e 0xff ) /* error, constant is out-of-bounds */ *string_buf_ptr++ = result; } \u003cstr\u003e\\\\[0-9]+ { /* generate error - bad escape sequence; something * like '\\48' or '\\0777777' */ } \u003cstr\u003e\\\\n *string_buf_ptr++ = '\\n'; \u003cstr\u003e\\\\t *string_buf_ptr++ = '\\t'; \u003cstr\u003e\\\\r *string_buf_ptr++ = '\\r'; \u003cstr\u003e\\\\b *string_buf_ptr++ = '\\b'; \u003cstr\u003e\\\\f *string_buf_ptr++ = '\\f'; \u003cstr\u003e\\\\(.|\\n) *string_buf_ptr++ = yytext[1]; \u003cstr\u003e[^\\\\\\n\\\"]+ { char *yptr = yytext; while ( *yptr ) *string_buf_ptr++ = *yptr++; } 像上面的代码，在一个 sc 下可能有很多规则，你可以使用 Flex 提供的 scope 语法来将太们写在一起 \u003cstr\u003e{ \"\\\\n\" *string_buf_ptr++ = '\\n'; \"\\\\t\" *string_buf_ptr++ = '\\t'; \"\\\\r\" *string_buf_ptr++ = '\\r'; \"\\\\b\" *string_buf_ptr++ = '\\b'; \"\\\\f\" *string_buf_ptr++ = '\\f'; } 当启用 %option stack 之后，你也可以使用内建的 sc 栈，可能你需要用到以下函数 将状态压入栈 (即切换到状态) void yy_push_state(int new_state); 将状态弹出栈 (即回到上一个状态) void yy_pop_state(); 获取栈顶元素 (即获取当前状态的值) int yy_top_state(); sc 栈是动态增长的，也没有内建大小限制，但是内存用尽时程序将退出。 ","date":"05-01","objectID":"/2022/flex_and_bison/:1:3","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#开始条件"},{"categories":["CompilerPrinciple"],"content":" 多输入缓冲区一些扫描器 (如 C/C++ 支持的 include) 需要从多个输入流中读取，因此无用简单地通过到达文件末尾才读取下一个输入的 YY_INPUT 来控制，因为可能从使用 include 到文件末尾需要很长时间。 针对这一问题，Flex 给出了多个缓冲区之间的创建和切换机制。连带的是更多的 flex 函数。 创建缓冲区 YY_BUFFER_STATE yy_create_buffer(FILE *file, int size); // c style YY_BUFFER_STATE yy_new_buffer(FILE* file, int size); // c++ style (use new and delete) YY_BUFFER_STATE 是不透明结构 struct yy_buffer_state 的指针。如果重定义了没有使用 yyin 的 YY_INPUT，需要可以安全地将 NULL 传给 file。 切换缓冲区 void yy_switch_to_buffer(YY_BUFFER_STATE new_buffer); 将扫描器输入切换到新的缓冲区上，但并不会改变 sc。 删除缓冲区 void yy_delete_buffer(YY_BUFFER_STATE buffer); 压入状态 void yypush_buffer_state(YY_BUFFER_STATE buffer); 将新的状态推送到 flex 维护的内部堆栈，推送的状态成为新的当前状态。 弹出状态 void yypop_buffer_state(YY_BUFFER_STATE buffer); 将当前状态弹出栈，并删除缓冲区，并将堆栈的下一个状态 (如果有) 设置为新的当前状态。 丢弃缓冲区 void yy_flush_buffer(YY_BUFFER_STATE buffer); 丢弃当前缓冲区的所有内容，扫描器的下次匹配将先调用 YY_INPUT() 重新填充缓冲区。 最后还有一些宏，比如 YY_CURRENT_BUFFER 将返回当前缓冲区的 YY_BUFFER_STATE 句柄，但是请不要将它作为左值。 看下示例吧！Talk is cheap. Show me the code. 首先示例是一个关于 include 功能的实现，使用 yypush_buffer_state 和 yypop_buffer_state 实现 (利用 Flex 自身维护堆栈) /* the \"incl\" state is used for picking up the name * of an include file */ %x incl %% include BEGIN(incl); [a-z]+ ECHO; [^a-z\\n]*\\n? ECHO; \u003cincl\u003e[ \\t]* /* eat the whitespace */ \u003cincl\u003e[^ \\t\\n]+ { /* got the include file name */ yyin = fopen(yytext, \"r\"); if (!yyin) error(...); yypush_buffer_state(yy_create_buffer(yyin, YY_BUF_SIZE)); BEGIN(INITIAL); } \u003c\u003cEOF\u003e\u003e { yypop_buffer_state(); if (!YY_CURRENT_BUFFER) yyterminate(); } 当然也可以自己管理输入文件的堆栈，就比如下面这个等价的例子 /* the \"incl\" state is used for picking up the name * of an include file */ %x incl %{ #define MAX_INCLUDE_DEPTH 10 YY_BUFFER_STATE include_stack[MAX_INCLUDE_DEPTH]; int include_stack_ptr = 0; %} %% include BEGIN(incl); [a-z]+ ECHO; [^a-z\\n]*\\n? ECHO; \u003cincl\u003e[ \\t]* /* eat the whitespace */ \u003cincl\u003e[^ \\t\\n]+ { /* got the include file name */ if (include_stack_ptr \u003e= MAX_INCLUDE_DEPTH) { fprintf(stderr, \"Includes nested too deeply\"); exit(1); } include_stack[include_stack_ptr++] = YY_CURRENT_BUFFER; yyin = fopen( yytext, \"r\" ); if (!yyin) error(...); yy_switch_to_buffer(yy_create_buffer(yyin, YY_BUF_SIZE)); BEGIN(INITIAL); } \u003c\u003cEOF\u003e\u003e { if (--include_stack_ptr == 0) { yyterminate(); } else { yy_delete_buffer(YY_CURRENT_BUFFER); yy_switch_to_buffer(include_stack[include_stack_ptr]); } } 也可以在一个内存缓冲区上而非文件上进行缓冲区操作。当然只是在创建阶段有区别，其他阶段没有任何区别。 YY_BUFFER_STATE yy_scan_string(const char *str); // c style string YY_BUFFER_STATE yy_scan_bytes(const char *bytes, int len); // string with end of non-NULL YY_BUFFER_STATE yy_scan_buffer(char *base, yy_size_t size); // no copy buffer 前两者会复制一份数据，这在希望修改缓冲区内容时是安全的，但你想避免复制时需要使用第三个函数。需要注意，它并不是末尾 non-NULL 字符串，它的最后两个字节必须是 YY_END_OF_BUFFER_CHAR。因此真正扫描的数据在 0 到 size-2。 ","date":"05-01","objectID":"/2022/flex_and_bison/:1:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#多输入缓冲区"},{"categories":["CompilerPrinciple"],"content":" Flex 的各种奇奇怪怪的定义 奇奇怪怪的宏定义 YY_USER_ACTION 提供了一种始终在匹配 action 之前执行的操作。当使用该宏时，变量 yy_act 用于指示当前动作的编号 (下标从 1 开始)，而 YY_NUM_RULES 指出了规则的总数。因此你可以这样统计每个规则被执行了多少次 int ctr[YY_NUM_RULES]; #define YY_USER_ACTION ++ctr[yy_act] YY_USER_INIT 提供了在第一次扫描之前或者内部初始化之前，执行的操作。比如说打开日志文件。 yy_set_interactive(is_interactive) 控制当前缓冲区是否是交互的。交互式的缓冲区性能差，但输入源是交互式的可以避免由于等待填充缓冲区导致的问题。也可以使用操作 %option always-interactive 或 %option never-interactive 指定是否为交互式缓冲区，但该宏会覆盖这两个操作。其值零表示为非交互的。 yy_set_bol(at_bol) 控制当前缓冲区是否开启行头的扫描上下文。零值表示为 ^ 规则无效。宏 YY_AT_BOL 可以给出当前的情况。 奇奇怪怪的变量 char *yytext 当前 token 的文本。你可以修改它但不能增长它。另外 %array 模式不能在生成 C++ 版本扫描器时使用 int yyleng 当前 token 的文本长度。 FILE *yyin 当前读取文件的指针。如果想修改需要在扫描开始之前或遇到 EOF 之后，否则 UB，这种情况请使用 yyrestart()。 void yyrestart(FILE *new_file); 要求 yyin 指向新的输入文件。 FILE *yyout 当前输出的文件。 YY_CURRENT_BUFFER 当前缓冲区的 YY_BUFFER_STATE 句柄。 YY_START 当前 sc 的值，通常用于 BEGIN action。 ","date":"05-01","objectID":"/2022/flex_and_bison/:1:5","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-的各种奇奇怪怪的定义"},{"categories":["CompilerPrinciple"],"content":" Flex 的各种奇奇怪怪的定义 奇奇怪怪的宏定义 YY_USER_ACTION 提供了一种始终在匹配 action 之前执行的操作。当使用该宏时，变量 yy_act 用于指示当前动作的编号 (下标从 1 开始)，而 YY_NUM_RULES 指出了规则的总数。因此你可以这样统计每个规则被执行了多少次 int ctr[YY_NUM_RULES]; #define YY_USER_ACTION ++ctr[yy_act] YY_USER_INIT 提供了在第一次扫描之前或者内部初始化之前，执行的操作。比如说打开日志文件。 yy_set_interactive(is_interactive) 控制当前缓冲区是否是交互的。交互式的缓冲区性能差，但输入源是交互式的可以避免由于等待填充缓冲区导致的问题。也可以使用操作 %option always-interactive 或 %option never-interactive 指定是否为交互式缓冲区，但该宏会覆盖这两个操作。其值零表示为非交互的。 yy_set_bol(at_bol) 控制当前缓冲区是否开启行头的扫描上下文。零值表示为 ^ 规则无效。宏 YY_AT_BOL 可以给出当前的情况。 奇奇怪怪的变量 char *yytext 当前 token 的文本。你可以修改它但不能增长它。另外 %array 模式不能在生成 C++ 版本扫描器时使用 int yyleng 当前 token 的文本长度。 FILE *yyin 当前读取文件的指针。如果想修改需要在扫描开始之前或遇到 EOF 之后，否则 UB，这种情况请使用 yyrestart()。 void yyrestart(FILE *new_file); 要求 yyin 指向新的输入文件。 FILE *yyout 当前输出的文件。 YY_CURRENT_BUFFER 当前缓冲区的 YY_BUFFER_STATE 句柄。 YY_START 当前 sc 的值，通常用于 BEGIN action。 ","date":"05-01","objectID":"/2022/flex_and_bison/:1:5","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#奇奇怪怪的宏定义"},{"categories":["CompilerPrinciple"],"content":" Flex 的各种奇奇怪怪的定义 奇奇怪怪的宏定义 YY_USER_ACTION 提供了一种始终在匹配 action 之前执行的操作。当使用该宏时，变量 yy_act 用于指示当前动作的编号 (下标从 1 开始)，而 YY_NUM_RULES 指出了规则的总数。因此你可以这样统计每个规则被执行了多少次 int ctr[YY_NUM_RULES]; #define YY_USER_ACTION ++ctr[yy_act] YY_USER_INIT 提供了在第一次扫描之前或者内部初始化之前，执行的操作。比如说打开日志文件。 yy_set_interactive(is_interactive) 控制当前缓冲区是否是交互的。交互式的缓冲区性能差，但输入源是交互式的可以避免由于等待填充缓冲区导致的问题。也可以使用操作 %option always-interactive 或 %option never-interactive 指定是否为交互式缓冲区，但该宏会覆盖这两个操作。其值零表示为非交互的。 yy_set_bol(at_bol) 控制当前缓冲区是否开启行头的扫描上下文。零值表示为 ^ 规则无效。宏 YY_AT_BOL 可以给出当前的情况。 奇奇怪怪的变量 char *yytext 当前 token 的文本。你可以修改它但不能增长它。另外 %array 模式不能在生成 C++ 版本扫描器时使用 int yyleng 当前 token 的文本长度。 FILE *yyin 当前读取文件的指针。如果想修改需要在扫描开始之前或遇到 EOF 之后，否则 UB，这种情况请使用 yyrestart()。 void yyrestart(FILE *new_file); 要求 yyin 指向新的输入文件。 FILE *yyout 当前输出的文件。 YY_CURRENT_BUFFER 当前缓冲区的 YY_BUFFER_STATE 句柄。 YY_START 当前 sc 的值，通常用于 BEGIN action。 ","date":"05-01","objectID":"/2022/flex_and_bison/:1:5","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#奇奇怪怪的变量"},{"categories":["CompilerPrinciple"],"content":" 扫描器操作有很多 scanner option，但通常只需要指定一些就够了 %option 8bit reentrant bison-bridge %option warn nodefault %option yylineno %option outfile=\"scanner.c\" header-file=\"scanner.h\" flex 你可以在第一部分中写入 %option，大部分选项都以名称的形式给出，你可以在前面加上 no 来表示否形式。这些名称与相应的命令行选项相同。 如果你有些强迫症，需要将没用的过程全部关闭，不让 flex 生成相关过程。下列函数是默认生成的： input, unput yy_push_state, yy_pop_state, yy_top_state yy_scan_buffer, yy_scan_bytes, yy_scan_string yyget_extra, yyset_extra, yyget_leng, yyget_text, yyget_lineno, yyset_lineno, yyget_in, yyset_in, yyget_out, yyset_out, yyget_lval, yyset_lval, yyget_lloc, yyset_lloc, yyget_debug, yyset_debug 指定文件名操作 指定导出的头文件名称 long name: header-file option name: header-file param: FILE comment: 与 --c++ 选项不兼容 指定导出的源文件名称 short name: o long name: outfile option name: outfile param: FILE 将生成的扫描器写入 stdout 而不是 lex.yy.c short name: t long name: stdout option name: stdout 修改构造扫描器模板 short name: S long name: skel param: FILE comment: 除非你是 flex 的开发人员，否则不要使用此选项 将序列化的扫描器 DFA 表写入文件 long name: tables-file param: FILE comment: 生成的扫描器将不会包含该表，并在运行时加载 检查序列化表的一致性 long name: tables-verify comment: 开发选项。序列化 DFA 表，并在运行时与代码内表进行匹配验证，而不是加载序列化表 影响扫描器行为的选项 不区分大小写 short name: i long name: case-insensitive option name: case-insensitive comment: 虽然匹配将忽略大小写，但 yytext 获得的数据还是保留了大小写 AT\u0026T lex 兼容 short name: l long name: lex-compat option name: lex-compat comment: 最大成度兼容 AT\u0026T lex，但不保证完全兼容。会拖慢扫描器性能，还会导致大量选项不可用。 交互式扫描器 short name: I long name: interactive option name: interactive comment: 以性能换取足够的交互性，默认采用这种模式，除非你使用了 -Cf 或 -CF 提高性能的选项。 7 bit 扫描器 short name: 7 long name: 7bit option name: 7bit comment: 只能识别输入中的 7 bit 字符的扫描器。优点是器生成的表格的大小仅有 8bit 的一半，需要同时指定 -Cf 或 -CF 的表压缩选项才会有显著提升。但输入包含 8bit 字符将挂起或崩溃。 8 bit 扫描器 short name: 8 long name: 8bit option name: 8bit 生成默认规则 long name: default option name: default 始终交互式扫描器 long name: always-interactive option name: always-interactive comment: 通常新的输入文件上，扫描器都会调用 isatty() 来确定扫描器的源是否是交互式的，因此一次读入一个字符。该选项会认为始终是交互式源，不会有相关调用。 绝不交互式扫描器 long name: never-interactive option name: never-interactive POSIX lex 兼容 short name: x long name: posix option name: posix comment: 最大成度兼容 POSIX 1003.2-1992 定义的 lex，由于最初实现是为 POSIX 定义所设计的，因此只有很少的行为不一致。已知的是 cat 与重复 {} 之间的优先级问题，大多数 POSIX 程序使用的扩展正则表达式 (ERE) 优先级与 flex 默认优先级一致，都是 cat 低于重复 (即 ab{3} 将产生 abbb)，而 POSIX 定义下是高于的 (即 ab{3} 将产生 ababab)。 启用 sc 栈 long name: stack option name: stack 初始化输入输出为标准 IO long name: stdinit option name: stdinit 读取行号 long name: yylineno option name: yylineno 文件结束判断 long name: yywrap option name: yywrap 代码级和 API 操作 GNU Bison 支持 long name: bison-bridge option name: bison-bridge comment: 指示扫描器将被 GNU Bison 调用，增加了对 Bison 兼容性，以及对一些 API 的修改 GNU Bison locations 支持 long name: bison-locations option name: bison-locations comment: 指示扫描器正在使用 GNU Bison %locations，yylex 将而额外增加一个 yylloc 参数。这个选项意味着启用了上一个选项。 不生成 #line 指令 short name: L long name: noline option name: noline comment: 不加这个选项时，Flex 生成的 action 错误消息将相对原始，有助于错误定位 生成可重入的 C 扫描器 short name: R long name: reentrant option name: reentrant comment: 生成可重入的扫描器，该扫描器可能是线程安全的。但是 API 可能与非可重入扫描器有所区别。因此可能需要修改代码，另外 --c++ 选项与该选项不兼容。 生成 C++ 扫描器 short name: + long name: c++ option name: c++ yytext 使用数组实现 long name: array option name: array yytext 使用指针实现 long name: pointer option name: pointer 修改默认的前缀名称 short name: P long name: prefix option name: prefix param: PREFIX comment: 将全局可见变量和函数名称默认的 yy 前缀修改为指定前缀，比如 prefix=foo 将 yytext 变为了 footext，另外默认生成的源文件也会从 lex.yy.c 改为 lex.foo.c。以下是所有受影响的名称 yy_create_buffer yy_delete_buffer yy_flex_debug yy_init_buffer yy_flush_buffer yy_load_buffer_state yy_switch_to_buffer yyin yyleng yylex yylineno yyout yyrestart yytext yywrap yyalloc yyrealloc yyfree 但是，如果是 C++ 扫描器只会影响到 yywrap 和 yyFlexLexer。另外你需要自己实现对应名称的 yywrap。 生成一个默认的 main long name: main optino name: main comment: 为扫描器生成一个默认的、简单调用 yylex 的 main 函数，此选项会开启 noyywrap 选项。 禁止使用 unistd.h long name: nounistd optino name: nounistd comment: 针对不存在 POSIX 的环境，flex 将不包含头文件 unistd.h。但某些选项可能依赖于该头文件。 C++ 类名称 long name: nounistd optino name: nounistd param: NAME comment: 告诉 Flex 你将使用 NAME 作为 yyFlexLexer 派生的子类名称。代码将生成在子类中的","date":"05-01","objectID":"/2022/flex_and_bison/:1:6","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#扫描器操作"},{"categories":["CompilerPrinciple"],"content":" 扫描器操作有很多 scanner option，但通常只需要指定一些就够了 %option 8bit reentrant bison-bridge %option warn nodefault %option yylineno %option outfile=\"scanner.c\" header-file=\"scanner.h\" flex 你可以在第一部分中写入 %option，大部分选项都以名称的形式给出，你可以在前面加上 no 来表示否形式。这些名称与相应的命令行选项相同。 如果你有些强迫症，需要将没用的过程全部关闭，不让 flex 生成相关过程。下列函数是默认生成的： input, unput yy_push_state, yy_pop_state, yy_top_state yy_scan_buffer, yy_scan_bytes, yy_scan_string yyget_extra, yyset_extra, yyget_leng, yyget_text, yyget_lineno, yyset_lineno, yyget_in, yyset_in, yyget_out, yyset_out, yyget_lval, yyset_lval, yyget_lloc, yyset_lloc, yyget_debug, yyset_debug 指定文件名操作 指定导出的头文件名称 long name: header-file option name: header-file param: FILE comment: 与 --c++ 选项不兼容 指定导出的源文件名称 short name: o long name: outfile option name: outfile param: FILE 将生成的扫描器写入 stdout 而不是 lex.yy.c short name: t long name: stdout option name: stdout 修改构造扫描器模板 short name: S long name: skel param: FILE comment: 除非你是 flex 的开发人员，否则不要使用此选项 将序列化的扫描器 DFA 表写入文件 long name: tables-file param: FILE comment: 生成的扫描器将不会包含该表，并在运行时加载 检查序列化表的一致性 long name: tables-verify comment: 开发选项。序列化 DFA 表，并在运行时与代码内表进行匹配验证，而不是加载序列化表 影响扫描器行为的选项 不区分大小写 short name: i long name: case-insensitive option name: case-insensitive comment: 虽然匹配将忽略大小写，但 yytext 获得的数据还是保留了大小写 AT\u0026T lex 兼容 short name: l long name: lex-compat option name: lex-compat comment: 最大成度兼容 AT\u0026T lex，但不保证完全兼容。会拖慢扫描器性能，还会导致大量选项不可用。 交互式扫描器 short name: I long name: interactive option name: interactive comment: 以性能换取足够的交互性，默认采用这种模式，除非你使用了 -Cf 或 -CF 提高性能的选项。 7 bit 扫描器 short name: 7 long name: 7bit option name: 7bit comment: 只能识别输入中的 7 bit 字符的扫描器。优点是器生成的表格的大小仅有 8bit 的一半，需要同时指定 -Cf 或 -CF 的表压缩选项才会有显著提升。但输入包含 8bit 字符将挂起或崩溃。 8 bit 扫描器 short name: 8 long name: 8bit option name: 8bit 生成默认规则 long name: default option name: default 始终交互式扫描器 long name: always-interactive option name: always-interactive comment: 通常新的输入文件上，扫描器都会调用 isatty() 来确定扫描器的源是否是交互式的，因此一次读入一个字符。该选项会认为始终是交互式源，不会有相关调用。 绝不交互式扫描器 long name: never-interactive option name: never-interactive POSIX lex 兼容 short name: x long name: posix option name: posix comment: 最大成度兼容 POSIX 1003.2-1992 定义的 lex，由于最初实现是为 POSIX 定义所设计的，因此只有很少的行为不一致。已知的是 cat 与重复 {} 之间的优先级问题，大多数 POSIX 程序使用的扩展正则表达式 (ERE) 优先级与 flex 默认优先级一致，都是 cat 低于重复 (即 ab{3} 将产生 abbb)，而 POSIX 定义下是高于的 (即 ab{3} 将产生 ababab)。 启用 sc 栈 long name: stack option name: stack 初始化输入输出为标准 IO long name: stdinit option name: stdinit 读取行号 long name: yylineno option name: yylineno 文件结束判断 long name: yywrap option name: yywrap 代码级和 API 操作 GNU Bison 支持 long name: bison-bridge option name: bison-bridge comment: 指示扫描器将被 GNU Bison 调用，增加了对 Bison 兼容性，以及对一些 API 的修改 GNU Bison locations 支持 long name: bison-locations option name: bison-locations comment: 指示扫描器正在使用 GNU Bison %locations，yylex 将而额外增加一个 yylloc 参数。这个选项意味着启用了上一个选项。 不生成 #line 指令 short name: L long name: noline option name: noline comment: 不加这个选项时，Flex 生成的 action 错误消息将相对原始，有助于错误定位 生成可重入的 C 扫描器 short name: R long name: reentrant option name: reentrant comment: 生成可重入的扫描器，该扫描器可能是线程安全的。但是 API 可能与非可重入扫描器有所区别。因此可能需要修改代码，另外 --c++ 选项与该选项不兼容。 生成 C++ 扫描器 short name: + long name: c++ option name: c++ yytext 使用数组实现 long name: array option name: array yytext 使用指针实现 long name: pointer option name: pointer 修改默认的前缀名称 short name: P long name: prefix option name: prefix param: PREFIX comment: 将全局可见变量和函数名称默认的 yy 前缀修改为指定前缀，比如 prefix=foo 将 yytext 变为了 footext，另外默认生成的源文件也会从 lex.yy.c 改为 lex.foo.c。以下是所有受影响的名称 yy_create_buffer yy_delete_buffer yy_flex_debug yy_init_buffer yy_flush_buffer yy_load_buffer_state yy_switch_to_buffer yyin yyleng yylex yylineno yyout yyrestart yytext yywrap yyalloc yyrealloc yyfree 但是，如果是 C++ 扫描器只会影响到 yywrap 和 yyFlexLexer。另外你需要自己实现对应名称的 yywrap。 生成一个默认的 main long name: main optino name: main comment: 为扫描器生成一个默认的、简单调用 yylex 的 main 函数，此选项会开启 noyywrap 选项。 禁止使用 unistd.h long name: nounistd optino name: nounistd comment: 针对不存在 POSIX 的环境，flex 将不包含头文件 unistd.h。但某些选项可能依赖于该头文件。 C++ 类名称 long name: nounistd optino name: nounistd param: NAME comment: 告诉 Flex 你将使用 NAME 作为 yyFlexLexer 派生的子类名称。代码将生成在子类中的","date":"05-01","objectID":"/2022/flex_and_bison/:1:6","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#指定文件名操作"},{"categories":["CompilerPrinciple"],"content":" 扫描器操作有很多 scanner option，但通常只需要指定一些就够了 %option 8bit reentrant bison-bridge %option warn nodefault %option yylineno %option outfile=\"scanner.c\" header-file=\"scanner.h\" flex 你可以在第一部分中写入 %option，大部分选项都以名称的形式给出，你可以在前面加上 no 来表示否形式。这些名称与相应的命令行选项相同。 如果你有些强迫症，需要将没用的过程全部关闭，不让 flex 生成相关过程。下列函数是默认生成的： input, unput yy_push_state, yy_pop_state, yy_top_state yy_scan_buffer, yy_scan_bytes, yy_scan_string yyget_extra, yyset_extra, yyget_leng, yyget_text, yyget_lineno, yyset_lineno, yyget_in, yyset_in, yyget_out, yyset_out, yyget_lval, yyset_lval, yyget_lloc, yyset_lloc, yyget_debug, yyset_debug 指定文件名操作 指定导出的头文件名称 long name: header-file option name: header-file param: FILE comment: 与 --c++ 选项不兼容 指定导出的源文件名称 short name: o long name: outfile option name: outfile param: FILE 将生成的扫描器写入 stdout 而不是 lex.yy.c short name: t long name: stdout option name: stdout 修改构造扫描器模板 short name: S long name: skel param: FILE comment: 除非你是 flex 的开发人员，否则不要使用此选项 将序列化的扫描器 DFA 表写入文件 long name: tables-file param: FILE comment: 生成的扫描器将不会包含该表，并在运行时加载 检查序列化表的一致性 long name: tables-verify comment: 开发选项。序列化 DFA 表，并在运行时与代码内表进行匹配验证，而不是加载序列化表 影响扫描器行为的选项 不区分大小写 short name: i long name: case-insensitive option name: case-insensitive comment: 虽然匹配将忽略大小写，但 yytext 获得的数据还是保留了大小写 AT\u0026T lex 兼容 short name: l long name: lex-compat option name: lex-compat comment: 最大成度兼容 AT\u0026T lex，但不保证完全兼容。会拖慢扫描器性能，还会导致大量选项不可用。 交互式扫描器 short name: I long name: interactive option name: interactive comment: 以性能换取足够的交互性，默认采用这种模式，除非你使用了 -Cf 或 -CF 提高性能的选项。 7 bit 扫描器 short name: 7 long name: 7bit option name: 7bit comment: 只能识别输入中的 7 bit 字符的扫描器。优点是器生成的表格的大小仅有 8bit 的一半，需要同时指定 -Cf 或 -CF 的表压缩选项才会有显著提升。但输入包含 8bit 字符将挂起或崩溃。 8 bit 扫描器 short name: 8 long name: 8bit option name: 8bit 生成默认规则 long name: default option name: default 始终交互式扫描器 long name: always-interactive option name: always-interactive comment: 通常新的输入文件上，扫描器都会调用 isatty() 来确定扫描器的源是否是交互式的，因此一次读入一个字符。该选项会认为始终是交互式源，不会有相关调用。 绝不交互式扫描器 long name: never-interactive option name: never-interactive POSIX lex 兼容 short name: x long name: posix option name: posix comment: 最大成度兼容 POSIX 1003.2-1992 定义的 lex，由于最初实现是为 POSIX 定义所设计的，因此只有很少的行为不一致。已知的是 cat 与重复 {} 之间的优先级问题，大多数 POSIX 程序使用的扩展正则表达式 (ERE) 优先级与 flex 默认优先级一致，都是 cat 低于重复 (即 ab{3} 将产生 abbb)，而 POSIX 定义下是高于的 (即 ab{3} 将产生 ababab)。 启用 sc 栈 long name: stack option name: stack 初始化输入输出为标准 IO long name: stdinit option name: stdinit 读取行号 long name: yylineno option name: yylineno 文件结束判断 long name: yywrap option name: yywrap 代码级和 API 操作 GNU Bison 支持 long name: bison-bridge option name: bison-bridge comment: 指示扫描器将被 GNU Bison 调用，增加了对 Bison 兼容性，以及对一些 API 的修改 GNU Bison locations 支持 long name: bison-locations option name: bison-locations comment: 指示扫描器正在使用 GNU Bison %locations，yylex 将而额外增加一个 yylloc 参数。这个选项意味着启用了上一个选项。 不生成 #line 指令 short name: L long name: noline option name: noline comment: 不加这个选项时，Flex 生成的 action 错误消息将相对原始，有助于错误定位 生成可重入的 C 扫描器 short name: R long name: reentrant option name: reentrant comment: 生成可重入的扫描器，该扫描器可能是线程安全的。但是 API 可能与非可重入扫描器有所区别。因此可能需要修改代码，另外 --c++ 选项与该选项不兼容。 生成 C++ 扫描器 short name: + long name: c++ option name: c++ yytext 使用数组实现 long name: array option name: array yytext 使用指针实现 long name: pointer option name: pointer 修改默认的前缀名称 short name: P long name: prefix option name: prefix param: PREFIX comment: 将全局可见变量和函数名称默认的 yy 前缀修改为指定前缀，比如 prefix=foo 将 yytext 变为了 footext，另外默认生成的源文件也会从 lex.yy.c 改为 lex.foo.c。以下是所有受影响的名称 yy_create_buffer yy_delete_buffer yy_flex_debug yy_init_buffer yy_flush_buffer yy_load_buffer_state yy_switch_to_buffer yyin yyleng yylex yylineno yyout yyrestart yytext yywrap yyalloc yyrealloc yyfree 但是，如果是 C++ 扫描器只会影响到 yywrap 和 yyFlexLexer。另外你需要自己实现对应名称的 yywrap。 生成一个默认的 main long name: main optino name: main comment: 为扫描器生成一个默认的、简单调用 yylex 的 main 函数，此选项会开启 noyywrap 选项。 禁止使用 unistd.h long name: nounistd optino name: nounistd comment: 针对不存在 POSIX 的环境，flex 将不包含头文件 unistd.h。但某些选项可能依赖于该头文件。 C++ 类名称 long name: nounistd optino name: nounistd param: NAME comment: 告诉 Flex 你将使用 NAME 作为 yyFlexLexer 派生的子类名称。代码将生成在子类中的","date":"05-01","objectID":"/2022/flex_and_bison/:1:6","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#影响扫描器行为的选项"},{"categories":["CompilerPrinciple"],"content":" 扫描器操作有很多 scanner option，但通常只需要指定一些就够了 %option 8bit reentrant bison-bridge %option warn nodefault %option yylineno %option outfile=\"scanner.c\" header-file=\"scanner.h\" flex 你可以在第一部分中写入 %option，大部分选项都以名称的形式给出，你可以在前面加上 no 来表示否形式。这些名称与相应的命令行选项相同。 如果你有些强迫症，需要将没用的过程全部关闭，不让 flex 生成相关过程。下列函数是默认生成的： input, unput yy_push_state, yy_pop_state, yy_top_state yy_scan_buffer, yy_scan_bytes, yy_scan_string yyget_extra, yyset_extra, yyget_leng, yyget_text, yyget_lineno, yyset_lineno, yyget_in, yyset_in, yyget_out, yyset_out, yyget_lval, yyset_lval, yyget_lloc, yyset_lloc, yyget_debug, yyset_debug 指定文件名操作 指定导出的头文件名称 long name: header-file option name: header-file param: FILE comment: 与 --c++ 选项不兼容 指定导出的源文件名称 short name: o long name: outfile option name: outfile param: FILE 将生成的扫描器写入 stdout 而不是 lex.yy.c short name: t long name: stdout option name: stdout 修改构造扫描器模板 short name: S long name: skel param: FILE comment: 除非你是 flex 的开发人员，否则不要使用此选项 将序列化的扫描器 DFA 表写入文件 long name: tables-file param: FILE comment: 生成的扫描器将不会包含该表，并在运行时加载 检查序列化表的一致性 long name: tables-verify comment: 开发选项。序列化 DFA 表，并在运行时与代码内表进行匹配验证，而不是加载序列化表 影响扫描器行为的选项 不区分大小写 short name: i long name: case-insensitive option name: case-insensitive comment: 虽然匹配将忽略大小写，但 yytext 获得的数据还是保留了大小写 AT\u0026T lex 兼容 short name: l long name: lex-compat option name: lex-compat comment: 最大成度兼容 AT\u0026T lex，但不保证完全兼容。会拖慢扫描器性能，还会导致大量选项不可用。 交互式扫描器 short name: I long name: interactive option name: interactive comment: 以性能换取足够的交互性，默认采用这种模式，除非你使用了 -Cf 或 -CF 提高性能的选项。 7 bit 扫描器 short name: 7 long name: 7bit option name: 7bit comment: 只能识别输入中的 7 bit 字符的扫描器。优点是器生成的表格的大小仅有 8bit 的一半，需要同时指定 -Cf 或 -CF 的表压缩选项才会有显著提升。但输入包含 8bit 字符将挂起或崩溃。 8 bit 扫描器 short name: 8 long name: 8bit option name: 8bit 生成默认规则 long name: default option name: default 始终交互式扫描器 long name: always-interactive option name: always-interactive comment: 通常新的输入文件上，扫描器都会调用 isatty() 来确定扫描器的源是否是交互式的，因此一次读入一个字符。该选项会认为始终是交互式源，不会有相关调用。 绝不交互式扫描器 long name: never-interactive option name: never-interactive POSIX lex 兼容 short name: x long name: posix option name: posix comment: 最大成度兼容 POSIX 1003.2-1992 定义的 lex，由于最初实现是为 POSIX 定义所设计的，因此只有很少的行为不一致。已知的是 cat 与重复 {} 之间的优先级问题，大多数 POSIX 程序使用的扩展正则表达式 (ERE) 优先级与 flex 默认优先级一致，都是 cat 低于重复 (即 ab{3} 将产生 abbb)，而 POSIX 定义下是高于的 (即 ab{3} 将产生 ababab)。 启用 sc 栈 long name: stack option name: stack 初始化输入输出为标准 IO long name: stdinit option name: stdinit 读取行号 long name: yylineno option name: yylineno 文件结束判断 long name: yywrap option name: yywrap 代码级和 API 操作 GNU Bison 支持 long name: bison-bridge option name: bison-bridge comment: 指示扫描器将被 GNU Bison 调用，增加了对 Bison 兼容性，以及对一些 API 的修改 GNU Bison locations 支持 long name: bison-locations option name: bison-locations comment: 指示扫描器正在使用 GNU Bison %locations，yylex 将而额外增加一个 yylloc 参数。这个选项意味着启用了上一个选项。 不生成 #line 指令 short name: L long name: noline option name: noline comment: 不加这个选项时，Flex 生成的 action 错误消息将相对原始，有助于错误定位 生成可重入的 C 扫描器 short name: R long name: reentrant option name: reentrant comment: 生成可重入的扫描器，该扫描器可能是线程安全的。但是 API 可能与非可重入扫描器有所区别。因此可能需要修改代码，另外 --c++ 选项与该选项不兼容。 生成 C++ 扫描器 short name: + long name: c++ option name: c++ yytext 使用数组实现 long name: array option name: array yytext 使用指针实现 long name: pointer option name: pointer 修改默认的前缀名称 short name: P long name: prefix option name: prefix param: PREFIX comment: 将全局可见变量和函数名称默认的 yy 前缀修改为指定前缀，比如 prefix=foo 将 yytext 变为了 footext，另外默认生成的源文件也会从 lex.yy.c 改为 lex.foo.c。以下是所有受影响的名称 yy_create_buffer yy_delete_buffer yy_flex_debug yy_init_buffer yy_flush_buffer yy_load_buffer_state yy_switch_to_buffer yyin yyleng yylex yylineno yyout yyrestart yytext yywrap yyalloc yyrealloc yyfree 但是，如果是 C++ 扫描器只会影响到 yywrap 和 yyFlexLexer。另外你需要自己实现对应名称的 yywrap。 生成一个默认的 main long name: main optino name: main comment: 为扫描器生成一个默认的、简单调用 yylex 的 main 函数，此选项会开启 noyywrap 选项。 禁止使用 unistd.h long name: nounistd optino name: nounistd comment: 针对不存在 POSIX 的环境，flex 将不包含头文件 unistd.h。但某些选项可能依赖于该头文件。 C++ 类名称 long name: nounistd optino name: nounistd param: NAME comment: 告诉 Flex 你将使用 NAME 作为 yyFlexLexer 派生的子类名称。代码将生成在子类中的","date":"05-01","objectID":"/2022/flex_and_bison/:1:6","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#代码级和-api-操作"},{"categories":["CompilerPrinciple"],"content":" 扫描器操作有很多 scanner option，但通常只需要指定一些就够了 %option 8bit reentrant bison-bridge %option warn nodefault %option yylineno %option outfile=\"scanner.c\" header-file=\"scanner.h\" flex 你可以在第一部分中写入 %option，大部分选项都以名称的形式给出，你可以在前面加上 no 来表示否形式。这些名称与相应的命令行选项相同。 如果你有些强迫症，需要将没用的过程全部关闭，不让 flex 生成相关过程。下列函数是默认生成的： input, unput yy_push_state, yy_pop_state, yy_top_state yy_scan_buffer, yy_scan_bytes, yy_scan_string yyget_extra, yyset_extra, yyget_leng, yyget_text, yyget_lineno, yyset_lineno, yyget_in, yyset_in, yyget_out, yyset_out, yyget_lval, yyset_lval, yyget_lloc, yyset_lloc, yyget_debug, yyset_debug 指定文件名操作 指定导出的头文件名称 long name: header-file option name: header-file param: FILE comment: 与 --c++ 选项不兼容 指定导出的源文件名称 short name: o long name: outfile option name: outfile param: FILE 将生成的扫描器写入 stdout 而不是 lex.yy.c short name: t long name: stdout option name: stdout 修改构造扫描器模板 short name: S long name: skel param: FILE comment: 除非你是 flex 的开发人员，否则不要使用此选项 将序列化的扫描器 DFA 表写入文件 long name: tables-file param: FILE comment: 生成的扫描器将不会包含该表，并在运行时加载 检查序列化表的一致性 long name: tables-verify comment: 开发选项。序列化 DFA 表，并在运行时与代码内表进行匹配验证，而不是加载序列化表 影响扫描器行为的选项 不区分大小写 short name: i long name: case-insensitive option name: case-insensitive comment: 虽然匹配将忽略大小写，但 yytext 获得的数据还是保留了大小写 AT\u0026T lex 兼容 short name: l long name: lex-compat option name: lex-compat comment: 最大成度兼容 AT\u0026T lex，但不保证完全兼容。会拖慢扫描器性能，还会导致大量选项不可用。 交互式扫描器 short name: I long name: interactive option name: interactive comment: 以性能换取足够的交互性，默认采用这种模式，除非你使用了 -Cf 或 -CF 提高性能的选项。 7 bit 扫描器 short name: 7 long name: 7bit option name: 7bit comment: 只能识别输入中的 7 bit 字符的扫描器。优点是器生成的表格的大小仅有 8bit 的一半，需要同时指定 -Cf 或 -CF 的表压缩选项才会有显著提升。但输入包含 8bit 字符将挂起或崩溃。 8 bit 扫描器 short name: 8 long name: 8bit option name: 8bit 生成默认规则 long name: default option name: default 始终交互式扫描器 long name: always-interactive option name: always-interactive comment: 通常新的输入文件上，扫描器都会调用 isatty() 来确定扫描器的源是否是交互式的，因此一次读入一个字符。该选项会认为始终是交互式源，不会有相关调用。 绝不交互式扫描器 long name: never-interactive option name: never-interactive POSIX lex 兼容 short name: x long name: posix option name: posix comment: 最大成度兼容 POSIX 1003.2-1992 定义的 lex，由于最初实现是为 POSIX 定义所设计的，因此只有很少的行为不一致。已知的是 cat 与重复 {} 之间的优先级问题，大多数 POSIX 程序使用的扩展正则表达式 (ERE) 优先级与 flex 默认优先级一致，都是 cat 低于重复 (即 ab{3} 将产生 abbb)，而 POSIX 定义下是高于的 (即 ab{3} 将产生 ababab)。 启用 sc 栈 long name: stack option name: stack 初始化输入输出为标准 IO long name: stdinit option name: stdinit 读取行号 long name: yylineno option name: yylineno 文件结束判断 long name: yywrap option name: yywrap 代码级和 API 操作 GNU Bison 支持 long name: bison-bridge option name: bison-bridge comment: 指示扫描器将被 GNU Bison 调用，增加了对 Bison 兼容性，以及对一些 API 的修改 GNU Bison locations 支持 long name: bison-locations option name: bison-locations comment: 指示扫描器正在使用 GNU Bison %locations，yylex 将而额外增加一个 yylloc 参数。这个选项意味着启用了上一个选项。 不生成 #line 指令 short name: L long name: noline option name: noline comment: 不加这个选项时，Flex 生成的 action 错误消息将相对原始，有助于错误定位 生成可重入的 C 扫描器 short name: R long name: reentrant option name: reentrant comment: 生成可重入的扫描器，该扫描器可能是线程安全的。但是 API 可能与非可重入扫描器有所区别。因此可能需要修改代码，另外 --c++ 选项与该选项不兼容。 生成 C++ 扫描器 short name: + long name: c++ option name: c++ yytext 使用数组实现 long name: array option name: array yytext 使用指针实现 long name: pointer option name: pointer 修改默认的前缀名称 short name: P long name: prefix option name: prefix param: PREFIX comment: 将全局可见变量和函数名称默认的 yy 前缀修改为指定前缀，比如 prefix=foo 将 yytext 变为了 footext，另外默认生成的源文件也会从 lex.yy.c 改为 lex.foo.c。以下是所有受影响的名称 yy_create_buffer yy_delete_buffer yy_flex_debug yy_init_buffer yy_flush_buffer yy_load_buffer_state yy_switch_to_buffer yyin yyleng yylex yylineno yyout yyrestart yytext yywrap yyalloc yyrealloc yyfree 但是，如果是 C++ 扫描器只会影响到 yywrap 和 yyFlexLexer。另外你需要自己实现对应名称的 yywrap。 生成一个默认的 main long name: main optino name: main comment: 为扫描器生成一个默认的、简单调用 yylex 的 main 函数，此选项会开启 noyywrap 选项。 禁止使用 unistd.h long name: nounistd optino name: nounistd comment: 针对不存在 POSIX 的环境，flex 将不包含头文件 unistd.h。但某些选项可能依赖于该头文件。 C++ 类名称 long name: nounistd optino name: nounistd param: NAME comment: 告诉 Flex 你将使用 NAME 作为 yyFlexLexer 派生的子类名称。代码将生成在子类中的","date":"05-01","objectID":"/2022/flex_and_bison/:1:6","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#生成扫描器速度和大小的选项"},{"categories":["CompilerPrinciple"],"content":" 扫描器操作有很多 scanner option，但通常只需要指定一些就够了 %option 8bit reentrant bison-bridge %option warn nodefault %option yylineno %option outfile=\"scanner.c\" header-file=\"scanner.h\" flex 你可以在第一部分中写入 %option，大部分选项都以名称的形式给出，你可以在前面加上 no 来表示否形式。这些名称与相应的命令行选项相同。 如果你有些强迫症，需要将没用的过程全部关闭，不让 flex 生成相关过程。下列函数是默认生成的： input, unput yy_push_state, yy_pop_state, yy_top_state yy_scan_buffer, yy_scan_bytes, yy_scan_string yyget_extra, yyset_extra, yyget_leng, yyget_text, yyget_lineno, yyset_lineno, yyget_in, yyset_in, yyget_out, yyset_out, yyget_lval, yyset_lval, yyget_lloc, yyset_lloc, yyget_debug, yyset_debug 指定文件名操作 指定导出的头文件名称 long name: header-file option name: header-file param: FILE comment: 与 --c++ 选项不兼容 指定导出的源文件名称 short name: o long name: outfile option name: outfile param: FILE 将生成的扫描器写入 stdout 而不是 lex.yy.c short name: t long name: stdout option name: stdout 修改构造扫描器模板 short name: S long name: skel param: FILE comment: 除非你是 flex 的开发人员，否则不要使用此选项 将序列化的扫描器 DFA 表写入文件 long name: tables-file param: FILE comment: 生成的扫描器将不会包含该表，并在运行时加载 检查序列化表的一致性 long name: tables-verify comment: 开发选项。序列化 DFA 表，并在运行时与代码内表进行匹配验证，而不是加载序列化表 影响扫描器行为的选项 不区分大小写 short name: i long name: case-insensitive option name: case-insensitive comment: 虽然匹配将忽略大小写，但 yytext 获得的数据还是保留了大小写 AT\u0026T lex 兼容 short name: l long name: lex-compat option name: lex-compat comment: 最大成度兼容 AT\u0026T lex，但不保证完全兼容。会拖慢扫描器性能，还会导致大量选项不可用。 交互式扫描器 short name: I long name: interactive option name: interactive comment: 以性能换取足够的交互性，默认采用这种模式，除非你使用了 -Cf 或 -CF 提高性能的选项。 7 bit 扫描器 short name: 7 long name: 7bit option name: 7bit comment: 只能识别输入中的 7 bit 字符的扫描器。优点是器生成的表格的大小仅有 8bit 的一半，需要同时指定 -Cf 或 -CF 的表压缩选项才会有显著提升。但输入包含 8bit 字符将挂起或崩溃。 8 bit 扫描器 short name: 8 long name: 8bit option name: 8bit 生成默认规则 long name: default option name: default 始终交互式扫描器 long name: always-interactive option name: always-interactive comment: 通常新的输入文件上，扫描器都会调用 isatty() 来确定扫描器的源是否是交互式的，因此一次读入一个字符。该选项会认为始终是交互式源，不会有相关调用。 绝不交互式扫描器 long name: never-interactive option name: never-interactive POSIX lex 兼容 short name: x long name: posix option name: posix comment: 最大成度兼容 POSIX 1003.2-1992 定义的 lex，由于最初实现是为 POSIX 定义所设计的，因此只有很少的行为不一致。已知的是 cat 与重复 {} 之间的优先级问题，大多数 POSIX 程序使用的扩展正则表达式 (ERE) 优先级与 flex 默认优先级一致，都是 cat 低于重复 (即 ab{3} 将产生 abbb)，而 POSIX 定义下是高于的 (即 ab{3} 将产生 ababab)。 启用 sc 栈 long name: stack option name: stack 初始化输入输出为标准 IO long name: stdinit option name: stdinit 读取行号 long name: yylineno option name: yylineno 文件结束判断 long name: yywrap option name: yywrap 代码级和 API 操作 GNU Bison 支持 long name: bison-bridge option name: bison-bridge comment: 指示扫描器将被 GNU Bison 调用，增加了对 Bison 兼容性，以及对一些 API 的修改 GNU Bison locations 支持 long name: bison-locations option name: bison-locations comment: 指示扫描器正在使用 GNU Bison %locations，yylex 将而额外增加一个 yylloc 参数。这个选项意味着启用了上一个选项。 不生成 #line 指令 short name: L long name: noline option name: noline comment: 不加这个选项时，Flex 生成的 action 错误消息将相对原始，有助于错误定位 生成可重入的 C 扫描器 short name: R long name: reentrant option name: reentrant comment: 生成可重入的扫描器，该扫描器可能是线程安全的。但是 API 可能与非可重入扫描器有所区别。因此可能需要修改代码，另外 --c++ 选项与该选项不兼容。 生成 C++ 扫描器 short name: + long name: c++ option name: c++ yytext 使用数组实现 long name: array option name: array yytext 使用指针实现 long name: pointer option name: pointer 修改默认的前缀名称 short name: P long name: prefix option name: prefix param: PREFIX comment: 将全局可见变量和函数名称默认的 yy 前缀修改为指定前缀，比如 prefix=foo 将 yytext 变为了 footext，另外默认生成的源文件也会从 lex.yy.c 改为 lex.foo.c。以下是所有受影响的名称 yy_create_buffer yy_delete_buffer yy_flex_debug yy_init_buffer yy_flush_buffer yy_load_buffer_state yy_switch_to_buffer yyin yyleng yylex yylineno yyout yyrestart yytext yywrap yyalloc yyrealloc yyfree 但是，如果是 C++ 扫描器只会影响到 yywrap 和 yyFlexLexer。另外你需要自己实现对应名称的 yywrap。 生成一个默认的 main long name: main optino name: main comment: 为扫描器生成一个默认的、简单调用 yylex 的 main 函数，此选项会开启 noyywrap 选项。 禁止使用 unistd.h long name: nounistd optino name: nounistd comment: 针对不存在 POSIX 的环境，flex 将不包含头文件 unistd.h。但某些选项可能依赖于该头文件。 C++ 类名称 long name: nounistd optino name: nounistd param: NAME comment: 告诉 Flex 你将使用 NAME 作为 yyFlexLexer 派生的子类名称。代码将生成在子类中的","date":"05-01","objectID":"/2022/flex_and_bison/:1:6","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-debugging-选项"},{"categories":["CompilerPrinciple"],"content":" Flex 的性能考虑Flex 的首要目标是构造高性能扫描器，因此已经针对大量规则进行了优化。但是除了表格与性能的取舍外，还有很多操作会降低性能，这里列出从严重影响性能到轻微影响性能的操作： REJECT 可变的尾随上下文 需要备份的模式集 %option yylineno %array %option interactive %option always-interactive ^ 运算符 yymore() 另外需要注意，unput 的实现可能会有大量的调用，而 yyless 很轻量，因此只是放回扫描的多余文本，请使用后者。 ","date":"05-01","objectID":"/2022/flex_and_bison/:1:7","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-的性能考虑"},{"categories":["CompilerPrinciple"],"content":" 可重入 C 扫描器flex 具有生成可移植的可重入 C 扫描器的能力。简单地说，即可以不需要与其他线程同步的情况下，创建多线程并行的扫描器。另外根据 info 的描述，所有的 C++ 扫描器都是可重入的。 Flex 可重入扫描器的用途你可以同时扫描两个或多个文件来对比 token 级别的差异，而非字符串级别的差异，比如 /* Example of maintaining more than one active scanner. */ do { int tok1 = yylex(scanner_1); int tok2 = yylex(scanner_2); if (tok1 != tok2) { printf(\"Files are different.\\n\"); } } while (tok1 \u0026\u0026 tok2); 另一个用途是创建递归扫描器，虽然也可以通过不可重入扫描器和缓冲区状态实现。下面是一个 eval 的实现 /* Example of recursive invocation. */ %option reentrant %% \"eval(\".+\")\" { yyscan_t scanner; YY_BUFFER_STATE buf; yylex_init( \u0026scanner ); yytext[yyleng-1] = ' '; buf = yy_scan_string( yytext + 5, scanner ); yylex( scanner ); yy_delete_buffer(buf,scanner); yylex_destroy( scanner ); } ... %% 可重入 API 概述可重入扫描器与不可重入扫描器有一定区别。 所有的函数需要加上参数 yyscanner 所有的全局变量被它们的相应的等价宏替代，比如 yytext 被替换为 #define yytext (((struct yyguts_t*)yyscanner)-\u003eyytext_r) yylex_init 和 yylex_destroy 必须分别在 yylex 之前和之后调用 int yylex_init(yyscan_t *ptr_yy_globals); int yylex_init_extra(YY_EXTRA_TYPE user_defined, yyscan_t *ptr_yy_globals); int yylex(yyscan_t yyscanner); int yylex_destroy(yyscan_t yyscanner); 使用访问器方法 (get/set) 对常见的 flex 变量进行访问，格式为 yyget_NAME 或 yyset_NAME，另外还有额外的参数 yyscanner /* Set the last character of yytext to NULL. */ void chop(yyscan_t scanner) { int len = yyget_leng(scanner); yyget_text(scanner)[len - 1] = '\\0'; } 用户特定的数据可以存储在 yyextra 中。可重入场景下，不能直接访问全局变量，因此用户的全局状态可以存储于 yyextra 中。 #define YY_EXTRA_TYPE void* YY_EXTRA_TYPE yyget_extra(yyscan_t scanner); void yyset_extra(YY_EXTRA_TYPE arbitrary_data, yyscan_t scanner); 一个 Flex 可重入扫描器的示例 /* This scanner prints \"//\" comments. */ %option reentrant stack noyywrap %x COMMENT %% \"//\" yy_push_state(COMMENT, yyscanner); .|\\n \u003cCOMMENT\u003e\\n yy_pop_state(yyscanner); \u003cCOMMENT\u003e[^\\n]+ fprintf(yyout, \"%s\\n\", yytext); %% int main(int argc, char *argv[]) { yyscan_t scanner; yylex_init(\u0026scanner); yylex(scanner); yylex_destroy(scanner); return 0; } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:8","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#可重入-c-扫描器"},{"categories":["CompilerPrinciple"],"content":" 可重入 C 扫描器flex 具有生成可移植的可重入 C 扫描器的能力。简单地说，即可以不需要与其他线程同步的情况下，创建多线程并行的扫描器。另外根据 info 的描述，所有的 C++ 扫描器都是可重入的。 Flex 可重入扫描器的用途你可以同时扫描两个或多个文件来对比 token 级别的差异，而非字符串级别的差异，比如 /* Example of maintaining more than one active scanner. */ do { int tok1 = yylex(scanner_1); int tok2 = yylex(scanner_2); if (tok1 != tok2) { printf(\"Files are different.\\n\"); } } while (tok1 \u0026\u0026 tok2); 另一个用途是创建递归扫描器，虽然也可以通过不可重入扫描器和缓冲区状态实现。下面是一个 eval 的实现 /* Example of recursive invocation. */ %option reentrant %% \"eval(\".+\")\" { yyscan_t scanner; YY_BUFFER_STATE buf; yylex_init( \u0026scanner ); yytext[yyleng-1] = ' '; buf = yy_scan_string( yytext + 5, scanner ); yylex( scanner ); yy_delete_buffer(buf,scanner); yylex_destroy( scanner ); } ... %% 可重入 API 概述可重入扫描器与不可重入扫描器有一定区别。 所有的函数需要加上参数 yyscanner 所有的全局变量被它们的相应的等价宏替代，比如 yytext 被替换为 #define yytext (((struct yyguts_t*)yyscanner)-\u003eyytext_r) yylex_init 和 yylex_destroy 必须分别在 yylex 之前和之后调用 int yylex_init(yyscan_t *ptr_yy_globals); int yylex_init_extra(YY_EXTRA_TYPE user_defined, yyscan_t *ptr_yy_globals); int yylex(yyscan_t yyscanner); int yylex_destroy(yyscan_t yyscanner); 使用访问器方法 (get/set) 对常见的 flex 变量进行访问，格式为 yyget_NAME 或 yyset_NAME，另外还有额外的参数 yyscanner /* Set the last character of yytext to NULL. */ void chop(yyscan_t scanner) { int len = yyget_leng(scanner); yyget_text(scanner)[len - 1] = '\\0'; } 用户特定的数据可以存储在 yyextra 中。可重入场景下，不能直接访问全局变量，因此用户的全局状态可以存储于 yyextra 中。 #define YY_EXTRA_TYPE void* YY_EXTRA_TYPE yyget_extra(yyscan_t scanner); void yyset_extra(YY_EXTRA_TYPE arbitrary_data, yyscan_t scanner); 一个 Flex 可重入扫描器的示例 /* This scanner prints \"//\" comments. */ %option reentrant stack noyywrap %x COMMENT %% \"//\" yy_push_state(COMMENT, yyscanner); .|\\n \\n yy_pop_state(yyscanner); [^\\n]+ fprintf(yyout, \"%s\\n\", yytext); %% int main(int argc, char *argv[]) { yyscan_t scanner; yylex_init(\u0026scanner); yylex(scanner); yylex_destroy(scanner); return 0; } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:8","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#flex-可重入扫描器的用途"},{"categories":["CompilerPrinciple"],"content":" 可重入 C 扫描器flex 具有生成可移植的可重入 C 扫描器的能力。简单地说，即可以不需要与其他线程同步的情况下，创建多线程并行的扫描器。另外根据 info 的描述，所有的 C++ 扫描器都是可重入的。 Flex 可重入扫描器的用途你可以同时扫描两个或多个文件来对比 token 级别的差异，而非字符串级别的差异，比如 /* Example of maintaining more than one active scanner. */ do { int tok1 = yylex(scanner_1); int tok2 = yylex(scanner_2); if (tok1 != tok2) { printf(\"Files are different.\\n\"); } } while (tok1 \u0026\u0026 tok2); 另一个用途是创建递归扫描器，虽然也可以通过不可重入扫描器和缓冲区状态实现。下面是一个 eval 的实现 /* Example of recursive invocation. */ %option reentrant %% \"eval(\".+\")\" { yyscan_t scanner; YY_BUFFER_STATE buf; yylex_init( \u0026scanner ); yytext[yyleng-1] = ' '; buf = yy_scan_string( yytext + 5, scanner ); yylex( scanner ); yy_delete_buffer(buf,scanner); yylex_destroy( scanner ); } ... %% 可重入 API 概述可重入扫描器与不可重入扫描器有一定区别。 所有的函数需要加上参数 yyscanner 所有的全局变量被它们的相应的等价宏替代，比如 yytext 被替换为 #define yytext (((struct yyguts_t*)yyscanner)-\u003eyytext_r) yylex_init 和 yylex_destroy 必须分别在 yylex 之前和之后调用 int yylex_init(yyscan_t *ptr_yy_globals); int yylex_init_extra(YY_EXTRA_TYPE user_defined, yyscan_t *ptr_yy_globals); int yylex(yyscan_t yyscanner); int yylex_destroy(yyscan_t yyscanner); 使用访问器方法 (get/set) 对常见的 flex 变量进行访问，格式为 yyget_NAME 或 yyset_NAME，另外还有额外的参数 yyscanner /* Set the last character of yytext to NULL. */ void chop(yyscan_t scanner) { int len = yyget_leng(scanner); yyget_text(scanner)[len - 1] = '\\0'; } 用户特定的数据可以存储在 yyextra 中。可重入场景下，不能直接访问全局变量，因此用户的全局状态可以存储于 yyextra 中。 #define YY_EXTRA_TYPE void* YY_EXTRA_TYPE yyget_extra(yyscan_t scanner); void yyset_extra(YY_EXTRA_TYPE arbitrary_data, yyscan_t scanner); 一个 Flex 可重入扫描器的示例 /* This scanner prints \"//\" comments. */ %option reentrant stack noyywrap %x COMMENT %% \"//\" yy_push_state(COMMENT, yyscanner); .|\\n \\n yy_pop_state(yyscanner); [^\\n]+ fprintf(yyout, \"%s\\n\", yytext); %% int main(int argc, char *argv[]) { yyscan_t scanner; yylex_init(\u0026scanner); yylex(scanner); yylex_destroy(scanner); return 0; } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:8","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#可重入-api-概述"},{"categories":["CompilerPrinciple"],"content":" 可重入 C 扫描器flex 具有生成可移植的可重入 C 扫描器的能力。简单地说，即可以不需要与其他线程同步的情况下，创建多线程并行的扫描器。另外根据 info 的描述，所有的 C++ 扫描器都是可重入的。 Flex 可重入扫描器的用途你可以同时扫描两个或多个文件来对比 token 级别的差异，而非字符串级别的差异，比如 /* Example of maintaining more than one active scanner. */ do { int tok1 = yylex(scanner_1); int tok2 = yylex(scanner_2); if (tok1 != tok2) { printf(\"Files are different.\\n\"); } } while (tok1 \u0026\u0026 tok2); 另一个用途是创建递归扫描器，虽然也可以通过不可重入扫描器和缓冲区状态实现。下面是一个 eval 的实现 /* Example of recursive invocation. */ %option reentrant %% \"eval(\".+\")\" { yyscan_t scanner; YY_BUFFER_STATE buf; yylex_init( \u0026scanner ); yytext[yyleng-1] = ' '; buf = yy_scan_string( yytext + 5, scanner ); yylex( scanner ); yy_delete_buffer(buf,scanner); yylex_destroy( scanner ); } ... %% 可重入 API 概述可重入扫描器与不可重入扫描器有一定区别。 所有的函数需要加上参数 yyscanner 所有的全局变量被它们的相应的等价宏替代，比如 yytext 被替换为 #define yytext (((struct yyguts_t*)yyscanner)-\u003eyytext_r) yylex_init 和 yylex_destroy 必须分别在 yylex 之前和之后调用 int yylex_init(yyscan_t *ptr_yy_globals); int yylex_init_extra(YY_EXTRA_TYPE user_defined, yyscan_t *ptr_yy_globals); int yylex(yyscan_t yyscanner); int yylex_destroy(yyscan_t yyscanner); 使用访问器方法 (get/set) 对常见的 flex 变量进行访问，格式为 yyget_NAME 或 yyset_NAME，另外还有额外的参数 yyscanner /* Set the last character of yytext to NULL. */ void chop(yyscan_t scanner) { int len = yyget_leng(scanner); yyget_text(scanner)[len - 1] = '\\0'; } 用户特定的数据可以存储在 yyextra 中。可重入场景下，不能直接访问全局变量，因此用户的全局状态可以存储于 yyextra 中。 #define YY_EXTRA_TYPE void* YY_EXTRA_TYPE yyget_extra(yyscan_t scanner); void yyset_extra(YY_EXTRA_TYPE arbitrary_data, yyscan_t scanner); 一个 Flex 可重入扫描器的示例 /* This scanner prints \"//\" comments. */ %option reentrant stack noyywrap %x COMMENT %% \"//\" yy_push_state(COMMENT, yyscanner); .|\\n \\n yy_pop_state(yyscanner); [^\\n]+ fprintf(yyout, \"%s\\n\", yytext); %% int main(int argc, char *argv[]) { yyscan_t scanner; yylex_init(\u0026scanner); yylex(scanner); yylex_destroy(scanner); return 0; } ","date":"05-01","objectID":"/2022/flex_and_bison/:1:8","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#一个-flex-可重入扫描器的示例"},{"categories":["CompilerPrinciple"],"content":" GNU BisonGNU Bison 与 Flex 一样，都是遵循 GPL 协议发布的软件，不同的是，它是 GNU 项目！ GNU Bison 是一个通用的语法分析生成器，它可以将上下文无关文法生成使用 LALR(1) 分析表的确定性 LR 或通用 LR (GLR) 解析器。另外还可以生成实验性的 IELR(1) 或规范 LR(1) 分析表。 Bison 兼容 Yacc 项目，因此无需任何修改 Yacc 语法就能在 Bison 上运行。你可以使用 C 或 C++ 进行编写程序，但 Bison 现在实现性的增添了 Java 支持。 ","date":"05-01","objectID":"/2022/flex_and_bison/:2:0","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#gnu-bison"},{"categories":["CompilerPrinciple"],"content":" Bison 概念 从形式规则到 Bison 语法形式语言是数学表达，因此 Bison 需要定义自己的语法来生成相关的分析器。 非终结符，即表达式左边的标识符，在 Bison 中用小写表示，比如 epxr, stmt 等。终结符或 token 在 BISON 中用大写表示，如 INTEGRE, RETURN 等。需要额外注意的是 error 作为保留标识用于错误处理。还有一种标记终结符的方法是使用 C 字符串常量的形式。示例 C 语言 return 语句的 Bison 语法 stmt: RETURN expr ';' ; 语义值如果一条规则表示的终结符是整数常量，那么任意整数常量都是有效的。也就是说，解析输入与具体的值是无关的：可以解析 x+4 的语法，也可以解析 x+1 或 x+5452。 但是对于被解析之后，精确值是十分重要的。无法区分精确值的编译器不是好的编译器！因此对每个 token，Bison 都会有一个 token 类型和一个语义值。 一般来说 token 类型是一个语法的终结符，比如说 INTEGER、IDENTIFIER 或 ',' 等。它可以提供决定 token 是否出现得正确和怎样组织其他 token 的所需的一切，比如整数字面量的值，或标识符的名称。而语法规则除了 token 类型外什么都不必知道。 每个分组还可以具有语义值以及非终结符。比如计算器程序，表达式通常是数字语义值，而编译器中语义值描述的是树结构。 语义行为同样地不止需要解析输入，还要对输入有一些对应的行为，Bison 对文法规则的行为 (action) 也是 C 代码段，每次解析器发现匹配的规则时都会执行相应的行为。 更多时候 action 的目的是根据部分语义值计算真个构造的语义值。比如说有一个规则是加法规则 expr: expr '+' expr { $$ = $1 + $3; }; 这个 action 说明了如何用两个子表达式的值生成 sum 表达式的语义值。 编写 GLR 解析器Bison 的确定性 LR(1) 解析算法无法在某些语法上决定如何在这个操作点上给出确定的操作，即产生了 归约/归约 冲突或 移入/归约 冲突。 有时需要更通用的解析文法，你的文件中声明 %glr-parser 就能生成 GLR (Generalized LR) 解析器。GLR 解析器与确定性解析器处理普通 Bison 文法相同，只有在发生冲突时，GLR 采用两者兼顾的权宜之计，有效复刻出解析器以遵循这两种方式。每个复刻的解析器可以继续复刻，因此可以尝试任意可能的结果。解析器也同步进行，它们都消耗给定的符号才进入下一阶段。每个复刻出的解析器在发生错误时就将消亡，而没有错误的解析器则会和其他解析器合并，因为已经将输入减少到了同一组相同的符号。 期间的所有解析器只会记录 action 而不会操作，如果解析器消亡那么 action 也随之消亡。只有合并时根据记录的 action，根据语法的优先级，或执行所有 action 后在结果值上调用用户定义的函数产生合并结果。 更多有关 GLR 的内容，可以查看 Scott 在 2000 年发表的 Tomita-Style Generalised LR Parsers。 无二义性 GLR 解析器示例 这个简单的示例是用 GLR 解析无二义性但无法成为 LR(1) 的文法，这种文法是典型需要向前看不止一个符号的文法。考虑在 Pascal 语言中出现的枚举声明与 subrange 类型。 type subrange = lo .. hi; type enum = (a, b, c); 原始的语言标准只允许数字字面量或常量标识符出现在 subrange 中，但扩展 Pascal (ISO/IEC 10206) 和更多的 Pascal 实现都允许任意表达式。这就可以比如这样的表达式 type subrange = (a) .. b; 但是枚举类型的声明与这很类似 type enum = (a); 在 .. 之前这都是相同的，当 LR(1) 文法解析到这里时不可能在两种形式上做出决定，但解析器必须做出这一点。如果是 subrange 的话 a 可以是一个常量或函数调用，而枚举的话则必须是一个标识符。如果将 (a) 解析成未指定的标识符从而稍后解决，但这通常需要在语义动作和大部分语法中进行大量扭曲。 你可能希望通过 lex 为当前定义和未定义的标识符返回不同的标记来区分两种状态。但声明出现在 local 但 a 为 extern 定义，那么需要重新定义 a 或使用 extern 的 a。所以这是行不通的。 简单的方法就是使用 GLR 算法，分裂成两个分支，同时解析两个语法规则，迟早会有一个分支因错误而消亡。在下一个 ; 之前有一个 .. 会导致枚举规则的分支解析失败，否则导致 subrange 的分支解析失败。因此只有一个分支会保存下来。如果两个分支都失败， GLR 则会像往常一样发出一个语法错误。所有的一切影响是解析器似乎猜到正确的分支，或者说这似乎比底层使用的 LR(1) 支持了更多的向前看符号。虽然示例是个 LR(2) 的文法，但 GLR 也可以针对 LR(k) 的情况做正确的处理。 一般来说 GLR 解析器可以采用二到三次最坏的情况时间，但 GLR 的某些语法解析可能需要指数的时间与空间，实际上这种情况对于许多语法来说不会发生。示例中仅在两个规则之间发生了冲突，且这两个冲突的类型声明上下文不能嵌套。因此任意时间存在的分支被限制在两个，解析时间依然是线性的。 虽然用户可以不加修改语法文件的情况下，将 LR 解析器替换为 GLR 解析器，用户甚至不会注意解析器在何时分叉。但需要注意的是， LR 解析器在冲突中会静态选择错误的替代方案，GLR 则会进行分叉继续向下分析，从而导致问题不那么明显。另外需要小心地与词法分析器进行交互，分叉后解析器不进行任何执行动作，因此无法通过解析器获取操作信息。好在 Bison 可以将复杂性从与词法分析器的交互转移到 GLR 解析器，但仍要检查其余情况的正确性。 二义性文法 GLR 解析器示例 从一个简化的 C++ 语法示例看起 %{ #include \u003cstdio.h\u003e #define YYSTYPE char const * int yylex (void); void yyerror (char const *); %} %token TYPENAME ID %right '=' %left '+' %glr-parser %% prog: %empty | prog stmt { printf (\"\\n\"); } ; stmt: expr ';' %dprec 1 | decl %dprec 2 ; expr: ID { printf (\"%s \", $$); } | TYPENAME '(' expr ')' { printf (\"%s \u003ccast\u003e \", $1); } | expr '+' expr { printf (\"+ \"); } | expr '=' expr { printf (\"= \"); } ; decl: TYPENAME declarator ';' { printf (\"%s \u003cdeclare\u003e \", $1); } | TYPENAME declarator '=' expr ';' { printf (\"%s \u003cinit-declare\u003e \", $1); } ; declarator: ID { printf (\"\\\"%s\\\" \", $1); } | '(' declarator ')' ; 如果解析一个二义性程序 T (x) = y + z; 这个语法将在 x 被解释为 ID 后 (假设 T 被解释成 TYPENAME) 分叉，因为规则 expr: ID 和 declarator: ID 都可以归约，这里产生归约/归约冲突。之后随着进行 expr 分支被归约为 stmt: expr ';' 而 decl 分支被归约为 stmt: decl，之后两个解析器都看到了 prog stmt 以及剩余相同的未处理输入，这里需要进行合并。但 bison 语法定义的 %dprec 声明将优先将示例解析为 decl。 当然 %dprec 仅在多个解析器存在的时候有效，比如以下这个例子，这里没有歧义，在看到 + 时 decl 分支将消亡，因此 bison 不会看 %dprec 定义 T (x) + y; 如果你不想解决歧义，而是像查看所有可能性，那就必须合并分支，而不是选择一个分支。因此需要更改 stmt 声明为 stmt: expr ';' %merge \u003cstmtMerge\u003e | decl %merge \u003cstmtMerge\u003e ; 并定义以下函数 static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1) { printf(\"\u003cOR\u003e \"); return \"\"; } 当然还要进行 C 声明 (类似 flex) %{ #define YYSTYPE char const * static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1); %} Bison 要求参与合并的产生体都要有相同的 merge 句柄，否则将无法处理歧义，解析器也会因存在不合法合并而报错。 GLR 语义行为 GLR 解析的性质与解析器的结构为语义值与行为产生了一些限制。 延迟语义行为 延迟行为不会与关联的归约一同执行，这可能会对在 GLR 解析器的语义行为中使用某","date":"05-01","objectID":"/2022/flex_and_bison/:2:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#bison-概念"},{"categories":["CompilerPrinciple"],"content":" Bison 概念 从形式规则到 Bison 语法形式语言是数学表达，因此 Bison 需要定义自己的语法来生成相关的分析器。 非终结符，即表达式左边的标识符，在 Bison 中用小写表示，比如 epxr, stmt 等。终结符或 token 在 BISON 中用大写表示，如 INTEGRE, RETURN 等。需要额外注意的是 error 作为保留标识用于错误处理。还有一种标记终结符的方法是使用 C 字符串常量的形式。示例 C 语言 return 语句的 Bison 语法 stmt: RETURN expr ';' ; 语义值如果一条规则表示的终结符是整数常量，那么任意整数常量都是有效的。也就是说，解析输入与具体的值是无关的：可以解析 x+4 的语法，也可以解析 x+1 或 x+5452。 但是对于被解析之后，精确值是十分重要的。无法区分精确值的编译器不是好的编译器！因此对每个 token，Bison 都会有一个 token 类型和一个语义值。 一般来说 token 类型是一个语法的终结符，比如说 INTEGER、IDENTIFIER 或 ',' 等。它可以提供决定 token 是否出现得正确和怎样组织其他 token 的所需的一切，比如整数字面量的值，或标识符的名称。而语法规则除了 token 类型外什么都不必知道。 每个分组还可以具有语义值以及非终结符。比如计算器程序，表达式通常是数字语义值，而编译器中语义值描述的是树结构。 语义行为同样地不止需要解析输入，还要对输入有一些对应的行为，Bison 对文法规则的行为 (action) 也是 C 代码段，每次解析器发现匹配的规则时都会执行相应的行为。 更多时候 action 的目的是根据部分语义值计算真个构造的语义值。比如说有一个规则是加法规则 expr: expr '+' expr { $$ = $1 + $3; }; 这个 action 说明了如何用两个子表达式的值生成 sum 表达式的语义值。 编写 GLR 解析器Bison 的确定性 LR(1) 解析算法无法在某些语法上决定如何在这个操作点上给出确定的操作，即产生了 归约/归约 冲突或 移入/归约 冲突。 有时需要更通用的解析文法，你的文件中声明 %glr-parser 就能生成 GLR (Generalized LR) 解析器。GLR 解析器与确定性解析器处理普通 Bison 文法相同，只有在发生冲突时，GLR 采用两者兼顾的权宜之计，有效复刻出解析器以遵循这两种方式。每个复刻的解析器可以继续复刻，因此可以尝试任意可能的结果。解析器也同步进行，它们都消耗给定的符号才进入下一阶段。每个复刻出的解析器在发生错误时就将消亡，而没有错误的解析器则会和其他解析器合并，因为已经将输入减少到了同一组相同的符号。 期间的所有解析器只会记录 action 而不会操作，如果解析器消亡那么 action 也随之消亡。只有合并时根据记录的 action，根据语法的优先级，或执行所有 action 后在结果值上调用用户定义的函数产生合并结果。 更多有关 GLR 的内容，可以查看 Scott 在 2000 年发表的 Tomita-Style Generalised LR Parsers。 无二义性 GLR 解析器示例 这个简单的示例是用 GLR 解析无二义性但无法成为 LR(1) 的文法，这种文法是典型需要向前看不止一个符号的文法。考虑在 Pascal 语言中出现的枚举声明与 subrange 类型。 type subrange = lo .. hi; type enum = (a, b, c); 原始的语言标准只允许数字字面量或常量标识符出现在 subrange 中，但扩展 Pascal (ISO/IEC 10206) 和更多的 Pascal 实现都允许任意表达式。这就可以比如这样的表达式 type subrange = (a) .. b; 但是枚举类型的声明与这很类似 type enum = (a); 在 .. 之前这都是相同的，当 LR(1) 文法解析到这里时不可能在两种形式上做出决定，但解析器必须做出这一点。如果是 subrange 的话 a 可以是一个常量或函数调用，而枚举的话则必须是一个标识符。如果将 (a) 解析成未指定的标识符从而稍后解决，但这通常需要在语义动作和大部分语法中进行大量扭曲。 你可能希望通过 lex 为当前定义和未定义的标识符返回不同的标记来区分两种状态。但声明出现在 local 但 a 为 extern 定义，那么需要重新定义 a 或使用 extern 的 a。所以这是行不通的。 简单的方法就是使用 GLR 算法，分裂成两个分支，同时解析两个语法规则，迟早会有一个分支因错误而消亡。在下一个 ; 之前有一个 .. 会导致枚举规则的分支解析失败，否则导致 subrange 的分支解析失败。因此只有一个分支会保存下来。如果两个分支都失败， GLR 则会像往常一样发出一个语法错误。所有的一切影响是解析器似乎猜到正确的分支，或者说这似乎比底层使用的 LR(1) 支持了更多的向前看符号。虽然示例是个 LR(2) 的文法，但 GLR 也可以针对 LR(k) 的情况做正确的处理。 一般来说 GLR 解析器可以采用二到三次最坏的情况时间，但 GLR 的某些语法解析可能需要指数的时间与空间，实际上这种情况对于许多语法来说不会发生。示例中仅在两个规则之间发生了冲突，且这两个冲突的类型声明上下文不能嵌套。因此任意时间存在的分支被限制在两个，解析时间依然是线性的。 虽然用户可以不加修改语法文件的情况下，将 LR 解析器替换为 GLR 解析器，用户甚至不会注意解析器在何时分叉。但需要注意的是， LR 解析器在冲突中会静态选择错误的替代方案，GLR 则会进行分叉继续向下分析，从而导致问题不那么明显。另外需要小心地与词法分析器进行交互，分叉后解析器不进行任何执行动作，因此无法通过解析器获取操作信息。好在 Bison 可以将复杂性从与词法分析器的交互转移到 GLR 解析器，但仍要检查其余情况的正确性。 二义性文法 GLR 解析器示例 从一个简化的 C++ 语法示例看起 %{ #include #define YYSTYPE char const * int yylex (void); void yyerror (char const *); %} %token TYPENAME ID %right '=' %left '+' %glr-parser %% prog: %empty | prog stmt { printf (\"\\n\"); } ; stmt: expr ';' %dprec 1 | decl %dprec 2 ; expr: ID { printf (\"%s \", $$); } | TYPENAME '(' expr ')' { printf (\"%s \", $1); } | expr '+' expr { printf (\"+ \"); } | expr '=' expr { printf (\"= \"); } ; decl: TYPENAME declarator ';' { printf (\"%s \", $1); } | TYPENAME declarator '=' expr ';' { printf (\"%s \", $1); } ; declarator: ID { printf (\"\\\"%s\\\" \", $1); } | '(' declarator ')' ; 如果解析一个二义性程序 T (x) = y + z; 这个语法将在 x 被解释为 ID 后 (假设 T 被解释成 TYPENAME) 分叉，因为规则 expr: ID 和 declarator: ID 都可以归约，这里产生归约/归约冲突。之后随着进行 expr 分支被归约为 stmt: expr ';' 而 decl 分支被归约为 stmt: decl，之后两个解析器都看到了 prog stmt 以及剩余相同的未处理输入，这里需要进行合并。但 bison 语法定义的 %dprec 声明将优先将示例解析为 decl。 当然 %dprec 仅在多个解析器存在的时候有效，比如以下这个例子，这里没有歧义，在看到 + 时 decl 分支将消亡，因此 bison 不会看 %dprec 定义 T (x) + y; 如果你不想解决歧义，而是像查看所有可能性，那就必须合并分支，而不是选择一个分支。因此需要更改 stmt 声明为 stmt: expr ';' %merge | decl %merge ; 并定义以下函数 static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1) { printf(\" \"); return \"\"; } 当然还要进行 C 声明 (类似 flex) %{ #define YYSTYPE char const * static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1); %} Bison 要求参与合并的产生体都要有相同的 merge 句柄，否则将无法处理歧义，解析器也会因存在不合法合并而报错。 GLR 语义行为 GLR 解析的性质与解析器的结构为语义值与行为产生了一些限制。 延迟语义行为 延迟行为不会与关联的归约一同执行，这可能会对在 GLR 解析器的语义行为中使用某","date":"05-01","objectID":"/2022/flex_and_bison/:2:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#从形式规则到-bison-语法"},{"categories":["CompilerPrinciple"],"content":" Bison 概念 从形式规则到 Bison 语法形式语言是数学表达，因此 Bison 需要定义自己的语法来生成相关的分析器。 非终结符，即表达式左边的标识符，在 Bison 中用小写表示，比如 epxr, stmt 等。终结符或 token 在 BISON 中用大写表示，如 INTEGRE, RETURN 等。需要额外注意的是 error 作为保留标识用于错误处理。还有一种标记终结符的方法是使用 C 字符串常量的形式。示例 C 语言 return 语句的 Bison 语法 stmt: RETURN expr ';' ; 语义值如果一条规则表示的终结符是整数常量，那么任意整数常量都是有效的。也就是说，解析输入与具体的值是无关的：可以解析 x+4 的语法，也可以解析 x+1 或 x+5452。 但是对于被解析之后，精确值是十分重要的。无法区分精确值的编译器不是好的编译器！因此对每个 token，Bison 都会有一个 token 类型和一个语义值。 一般来说 token 类型是一个语法的终结符，比如说 INTEGER、IDENTIFIER 或 ',' 等。它可以提供决定 token 是否出现得正确和怎样组织其他 token 的所需的一切，比如整数字面量的值，或标识符的名称。而语法规则除了 token 类型外什么都不必知道。 每个分组还可以具有语义值以及非终结符。比如计算器程序，表达式通常是数字语义值，而编译器中语义值描述的是树结构。 语义行为同样地不止需要解析输入，还要对输入有一些对应的行为，Bison 对文法规则的行为 (action) 也是 C 代码段，每次解析器发现匹配的规则时都会执行相应的行为。 更多时候 action 的目的是根据部分语义值计算真个构造的语义值。比如说有一个规则是加法规则 expr: expr '+' expr { $$ = $1 + $3; }; 这个 action 说明了如何用两个子表达式的值生成 sum 表达式的语义值。 编写 GLR 解析器Bison 的确定性 LR(1) 解析算法无法在某些语法上决定如何在这个操作点上给出确定的操作，即产生了 归约/归约 冲突或 移入/归约 冲突。 有时需要更通用的解析文法，你的文件中声明 %glr-parser 就能生成 GLR (Generalized LR) 解析器。GLR 解析器与确定性解析器处理普通 Bison 文法相同，只有在发生冲突时，GLR 采用两者兼顾的权宜之计，有效复刻出解析器以遵循这两种方式。每个复刻的解析器可以继续复刻，因此可以尝试任意可能的结果。解析器也同步进行，它们都消耗给定的符号才进入下一阶段。每个复刻出的解析器在发生错误时就将消亡，而没有错误的解析器则会和其他解析器合并，因为已经将输入减少到了同一组相同的符号。 期间的所有解析器只会记录 action 而不会操作，如果解析器消亡那么 action 也随之消亡。只有合并时根据记录的 action，根据语法的优先级，或执行所有 action 后在结果值上调用用户定义的函数产生合并结果。 更多有关 GLR 的内容，可以查看 Scott 在 2000 年发表的 Tomita-Style Generalised LR Parsers。 无二义性 GLR 解析器示例 这个简单的示例是用 GLR 解析无二义性但无法成为 LR(1) 的文法，这种文法是典型需要向前看不止一个符号的文法。考虑在 Pascal 语言中出现的枚举声明与 subrange 类型。 type subrange = lo .. hi; type enum = (a, b, c); 原始的语言标准只允许数字字面量或常量标识符出现在 subrange 中，但扩展 Pascal (ISO/IEC 10206) 和更多的 Pascal 实现都允许任意表达式。这就可以比如这样的表达式 type subrange = (a) .. b; 但是枚举类型的声明与这很类似 type enum = (a); 在 .. 之前这都是相同的，当 LR(1) 文法解析到这里时不可能在两种形式上做出决定，但解析器必须做出这一点。如果是 subrange 的话 a 可以是一个常量或函数调用，而枚举的话则必须是一个标识符。如果将 (a) 解析成未指定的标识符从而稍后解决，但这通常需要在语义动作和大部分语法中进行大量扭曲。 你可能希望通过 lex 为当前定义和未定义的标识符返回不同的标记来区分两种状态。但声明出现在 local 但 a 为 extern 定义，那么需要重新定义 a 或使用 extern 的 a。所以这是行不通的。 简单的方法就是使用 GLR 算法，分裂成两个分支，同时解析两个语法规则，迟早会有一个分支因错误而消亡。在下一个 ; 之前有一个 .. 会导致枚举规则的分支解析失败，否则导致 subrange 的分支解析失败。因此只有一个分支会保存下来。如果两个分支都失败， GLR 则会像往常一样发出一个语法错误。所有的一切影响是解析器似乎猜到正确的分支，或者说这似乎比底层使用的 LR(1) 支持了更多的向前看符号。虽然示例是个 LR(2) 的文法，但 GLR 也可以针对 LR(k) 的情况做正确的处理。 一般来说 GLR 解析器可以采用二到三次最坏的情况时间，但 GLR 的某些语法解析可能需要指数的时间与空间，实际上这种情况对于许多语法来说不会发生。示例中仅在两个规则之间发生了冲突，且这两个冲突的类型声明上下文不能嵌套。因此任意时间存在的分支被限制在两个，解析时间依然是线性的。 虽然用户可以不加修改语法文件的情况下，将 LR 解析器替换为 GLR 解析器，用户甚至不会注意解析器在何时分叉。但需要注意的是， LR 解析器在冲突中会静态选择错误的替代方案，GLR 则会进行分叉继续向下分析，从而导致问题不那么明显。另外需要小心地与词法分析器进行交互，分叉后解析器不进行任何执行动作，因此无法通过解析器获取操作信息。好在 Bison 可以将复杂性从与词法分析器的交互转移到 GLR 解析器，但仍要检查其余情况的正确性。 二义性文法 GLR 解析器示例 从一个简化的 C++ 语法示例看起 %{ #include #define YYSTYPE char const * int yylex (void); void yyerror (char const *); %} %token TYPENAME ID %right '=' %left '+' %glr-parser %% prog: %empty | prog stmt { printf (\"\\n\"); } ; stmt: expr ';' %dprec 1 | decl %dprec 2 ; expr: ID { printf (\"%s \", $$); } | TYPENAME '(' expr ')' { printf (\"%s \", $1); } | expr '+' expr { printf (\"+ \"); } | expr '=' expr { printf (\"= \"); } ; decl: TYPENAME declarator ';' { printf (\"%s \", $1); } | TYPENAME declarator '=' expr ';' { printf (\"%s \", $1); } ; declarator: ID { printf (\"\\\"%s\\\" \", $1); } | '(' declarator ')' ; 如果解析一个二义性程序 T (x) = y + z; 这个语法将在 x 被解释为 ID 后 (假设 T 被解释成 TYPENAME) 分叉，因为规则 expr: ID 和 declarator: ID 都可以归约，这里产生归约/归约冲突。之后随着进行 expr 分支被归约为 stmt: expr ';' 而 decl 分支被归约为 stmt: decl，之后两个解析器都看到了 prog stmt 以及剩余相同的未处理输入，这里需要进行合并。但 bison 语法定义的 %dprec 声明将优先将示例解析为 decl。 当然 %dprec 仅在多个解析器存在的时候有效，比如以下这个例子，这里没有歧义，在看到 + 时 decl 分支将消亡，因此 bison 不会看 %dprec 定义 T (x) + y; 如果你不想解决歧义，而是像查看所有可能性，那就必须合并分支，而不是选择一个分支。因此需要更改 stmt 声明为 stmt: expr ';' %merge | decl %merge ; 并定义以下函数 static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1) { printf(\" \"); return \"\"; } 当然还要进行 C 声明 (类似 flex) %{ #define YYSTYPE char const * static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1); %} Bison 要求参与合并的产生体都要有相同的 merge 句柄，否则将无法处理歧义，解析器也会因存在不合法合并而报错。 GLR 语义行为 GLR 解析的性质与解析器的结构为语义值与行为产生了一些限制。 延迟语义行为 延迟行为不会与关联的归约一同执行，这可能会对在 GLR 解析器的语义行为中使用某","date":"05-01","objectID":"/2022/flex_and_bison/:2:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#语义值"},{"categories":["CompilerPrinciple"],"content":" Bison 概念 从形式规则到 Bison 语法形式语言是数学表达，因此 Bison 需要定义自己的语法来生成相关的分析器。 非终结符，即表达式左边的标识符，在 Bison 中用小写表示，比如 epxr, stmt 等。终结符或 token 在 BISON 中用大写表示，如 INTEGRE, RETURN 等。需要额外注意的是 error 作为保留标识用于错误处理。还有一种标记终结符的方法是使用 C 字符串常量的形式。示例 C 语言 return 语句的 Bison 语法 stmt: RETURN expr ';' ; 语义值如果一条规则表示的终结符是整数常量，那么任意整数常量都是有效的。也就是说，解析输入与具体的值是无关的：可以解析 x+4 的语法，也可以解析 x+1 或 x+5452。 但是对于被解析之后，精确值是十分重要的。无法区分精确值的编译器不是好的编译器！因此对每个 token，Bison 都会有一个 token 类型和一个语义值。 一般来说 token 类型是一个语法的终结符，比如说 INTEGER、IDENTIFIER 或 ',' 等。它可以提供决定 token 是否出现得正确和怎样组织其他 token 的所需的一切，比如整数字面量的值，或标识符的名称。而语法规则除了 token 类型外什么都不必知道。 每个分组还可以具有语义值以及非终结符。比如计算器程序，表达式通常是数字语义值，而编译器中语义值描述的是树结构。 语义行为同样地不止需要解析输入，还要对输入有一些对应的行为，Bison 对文法规则的行为 (action) 也是 C 代码段，每次解析器发现匹配的规则时都会执行相应的行为。 更多时候 action 的目的是根据部分语义值计算真个构造的语义值。比如说有一个规则是加法规则 expr: expr '+' expr { $$ = $1 + $3; }; 这个 action 说明了如何用两个子表达式的值生成 sum 表达式的语义值。 编写 GLR 解析器Bison 的确定性 LR(1) 解析算法无法在某些语法上决定如何在这个操作点上给出确定的操作，即产生了 归约/归约 冲突或 移入/归约 冲突。 有时需要更通用的解析文法，你的文件中声明 %glr-parser 就能生成 GLR (Generalized LR) 解析器。GLR 解析器与确定性解析器处理普通 Bison 文法相同，只有在发生冲突时，GLR 采用两者兼顾的权宜之计，有效复刻出解析器以遵循这两种方式。每个复刻的解析器可以继续复刻，因此可以尝试任意可能的结果。解析器也同步进行，它们都消耗给定的符号才进入下一阶段。每个复刻出的解析器在发生错误时就将消亡，而没有错误的解析器则会和其他解析器合并，因为已经将输入减少到了同一组相同的符号。 期间的所有解析器只会记录 action 而不会操作，如果解析器消亡那么 action 也随之消亡。只有合并时根据记录的 action，根据语法的优先级，或执行所有 action 后在结果值上调用用户定义的函数产生合并结果。 更多有关 GLR 的内容，可以查看 Scott 在 2000 年发表的 Tomita-Style Generalised LR Parsers。 无二义性 GLR 解析器示例 这个简单的示例是用 GLR 解析无二义性但无法成为 LR(1) 的文法，这种文法是典型需要向前看不止一个符号的文法。考虑在 Pascal 语言中出现的枚举声明与 subrange 类型。 type subrange = lo .. hi; type enum = (a, b, c); 原始的语言标准只允许数字字面量或常量标识符出现在 subrange 中，但扩展 Pascal (ISO/IEC 10206) 和更多的 Pascal 实现都允许任意表达式。这就可以比如这样的表达式 type subrange = (a) .. b; 但是枚举类型的声明与这很类似 type enum = (a); 在 .. 之前这都是相同的，当 LR(1) 文法解析到这里时不可能在两种形式上做出决定，但解析器必须做出这一点。如果是 subrange 的话 a 可以是一个常量或函数调用，而枚举的话则必须是一个标识符。如果将 (a) 解析成未指定的标识符从而稍后解决，但这通常需要在语义动作和大部分语法中进行大量扭曲。 你可能希望通过 lex 为当前定义和未定义的标识符返回不同的标记来区分两种状态。但声明出现在 local 但 a 为 extern 定义，那么需要重新定义 a 或使用 extern 的 a。所以这是行不通的。 简单的方法就是使用 GLR 算法，分裂成两个分支，同时解析两个语法规则，迟早会有一个分支因错误而消亡。在下一个 ; 之前有一个 .. 会导致枚举规则的分支解析失败，否则导致 subrange 的分支解析失败。因此只有一个分支会保存下来。如果两个分支都失败， GLR 则会像往常一样发出一个语法错误。所有的一切影响是解析器似乎猜到正确的分支，或者说这似乎比底层使用的 LR(1) 支持了更多的向前看符号。虽然示例是个 LR(2) 的文法，但 GLR 也可以针对 LR(k) 的情况做正确的处理。 一般来说 GLR 解析器可以采用二到三次最坏的情况时间，但 GLR 的某些语法解析可能需要指数的时间与空间，实际上这种情况对于许多语法来说不会发生。示例中仅在两个规则之间发生了冲突，且这两个冲突的类型声明上下文不能嵌套。因此任意时间存在的分支被限制在两个，解析时间依然是线性的。 虽然用户可以不加修改语法文件的情况下，将 LR 解析器替换为 GLR 解析器，用户甚至不会注意解析器在何时分叉。但需要注意的是， LR 解析器在冲突中会静态选择错误的替代方案，GLR 则会进行分叉继续向下分析，从而导致问题不那么明显。另外需要小心地与词法分析器进行交互，分叉后解析器不进行任何执行动作，因此无法通过解析器获取操作信息。好在 Bison 可以将复杂性从与词法分析器的交互转移到 GLR 解析器，但仍要检查其余情况的正确性。 二义性文法 GLR 解析器示例 从一个简化的 C++ 语法示例看起 %{ #include #define YYSTYPE char const * int yylex (void); void yyerror (char const *); %} %token TYPENAME ID %right '=' %left '+' %glr-parser %% prog: %empty | prog stmt { printf (\"\\n\"); } ; stmt: expr ';' %dprec 1 | decl %dprec 2 ; expr: ID { printf (\"%s \", $$); } | TYPENAME '(' expr ')' { printf (\"%s \", $1); } | expr '+' expr { printf (\"+ \"); } | expr '=' expr { printf (\"= \"); } ; decl: TYPENAME declarator ';' { printf (\"%s \", $1); } | TYPENAME declarator '=' expr ';' { printf (\"%s \", $1); } ; declarator: ID { printf (\"\\\"%s\\\" \", $1); } | '(' declarator ')' ; 如果解析一个二义性程序 T (x) = y + z; 这个语法将在 x 被解释为 ID 后 (假设 T 被解释成 TYPENAME) 分叉，因为规则 expr: ID 和 declarator: ID 都可以归约，这里产生归约/归约冲突。之后随着进行 expr 分支被归约为 stmt: expr ';' 而 decl 分支被归约为 stmt: decl，之后两个解析器都看到了 prog stmt 以及剩余相同的未处理输入，这里需要进行合并。但 bison 语法定义的 %dprec 声明将优先将示例解析为 decl。 当然 %dprec 仅在多个解析器存在的时候有效，比如以下这个例子，这里没有歧义，在看到 + 时 decl 分支将消亡，因此 bison 不会看 %dprec 定义 T (x) + y; 如果你不想解决歧义，而是像查看所有可能性，那就必须合并分支，而不是选择一个分支。因此需要更改 stmt 声明为 stmt: expr ';' %merge | decl %merge ; 并定义以下函数 static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1) { printf(\" \"); return \"\"; } 当然还要进行 C 声明 (类似 flex) %{ #define YYSTYPE char const * static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1); %} Bison 要求参与合并的产生体都要有相同的 merge 句柄，否则将无法处理歧义，解析器也会因存在不合法合并而报错。 GLR 语义行为 GLR 解析的性质与解析器的结构为语义值与行为产生了一些限制。 延迟语义行为 延迟行为不会与关联的归约一同执行，这可能会对在 GLR 解析器的语义行为中使用某","date":"05-01","objectID":"/2022/flex_and_bison/:2:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#语义行为"},{"categories":["CompilerPrinciple"],"content":" Bison 概念 从形式规则到 Bison 语法形式语言是数学表达，因此 Bison 需要定义自己的语法来生成相关的分析器。 非终结符，即表达式左边的标识符，在 Bison 中用小写表示，比如 epxr, stmt 等。终结符或 token 在 BISON 中用大写表示，如 INTEGRE, RETURN 等。需要额外注意的是 error 作为保留标识用于错误处理。还有一种标记终结符的方法是使用 C 字符串常量的形式。示例 C 语言 return 语句的 Bison 语法 stmt: RETURN expr ';' ; 语义值如果一条规则表示的终结符是整数常量，那么任意整数常量都是有效的。也就是说，解析输入与具体的值是无关的：可以解析 x+4 的语法，也可以解析 x+1 或 x+5452。 但是对于被解析之后，精确值是十分重要的。无法区分精确值的编译器不是好的编译器！因此对每个 token，Bison 都会有一个 token 类型和一个语义值。 一般来说 token 类型是一个语法的终结符，比如说 INTEGER、IDENTIFIER 或 ',' 等。它可以提供决定 token 是否出现得正确和怎样组织其他 token 的所需的一切，比如整数字面量的值，或标识符的名称。而语法规则除了 token 类型外什么都不必知道。 每个分组还可以具有语义值以及非终结符。比如计算器程序，表达式通常是数字语义值，而编译器中语义值描述的是树结构。 语义行为同样地不止需要解析输入，还要对输入有一些对应的行为，Bison 对文法规则的行为 (action) 也是 C 代码段，每次解析器发现匹配的规则时都会执行相应的行为。 更多时候 action 的目的是根据部分语义值计算真个构造的语义值。比如说有一个规则是加法规则 expr: expr '+' expr { $$ = $1 + $3; }; 这个 action 说明了如何用两个子表达式的值生成 sum 表达式的语义值。 编写 GLR 解析器Bison 的确定性 LR(1) 解析算法无法在某些语法上决定如何在这个操作点上给出确定的操作，即产生了 归约/归约 冲突或 移入/归约 冲突。 有时需要更通用的解析文法，你的文件中声明 %glr-parser 就能生成 GLR (Generalized LR) 解析器。GLR 解析器与确定性解析器处理普通 Bison 文法相同，只有在发生冲突时，GLR 采用两者兼顾的权宜之计，有效复刻出解析器以遵循这两种方式。每个复刻的解析器可以继续复刻，因此可以尝试任意可能的结果。解析器也同步进行，它们都消耗给定的符号才进入下一阶段。每个复刻出的解析器在发生错误时就将消亡，而没有错误的解析器则会和其他解析器合并，因为已经将输入减少到了同一组相同的符号。 期间的所有解析器只会记录 action 而不会操作，如果解析器消亡那么 action 也随之消亡。只有合并时根据记录的 action，根据语法的优先级，或执行所有 action 后在结果值上调用用户定义的函数产生合并结果。 更多有关 GLR 的内容，可以查看 Scott 在 2000 年发表的 Tomita-Style Generalised LR Parsers。 无二义性 GLR 解析器示例 这个简单的示例是用 GLR 解析无二义性但无法成为 LR(1) 的文法，这种文法是典型需要向前看不止一个符号的文法。考虑在 Pascal 语言中出现的枚举声明与 subrange 类型。 type subrange = lo .. hi; type enum = (a, b, c); 原始的语言标准只允许数字字面量或常量标识符出现在 subrange 中，但扩展 Pascal (ISO/IEC 10206) 和更多的 Pascal 实现都允许任意表达式。这就可以比如这样的表达式 type subrange = (a) .. b; 但是枚举类型的声明与这很类似 type enum = (a); 在 .. 之前这都是相同的，当 LR(1) 文法解析到这里时不可能在两种形式上做出决定，但解析器必须做出这一点。如果是 subrange 的话 a 可以是一个常量或函数调用，而枚举的话则必须是一个标识符。如果将 (a) 解析成未指定的标识符从而稍后解决，但这通常需要在语义动作和大部分语法中进行大量扭曲。 你可能希望通过 lex 为当前定义和未定义的标识符返回不同的标记来区分两种状态。但声明出现在 local 但 a 为 extern 定义，那么需要重新定义 a 或使用 extern 的 a。所以这是行不通的。 简单的方法就是使用 GLR 算法，分裂成两个分支，同时解析两个语法规则，迟早会有一个分支因错误而消亡。在下一个 ; 之前有一个 .. 会导致枚举规则的分支解析失败，否则导致 subrange 的分支解析失败。因此只有一个分支会保存下来。如果两个分支都失败， GLR 则会像往常一样发出一个语法错误。所有的一切影响是解析器似乎猜到正确的分支，或者说这似乎比底层使用的 LR(1) 支持了更多的向前看符号。虽然示例是个 LR(2) 的文法，但 GLR 也可以针对 LR(k) 的情况做正确的处理。 一般来说 GLR 解析器可以采用二到三次最坏的情况时间，但 GLR 的某些语法解析可能需要指数的时间与空间，实际上这种情况对于许多语法来说不会发生。示例中仅在两个规则之间发生了冲突，且这两个冲突的类型声明上下文不能嵌套。因此任意时间存在的分支被限制在两个，解析时间依然是线性的。 虽然用户可以不加修改语法文件的情况下，将 LR 解析器替换为 GLR 解析器，用户甚至不会注意解析器在何时分叉。但需要注意的是， LR 解析器在冲突中会静态选择错误的替代方案，GLR 则会进行分叉继续向下分析，从而导致问题不那么明显。另外需要小心地与词法分析器进行交互，分叉后解析器不进行任何执行动作，因此无法通过解析器获取操作信息。好在 Bison 可以将复杂性从与词法分析器的交互转移到 GLR 解析器，但仍要检查其余情况的正确性。 二义性文法 GLR 解析器示例 从一个简化的 C++ 语法示例看起 %{ #include #define YYSTYPE char const * int yylex (void); void yyerror (char const *); %} %token TYPENAME ID %right '=' %left '+' %glr-parser %% prog: %empty | prog stmt { printf (\"\\n\"); } ; stmt: expr ';' %dprec 1 | decl %dprec 2 ; expr: ID { printf (\"%s \", $$); } | TYPENAME '(' expr ')' { printf (\"%s \", $1); } | expr '+' expr { printf (\"+ \"); } | expr '=' expr { printf (\"= \"); } ; decl: TYPENAME declarator ';' { printf (\"%s \", $1); } | TYPENAME declarator '=' expr ';' { printf (\"%s \", $1); } ; declarator: ID { printf (\"\\\"%s\\\" \", $1); } | '(' declarator ')' ; 如果解析一个二义性程序 T (x) = y + z; 这个语法将在 x 被解释为 ID 后 (假设 T 被解释成 TYPENAME) 分叉，因为规则 expr: ID 和 declarator: ID 都可以归约，这里产生归约/归约冲突。之后随着进行 expr 分支被归约为 stmt: expr ';' 而 decl 分支被归约为 stmt: decl，之后两个解析器都看到了 prog stmt 以及剩余相同的未处理输入，这里需要进行合并。但 bison 语法定义的 %dprec 声明将优先将示例解析为 decl。 当然 %dprec 仅在多个解析器存在的时候有效，比如以下这个例子，这里没有歧义，在看到 + 时 decl 分支将消亡，因此 bison 不会看 %dprec 定义 T (x) + y; 如果你不想解决歧义，而是像查看所有可能性，那就必须合并分支，而不是选择一个分支。因此需要更改 stmt 声明为 stmt: expr ';' %merge | decl %merge ; 并定义以下函数 static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1) { printf(\" \"); return \"\"; } 当然还要进行 C 声明 (类似 flex) %{ #define YYSTYPE char const * static YYSTYPE stmtMerge(YYSTYPE x0, YYSTYPE x1); %} Bison 要求参与合并的产生体都要有相同的 merge 句柄，否则将无法处理歧义，解析器也会因存在不合法合并而报错。 GLR 语义行为 GLR 解析的性质与解析器的结构为语义值与行为产生了一些限制。 延迟语义行为 延迟行为不会与关联的归约一同执行，这可能会对在 GLR 解析器的语义行为中使用某","date":"05-01","objectID":"/2022/flex_and_bison/:2:1","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#编写-glr-解析器"},{"categories":["CompilerPrinciple"],"content":" Bison 语法文件Bison 输入上下文无关的语法，并生成识别语法的 C 语言代码。 语法文件类似于以下结构 %{ PROLOGUE %} BISON DECLARATIONS %% GRAMMAR RULES %% EPILOGUE 并且可以使用 C 和 C++ 样式的注释。 序言 (Prologue) 部分可以包含宏定义以及函数与变量的声明，这将拷贝到生成文件的开头。这部分内容存放在 %{ 与 %} 块中。这与 Flex 类似。序言可以与 Bison 的声明混在一起，可以与 C 与 Bison 相互引用。但是通常将序言放在所有 Bison 声明之前更安全。比如任何的功能测试宏的定义 _GNU_SOURCE 和 _POSIX_C_SOURCE 都应如此。 声明 (Declaration) 部分包含定义终结符、非终结符、指定优先级等。一些简单的语法可能不需要声明。声明主要定义用于指定语法的符号和语义值的数据类型。 规则 (Rule) 部分至少有一个规则，用来编写语法分析处理行为。 结尾 (Epilogue) 会将所有代码复制到生成的解析器的末尾，和之前 Flex 的结尾一样。 Bison 语法 (Grammar) Bison 语法规则 语法规则很简单 RESULT: COMPONENTS ...; 其中 RESULT (结果) 是该规则的非终结符，COMPONENTS (组件) 是有改规则组合在一起的各种终结符和非终结符。散布在组件之间的可以是确定规则的语义行为，通常是 C 语言行为，但 Bison 不会检查其正确性，只会完完整整地复制代码。 { C STATEMENTS } 也可以用 | 连接多个规则。 RESULT: RULE1-COMPONENTS ... | RULE2-COMPONENTS ... ... ; 如果一个规则的 COMPONENTS 为空，则称为 empty。那么 RESULT 可以匹配空字符串。 RUSULT: | \";\" ; 上一个写法可能不那么好看，有个更好的写法是 RUSULT: %empty | \";\" ; 如果添加 -Wempty-rule 将警告没有 %empty 的空规则，如果想关掉则使用 -Wno-empty-rule。另外这是 Bison 的扩展，如果想兼容 POSIX Yacc，则写法是 RUSULT: /* empty */ | \";\" ; Bison 递归规则 当一个规则的非终结符也出现在右部时，这就是递归。递归几乎是 Bison 语法必须的部分，因为这是定义任意数量的特定事物序列的唯一方法。下面示例中，expseq1 是左递归的，而 expseq2 是右递归的，但这两个非终结符是等价的。 expseq1: exp | expseq1 ',' exp ; expseq2: exp | exp ',' expseq2 ; 但是在编写时，应该更多地使用左递归，即迭代，它可以在有限堆栈空间上解析任意数量的元素，但右递归 (递归) 所用堆栈空间与元素数量成正比。 另外还有间接或相互执行的递归。 expr: primary | primary '+' primary ; primary: constant | '(' expr ')' ; 定义语言语义语言的语法规则仅约定语法，语义是由各种标记和分组相关的语义值，以及在识别各个分组时所采取的动作确定的。 语义值的数据类型 一个简单的程序中，语义值采用相同的类型就够了，比如计算器。Bison 通常将其设置为 int，如果要指定其他类型，则需要 %define api.value.type {double} 或者使用 C/C++ 的预处理器来定义 YYSEYPE #define YYSTYPE double 这个宏必须写在 Prologue 中，如果需要对 POSIX Yacc 的兼容性，则需要使用它。 但是你可能需要不止一种语义值的数据类型，但是想用多数据类型就要做两件事 指定数据类型的整个集合，有以下几种选择 让 Bison 根据分配的标签计算 使用 Bison 的 %union 声明 使用 %define 将变量 api.value.type 定义为联合类型 使用 typedef 或 #define 将 YYSTYPE 定义为联合类型，其成员名称是类型标签 使用语义值的每个符号选择其中一种类型 生成语义值类型 用 %define 定义变量 api.value.type 为 union，用 Bison 提供的 %type 与 %token 定义真正的类型。如下 %define api.value.type union %token \u003cint\u003e INT \"integer\" %token \u003cint\u003e 'n' %type \u003cint\u003e expr %token \u003cchar const *\u003e ID \"identifier\" 生成适当的 YYSTYPE 值来支持每种符号类型。由 token 声明的标识符 (如上面的 INT 和 ID)，在 YYSTYPE 中以字段名称的形式出现。而 ’n’ 并不是指定名称的字段，因此编写代码时不应依赖它们 // For an \"integer\" yylval.INT = 42; return INT; // For an 'n', also declared as int *((int*)\u0026yylval) = 42; return 'n'; // For an \"identifier\" yylval.ID = \"42\"; return ID; 为了避免名称冲突，来可以用 %define 来指定 api.token.prefix 来定义前缀。当然这又是个 Bison 扩展， // %define api.token.prefix {TOK_} // For an \"integer\" yylval.TOK_INT = 42; return TOK_INT; Union 声明 %union 声明语义值指定了可能包含的数据类型的集合，其中包含与 C 的 union 中的内容相同。 %union { double val; symrec *tptr; } 还可以为 union 加上 vaule 标签，在定义了 api.vaule.union.name 时可以生成类型的名称，在不定义时默认使用 YYSTYPE。由于 POSIX 可以多次声明 union，最终将这些 union 串联起来，因此只有第一个 union 定义可以加 value 标签。 如果语法至少包含一个 \u003ctype\u003e 标签，则可以使用自己定义的 YYSTYPE， // parser.h union YYSTYPE { double val; symrec *tptr; }; 并修改为 %{ #include \"parser.h\" %} %define api.value.type {union YYSTYPE} %type \u003cval\u003e expr %token \u003ctptr\u003e ID Bison 行为 每个行为都可以在识别时带有一个 C 代码行为，这些行为的任务多是根据与标记或较小分组关联的语义值计算由规则构建的分组的语义值。 之前也见到了， expr : | expr '+' expr { $$ = $1 + $3; } 当然还可以给每个位置命名 expr[result]: | expr[left] '+' expr[right] { $result = $left + $right; } 如果没为规则指定操作，Bison 会使用默认的 $$ = $1，空规则应该具有显式的行为，除非规则的值无关紧要。 另外指定位置为 0 或负数是十分危险的行为，除非你确定上下文的规则了，否则不要使用它。比如下面这个示例，$0 总是指 foo 中定义在 bar 之前的 expr，如果存在还可以通过 yylval 访问前瞻语义值。 foo: expr bar '+' expr { ... } | expr bar '0' expr { ... } ; bar: %empty { previsous_expr = $0; } ; 行为中值的数据类型 如果语义值是单一数据类型，那么 $$ 与 $N 始终是相同的数据类型；但多种类型的语义值，必须为每个可以具有语义值的终结符、非终结符选择类型，每次使用 $$ 和 $N 时，它的数据类型由规则的引用符号决定。比如 expr : | expr '+' expr { $$ = $1 + $3; } 当然也可以在引用值时指明数据类型，比如说写成 $\u003cINT\u003e1。 规则中行为 有时将行为放在规则中间很有用，它可以在解析器识别下一个组件前执行。 中间规则只能引用之前的 $N，而不能引用之后的位置。规则中行为往往算作规则的组成部分，并且也具有语义值，另外行为可以通过给 $$ 设置值，规则后面的行为可以用 $N 引用这个值，由于没有符号来命名行为，因此无法提前为该值声明数据类型，每次引用都需要指定数据类型 $\u003cTYPE\u003eN。并无法通过规则中行为设置整体的值，唯一的方法就是规则末尾的行为。示例处理一个 let (VARIABLE) STATEMENT 的 let 语句，需要在 STATEMENT 期间临时创建一个名为 VARIABLE 的变量，在解析 STATEMENT 时必须将 VARIABLE 放入符号表，并在之后将其删除。 stmt: \"let\" '(' var ')' { $\u003ccontext\u003e$ = push=context(); declare_variable($3); ","date":"05-01","objectID":"/2022/flex_and_bison/:2:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#bison-语法文件"},{"categories":["CompilerPrinciple"],"content":" Bison 语法文件Bison 输入上下文无关的语法，并生成识别语法的 C 语言代码。 语法文件类似于以下结构 %{ PROLOGUE %} BISON DECLARATIONS %% GRAMMAR RULES %% EPILOGUE 并且可以使用 C 和 C++ 样式的注释。 序言 (Prologue) 部分可以包含宏定义以及函数与变量的声明，这将拷贝到生成文件的开头。这部分内容存放在 %{ 与 %} 块中。这与 Flex 类似。序言可以与 Bison 的声明混在一起，可以与 C 与 Bison 相互引用。但是通常将序言放在所有 Bison 声明之前更安全。比如任何的功能测试宏的定义 _GNU_SOURCE 和 _POSIX_C_SOURCE 都应如此。 声明 (Declaration) 部分包含定义终结符、非终结符、指定优先级等。一些简单的语法可能不需要声明。声明主要定义用于指定语法的符号和语义值的数据类型。 规则 (Rule) 部分至少有一个规则，用来编写语法分析处理行为。 结尾 (Epilogue) 会将所有代码复制到生成的解析器的末尾，和之前 Flex 的结尾一样。 Bison 语法 (Grammar) Bison 语法规则 语法规则很简单 RESULT: COMPONENTS ...; 其中 RESULT (结果) 是该规则的非终结符，COMPONENTS (组件) 是有改规则组合在一起的各种终结符和非终结符。散布在组件之间的可以是确定规则的语义行为，通常是 C 语言行为，但 Bison 不会检查其正确性，只会完完整整地复制代码。 { C STATEMENTS } 也可以用 | 连接多个规则。 RESULT: RULE1-COMPONENTS ... | RULE2-COMPONENTS ... ... ; 如果一个规则的 COMPONENTS 为空，则称为 empty。那么 RESULT 可以匹配空字符串。 RUSULT: | \";\" ; 上一个写法可能不那么好看，有个更好的写法是 RUSULT: %empty | \";\" ; 如果添加 -Wempty-rule 将警告没有 %empty 的空规则，如果想关掉则使用 -Wno-empty-rule。另外这是 Bison 的扩展，如果想兼容 POSIX Yacc，则写法是 RUSULT: /* empty */ | \";\" ; Bison 递归规则 当一个规则的非终结符也出现在右部时，这就是递归。递归几乎是 Bison 语法必须的部分，因为这是定义任意数量的特定事物序列的唯一方法。下面示例中，expseq1 是左递归的，而 expseq2 是右递归的，但这两个非终结符是等价的。 expseq1: exp | expseq1 ',' exp ; expseq2: exp | exp ',' expseq2 ; 但是在编写时，应该更多地使用左递归，即迭代，它可以在有限堆栈空间上解析任意数量的元素，但右递归 (递归) 所用堆栈空间与元素数量成正比。 另外还有间接或相互执行的递归。 expr: primary | primary '+' primary ; primary: constant | '(' expr ')' ; 定义语言语义语言的语法规则仅约定语法，语义是由各种标记和分组相关的语义值，以及在识别各个分组时所采取的动作确定的。 语义值的数据类型 一个简单的程序中，语义值采用相同的类型就够了，比如计算器。Bison 通常将其设置为 int，如果要指定其他类型，则需要 %define api.value.type {double} 或者使用 C/C++ 的预处理器来定义 YYSEYPE #define YYSTYPE double 这个宏必须写在 Prologue 中，如果需要对 POSIX Yacc 的兼容性，则需要使用它。 但是你可能需要不止一种语义值的数据类型，但是想用多数据类型就要做两件事 指定数据类型的整个集合，有以下几种选择 让 Bison 根据分配的标签计算 使用 Bison 的 %union 声明 使用 %define 将变量 api.value.type 定义为联合类型 使用 typedef 或 #define 将 YYSTYPE 定义为联合类型，其成员名称是类型标签 使用语义值的每个符号选择其中一种类型 生成语义值类型 用 %define 定义变量 api.value.type 为 union，用 Bison 提供的 %type 与 %token 定义真正的类型。如下 %define api.value.type union %token INT \"integer\" %token 'n' %type expr %token ID \"identifier\" 生成适当的 YYSTYPE 值来支持每种符号类型。由 token 声明的标识符 (如上面的 INT 和 ID)，在 YYSTYPE 中以字段名称的形式出现。而 ’n’ 并不是指定名称的字段，因此编写代码时不应依赖它们 // For an \"integer\" yylval.INT = 42; return INT; // For an 'n', also declared as int *((int*)\u0026yylval) = 42; return 'n'; // For an \"identifier\" yylval.ID = \"42\"; return ID; 为了避免名称冲突，来可以用 %define 来指定 api.token.prefix 来定义前缀。当然这又是个 Bison 扩展， // %define api.token.prefix {TOK_} // For an \"integer\" yylval.TOK_INT = 42; return TOK_INT; Union 声明 %union 声明语义值指定了可能包含的数据类型的集合，其中包含与 C 的 union 中的内容相同。 %union { double val; symrec *tptr; } 还可以为 union 加上 vaule 标签，在定义了 api.vaule.union.name 时可以生成类型的名称，在不定义时默认使用 YYSTYPE。由于 POSIX 可以多次声明 union，最终将这些 union 串联起来，因此只有第一个 union 定义可以加 value 标签。 如果语法至少包含一个 标签，则可以使用自己定义的 YYSTYPE， // parser.h union YYSTYPE { double val; symrec *tptr; }; 并修改为 %{ #include \"parser.h\" %} %define api.value.type {union YYSTYPE} %type expr %token ID Bison 行为 每个行为都可以在识别时带有一个 C 代码行为，这些行为的任务多是根据与标记或较小分组关联的语义值计算由规则构建的分组的语义值。 之前也见到了， expr : | expr '+' expr { $$ = $1 + $3; } 当然还可以给每个位置命名 expr[result]: | expr[left] '+' expr[right] { $result = $left + $right; } 如果没为规则指定操作，Bison 会使用默认的 $$ = $1，空规则应该具有显式的行为，除非规则的值无关紧要。 另外指定位置为 0 或负数是十分危险的行为，除非你确定上下文的规则了，否则不要使用它。比如下面这个示例，$0 总是指 foo 中定义在 bar 之前的 expr，如果存在还可以通过 yylval 访问前瞻语义值。 foo: expr bar '+' expr { ... } | expr bar '0' expr { ... } ; bar: %empty { previsous_expr = $0; } ; 行为中值的数据类型 如果语义值是单一数据类型，那么 $$ 与 $N 始终是相同的数据类型；但多种类型的语义值，必须为每个可以具有语义值的终结符、非终结符选择类型，每次使用 $$ 和 $N 时，它的数据类型由规则的引用符号决定。比如 expr : | expr '+' expr { $$ = $1 + $3; } 当然也可以在引用值时指明数据类型，比如说写成 $1。 规则中行为 有时将行为放在规则中间很有用，它可以在解析器识别下一个组件前执行。 中间规则只能引用之前的 $N，而不能引用之后的位置。规则中行为往往算作规则的组成部分，并且也具有语义值，另外行为可以通过给 $$ 设置值，规则后面的行为可以用 $N 引用这个值，由于没有符号来命名行为，因此无法提前为该值声明数据类型，每次引用都需要指定数据类型 $N。并无法通过规则中行为设置整体的值，唯一的方法就是规则末尾的行为。示例处理一个 let (VARIABLE) STATEMENT 的 let 语句，需要在 STATEMENT 期间临时创建一个名为 VARIABLE 的变量，在解析 STATEMENT 时必须将 VARIABLE 放入符号表，并在之后将其删除。 stmt: \"let\" '(' var ')' { $$ = push=context(); declare_variable($3); ","date":"05-01","objectID":"/2022/flex_and_bison/:2:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#bison-语法--grammar"},{"categories":["CompilerPrinciple"],"content":" Bison 语法文件Bison 输入上下文无关的语法，并生成识别语法的 C 语言代码。 语法文件类似于以下结构 %{ PROLOGUE %} BISON DECLARATIONS %% GRAMMAR RULES %% EPILOGUE 并且可以使用 C 和 C++ 样式的注释。 序言 (Prologue) 部分可以包含宏定义以及函数与变量的声明，这将拷贝到生成文件的开头。这部分内容存放在 %{ 与 %} 块中。这与 Flex 类似。序言可以与 Bison 的声明混在一起，可以与 C 与 Bison 相互引用。但是通常将序言放在所有 Bison 声明之前更安全。比如任何的功能测试宏的定义 _GNU_SOURCE 和 _POSIX_C_SOURCE 都应如此。 声明 (Declaration) 部分包含定义终结符、非终结符、指定优先级等。一些简单的语法可能不需要声明。声明主要定义用于指定语法的符号和语义值的数据类型。 规则 (Rule) 部分至少有一个规则，用来编写语法分析处理行为。 结尾 (Epilogue) 会将所有代码复制到生成的解析器的末尾，和之前 Flex 的结尾一样。 Bison 语法 (Grammar) Bison 语法规则 语法规则很简单 RESULT: COMPONENTS ...; 其中 RESULT (结果) 是该规则的非终结符，COMPONENTS (组件) 是有改规则组合在一起的各种终结符和非终结符。散布在组件之间的可以是确定规则的语义行为，通常是 C 语言行为，但 Bison 不会检查其正确性，只会完完整整地复制代码。 { C STATEMENTS } 也可以用 | 连接多个规则。 RESULT: RULE1-COMPONENTS ... | RULE2-COMPONENTS ... ... ; 如果一个规则的 COMPONENTS 为空，则称为 empty。那么 RESULT 可以匹配空字符串。 RUSULT: | \";\" ; 上一个写法可能不那么好看，有个更好的写法是 RUSULT: %empty | \";\" ; 如果添加 -Wempty-rule 将警告没有 %empty 的空规则，如果想关掉则使用 -Wno-empty-rule。另外这是 Bison 的扩展，如果想兼容 POSIX Yacc，则写法是 RUSULT: /* empty */ | \";\" ; Bison 递归规则 当一个规则的非终结符也出现在右部时，这就是递归。递归几乎是 Bison 语法必须的部分，因为这是定义任意数量的特定事物序列的唯一方法。下面示例中，expseq1 是左递归的，而 expseq2 是右递归的，但这两个非终结符是等价的。 expseq1: exp | expseq1 ',' exp ; expseq2: exp | exp ',' expseq2 ; 但是在编写时，应该更多地使用左递归，即迭代，它可以在有限堆栈空间上解析任意数量的元素，但右递归 (递归) 所用堆栈空间与元素数量成正比。 另外还有间接或相互执行的递归。 expr: primary | primary '+' primary ; primary: constant | '(' expr ')' ; 定义语言语义语言的语法规则仅约定语法，语义是由各种标记和分组相关的语义值，以及在识别各个分组时所采取的动作确定的。 语义值的数据类型 一个简单的程序中，语义值采用相同的类型就够了，比如计算器。Bison 通常将其设置为 int，如果要指定其他类型，则需要 %define api.value.type {double} 或者使用 C/C++ 的预处理器来定义 YYSEYPE #define YYSTYPE double 这个宏必须写在 Prologue 中，如果需要对 POSIX Yacc 的兼容性，则需要使用它。 但是你可能需要不止一种语义值的数据类型，但是想用多数据类型就要做两件事 指定数据类型的整个集合，有以下几种选择 让 Bison 根据分配的标签计算 使用 Bison 的 %union 声明 使用 %define 将变量 api.value.type 定义为联合类型 使用 typedef 或 #define 将 YYSTYPE 定义为联合类型，其成员名称是类型标签 使用语义值的每个符号选择其中一种类型 生成语义值类型 用 %define 定义变量 api.value.type 为 union，用 Bison 提供的 %type 与 %token 定义真正的类型。如下 %define api.value.type union %token INT \"integer\" %token 'n' %type expr %token ID \"identifier\" 生成适当的 YYSTYPE 值来支持每种符号类型。由 token 声明的标识符 (如上面的 INT 和 ID)，在 YYSTYPE 中以字段名称的形式出现。而 ’n’ 并不是指定名称的字段，因此编写代码时不应依赖它们 // For an \"integer\" yylval.INT = 42; return INT; // For an 'n', also declared as int *((int*)\u0026yylval) = 42; return 'n'; // For an \"identifier\" yylval.ID = \"42\"; return ID; 为了避免名称冲突，来可以用 %define 来指定 api.token.prefix 来定义前缀。当然这又是个 Bison 扩展， // %define api.token.prefix {TOK_} // For an \"integer\" yylval.TOK_INT = 42; return TOK_INT; Union 声明 %union 声明语义值指定了可能包含的数据类型的集合，其中包含与 C 的 union 中的内容相同。 %union { double val; symrec *tptr; } 还可以为 union 加上 vaule 标签，在定义了 api.vaule.union.name 时可以生成类型的名称，在不定义时默认使用 YYSTYPE。由于 POSIX 可以多次声明 union，最终将这些 union 串联起来，因此只有第一个 union 定义可以加 value 标签。 如果语法至少包含一个 标签，则可以使用自己定义的 YYSTYPE， // parser.h union YYSTYPE { double val; symrec *tptr; }; 并修改为 %{ #include \"parser.h\" %} %define api.value.type {union YYSTYPE} %type expr %token ID Bison 行为 每个行为都可以在识别时带有一个 C 代码行为，这些行为的任务多是根据与标记或较小分组关联的语义值计算由规则构建的分组的语义值。 之前也见到了， expr : | expr '+' expr { $$ = $1 + $3; } 当然还可以给每个位置命名 expr[result]: | expr[left] '+' expr[right] { $result = $left + $right; } 如果没为规则指定操作，Bison 会使用默认的 $$ = $1，空规则应该具有显式的行为，除非规则的值无关紧要。 另外指定位置为 0 或负数是十分危险的行为，除非你确定上下文的规则了，否则不要使用它。比如下面这个示例，$0 总是指 foo 中定义在 bar 之前的 expr，如果存在还可以通过 yylval 访问前瞻语义值。 foo: expr bar '+' expr { ... } | expr bar '0' expr { ... } ; bar: %empty { previsous_expr = $0; } ; 行为中值的数据类型 如果语义值是单一数据类型，那么 $$ 与 $N 始终是相同的数据类型；但多种类型的语义值，必须为每个可以具有语义值的终结符、非终结符选择类型，每次使用 $$ 和 $N 时，它的数据类型由规则的引用符号决定。比如 expr : | expr '+' expr { $$ = $1 + $3; } 当然也可以在引用值时指明数据类型，比如说写成 $1。 规则中行为 有时将行为放在规则中间很有用，它可以在解析器识别下一个组件前执行。 中间规则只能引用之前的 $N，而不能引用之后的位置。规则中行为往往算作规则的组成部分，并且也具有语义值，另外行为可以通过给 $$ 设置值，规则后面的行为可以用 $N 引用这个值，由于没有符号来命名行为，因此无法提前为该值声明数据类型，每次引用都需要指定数据类型 $N。并无法通过规则中行为设置整体的值，唯一的方法就是规则末尾的行为。示例处理一个 let (VARIABLE) STATEMENT 的 let 语句，需要在 STATEMENT 期间临时创建一个名为 VARIABLE 的变量，在解析 STATEMENT 时必须将 VARIABLE 放入符号表，并在之后将其删除。 stmt: \"let\" '(' var ')' { $$ = push=context(); declare_variable($3); ","date":"05-01","objectID":"/2022/flex_and_bison/:2:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#定义语言语义"},{"categories":["CompilerPrinciple"],"content":" Bison 语法文件Bison 输入上下文无关的语法，并生成识别语法的 C 语言代码。 语法文件类似于以下结构 %{ PROLOGUE %} BISON DECLARATIONS %% GRAMMAR RULES %% EPILOGUE 并且可以使用 C 和 C++ 样式的注释。 序言 (Prologue) 部分可以包含宏定义以及函数与变量的声明，这将拷贝到生成文件的开头。这部分内容存放在 %{ 与 %} 块中。这与 Flex 类似。序言可以与 Bison 的声明混在一起，可以与 C 与 Bison 相互引用。但是通常将序言放在所有 Bison 声明之前更安全。比如任何的功能测试宏的定义 _GNU_SOURCE 和 _POSIX_C_SOURCE 都应如此。 声明 (Declaration) 部分包含定义终结符、非终结符、指定优先级等。一些简单的语法可能不需要声明。声明主要定义用于指定语法的符号和语义值的数据类型。 规则 (Rule) 部分至少有一个规则，用来编写语法分析处理行为。 结尾 (Epilogue) 会将所有代码复制到生成的解析器的末尾，和之前 Flex 的结尾一样。 Bison 语法 (Grammar) Bison 语法规则 语法规则很简单 RESULT: COMPONENTS ...; 其中 RESULT (结果) 是该规则的非终结符，COMPONENTS (组件) 是有改规则组合在一起的各种终结符和非终结符。散布在组件之间的可以是确定规则的语义行为，通常是 C 语言行为，但 Bison 不会检查其正确性，只会完完整整地复制代码。 { C STATEMENTS } 也可以用 | 连接多个规则。 RESULT: RULE1-COMPONENTS ... | RULE2-COMPONENTS ... ... ; 如果一个规则的 COMPONENTS 为空，则称为 empty。那么 RESULT 可以匹配空字符串。 RUSULT: | \";\" ; 上一个写法可能不那么好看，有个更好的写法是 RUSULT: %empty | \";\" ; 如果添加 -Wempty-rule 将警告没有 %empty 的空规则，如果想关掉则使用 -Wno-empty-rule。另外这是 Bison 的扩展，如果想兼容 POSIX Yacc，则写法是 RUSULT: /* empty */ | \";\" ; Bison 递归规则 当一个规则的非终结符也出现在右部时，这就是递归。递归几乎是 Bison 语法必须的部分，因为这是定义任意数量的特定事物序列的唯一方法。下面示例中，expseq1 是左递归的，而 expseq2 是右递归的，但这两个非终结符是等价的。 expseq1: exp | expseq1 ',' exp ; expseq2: exp | exp ',' expseq2 ; 但是在编写时，应该更多地使用左递归，即迭代，它可以在有限堆栈空间上解析任意数量的元素，但右递归 (递归) 所用堆栈空间与元素数量成正比。 另外还有间接或相互执行的递归。 expr: primary | primary '+' primary ; primary: constant | '(' expr ')' ; 定义语言语义语言的语法规则仅约定语法，语义是由各种标记和分组相关的语义值，以及在识别各个分组时所采取的动作确定的。 语义值的数据类型 一个简单的程序中，语义值采用相同的类型就够了，比如计算器。Bison 通常将其设置为 int，如果要指定其他类型，则需要 %define api.value.type {double} 或者使用 C/C++ 的预处理器来定义 YYSEYPE #define YYSTYPE double 这个宏必须写在 Prologue 中，如果需要对 POSIX Yacc 的兼容性，则需要使用它。 但是你可能需要不止一种语义值的数据类型，但是想用多数据类型就要做两件事 指定数据类型的整个集合，有以下几种选择 让 Bison 根据分配的标签计算 使用 Bison 的 %union 声明 使用 %define 将变量 api.value.type 定义为联合类型 使用 typedef 或 #define 将 YYSTYPE 定义为联合类型，其成员名称是类型标签 使用语义值的每个符号选择其中一种类型 生成语义值类型 用 %define 定义变量 api.value.type 为 union，用 Bison 提供的 %type 与 %token 定义真正的类型。如下 %define api.value.type union %token INT \"integer\" %token 'n' %type expr %token ID \"identifier\" 生成适当的 YYSTYPE 值来支持每种符号类型。由 token 声明的标识符 (如上面的 INT 和 ID)，在 YYSTYPE 中以字段名称的形式出现。而 ’n’ 并不是指定名称的字段，因此编写代码时不应依赖它们 // For an \"integer\" yylval.INT = 42; return INT; // For an 'n', also declared as int *((int*)\u0026yylval) = 42; return 'n'; // For an \"identifier\" yylval.ID = \"42\"; return ID; 为了避免名称冲突，来可以用 %define 来指定 api.token.prefix 来定义前缀。当然这又是个 Bison 扩展， // %define api.token.prefix {TOK_} // For an \"integer\" yylval.TOK_INT = 42; return TOK_INT; Union 声明 %union 声明语义值指定了可能包含的数据类型的集合，其中包含与 C 的 union 中的内容相同。 %union { double val; symrec *tptr; } 还可以为 union 加上 vaule 标签，在定义了 api.vaule.union.name 时可以生成类型的名称，在不定义时默认使用 YYSTYPE。由于 POSIX 可以多次声明 union，最终将这些 union 串联起来，因此只有第一个 union 定义可以加 value 标签。 如果语法至少包含一个 标签，则可以使用自己定义的 YYSTYPE， // parser.h union YYSTYPE { double val; symrec *tptr; }; 并修改为 %{ #include \"parser.h\" %} %define api.value.type {union YYSTYPE} %type expr %token ID Bison 行为 每个行为都可以在识别时带有一个 C 代码行为，这些行为的任务多是根据与标记或较小分组关联的语义值计算由规则构建的分组的语义值。 之前也见到了， expr : | expr '+' expr { $$ = $1 + $3; } 当然还可以给每个位置命名 expr[result]: | expr[left] '+' expr[right] { $result = $left + $right; } 如果没为规则指定操作，Bison 会使用默认的 $$ = $1，空规则应该具有显式的行为，除非规则的值无关紧要。 另外指定位置为 0 或负数是十分危险的行为，除非你确定上下文的规则了，否则不要使用它。比如下面这个示例，$0 总是指 foo 中定义在 bar 之前的 expr，如果存在还可以通过 yylval 访问前瞻语义值。 foo: expr bar '+' expr { ... } | expr bar '0' expr { ... } ; bar: %empty { previsous_expr = $0; } ; 行为中值的数据类型 如果语义值是单一数据类型，那么 $$ 与 $N 始终是相同的数据类型；但多种类型的语义值，必须为每个可以具有语义值的终结符、非终结符选择类型，每次使用 $$ 和 $N 时，它的数据类型由规则的引用符号决定。比如 expr : | expr '+' expr { $$ = $1 + $3; } 当然也可以在引用值时指明数据类型，比如说写成 $1。 规则中行为 有时将行为放在规则中间很有用，它可以在解析器识别下一个组件前执行。 中间规则只能引用之前的 $N，而不能引用之后的位置。规则中行为往往算作规则的组成部分，并且也具有语义值，另外行为可以通过给 $$ 设置值，规则后面的行为可以用 $N 引用这个值，由于没有符号来命名行为，因此无法提前为该值声明数据类型，每次引用都需要指定数据类型 $N。并无法通过规则中行为设置整体的值，唯一的方法就是规则末尾的行为。示例处理一个 let (VARIABLE) STATEMENT 的 let 语句，需要在 STATEMENT 期间临时创建一个名为 VARIABLE 的变量，在解析 STATEMENT 时必须将 VARIABLE 放入符号表，并在之后将其删除。 stmt: \"let\" '(' var ')' { $$ = push=context(); declare_variable($3); ","date":"05-01","objectID":"/2022/flex_and_bison/:2:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#追踪位置"},{"categories":["CompilerPrinciple"],"content":" Bison 语法文件Bison 输入上下文无关的语法，并生成识别语法的 C 语言代码。 语法文件类似于以下结构 %{ PROLOGUE %} BISON DECLARATIONS %% GRAMMAR RULES %% EPILOGUE 并且可以使用 C 和 C++ 样式的注释。 序言 (Prologue) 部分可以包含宏定义以及函数与变量的声明，这将拷贝到生成文件的开头。这部分内容存放在 %{ 与 %} 块中。这与 Flex 类似。序言可以与 Bison 的声明混在一起，可以与 C 与 Bison 相互引用。但是通常将序言放在所有 Bison 声明之前更安全。比如任何的功能测试宏的定义 _GNU_SOURCE 和 _POSIX_C_SOURCE 都应如此。 声明 (Declaration) 部分包含定义终结符、非终结符、指定优先级等。一些简单的语法可能不需要声明。声明主要定义用于指定语法的符号和语义值的数据类型。 规则 (Rule) 部分至少有一个规则，用来编写语法分析处理行为。 结尾 (Epilogue) 会将所有代码复制到生成的解析器的末尾，和之前 Flex 的结尾一样。 Bison 语法 (Grammar) Bison 语法规则 语法规则很简单 RESULT: COMPONENTS ...; 其中 RESULT (结果) 是该规则的非终结符，COMPONENTS (组件) 是有改规则组合在一起的各种终结符和非终结符。散布在组件之间的可以是确定规则的语义行为，通常是 C 语言行为，但 Bison 不会检查其正确性，只会完完整整地复制代码。 { C STATEMENTS } 也可以用 | 连接多个规则。 RESULT: RULE1-COMPONENTS ... | RULE2-COMPONENTS ... ... ; 如果一个规则的 COMPONENTS 为空，则称为 empty。那么 RESULT 可以匹配空字符串。 RUSULT: | \";\" ; 上一个写法可能不那么好看，有个更好的写法是 RUSULT: %empty | \";\" ; 如果添加 -Wempty-rule 将警告没有 %empty 的空规则，如果想关掉则使用 -Wno-empty-rule。另外这是 Bison 的扩展，如果想兼容 POSIX Yacc，则写法是 RUSULT: /* empty */ | \";\" ; Bison 递归规则 当一个规则的非终结符也出现在右部时，这就是递归。递归几乎是 Bison 语法必须的部分，因为这是定义任意数量的特定事物序列的唯一方法。下面示例中，expseq1 是左递归的，而 expseq2 是右递归的，但这两个非终结符是等价的。 expseq1: exp | expseq1 ',' exp ; expseq2: exp | exp ',' expseq2 ; 但是在编写时，应该更多地使用左递归，即迭代，它可以在有限堆栈空间上解析任意数量的元素，但右递归 (递归) 所用堆栈空间与元素数量成正比。 另外还有间接或相互执行的递归。 expr: primary | primary '+' primary ; primary: constant | '(' expr ')' ; 定义语言语义语言的语法规则仅约定语法，语义是由各种标记和分组相关的语义值，以及在识别各个分组时所采取的动作确定的。 语义值的数据类型 一个简单的程序中，语义值采用相同的类型就够了，比如计算器。Bison 通常将其设置为 int，如果要指定其他类型，则需要 %define api.value.type {double} 或者使用 C/C++ 的预处理器来定义 YYSEYPE #define YYSTYPE double 这个宏必须写在 Prologue 中，如果需要对 POSIX Yacc 的兼容性，则需要使用它。 但是你可能需要不止一种语义值的数据类型，但是想用多数据类型就要做两件事 指定数据类型的整个集合，有以下几种选择 让 Bison 根据分配的标签计算 使用 Bison 的 %union 声明 使用 %define 将变量 api.value.type 定义为联合类型 使用 typedef 或 #define 将 YYSTYPE 定义为联合类型，其成员名称是类型标签 使用语义值的每个符号选择其中一种类型 生成语义值类型 用 %define 定义变量 api.value.type 为 union，用 Bison 提供的 %type 与 %token 定义真正的类型。如下 %define api.value.type union %token INT \"integer\" %token 'n' %type expr %token ID \"identifier\" 生成适当的 YYSTYPE 值来支持每种符号类型。由 token 声明的标识符 (如上面的 INT 和 ID)，在 YYSTYPE 中以字段名称的形式出现。而 ’n’ 并不是指定名称的字段，因此编写代码时不应依赖它们 // For an \"integer\" yylval.INT = 42; return INT; // For an 'n', also declared as int *((int*)\u0026yylval) = 42; return 'n'; // For an \"identifier\" yylval.ID = \"42\"; return ID; 为了避免名称冲突，来可以用 %define 来指定 api.token.prefix 来定义前缀。当然这又是个 Bison 扩展， // %define api.token.prefix {TOK_} // For an \"integer\" yylval.TOK_INT = 42; return TOK_INT; Union 声明 %union 声明语义值指定了可能包含的数据类型的集合，其中包含与 C 的 union 中的内容相同。 %union { double val; symrec *tptr; } 还可以为 union 加上 vaule 标签，在定义了 api.vaule.union.name 时可以生成类型的名称，在不定义时默认使用 YYSTYPE。由于 POSIX 可以多次声明 union，最终将这些 union 串联起来，因此只有第一个 union 定义可以加 value 标签。 如果语法至少包含一个 标签，则可以使用自己定义的 YYSTYPE， // parser.h union YYSTYPE { double val; symrec *tptr; }; 并修改为 %{ #include \"parser.h\" %} %define api.value.type {union YYSTYPE} %type expr %token ID Bison 行为 每个行为都可以在识别时带有一个 C 代码行为，这些行为的任务多是根据与标记或较小分组关联的语义值计算由规则构建的分组的语义值。 之前也见到了， expr : | expr '+' expr { $$ = $1 + $3; } 当然还可以给每个位置命名 expr[result]: | expr[left] '+' expr[right] { $result = $left + $right; } 如果没为规则指定操作，Bison 会使用默认的 $$ = $1，空规则应该具有显式的行为，除非规则的值无关紧要。 另外指定位置为 0 或负数是十分危险的行为，除非你确定上下文的规则了，否则不要使用它。比如下面这个示例，$0 总是指 foo 中定义在 bar 之前的 expr，如果存在还可以通过 yylval 访问前瞻语义值。 foo: expr bar '+' expr { ... } | expr bar '0' expr { ... } ; bar: %empty { previsous_expr = $0; } ; 行为中值的数据类型 如果语义值是单一数据类型，那么 $$ 与 $N 始终是相同的数据类型；但多种类型的语义值，必须为每个可以具有语义值的终结符、非终结符选择类型，每次使用 $$ 和 $N 时，它的数据类型由规则的引用符号决定。比如 expr : | expr '+' expr { $$ = $1 + $3; } 当然也可以在引用值时指明数据类型，比如说写成 $1。 规则中行为 有时将行为放在规则中间很有用，它可以在解析器识别下一个组件前执行。 中间规则只能引用之前的 $N，而不能引用之后的位置。规则中行为往往算作规则的组成部分，并且也具有语义值，另外行为可以通过给 $$ 设置值，规则后面的行为可以用 $N 引用这个值，由于没有符号来命名行为，因此无法提前为该值声明数据类型，每次引用都需要指定数据类型 $N。并无法通过规则中行为设置整体的值，唯一的方法就是规则末尾的行为。示例处理一个 let (VARIABLE) STATEMENT 的 let 语句，需要在 STATEMENT 期间临时创建一个名为 VARIABLE 的变量，在解析 STATEMENT 时必须将 VARIABLE 放入符号表，并在之后将其删除。 stmt: \"let\" '(' var ')' { $$ = push=context(); declare_variable($3); ","date":"05-01","objectID":"/2022/flex_and_bison/:2:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#名称引用"},{"categories":["CompilerPrinciple"],"content":" Bison 语法文件Bison 输入上下文无关的语法，并生成识别语法的 C 语言代码。 语法文件类似于以下结构 %{ PROLOGUE %} BISON DECLARATIONS %% GRAMMAR RULES %% EPILOGUE 并且可以使用 C 和 C++ 样式的注释。 序言 (Prologue) 部分可以包含宏定义以及函数与变量的声明，这将拷贝到生成文件的开头。这部分内容存放在 %{ 与 %} 块中。这与 Flex 类似。序言可以与 Bison 的声明混在一起，可以与 C 与 Bison 相互引用。但是通常将序言放在所有 Bison 声明之前更安全。比如任何的功能测试宏的定义 _GNU_SOURCE 和 _POSIX_C_SOURCE 都应如此。 声明 (Declaration) 部分包含定义终结符、非终结符、指定优先级等。一些简单的语法可能不需要声明。声明主要定义用于指定语法的符号和语义值的数据类型。 规则 (Rule) 部分至少有一个规则，用来编写语法分析处理行为。 结尾 (Epilogue) 会将所有代码复制到生成的解析器的末尾，和之前 Flex 的结尾一样。 Bison 语法 (Grammar) Bison 语法规则 语法规则很简单 RESULT: COMPONENTS ...; 其中 RESULT (结果) 是该规则的非终结符，COMPONENTS (组件) 是有改规则组合在一起的各种终结符和非终结符。散布在组件之间的可以是确定规则的语义行为，通常是 C 语言行为，但 Bison 不会检查其正确性，只会完完整整地复制代码。 { C STATEMENTS } 也可以用 | 连接多个规则。 RESULT: RULE1-COMPONENTS ... | RULE2-COMPONENTS ... ... ; 如果一个规则的 COMPONENTS 为空，则称为 empty。那么 RESULT 可以匹配空字符串。 RUSULT: | \";\" ; 上一个写法可能不那么好看，有个更好的写法是 RUSULT: %empty | \";\" ; 如果添加 -Wempty-rule 将警告没有 %empty 的空规则，如果想关掉则使用 -Wno-empty-rule。另外这是 Bison 的扩展，如果想兼容 POSIX Yacc，则写法是 RUSULT: /* empty */ | \";\" ; Bison 递归规则 当一个规则的非终结符也出现在右部时，这就是递归。递归几乎是 Bison 语法必须的部分，因为这是定义任意数量的特定事物序列的唯一方法。下面示例中，expseq1 是左递归的，而 expseq2 是右递归的，但这两个非终结符是等价的。 expseq1: exp | expseq1 ',' exp ; expseq2: exp | exp ',' expseq2 ; 但是在编写时，应该更多地使用左递归，即迭代，它可以在有限堆栈空间上解析任意数量的元素，但右递归 (递归) 所用堆栈空间与元素数量成正比。 另外还有间接或相互执行的递归。 expr: primary | primary '+' primary ; primary: constant | '(' expr ')' ; 定义语言语义语言的语法规则仅约定语法，语义是由各种标记和分组相关的语义值，以及在识别各个分组时所采取的动作确定的。 语义值的数据类型 一个简单的程序中，语义值采用相同的类型就够了，比如计算器。Bison 通常将其设置为 int，如果要指定其他类型，则需要 %define api.value.type {double} 或者使用 C/C++ 的预处理器来定义 YYSEYPE #define YYSTYPE double 这个宏必须写在 Prologue 中，如果需要对 POSIX Yacc 的兼容性，则需要使用它。 但是你可能需要不止一种语义值的数据类型，但是想用多数据类型就要做两件事 指定数据类型的整个集合，有以下几种选择 让 Bison 根据分配的标签计算 使用 Bison 的 %union 声明 使用 %define 将变量 api.value.type 定义为联合类型 使用 typedef 或 #define 将 YYSTYPE 定义为联合类型，其成员名称是类型标签 使用语义值的每个符号选择其中一种类型 生成语义值类型 用 %define 定义变量 api.value.type 为 union，用 Bison 提供的 %type 与 %token 定义真正的类型。如下 %define api.value.type union %token INT \"integer\" %token 'n' %type expr %token ID \"identifier\" 生成适当的 YYSTYPE 值来支持每种符号类型。由 token 声明的标识符 (如上面的 INT 和 ID)，在 YYSTYPE 中以字段名称的形式出现。而 ’n’ 并不是指定名称的字段，因此编写代码时不应依赖它们 // For an \"integer\" yylval.INT = 42; return INT; // For an 'n', also declared as int *((int*)\u0026yylval) = 42; return 'n'; // For an \"identifier\" yylval.ID = \"42\"; return ID; 为了避免名称冲突，来可以用 %define 来指定 api.token.prefix 来定义前缀。当然这又是个 Bison 扩展， // %define api.token.prefix {TOK_} // For an \"integer\" yylval.TOK_INT = 42; return TOK_INT; Union 声明 %union 声明语义值指定了可能包含的数据类型的集合，其中包含与 C 的 union 中的内容相同。 %union { double val; symrec *tptr; } 还可以为 union 加上 vaule 标签，在定义了 api.vaule.union.name 时可以生成类型的名称，在不定义时默认使用 YYSTYPE。由于 POSIX 可以多次声明 union，最终将这些 union 串联起来，因此只有第一个 union 定义可以加 value 标签。 如果语法至少包含一个 标签，则可以使用自己定义的 YYSTYPE， // parser.h union YYSTYPE { double val; symrec *tptr; }; 并修改为 %{ #include \"parser.h\" %} %define api.value.type {union YYSTYPE} %type expr %token ID Bison 行为 每个行为都可以在识别时带有一个 C 代码行为，这些行为的任务多是根据与标记或较小分组关联的语义值计算由规则构建的分组的语义值。 之前也见到了， expr : | expr '+' expr { $$ = $1 + $3; } 当然还可以给每个位置命名 expr[result]: | expr[left] '+' expr[right] { $result = $left + $right; } 如果没为规则指定操作，Bison 会使用默认的 $$ = $1，空规则应该具有显式的行为，除非规则的值无关紧要。 另外指定位置为 0 或负数是十分危险的行为，除非你确定上下文的规则了，否则不要使用它。比如下面这个示例，$0 总是指 foo 中定义在 bar 之前的 expr，如果存在还可以通过 yylval 访问前瞻语义值。 foo: expr bar '+' expr { ... } | expr bar '0' expr { ... } ; bar: %empty { previsous_expr = $0; } ; 行为中值的数据类型 如果语义值是单一数据类型，那么 $$ 与 $N 始终是相同的数据类型；但多种类型的语义值，必须为每个可以具有语义值的终结符、非终结符选择类型，每次使用 $$ 和 $N 时，它的数据类型由规则的引用符号决定。比如 expr : | expr '+' expr { $$ = $1 + $3; } 当然也可以在引用值时指明数据类型，比如说写成 $1。 规则中行为 有时将行为放在规则中间很有用，它可以在解析器识别下一个组件前执行。 中间规则只能引用之前的 $N，而不能引用之后的位置。规则中行为往往算作规则的组成部分，并且也具有语义值，另外行为可以通过给 $$ 设置值，规则后面的行为可以用 $N 引用这个值，由于没有符号来命名行为，因此无法提前为该值声明数据类型，每次引用都需要指定数据类型 $N。并无法通过规则中行为设置整体的值，唯一的方法就是规则末尾的行为。示例处理一个 let (VARIABLE) STATEMENT 的 let 语句，需要在 STATEMENT 期间临时创建一个名为 VARIABLE 的变量，在解析 STATEMENT 时必须将 VARIABLE 放入符号表，并在之后将其删除。 stmt: \"let\" '(' var ')' { $$ = push=context(); declare_variable($3); ","date":"05-01","objectID":"/2022/flex_and_bison/:2:2","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#声明"},{"categories":["CompilerPrinciple"],"content":" 解析器 C 语言接口Bison 的解析器实际上是名为 yyparse 的 C 函数。我们需要对其进行约定。请记住解析器出于内部的目的使用了很多 yy 或 YY 开头的 C 标识符，请小心冲突。 解析器函数 yyparse你需要调用 yyparse 来进行解析。此函数读取标记、执行行为，并在遇到输入结束或不可恢复的语法错误时最终返回。你还可以编写一个行为，指示 yyparse 立即返回，而无需继续执行。 int yyparse(void); // RETURN YYACCEPT (0) if report success, // RETURN YYABORT (1) if report failure, // RETURN 2 if memory exhaustion. 如果使用纯解析器，可以声明 %parse-param 为 yyparse 和 yyerror 定义额外的参数。比如 %parse-param {int *nastiness} {int *randomness} 声明后，这两个的函数如下 void yyerror(int *nastiness, int *randomness, const char *msg); void yyparse(int *nastiness, int *randomness); 当然同时使用 %define api.pure full (或仅 %define api.pure) 和 %locations， yyerror 将生成不一样的签名。 void yyerror(YYLTYPE *llocp, int *nastiness, int *randomness, const char *msg); 在调用时就可以这样使用 int nastiness, randomness; value = yyparse(\u0026nastiness, \u0026randomness); 在语法的规则行为中，也可以使用由 yyparse 传入的参数 exp: ... { ...; *randomness += 1; ... } 词法分析函数 yylex该函数从输入流中识别并返回 token，不会由 bison 创建，你可以使用 Flex 创建它。 yylex 调用约定 yylex 必须返回 token 类型的整数值，零或负数表示输入结束。当然 token 只有一个字符时也可以直接返回该字符。 int yylex(void) { if (c == EOF) { return 0; } if (c == '+' || c == '-') { return c; } // ... return INT; } 如果语法使用字符串字面量 token，yylex 可以通过两种方式确定它们的标记类型代码： 如果语法将符号 token 名称定义为字符串字面量别名，则 yylex 可以像所有其他符号一样使用这些符号名称。在这种情况下，在语法文件中使用字符串字面量 token 对 yylex 没有影响。 yylex 可以在 yytname 表中找到多字符 token，表中的索引是该 token 的编码，该 token 的名字用双引号 (”) 包围并记录在表中。 for (i = 0; i \u003c YYNTOKENS; i++) { if (yytname[i] != 0 \u0026\u0026 yytname[i][0] == '\"' \u0026\u0026 !strncmp(yytname[i] + 1, token_buffer, strlen(token_buffer)) \u0026\u0026 yytname[i][strlen(token_buffer) + 1] == '\"' \u0026\u0026 yytname[i][strlen(token_buffer) + 2] == 0) { break; } } 当使用 %token-table 声明时才会生成 yytname 表。 Token 的语义值 不可重入的解析器，所有语义值都存储在 yylval 中，如果使用单一语义值类型 (默认为 int)，可以用这种方式在 yylex 中使用 yylval = value; // 将值压入 Bison 栈 return INT; // 返回 token 类型 当使用 %union 定义了多类型时，需要正确使用各个 union 字段。比如 union 如下定义 %union { int intval; doubal val; symrec *tptr; } yylex 中需要如下使用 yylval.intval = value; return INT; Token 的文本位置 如果在行为中使用 @N 功能来跟踪 token 和分组的文本位置，那么必须在 yylex 中提供此信息。但相对的，这会明显拖慢解析器的速度。 通常情况下只需要正确处理 yylloc 的成员即可，其类型通常为 YYLTYPE，定义通常如下 typedef struct YYLTYPE { int first_line; int first_column; int last_line; int last_column; } YYLTYPE; 纯解析器的调用约定 如果你用来可重入的解析器，那不能使用全局变量 yylloc 和 yylval，需要将这两个变量以参数的形式传递给 yylex。原型如下 int yylex(YYSTYPE *lvalp, YYLTYPE *llocp); 如果没有使用位置参数的话，将不会定义 YYSTYPE，也就不需要由参数 lvalp 了。如果还需要其他参数，可以使用 %lex-param 来声明其他参数，用法和之前的 %parse-param 一样。如果想对 yyparse 和 yylex 都传入某个参数可以使用 %param。 错误处理函数 yyerror每当 Bison 解析器读取不能满足任何语法规则的标记时，它就会检测到语法错误。语法中的行为也可以使用宏 YYERROR 显式声明错误。Bison 解析器希望通过调用名为 yyerror 的函数来报告错误，用户必须实现该函数。函数原型如下 void yyerror(char const *s); 在 yyerror 返回后，yyparse 还会尝试使用编写的错误恢复规则，如果无法恢复，yyparse 将返回 1。 当你使用纯解析器时 (%define api.pure full)，将会生成原型为 void yyerror(YYLTYPE *locp, char const *msg); 解析器 i18nBison 支持解析器的国际化 (i18n)，我想这是个我用不上的功能。Bison 采用的 i18n 方案是通用的 gettext。 ","date":"05-01","objectID":"/2022/flex_and_bison/:2:3","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#解析器-c-语言接口"},{"categories":["CompilerPrinciple"],"content":" 解析器 C 语言接口Bison 的解析器实际上是名为 yyparse 的 C 函数。我们需要对其进行约定。请记住解析器出于内部的目的使用了很多 yy 或 YY 开头的 C 标识符，请小心冲突。 解析器函数 yyparse你需要调用 yyparse 来进行解析。此函数读取标记、执行行为，并在遇到输入结束或不可恢复的语法错误时最终返回。你还可以编写一个行为，指示 yyparse 立即返回，而无需继续执行。 int yyparse(void); // RETURN YYACCEPT (0) if report success, // RETURN YYABORT (1) if report failure, // RETURN 2 if memory exhaustion. 如果使用纯解析器，可以声明 %parse-param 为 yyparse 和 yyerror 定义额外的参数。比如 %parse-param {int *nastiness} {int *randomness} 声明后，这两个的函数如下 void yyerror(int *nastiness, int *randomness, const char *msg); void yyparse(int *nastiness, int *randomness); 当然同时使用 %define api.pure full (或仅 %define api.pure) 和 %locations， yyerror 将生成不一样的签名。 void yyerror(YYLTYPE *llocp, int *nastiness, int *randomness, const char *msg); 在调用时就可以这样使用 int nastiness, randomness; value = yyparse(\u0026nastiness, \u0026randomness); 在语法的规则行为中，也可以使用由 yyparse 传入的参数 exp: ... { ...; *randomness += 1; ... } 词法分析函数 yylex该函数从输入流中识别并返回 token，不会由 bison 创建，你可以使用 Flex 创建它。 yylex 调用约定 yylex 必须返回 token 类型的整数值，零或负数表示输入结束。当然 token 只有一个字符时也可以直接返回该字符。 int yylex(void) { if (c == EOF) { return 0; } if (c == '+' || c == '-') { return c; } // ... return INT; } 如果语法使用字符串字面量 token，yylex 可以通过两种方式确定它们的标记类型代码： 如果语法将符号 token 名称定义为字符串字面量别名，则 yylex 可以像所有其他符号一样使用这些符号名称。在这种情况下，在语法文件中使用字符串字面量 token 对 yylex 没有影响。 yylex 可以在 yytname 表中找到多字符 token，表中的索引是该 token 的编码，该 token 的名字用双引号 (”) 包围并记录在表中。 for (i = 0; i \u003c YYNTOKENS; i++) { if (yytname[i] != 0 \u0026\u0026 yytname[i][0] == '\"' \u0026\u0026 !strncmp(yytname[i] + 1, token_buffer, strlen(token_buffer)) \u0026\u0026 yytname[i][strlen(token_buffer) + 1] == '\"' \u0026\u0026 yytname[i][strlen(token_buffer) + 2] == 0) { break; } } 当使用 %token-table 声明时才会生成 yytname 表。 Token 的语义值 不可重入的解析器，所有语义值都存储在 yylval 中，如果使用单一语义值类型 (默认为 int)，可以用这种方式在 yylex 中使用 yylval = value; // 将值压入 Bison 栈 return INT; // 返回 token 类型 当使用 %union 定义了多类型时，需要正确使用各个 union 字段。比如 union 如下定义 %union { int intval; doubal val; symrec *tptr; } yylex 中需要如下使用 yylval.intval = value; return INT; Token 的文本位置 如果在行为中使用 @N 功能来跟踪 token 和分组的文本位置，那么必须在 yylex 中提供此信息。但相对的，这会明显拖慢解析器的速度。 通常情况下只需要正确处理 yylloc 的成员即可，其类型通常为 YYLTYPE，定义通常如下 typedef struct YYLTYPE { int first_line; int first_column; int last_line; int last_column; } YYLTYPE; 纯解析器的调用约定 如果你用来可重入的解析器，那不能使用全局变量 yylloc 和 yylval，需要将这两个变量以参数的形式传递给 yylex。原型如下 int yylex(YYSTYPE *lvalp, YYLTYPE *llocp); 如果没有使用位置参数的话，将不会定义 YYSTYPE，也就不需要由参数 lvalp 了。如果还需要其他参数，可以使用 %lex-param 来声明其他参数，用法和之前的 %parse-param 一样。如果想对 yyparse 和 yylex 都传入某个参数可以使用 %param。 错误处理函数 yyerror每当 Bison 解析器读取不能满足任何语法规则的标记时，它就会检测到语法错误。语法中的行为也可以使用宏 YYERROR 显式声明错误。Bison 解析器希望通过调用名为 yyerror 的函数来报告错误，用户必须实现该函数。函数原型如下 void yyerror(char const *s); 在 yyerror 返回后，yyparse 还会尝试使用编写的错误恢复规则，如果无法恢复，yyparse 将返回 1。 当你使用纯解析器时 (%define api.pure full)，将会生成原型为 void yyerror(YYLTYPE *locp, char const *msg); 解析器 i18nBison 支持解析器的国际化 (i18n)，我想这是个我用不上的功能。Bison 采用的 i18n 方案是通用的 gettext。 ","date":"05-01","objectID":"/2022/flex_and_bison/:2:3","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#解析器函数-yyparse"},{"categories":["CompilerPrinciple"],"content":" 解析器 C 语言接口Bison 的解析器实际上是名为 yyparse 的 C 函数。我们需要对其进行约定。请记住解析器出于内部的目的使用了很多 yy 或 YY 开头的 C 标识符，请小心冲突。 解析器函数 yyparse你需要调用 yyparse 来进行解析。此函数读取标记、执行行为，并在遇到输入结束或不可恢复的语法错误时最终返回。你还可以编写一个行为，指示 yyparse 立即返回，而无需继续执行。 int yyparse(void); // RETURN YYACCEPT (0) if report success, // RETURN YYABORT (1) if report failure, // RETURN 2 if memory exhaustion. 如果使用纯解析器，可以声明 %parse-param 为 yyparse 和 yyerror 定义额外的参数。比如 %parse-param {int *nastiness} {int *randomness} 声明后，这两个的函数如下 void yyerror(int *nastiness, int *randomness, const char *msg); void yyparse(int *nastiness, int *randomness); 当然同时使用 %define api.pure full (或仅 %define api.pure) 和 %locations， yyerror 将生成不一样的签名。 void yyerror(YYLTYPE *llocp, int *nastiness, int *randomness, const char *msg); 在调用时就可以这样使用 int nastiness, randomness; value = yyparse(\u0026nastiness, \u0026randomness); 在语法的规则行为中，也可以使用由 yyparse 传入的参数 exp: ... { ...; *randomness += 1; ... } 词法分析函数 yylex该函数从输入流中识别并返回 token，不会由 bison 创建，你可以使用 Flex 创建它。 yylex 调用约定 yylex 必须返回 token 类型的整数值，零或负数表示输入结束。当然 token 只有一个字符时也可以直接返回该字符。 int yylex(void) { if (c == EOF) { return 0; } if (c == '+' || c == '-') { return c; } // ... return INT; } 如果语法使用字符串字面量 token，yylex 可以通过两种方式确定它们的标记类型代码： 如果语法将符号 token 名称定义为字符串字面量别名，则 yylex 可以像所有其他符号一样使用这些符号名称。在这种情况下，在语法文件中使用字符串字面量 token 对 yylex 没有影响。 yylex 可以在 yytname 表中找到多字符 token，表中的索引是该 token 的编码，该 token 的名字用双引号 (”) 包围并记录在表中。 for (i = 0; i \u003c YYNTOKENS; i++) { if (yytname[i] != 0 \u0026\u0026 yytname[i][0] == '\"' \u0026\u0026 !strncmp(yytname[i] + 1, token_buffer, strlen(token_buffer)) \u0026\u0026 yytname[i][strlen(token_buffer) + 1] == '\"' \u0026\u0026 yytname[i][strlen(token_buffer) + 2] == 0) { break; } } 当使用 %token-table 声明时才会生成 yytname 表。 Token 的语义值 不可重入的解析器，所有语义值都存储在 yylval 中，如果使用单一语义值类型 (默认为 int)，可以用这种方式在 yylex 中使用 yylval = value; // 将值压入 Bison 栈 return INT; // 返回 token 类型 当使用 %union 定义了多类型时，需要正确使用各个 union 字段。比如 union 如下定义 %union { int intval; doubal val; symrec *tptr; } yylex 中需要如下使用 yylval.intval = value; return INT; Token 的文本位置 如果在行为中使用 @N 功能来跟踪 token 和分组的文本位置，那么必须在 yylex 中提供此信息。但相对的，这会明显拖慢解析器的速度。 通常情况下只需要正确处理 yylloc 的成员即可，其类型通常为 YYLTYPE，定义通常如下 typedef struct YYLTYPE { int first_line; int first_column; int last_line; int last_column; } YYLTYPE; 纯解析器的调用约定 如果你用来可重入的解析器，那不能使用全局变量 yylloc 和 yylval，需要将这两个变量以参数的形式传递给 yylex。原型如下 int yylex(YYSTYPE *lvalp, YYLTYPE *llocp); 如果没有使用位置参数的话，将不会定义 YYSTYPE，也就不需要由参数 lvalp 了。如果还需要其他参数，可以使用 %lex-param 来声明其他参数，用法和之前的 %parse-param 一样。如果想对 yyparse 和 yylex 都传入某个参数可以使用 %param。 错误处理函数 yyerror每当 Bison 解析器读取不能满足任何语法规则的标记时，它就会检测到语法错误。语法中的行为也可以使用宏 YYERROR 显式声明错误。Bison 解析器希望通过调用名为 yyerror 的函数来报告错误，用户必须实现该函数。函数原型如下 void yyerror(char const *s); 在 yyerror 返回后，yyparse 还会尝试使用编写的错误恢复规则，如果无法恢复，yyparse 将返回 1。 当你使用纯解析器时 (%define api.pure full)，将会生成原型为 void yyerror(YYLTYPE *locp, char const *msg); 解析器 i18nBison 支持解析器的国际化 (i18n)，我想这是个我用不上的功能。Bison 采用的 i18n 方案是通用的 gettext。 ","date":"05-01","objectID":"/2022/flex_and_bison/:2:3","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#词法分析函数-yylex"},{"categories":["CompilerPrinciple"],"content":" 解析器 C 语言接口Bison 的解析器实际上是名为 yyparse 的 C 函数。我们需要对其进行约定。请记住解析器出于内部的目的使用了很多 yy 或 YY 开头的 C 标识符，请小心冲突。 解析器函数 yyparse你需要调用 yyparse 来进行解析。此函数读取标记、执行行为，并在遇到输入结束或不可恢复的语法错误时最终返回。你还可以编写一个行为，指示 yyparse 立即返回，而无需继续执行。 int yyparse(void); // RETURN YYACCEPT (0) if report success, // RETURN YYABORT (1) if report failure, // RETURN 2 if memory exhaustion. 如果使用纯解析器，可以声明 %parse-param 为 yyparse 和 yyerror 定义额外的参数。比如 %parse-param {int *nastiness} {int *randomness} 声明后，这两个的函数如下 void yyerror(int *nastiness, int *randomness, const char *msg); void yyparse(int *nastiness, int *randomness); 当然同时使用 %define api.pure full (或仅 %define api.pure) 和 %locations， yyerror 将生成不一样的签名。 void yyerror(YYLTYPE *llocp, int *nastiness, int *randomness, const char *msg); 在调用时就可以这样使用 int nastiness, randomness; value = yyparse(\u0026nastiness, \u0026randomness); 在语法的规则行为中，也可以使用由 yyparse 传入的参数 exp: ... { ...; *randomness += 1; ... } 词法分析函数 yylex该函数从输入流中识别并返回 token，不会由 bison 创建，你可以使用 Flex 创建它。 yylex 调用约定 yylex 必须返回 token 类型的整数值，零或负数表示输入结束。当然 token 只有一个字符时也可以直接返回该字符。 int yylex(void) { if (c == EOF) { return 0; } if (c == '+' || c == '-') { return c; } // ... return INT; } 如果语法使用字符串字面量 token，yylex 可以通过两种方式确定它们的标记类型代码： 如果语法将符号 token 名称定义为字符串字面量别名，则 yylex 可以像所有其他符号一样使用这些符号名称。在这种情况下，在语法文件中使用字符串字面量 token 对 yylex 没有影响。 yylex 可以在 yytname 表中找到多字符 token，表中的索引是该 token 的编码，该 token 的名字用双引号 (”) 包围并记录在表中。 for (i = 0; i \u003c YYNTOKENS; i++) { if (yytname[i] != 0 \u0026\u0026 yytname[i][0] == '\"' \u0026\u0026 !strncmp(yytname[i] + 1, token_buffer, strlen(token_buffer)) \u0026\u0026 yytname[i][strlen(token_buffer) + 1] == '\"' \u0026\u0026 yytname[i][strlen(token_buffer) + 2] == 0) { break; } } 当使用 %token-table 声明时才会生成 yytname 表。 Token 的语义值 不可重入的解析器，所有语义值都存储在 yylval 中，如果使用单一语义值类型 (默认为 int)，可以用这种方式在 yylex 中使用 yylval = value; // 将值压入 Bison 栈 return INT; // 返回 token 类型 当使用 %union 定义了多类型时，需要正确使用各个 union 字段。比如 union 如下定义 %union { int intval; doubal val; symrec *tptr; } yylex 中需要如下使用 yylval.intval = value; return INT; Token 的文本位置 如果在行为中使用 @N 功能来跟踪 token 和分组的文本位置，那么必须在 yylex 中提供此信息。但相对的，这会明显拖慢解析器的速度。 通常情况下只需要正确处理 yylloc 的成员即可，其类型通常为 YYLTYPE，定义通常如下 typedef struct YYLTYPE { int first_line; int first_column; int last_line; int last_column; } YYLTYPE; 纯解析器的调用约定 如果你用来可重入的解析器，那不能使用全局变量 yylloc 和 yylval，需要将这两个变量以参数的形式传递给 yylex。原型如下 int yylex(YYSTYPE *lvalp, YYLTYPE *llocp); 如果没有使用位置参数的话，将不会定义 YYSTYPE，也就不需要由参数 lvalp 了。如果还需要其他参数，可以使用 %lex-param 来声明其他参数，用法和之前的 %parse-param 一样。如果想对 yyparse 和 yylex 都传入某个参数可以使用 %param。 错误处理函数 yyerror每当 Bison 解析器读取不能满足任何语法规则的标记时，它就会检测到语法错误。语法中的行为也可以使用宏 YYERROR 显式声明错误。Bison 解析器希望通过调用名为 yyerror 的函数来报告错误，用户必须实现该函数。函数原型如下 void yyerror(char const *s); 在 yyerror 返回后，yyparse 还会尝试使用编写的错误恢复规则，如果无法恢复，yyparse 将返回 1。 当你使用纯解析器时 (%define api.pure full)，将会生成原型为 void yyerror(YYLTYPE *locp, char const *msg); 解析器 i18nBison 支持解析器的国际化 (i18n)，我想这是个我用不上的功能。Bison 采用的 i18n 方案是通用的 gettext。 ","date":"05-01","objectID":"/2022/flex_and_bison/:2:3","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#错误处理函数-yyerror"},{"categories":["CompilerPrinciple"],"content":" 解析器 C 语言接口Bison 的解析器实际上是名为 yyparse 的 C 函数。我们需要对其进行约定。请记住解析器出于内部的目的使用了很多 yy 或 YY 开头的 C 标识符，请小心冲突。 解析器函数 yyparse你需要调用 yyparse 来进行解析。此函数读取标记、执行行为，并在遇到输入结束或不可恢复的语法错误时最终返回。你还可以编写一个行为，指示 yyparse 立即返回，而无需继续执行。 int yyparse(void); // RETURN YYACCEPT (0) if report success, // RETURN YYABORT (1) if report failure, // RETURN 2 if memory exhaustion. 如果使用纯解析器，可以声明 %parse-param 为 yyparse 和 yyerror 定义额外的参数。比如 %parse-param {int *nastiness} {int *randomness} 声明后，这两个的函数如下 void yyerror(int *nastiness, int *randomness, const char *msg); void yyparse(int *nastiness, int *randomness); 当然同时使用 %define api.pure full (或仅 %define api.pure) 和 %locations， yyerror 将生成不一样的签名。 void yyerror(YYLTYPE *llocp, int *nastiness, int *randomness, const char *msg); 在调用时就可以这样使用 int nastiness, randomness; value = yyparse(\u0026nastiness, \u0026randomness); 在语法的规则行为中，也可以使用由 yyparse 传入的参数 exp: ... { ...; *randomness += 1; ... } 词法分析函数 yylex该函数从输入流中识别并返回 token，不会由 bison 创建，你可以使用 Flex 创建它。 yylex 调用约定 yylex 必须返回 token 类型的整数值，零或负数表示输入结束。当然 token 只有一个字符时也可以直接返回该字符。 int yylex(void) { if (c == EOF) { return 0; } if (c == '+' || c == '-') { return c; } // ... return INT; } 如果语法使用字符串字面量 token，yylex 可以通过两种方式确定它们的标记类型代码： 如果语法将符号 token 名称定义为字符串字面量别名，则 yylex 可以像所有其他符号一样使用这些符号名称。在这种情况下，在语法文件中使用字符串字面量 token 对 yylex 没有影响。 yylex 可以在 yytname 表中找到多字符 token，表中的索引是该 token 的编码，该 token 的名字用双引号 (”) 包围并记录在表中。 for (i = 0; i \u003c YYNTOKENS; i++) { if (yytname[i] != 0 \u0026\u0026 yytname[i][0] == '\"' \u0026\u0026 !strncmp(yytname[i] + 1, token_buffer, strlen(token_buffer)) \u0026\u0026 yytname[i][strlen(token_buffer) + 1] == '\"' \u0026\u0026 yytname[i][strlen(token_buffer) + 2] == 0) { break; } } 当使用 %token-table 声明时才会生成 yytname 表。 Token 的语义值 不可重入的解析器，所有语义值都存储在 yylval 中，如果使用单一语义值类型 (默认为 int)，可以用这种方式在 yylex 中使用 yylval = value; // 将值压入 Bison 栈 return INT; // 返回 token 类型 当使用 %union 定义了多类型时，需要正确使用各个 union 字段。比如 union 如下定义 %union { int intval; doubal val; symrec *tptr; } yylex 中需要如下使用 yylval.intval = value; return INT; Token 的文本位置 如果在行为中使用 @N 功能来跟踪 token 和分组的文本位置，那么必须在 yylex 中提供此信息。但相对的，这会明显拖慢解析器的速度。 通常情况下只需要正确处理 yylloc 的成员即可，其类型通常为 YYLTYPE，定义通常如下 typedef struct YYLTYPE { int first_line; int first_column; int last_line; int last_column; } YYLTYPE; 纯解析器的调用约定 如果你用来可重入的解析器，那不能使用全局变量 yylloc 和 yylval，需要将这两个变量以参数的形式传递给 yylex。原型如下 int yylex(YYSTYPE *lvalp, YYLTYPE *llocp); 如果没有使用位置参数的话，将不会定义 YYSTYPE，也就不需要由参数 lvalp 了。如果还需要其他参数，可以使用 %lex-param 来声明其他参数，用法和之前的 %parse-param 一样。如果想对 yyparse 和 yylex 都传入某个参数可以使用 %param。 错误处理函数 yyerror每当 Bison 解析器读取不能满足任何语法规则的标记时，它就会检测到语法错误。语法中的行为也可以使用宏 YYERROR 显式声明错误。Bison 解析器希望通过调用名为 yyerror 的函数来报告错误，用户必须实现该函数。函数原型如下 void yyerror(char const *s); 在 yyerror 返回后，yyparse 还会尝试使用编写的错误恢复规则，如果无法恢复，yyparse 将返回 1。 当你使用纯解析器时 (%define api.pure full)，将会生成原型为 void yyerror(YYLTYPE *locp, char const *msg); 解析器 i18nBison 支持解析器的国际化 (i18n)，我想这是个我用不上的功能。Bison 采用的 i18n 方案是通用的 gettext。 ","date":"05-01","objectID":"/2022/flex_and_bison/:2:3","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#解析器-i18n"},{"categories":["CompilerPrinciple"],"content":" Bison 解析器算法Bison 在读取 token 时，会将 token 及其语义值一起推送到名为 parser stack 的栈上，而这个行为被称为移入。但不会总是移入，当最后 N 个元素与语法规则相匹配时，元素将会组合，这个步骤称为归约。当解析器通过移入与归约，直到将整个输入串归约成单个分组时，我们将剩下的这个符号称作开始符号。而解析器的整个操作是自下而上的。 向前看符号就像之前的理论部分，解析器向前看一个符号，来确定下一个动作是什么。如果我们写下一个阶乘代码 expr: term '+' expr | term ; term: '(' expr ')' | term '!' | \"number\" ; 现在假设输入上的 1+2 已被读入并移入栈 如果后面是 ) 那么栈顶的三个元素将被归约为 expr，这是唯一有效的操作，因为移入之后没有规则继续 如果后面是 ! 那么会移入符号，以便 2! 可以归约成一项。如果在移入之前归约，那么将没有规则可以继续 可以通过 yychar 查看向前看符号。 移入归约冲突经典的悬空 else 问题 if_stmt: \"if\" expr \"then\" stmt | \"if\" expr \"then\" stmt \"else\" stmt ; 当 else 成为向前看符号时，移入规则有效，归约规则也有效，这就产生了一个移入规约冲突。但是 Bison 解析器更喜欢采用移入规则解决这种冲突。 悬空 else 问题往往的解决方式是，通常原则是 else 匹配最近的 if，那么下面这两行代码等价 if x then if y then win; else lose; if x then do; if y then win; else lose; end; 如果选用归约规则，与通常原则将大相径庭。就是下面这两行例子 if x then if y then win; else lose; if x then do; if y then win; end; else lose; 既然移入/归约冲突都是移入优先，那用之前介绍的 %expect N 可以吗？ 警告 不建议使用 %expect N (除 %expect 0)，即使移入归约冲突的数量正确，不代表发生错误的原因是预期的 操作符优先级算数表达式中也经常出现移入归约冲突，但这里移入不总是首选。优先级则是处理这类问题的一种解决方法。 想想这段代码， expr: expr '+' expr | expr '*' expr | expr '\u003c' expr | '(' expr ')' | ... ; 遇到 1+2*5 和 1+1+1 时，移入归约冲突，此时应该怎么选择，这就是优先级和结合性的作用。至于定义已经在声明运算符优先级中介绍过了。 当然也可以使用优先级去解决悬空 else 问题。比如说，token ELSE 的优先级总是高于 token THEN，这样在悬空 else 问题上，每次都优先移入 else 而非归约。 %precedence THEN %precedence ELSE 有个很奇怪的地方就是，往往优先级是上下文相关的。最直接的例子是一元 ‘-’ (负号) 与二元 ‘-’ (减号)，比如 C 语言的定义中，符号的优先级为 2 (越小越优先)，而乘号为 3，减号为 4。但是减号与负号的区别在于上下文的不同。另一个问题，Bison 中的优先级声明只能一 token 一次，这时就需要 %prec 修饰符在规则中进行修饰。 %prec TERMINAL-SYMBOL 首先在规则中声明这个上下文相关的符号为一个不存在的 (虚构的) token type，在声明部分声明这个 token type 的优先级。 ... %left '+' '-' %left '*' %right UMINUS %% expr: ... | expr '-' expr | ... | '-' expr %prec UMINUS | ... ; 归约归约冲突如果有多个规则可以应用咋同一个输入上，会发生归约归约冲突，通常这是严重的语法错误。比如下面这个示例 sequence: %empty { printf (\"empty sequence\\n\"); } | maybeword | sequence word { printf (\"added word %s\\n\", $2); } ; maybeword: %empty { printf (\"empty maybeword\\n\"); } | word { printf (\"single word %s\\n\", $1); } ; 比如现在栈顶是 word，word 可以被归约为 maybeword，也可以被归约为 sequence。Bison 会选择首先出现在语法中的规则进行归约，但这可能超出编码预期，因此尽量不要依赖这种方式，而是选择消除所有的归约归约冲突。比如将 sequence 修改为 sequence: %empty { printf (\"empty sequence\\n\"); } | sequence word { printf (\"added word %s\\n\", $2); } ; 当然有可能有其他方式产生归约归约冲突，比如下面这个例子。虽然每个规则独立看是没有问题的，但三个规则放在一起将产生错误：空输入可以被无限多种方式解析。 sequence: %empty | sequence words | sequence redirects ; words: %empty | words word ; redirects: %empty | redirects redirect ; 稍加修改，你会得到一个看起来不错的方法，空输入再也不会产生冲突了。 sequence: %empty | sequence words | sequence redirects ; words: word | words word ; redirects: redirect | redirects redirect ; 但是，如果输入为 “word word”，很明显可以被归约 words words 或 words，这有一个二义性的移入归约冲突，第二个 word 是移入还是将栈中的 word 归约。 可以用优先级解决这个问题 %precedence \"word\" %precedence \"sequence\" %% sequence: %empty | sequence word %prec \"sequence\" | sequence redirect %prec \"sequence\" ; words: word | words \"word\" ; 当然结合性也能解决这个问题 %right \"word\" \"redirect\" %% sequence: %empty | sequence word %prec \"word\" | sequence redirect %prec \"redirect\" ; 神秘冲突有些归约归约冲突看起来根本没什么依据。这在 info 中称为 神秘 (Mysterious) 冲突。比如 def: param_spec return_spec ','; param_spec: type | name_list ':' type ; return_spec: type | name ':' type ; type: \"id\"; name: \"id\"; name_list: name | name ',' name_list ; 这个文法是一个 LR(1) 文法，从 param_spec 开始，如果 id 后面是一个 ‘:’ 或 ‘,’ 它将被归约为 name，如果后面是一个 id 则被归约为 type。但是问题在于这不是一个 LALR(1) 文法，看起来 param_spec 和 return_spec 太像了，以至于 Bison 无法处理。 对于许多语法 (特别是非 LR(1) 语法)，LALR(1) 的局限性造成了各种问题，因此最直接的解决方法是构造不那么高效的规范 LR(1) 分析表或 IELR(1) 分析表。 信息 从 GNU Bison 3.1 开始 LR(LR,LALR,IELR) 分析表不再是实验性功能 如果只用 LALR(1) 解决这个问题，你可以添加一些东西，让这两个生成体不那么相同。以此来解决这个问题。 return_spec: type | name ':' type | \"id\" \"bogus\" /* This rule is never used. */ ; 很明显，yylex 不能解析出一个 bogus 的 token。在这个示例中，更好的解决方法是 return_spec: type | \"id\" ':' type ; 调整 LR构造 LR 解析器是因为历史原因的选择，现代化的 Bison 给出了一些更好的选择。 LR 表构建 只要修改一下下声明，就可以从历史遗留的 LALR(1) 分析表切换到 IELR 或 canonical LR 分析表。当然，主要是由于 LALR(1) 文法不够强大，不能完全解析 LR(1) 文法。有些需要复杂重构的 LALR 文法，或许在切换到 LR(1) 文法后，就可以完全消除冲突。 先说说怎么指定不同的分析表。 %define lr.type TYPE 可选值如下 lalr (default) ielr canonical-lr 当然每种 LR 分析表都有其特点，在龙书中也详细讲了 Canonic","date":"05-01","objectID":"/2022/flex_and_bison/:2:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#bison-解析器算法"},{"categories":["CompilerPrinciple"],"content":" Bison 解析器算法Bison 在读取 token 时，会将 token 及其语义值一起推送到名为 parser stack 的栈上，而这个行为被称为移入。但不会总是移入，当最后 N 个元素与语法规则相匹配时，元素将会组合，这个步骤称为归约。当解析器通过移入与归约，直到将整个输入串归约成单个分组时，我们将剩下的这个符号称作开始符号。而解析器的整个操作是自下而上的。 向前看符号就像之前的理论部分，解析器向前看一个符号，来确定下一个动作是什么。如果我们写下一个阶乘代码 expr: term '+' expr | term ; term: '(' expr ')' | term '!' | \"number\" ; 现在假设输入上的 1+2 已被读入并移入栈 如果后面是 ) 那么栈顶的三个元素将被归约为 expr，这是唯一有效的操作，因为移入之后没有规则继续 如果后面是 ! 那么会移入符号，以便 2! 可以归约成一项。如果在移入之前归约，那么将没有规则可以继续 可以通过 yychar 查看向前看符号。 移入归约冲突经典的悬空 else 问题 if_stmt: \"if\" expr \"then\" stmt | \"if\" expr \"then\" stmt \"else\" stmt ; 当 else 成为向前看符号时，移入规则有效，归约规则也有效，这就产生了一个移入规约冲突。但是 Bison 解析器更喜欢采用移入规则解决这种冲突。 悬空 else 问题往往的解决方式是，通常原则是 else 匹配最近的 if，那么下面这两行代码等价 if x then if y then win; else lose; if x then do; if y then win; else lose; end; 如果选用归约规则，与通常原则将大相径庭。就是下面这两行例子 if x then if y then win; else lose; if x then do; if y then win; end; else lose; 既然移入/归约冲突都是移入优先，那用之前介绍的 %expect N 可以吗？ 警告 不建议使用 %expect N (除 %expect 0)，即使移入归约冲突的数量正确，不代表发生错误的原因是预期的 操作符优先级算数表达式中也经常出现移入归约冲突，但这里移入不总是首选。优先级则是处理这类问题的一种解决方法。 想想这段代码， expr: expr '+' expr | expr '*' expr | expr '\u003c' expr | '(' expr ')' | ... ; 遇到 1+2*5 和 1+1+1 时，移入归约冲突，此时应该怎么选择，这就是优先级和结合性的作用。至于定义已经在声明运算符优先级中介绍过了。 当然也可以使用优先级去解决悬空 else 问题。比如说，token ELSE 的优先级总是高于 token THEN，这样在悬空 else 问题上，每次都优先移入 else 而非归约。 %precedence THEN %precedence ELSE 有个很奇怪的地方就是，往往优先级是上下文相关的。最直接的例子是一元 ‘-’ (负号) 与二元 ‘-’ (减号)，比如 C 语言的定义中，符号的优先级为 2 (越小越优先)，而乘号为 3，减号为 4。但是减号与负号的区别在于上下文的不同。另一个问题，Bison 中的优先级声明只能一 token 一次，这时就需要 %prec 修饰符在规则中进行修饰。 %prec TERMINAL-SYMBOL 首先在规则中声明这个上下文相关的符号为一个不存在的 (虚构的) token type，在声明部分声明这个 token type 的优先级。 ... %left '+' '-' %left '*' %right UMINUS %% expr: ... | expr '-' expr | ... | '-' expr %prec UMINUS | ... ; 归约归约冲突如果有多个规则可以应用咋同一个输入上，会发生归约归约冲突，通常这是严重的语法错误。比如下面这个示例 sequence: %empty { printf (\"empty sequence\\n\"); } | maybeword | sequence word { printf (\"added word %s\\n\", $2); } ; maybeword: %empty { printf (\"empty maybeword\\n\"); } | word { printf (\"single word %s\\n\", $1); } ; 比如现在栈顶是 word，word 可以被归约为 maybeword，也可以被归约为 sequence。Bison 会选择首先出现在语法中的规则进行归约，但这可能超出编码预期，因此尽量不要依赖这种方式，而是选择消除所有的归约归约冲突。比如将 sequence 修改为 sequence: %empty { printf (\"empty sequence\\n\"); } | sequence word { printf (\"added word %s\\n\", $2); } ; 当然有可能有其他方式产生归约归约冲突，比如下面这个例子。虽然每个规则独立看是没有问题的，但三个规则放在一起将产生错误：空输入可以被无限多种方式解析。 sequence: %empty | sequence words | sequence redirects ; words: %empty | words word ; redirects: %empty | redirects redirect ; 稍加修改，你会得到一个看起来不错的方法，空输入再也不会产生冲突了。 sequence: %empty | sequence words | sequence redirects ; words: word | words word ; redirects: redirect | redirects redirect ; 但是，如果输入为 “word word”，很明显可以被归约 words words 或 words，这有一个二义性的移入归约冲突，第二个 word 是移入还是将栈中的 word 归约。 可以用优先级解决这个问题 %precedence \"word\" %precedence \"sequence\" %% sequence: %empty | sequence word %prec \"sequence\" | sequence redirect %prec \"sequence\" ; words: word | words \"word\" ; 当然结合性也能解决这个问题 %right \"word\" \"redirect\" %% sequence: %empty | sequence word %prec \"word\" | sequence redirect %prec \"redirect\" ; 神秘冲突有些归约归约冲突看起来根本没什么依据。这在 info 中称为 神秘 (Mysterious) 冲突。比如 def: param_spec return_spec ','; param_spec: type | name_list ':' type ; return_spec: type | name ':' type ; type: \"id\"; name: \"id\"; name_list: name | name ',' name_list ; 这个文法是一个 LR(1) 文法，从 param_spec 开始，如果 id 后面是一个 ‘:’ 或 ‘,’ 它将被归约为 name，如果后面是一个 id 则被归约为 type。但是问题在于这不是一个 LALR(1) 文法，看起来 param_spec 和 return_spec 太像了，以至于 Bison 无法处理。 对于许多语法 (特别是非 LR(1) 语法)，LALR(1) 的局限性造成了各种问题，因此最直接的解决方法是构造不那么高效的规范 LR(1) 分析表或 IELR(1) 分析表。 信息 从 GNU Bison 3.1 开始 LR(LR,LALR,IELR) 分析表不再是实验性功能 如果只用 LALR(1) 解决这个问题，你可以添加一些东西，让这两个生成体不那么相同。以此来解决这个问题。 return_spec: type | name ':' type | \"id\" \"bogus\" /* This rule is never used. */ ; 很明显，yylex 不能解析出一个 bogus 的 token。在这个示例中，更好的解决方法是 return_spec: type | \"id\" ':' type ; 调整 LR构造 LR 解析器是因为历史原因的选择，现代化的 Bison 给出了一些更好的选择。 LR 表构建 只要修改一下下声明，就可以从历史遗留的 LALR(1) 分析表切换到 IELR 或 canonical LR 分析表。当然，主要是由于 LALR(1) 文法不够强大，不能完全解析 LR(1) 文法。有些需要复杂重构的 LALR 文法，或许在切换到 LR(1) 文法后，就可以完全消除冲突。 先说说怎么指定不同的分析表。 %define lr.type TYPE 可选值如下 lalr (default) ielr canonical-lr 当然每种 LR 分析表都有其特点，在龙书中也详细讲了 Canonic","date":"05-01","objectID":"/2022/flex_and_bison/:2:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#向前看符号"},{"categories":["CompilerPrinciple"],"content":" Bison 解析器算法Bison 在读取 token 时，会将 token 及其语义值一起推送到名为 parser stack 的栈上，而这个行为被称为移入。但不会总是移入，当最后 N 个元素与语法规则相匹配时，元素将会组合，这个步骤称为归约。当解析器通过移入与归约，直到将整个输入串归约成单个分组时，我们将剩下的这个符号称作开始符号。而解析器的整个操作是自下而上的。 向前看符号就像之前的理论部分，解析器向前看一个符号，来确定下一个动作是什么。如果我们写下一个阶乘代码 expr: term '+' expr | term ; term: '(' expr ')' | term '!' | \"number\" ; 现在假设输入上的 1+2 已被读入并移入栈 如果后面是 ) 那么栈顶的三个元素将被归约为 expr，这是唯一有效的操作，因为移入之后没有规则继续 如果后面是 ! 那么会移入符号，以便 2! 可以归约成一项。如果在移入之前归约，那么将没有规则可以继续 可以通过 yychar 查看向前看符号。 移入归约冲突经典的悬空 else 问题 if_stmt: \"if\" expr \"then\" stmt | \"if\" expr \"then\" stmt \"else\" stmt ; 当 else 成为向前看符号时，移入规则有效，归约规则也有效，这就产生了一个移入规约冲突。但是 Bison 解析器更喜欢采用移入规则解决这种冲突。 悬空 else 问题往往的解决方式是，通常原则是 else 匹配最近的 if，那么下面这两行代码等价 if x then if y then win; else lose; if x then do; if y then win; else lose; end; 如果选用归约规则，与通常原则将大相径庭。就是下面这两行例子 if x then if y then win; else lose; if x then do; if y then win; end; else lose; 既然移入/归约冲突都是移入优先，那用之前介绍的 %expect N 可以吗？ 警告 不建议使用 %expect N (除 %expect 0)，即使移入归约冲突的数量正确，不代表发生错误的原因是预期的 操作符优先级算数表达式中也经常出现移入归约冲突，但这里移入不总是首选。优先级则是处理这类问题的一种解决方法。 想想这段代码， expr: expr '+' expr | expr '*' expr | expr '\u003c' expr | '(' expr ')' | ... ; 遇到 1+2*5 和 1+1+1 时，移入归约冲突，此时应该怎么选择，这就是优先级和结合性的作用。至于定义已经在声明运算符优先级中介绍过了。 当然也可以使用优先级去解决悬空 else 问题。比如说，token ELSE 的优先级总是高于 token THEN，这样在悬空 else 问题上，每次都优先移入 else 而非归约。 %precedence THEN %precedence ELSE 有个很奇怪的地方就是，往往优先级是上下文相关的。最直接的例子是一元 ‘-’ (负号) 与二元 ‘-’ (减号)，比如 C 语言的定义中，符号的优先级为 2 (越小越优先)，而乘号为 3，减号为 4。但是减号与负号的区别在于上下文的不同。另一个问题，Bison 中的优先级声明只能一 token 一次，这时就需要 %prec 修饰符在规则中进行修饰。 %prec TERMINAL-SYMBOL 首先在规则中声明这个上下文相关的符号为一个不存在的 (虚构的) token type，在声明部分声明这个 token type 的优先级。 ... %left '+' '-' %left '*' %right UMINUS %% expr: ... | expr '-' expr | ... | '-' expr %prec UMINUS | ... ; 归约归约冲突如果有多个规则可以应用咋同一个输入上，会发生归约归约冲突，通常这是严重的语法错误。比如下面这个示例 sequence: %empty { printf (\"empty sequence\\n\"); } | maybeword | sequence word { printf (\"added word %s\\n\", $2); } ; maybeword: %empty { printf (\"empty maybeword\\n\"); } | word { printf (\"single word %s\\n\", $1); } ; 比如现在栈顶是 word，word 可以被归约为 maybeword，也可以被归约为 sequence。Bison 会选择首先出现在语法中的规则进行归约，但这可能超出编码预期，因此尽量不要依赖这种方式，而是选择消除所有的归约归约冲突。比如将 sequence 修改为 sequence: %empty { printf (\"empty sequence\\n\"); } | sequence word { printf (\"added word %s\\n\", $2); } ; 当然有可能有其他方式产生归约归约冲突，比如下面这个例子。虽然每个规则独立看是没有问题的，但三个规则放在一起将产生错误：空输入可以被无限多种方式解析。 sequence: %empty | sequence words | sequence redirects ; words: %empty | words word ; redirects: %empty | redirects redirect ; 稍加修改，你会得到一个看起来不错的方法，空输入再也不会产生冲突了。 sequence: %empty | sequence words | sequence redirects ; words: word | words word ; redirects: redirect | redirects redirect ; 但是，如果输入为 “word word”，很明显可以被归约 words words 或 words，这有一个二义性的移入归约冲突，第二个 word 是移入还是将栈中的 word 归约。 可以用优先级解决这个问题 %precedence \"word\" %precedence \"sequence\" %% sequence: %empty | sequence word %prec \"sequence\" | sequence redirect %prec \"sequence\" ; words: word | words \"word\" ; 当然结合性也能解决这个问题 %right \"word\" \"redirect\" %% sequence: %empty | sequence word %prec \"word\" | sequence redirect %prec \"redirect\" ; 神秘冲突有些归约归约冲突看起来根本没什么依据。这在 info 中称为 神秘 (Mysterious) 冲突。比如 def: param_spec return_spec ','; param_spec: type | name_list ':' type ; return_spec: type | name ':' type ; type: \"id\"; name: \"id\"; name_list: name | name ',' name_list ; 这个文法是一个 LR(1) 文法，从 param_spec 开始，如果 id 后面是一个 ‘:’ 或 ‘,’ 它将被归约为 name，如果后面是一个 id 则被归约为 type。但是问题在于这不是一个 LALR(1) 文法，看起来 param_spec 和 return_spec 太像了，以至于 Bison 无法处理。 对于许多语法 (特别是非 LR(1) 语法)，LALR(1) 的局限性造成了各种问题，因此最直接的解决方法是构造不那么高效的规范 LR(1) 分析表或 IELR(1) 分析表。 信息 从 GNU Bison 3.1 开始 LR(LR,LALR,IELR) 分析表不再是实验性功能 如果只用 LALR(1) 解决这个问题，你可以添加一些东西，让这两个生成体不那么相同。以此来解决这个问题。 return_spec: type | name ':' type | \"id\" \"bogus\" /* This rule is never used. */ ; 很明显，yylex 不能解析出一个 bogus 的 token。在这个示例中，更好的解决方法是 return_spec: type | \"id\" ':' type ; 调整 LR构造 LR 解析器是因为历史原因的选择，现代化的 Bison 给出了一些更好的选择。 LR 表构建 只要修改一下下声明，就可以从历史遗留的 LALR(1) 分析表切换到 IELR 或 canonical LR 分析表。当然，主要是由于 LALR(1) 文法不够强大，不能完全解析 LR(1) 文法。有些需要复杂重构的 LALR 文法，或许在切换到 LR(1) 文法后，就可以完全消除冲突。 先说说怎么指定不同的分析表。 %define lr.type TYPE 可选值如下 lalr (default) ielr canonical-lr 当然每种 LR 分析表都有其特点，在龙书中也详细讲了 Canonic","date":"05-01","objectID":"/2022/flex_and_bison/:2:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#移入归约冲突"},{"categories":["CompilerPrinciple"],"content":" Bison 解析器算法Bison 在读取 token 时，会将 token 及其语义值一起推送到名为 parser stack 的栈上，而这个行为被称为移入。但不会总是移入，当最后 N 个元素与语法规则相匹配时，元素将会组合，这个步骤称为归约。当解析器通过移入与归约，直到将整个输入串归约成单个分组时，我们将剩下的这个符号称作开始符号。而解析器的整个操作是自下而上的。 向前看符号就像之前的理论部分，解析器向前看一个符号，来确定下一个动作是什么。如果我们写下一个阶乘代码 expr: term '+' expr | term ; term: '(' expr ')' | term '!' | \"number\" ; 现在假设输入上的 1+2 已被读入并移入栈 如果后面是 ) 那么栈顶的三个元素将被归约为 expr，这是唯一有效的操作，因为移入之后没有规则继续 如果后面是 ! 那么会移入符号，以便 2! 可以归约成一项。如果在移入之前归约，那么将没有规则可以继续 可以通过 yychar 查看向前看符号。 移入归约冲突经典的悬空 else 问题 if_stmt: \"if\" expr \"then\" stmt | \"if\" expr \"then\" stmt \"else\" stmt ; 当 else 成为向前看符号时，移入规则有效，归约规则也有效，这就产生了一个移入规约冲突。但是 Bison 解析器更喜欢采用移入规则解决这种冲突。 悬空 else 问题往往的解决方式是，通常原则是 else 匹配最近的 if，那么下面这两行代码等价 if x then if y then win; else lose; if x then do; if y then win; else lose; end; 如果选用归约规则，与通常原则将大相径庭。就是下面这两行例子 if x then if y then win; else lose; if x then do; if y then win; end; else lose; 既然移入/归约冲突都是移入优先，那用之前介绍的 %expect N 可以吗？ 警告 不建议使用 %expect N (除 %expect 0)，即使移入归约冲突的数量正确，不代表发生错误的原因是预期的 操作符优先级算数表达式中也经常出现移入归约冲突，但这里移入不总是首选。优先级则是处理这类问题的一种解决方法。 想想这段代码， expr: expr '+' expr | expr '*' expr | expr '\u003c' expr | '(' expr ')' | ... ; 遇到 1+2*5 和 1+1+1 时，移入归约冲突，此时应该怎么选择，这就是优先级和结合性的作用。至于定义已经在声明运算符优先级中介绍过了。 当然也可以使用优先级去解决悬空 else 问题。比如说，token ELSE 的优先级总是高于 token THEN，这样在悬空 else 问题上，每次都优先移入 else 而非归约。 %precedence THEN %precedence ELSE 有个很奇怪的地方就是，往往优先级是上下文相关的。最直接的例子是一元 ‘-’ (负号) 与二元 ‘-’ (减号)，比如 C 语言的定义中，符号的优先级为 2 (越小越优先)，而乘号为 3，减号为 4。但是减号与负号的区别在于上下文的不同。另一个问题，Bison 中的优先级声明只能一 token 一次，这时就需要 %prec 修饰符在规则中进行修饰。 %prec TERMINAL-SYMBOL 首先在规则中声明这个上下文相关的符号为一个不存在的 (虚构的) token type，在声明部分声明这个 token type 的优先级。 ... %left '+' '-' %left '*' %right UMINUS %% expr: ... | expr '-' expr | ... | '-' expr %prec UMINUS | ... ; 归约归约冲突如果有多个规则可以应用咋同一个输入上，会发生归约归约冲突，通常这是严重的语法错误。比如下面这个示例 sequence: %empty { printf (\"empty sequence\\n\"); } | maybeword | sequence word { printf (\"added word %s\\n\", $2); } ; maybeword: %empty { printf (\"empty maybeword\\n\"); } | word { printf (\"single word %s\\n\", $1); } ; 比如现在栈顶是 word，word 可以被归约为 maybeword，也可以被归约为 sequence。Bison 会选择首先出现在语法中的规则进行归约，但这可能超出编码预期，因此尽量不要依赖这种方式，而是选择消除所有的归约归约冲突。比如将 sequence 修改为 sequence: %empty { printf (\"empty sequence\\n\"); } | sequence word { printf (\"added word %s\\n\", $2); } ; 当然有可能有其他方式产生归约归约冲突，比如下面这个例子。虽然每个规则独立看是没有问题的，但三个规则放在一起将产生错误：空输入可以被无限多种方式解析。 sequence: %empty | sequence words | sequence redirects ; words: %empty | words word ; redirects: %empty | redirects redirect ; 稍加修改，你会得到一个看起来不错的方法，空输入再也不会产生冲突了。 sequence: %empty | sequence words | sequence redirects ; words: word | words word ; redirects: redirect | redirects redirect ; 但是，如果输入为 “word word”，很明显可以被归约 words words 或 words，这有一个二义性的移入归约冲突，第二个 word 是移入还是将栈中的 word 归约。 可以用优先级解决这个问题 %precedence \"word\" %precedence \"sequence\" %% sequence: %empty | sequence word %prec \"sequence\" | sequence redirect %prec \"sequence\" ; words: word | words \"word\" ; 当然结合性也能解决这个问题 %right \"word\" \"redirect\" %% sequence: %empty | sequence word %prec \"word\" | sequence redirect %prec \"redirect\" ; 神秘冲突有些归约归约冲突看起来根本没什么依据。这在 info 中称为 神秘 (Mysterious) 冲突。比如 def: param_spec return_spec ','; param_spec: type | name_list ':' type ; return_spec: type | name ':' type ; type: \"id\"; name: \"id\"; name_list: name | name ',' name_list ; 这个文法是一个 LR(1) 文法，从 param_spec 开始，如果 id 后面是一个 ‘:’ 或 ‘,’ 它将被归约为 name，如果后面是一个 id 则被归约为 type。但是问题在于这不是一个 LALR(1) 文法，看起来 param_spec 和 return_spec 太像了，以至于 Bison 无法处理。 对于许多语法 (特别是非 LR(1) 语法)，LALR(1) 的局限性造成了各种问题，因此最直接的解决方法是构造不那么高效的规范 LR(1) 分析表或 IELR(1) 分析表。 信息 从 GNU Bison 3.1 开始 LR(LR,LALR,IELR) 分析表不再是实验性功能 如果只用 LALR(1) 解决这个问题，你可以添加一些东西，让这两个生成体不那么相同。以此来解决这个问题。 return_spec: type | name ':' type | \"id\" \"bogus\" /* This rule is never used. */ ; 很明显，yylex 不能解析出一个 bogus 的 token。在这个示例中，更好的解决方法是 return_spec: type | \"id\" ':' type ; 调整 LR构造 LR 解析器是因为历史原因的选择，现代化的 Bison 给出了一些更好的选择。 LR 表构建 只要修改一下下声明，就可以从历史遗留的 LALR(1) 分析表切换到 IELR 或 canonical LR 分析表。当然，主要是由于 LALR(1) 文法不够强大，不能完全解析 LR(1) 文法。有些需要复杂重构的 LALR 文法，或许在切换到 LR(1) 文法后，就可以完全消除冲突。 先说说怎么指定不同的分析表。 %define lr.type TYPE 可选值如下 lalr (default) ielr canonical-lr 当然每种 LR 分析表都有其特点，在龙书中也详细讲了 Canonic","date":"05-01","objectID":"/2022/flex_and_bison/:2:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#操作符优先级"},{"categories":["CompilerPrinciple"],"content":" Bison 解析器算法Bison 在读取 token 时，会将 token 及其语义值一起推送到名为 parser stack 的栈上，而这个行为被称为移入。但不会总是移入，当最后 N 个元素与语法规则相匹配时，元素将会组合，这个步骤称为归约。当解析器通过移入与归约，直到将整个输入串归约成单个分组时，我们将剩下的这个符号称作开始符号。而解析器的整个操作是自下而上的。 向前看符号就像之前的理论部分，解析器向前看一个符号，来确定下一个动作是什么。如果我们写下一个阶乘代码 expr: term '+' expr | term ; term: '(' expr ')' | term '!' | \"number\" ; 现在假设输入上的 1+2 已被读入并移入栈 如果后面是 ) 那么栈顶的三个元素将被归约为 expr，这是唯一有效的操作，因为移入之后没有规则继续 如果后面是 ! 那么会移入符号，以便 2! 可以归约成一项。如果在移入之前归约，那么将没有规则可以继续 可以通过 yychar 查看向前看符号。 移入归约冲突经典的悬空 else 问题 if_stmt: \"if\" expr \"then\" stmt | \"if\" expr \"then\" stmt \"else\" stmt ; 当 else 成为向前看符号时，移入规则有效，归约规则也有效，这就产生了一个移入规约冲突。但是 Bison 解析器更喜欢采用移入规则解决这种冲突。 悬空 else 问题往往的解决方式是，通常原则是 else 匹配最近的 if，那么下面这两行代码等价 if x then if y then win; else lose; if x then do; if y then win; else lose; end; 如果选用归约规则，与通常原则将大相径庭。就是下面这两行例子 if x then if y then win; else lose; if x then do; if y then win; end; else lose; 既然移入/归约冲突都是移入优先，那用之前介绍的 %expect N 可以吗？ 警告 不建议使用 %expect N (除 %expect 0)，即使移入归约冲突的数量正确，不代表发生错误的原因是预期的 操作符优先级算数表达式中也经常出现移入归约冲突，但这里移入不总是首选。优先级则是处理这类问题的一种解决方法。 想想这段代码， expr: expr '+' expr | expr '*' expr | expr '\u003c' expr | '(' expr ')' | ... ; 遇到 1+2*5 和 1+1+1 时，移入归约冲突，此时应该怎么选择，这就是优先级和结合性的作用。至于定义已经在声明运算符优先级中介绍过了。 当然也可以使用优先级去解决悬空 else 问题。比如说，token ELSE 的优先级总是高于 token THEN，这样在悬空 else 问题上，每次都优先移入 else 而非归约。 %precedence THEN %precedence ELSE 有个很奇怪的地方就是，往往优先级是上下文相关的。最直接的例子是一元 ‘-’ (负号) 与二元 ‘-’ (减号)，比如 C 语言的定义中，符号的优先级为 2 (越小越优先)，而乘号为 3，减号为 4。但是减号与负号的区别在于上下文的不同。另一个问题，Bison 中的优先级声明只能一 token 一次，这时就需要 %prec 修饰符在规则中进行修饰。 %prec TERMINAL-SYMBOL 首先在规则中声明这个上下文相关的符号为一个不存在的 (虚构的) token type，在声明部分声明这个 token type 的优先级。 ... %left '+' '-' %left '*' %right UMINUS %% expr: ... | expr '-' expr | ... | '-' expr %prec UMINUS | ... ; 归约归约冲突如果有多个规则可以应用咋同一个输入上，会发生归约归约冲突，通常这是严重的语法错误。比如下面这个示例 sequence: %empty { printf (\"empty sequence\\n\"); } | maybeword | sequence word { printf (\"added word %s\\n\", $2); } ; maybeword: %empty { printf (\"empty maybeword\\n\"); } | word { printf (\"single word %s\\n\", $1); } ; 比如现在栈顶是 word，word 可以被归约为 maybeword，也可以被归约为 sequence。Bison 会选择首先出现在语法中的规则进行归约，但这可能超出编码预期，因此尽量不要依赖这种方式，而是选择消除所有的归约归约冲突。比如将 sequence 修改为 sequence: %empty { printf (\"empty sequence\\n\"); } | sequence word { printf (\"added word %s\\n\", $2); } ; 当然有可能有其他方式产生归约归约冲突，比如下面这个例子。虽然每个规则独立看是没有问题的，但三个规则放在一起将产生错误：空输入可以被无限多种方式解析。 sequence: %empty | sequence words | sequence redirects ; words: %empty | words word ; redirects: %empty | redirects redirect ; 稍加修改，你会得到一个看起来不错的方法，空输入再也不会产生冲突了。 sequence: %empty | sequence words | sequence redirects ; words: word | words word ; redirects: redirect | redirects redirect ; 但是，如果输入为 “word word”，很明显可以被归约 words words 或 words，这有一个二义性的移入归约冲突，第二个 word 是移入还是将栈中的 word 归约。 可以用优先级解决这个问题 %precedence \"word\" %precedence \"sequence\" %% sequence: %empty | sequence word %prec \"sequence\" | sequence redirect %prec \"sequence\" ; words: word | words \"word\" ; 当然结合性也能解决这个问题 %right \"word\" \"redirect\" %% sequence: %empty | sequence word %prec \"word\" | sequence redirect %prec \"redirect\" ; 神秘冲突有些归约归约冲突看起来根本没什么依据。这在 info 中称为 神秘 (Mysterious) 冲突。比如 def: param_spec return_spec ','; param_spec: type | name_list ':' type ; return_spec: type | name ':' type ; type: \"id\"; name: \"id\"; name_list: name | name ',' name_list ; 这个文法是一个 LR(1) 文法，从 param_spec 开始，如果 id 后面是一个 ‘:’ 或 ‘,’ 它将被归约为 name，如果后面是一个 id 则被归约为 type。但是问题在于这不是一个 LALR(1) 文法，看起来 param_spec 和 return_spec 太像了，以至于 Bison 无法处理。 对于许多语法 (特别是非 LR(1) 语法)，LALR(1) 的局限性造成了各种问题，因此最直接的解决方法是构造不那么高效的规范 LR(1) 分析表或 IELR(1) 分析表。 信息 从 GNU Bison 3.1 开始 LR(LR,LALR,IELR) 分析表不再是实验性功能 如果只用 LALR(1) 解决这个问题，你可以添加一些东西，让这两个生成体不那么相同。以此来解决这个问题。 return_spec: type | name ':' type | \"id\" \"bogus\" /* This rule is never used. */ ; 很明显，yylex 不能解析出一个 bogus 的 token。在这个示例中，更好的解决方法是 return_spec: type | \"id\" ':' type ; 调整 LR构造 LR 解析器是因为历史原因的选择，现代化的 Bison 给出了一些更好的选择。 LR 表构建 只要修改一下下声明，就可以从历史遗留的 LALR(1) 分析表切换到 IELR 或 canonical LR 分析表。当然，主要是由于 LALR(1) 文法不够强大，不能完全解析 LR(1) 文法。有些需要复杂重构的 LALR 文法，或许在切换到 LR(1) 文法后，就可以完全消除冲突。 先说说怎么指定不同的分析表。 %define lr.type TYPE 可选值如下 lalr (default) ielr canonical-lr 当然每种 LR 分析表都有其特点，在龙书中也详细讲了 Canonic","date":"05-01","objectID":"/2022/flex_and_bison/:2:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#归约归约冲突"},{"categories":["CompilerPrinciple"],"content":" Bison 解析器算法Bison 在读取 token 时，会将 token 及其语义值一起推送到名为 parser stack 的栈上，而这个行为被称为移入。但不会总是移入，当最后 N 个元素与语法规则相匹配时，元素将会组合，这个步骤称为归约。当解析器通过移入与归约，直到将整个输入串归约成单个分组时，我们将剩下的这个符号称作开始符号。而解析器的整个操作是自下而上的。 向前看符号就像之前的理论部分，解析器向前看一个符号，来确定下一个动作是什么。如果我们写下一个阶乘代码 expr: term '+' expr | term ; term: '(' expr ')' | term '!' | \"number\" ; 现在假设输入上的 1+2 已被读入并移入栈 如果后面是 ) 那么栈顶的三个元素将被归约为 expr，这是唯一有效的操作，因为移入之后没有规则继续 如果后面是 ! 那么会移入符号，以便 2! 可以归约成一项。如果在移入之前归约，那么将没有规则可以继续 可以通过 yychar 查看向前看符号。 移入归约冲突经典的悬空 else 问题 if_stmt: \"if\" expr \"then\" stmt | \"if\" expr \"then\" stmt \"else\" stmt ; 当 else 成为向前看符号时，移入规则有效，归约规则也有效，这就产生了一个移入规约冲突。但是 Bison 解析器更喜欢采用移入规则解决这种冲突。 悬空 else 问题往往的解决方式是，通常原则是 else 匹配最近的 if，那么下面这两行代码等价 if x then if y then win; else lose; if x then do; if y then win; else lose; end; 如果选用归约规则，与通常原则将大相径庭。就是下面这两行例子 if x then if y then win; else lose; if x then do; if y then win; end; else lose; 既然移入/归约冲突都是移入优先，那用之前介绍的 %expect N 可以吗？ 警告 不建议使用 %expect N (除 %expect 0)，即使移入归约冲突的数量正确，不代表发生错误的原因是预期的 操作符优先级算数表达式中也经常出现移入归约冲突，但这里移入不总是首选。优先级则是处理这类问题的一种解决方法。 想想这段代码， expr: expr '+' expr | expr '*' expr | expr '\u003c' expr | '(' expr ')' | ... ; 遇到 1+2*5 和 1+1+1 时，移入归约冲突，此时应该怎么选择，这就是优先级和结合性的作用。至于定义已经在声明运算符优先级中介绍过了。 当然也可以使用优先级去解决悬空 else 问题。比如说，token ELSE 的优先级总是高于 token THEN，这样在悬空 else 问题上，每次都优先移入 else 而非归约。 %precedence THEN %precedence ELSE 有个很奇怪的地方就是，往往优先级是上下文相关的。最直接的例子是一元 ‘-’ (负号) 与二元 ‘-’ (减号)，比如 C 语言的定义中，符号的优先级为 2 (越小越优先)，而乘号为 3，减号为 4。但是减号与负号的区别在于上下文的不同。另一个问题，Bison 中的优先级声明只能一 token 一次，这时就需要 %prec 修饰符在规则中进行修饰。 %prec TERMINAL-SYMBOL 首先在规则中声明这个上下文相关的符号为一个不存在的 (虚构的) token type，在声明部分声明这个 token type 的优先级。 ... %left '+' '-' %left '*' %right UMINUS %% expr: ... | expr '-' expr | ... | '-' expr %prec UMINUS | ... ; 归约归约冲突如果有多个规则可以应用咋同一个输入上，会发生归约归约冲突，通常这是严重的语法错误。比如下面这个示例 sequence: %empty { printf (\"empty sequence\\n\"); } | maybeword | sequence word { printf (\"added word %s\\n\", $2); } ; maybeword: %empty { printf (\"empty maybeword\\n\"); } | word { printf (\"single word %s\\n\", $1); } ; 比如现在栈顶是 word，word 可以被归约为 maybeword，也可以被归约为 sequence。Bison 会选择首先出现在语法中的规则进行归约，但这可能超出编码预期，因此尽量不要依赖这种方式，而是选择消除所有的归约归约冲突。比如将 sequence 修改为 sequence: %empty { printf (\"empty sequence\\n\"); } | sequence word { printf (\"added word %s\\n\", $2); } ; 当然有可能有其他方式产生归约归约冲突，比如下面这个例子。虽然每个规则独立看是没有问题的，但三个规则放在一起将产生错误：空输入可以被无限多种方式解析。 sequence: %empty | sequence words | sequence redirects ; words: %empty | words word ; redirects: %empty | redirects redirect ; 稍加修改，你会得到一个看起来不错的方法，空输入再也不会产生冲突了。 sequence: %empty | sequence words | sequence redirects ; words: word | words word ; redirects: redirect | redirects redirect ; 但是，如果输入为 “word word”，很明显可以被归约 words words 或 words，这有一个二义性的移入归约冲突，第二个 word 是移入还是将栈中的 word 归约。 可以用优先级解决这个问题 %precedence \"word\" %precedence \"sequence\" %% sequence: %empty | sequence word %prec \"sequence\" | sequence redirect %prec \"sequence\" ; words: word | words \"word\" ; 当然结合性也能解决这个问题 %right \"word\" \"redirect\" %% sequence: %empty | sequence word %prec \"word\" | sequence redirect %prec \"redirect\" ; 神秘冲突有些归约归约冲突看起来根本没什么依据。这在 info 中称为 神秘 (Mysterious) 冲突。比如 def: param_spec return_spec ','; param_spec: type | name_list ':' type ; return_spec: type | name ':' type ; type: \"id\"; name: \"id\"; name_list: name | name ',' name_list ; 这个文法是一个 LR(1) 文法，从 param_spec 开始，如果 id 后面是一个 ‘:’ 或 ‘,’ 它将被归约为 name，如果后面是一个 id 则被归约为 type。但是问题在于这不是一个 LALR(1) 文法，看起来 param_spec 和 return_spec 太像了，以至于 Bison 无法处理。 对于许多语法 (特别是非 LR(1) 语法)，LALR(1) 的局限性造成了各种问题，因此最直接的解决方法是构造不那么高效的规范 LR(1) 分析表或 IELR(1) 分析表。 信息 从 GNU Bison 3.1 开始 LR(LR,LALR,IELR) 分析表不再是实验性功能 如果只用 LALR(1) 解决这个问题，你可以添加一些东西，让这两个生成体不那么相同。以此来解决这个问题。 return_spec: type | name ':' type | \"id\" \"bogus\" /* This rule is never used. */ ; 很明显，yylex 不能解析出一个 bogus 的 token。在这个示例中，更好的解决方法是 return_spec: type | \"id\" ':' type ; 调整 LR构造 LR 解析器是因为历史原因的选择，现代化的 Bison 给出了一些更好的选择。 LR 表构建 只要修改一下下声明，就可以从历史遗留的 LALR(1) 分析表切换到 IELR 或 canonical LR 分析表。当然，主要是由于 LALR(1) 文法不够强大，不能完全解析 LR(1) 文法。有些需要复杂重构的 LALR 文法，或许在切换到 LR(1) 文法后，就可以完全消除冲突。 先说说怎么指定不同的分析表。 %define lr.type TYPE 可选值如下 lalr (default) ielr canonical-lr 当然每种 LR 分析表都有其特点，在龙书中也详细讲了 Canonic","date":"05-01","objectID":"/2022/flex_and_bison/:2:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#神秘冲突"},{"categories":["CompilerPrinciple"],"content":" Bison 解析器算法Bison 在读取 token 时，会将 token 及其语义值一起推送到名为 parser stack 的栈上，而这个行为被称为移入。但不会总是移入，当最后 N 个元素与语法规则相匹配时，元素将会组合，这个步骤称为归约。当解析器通过移入与归约，直到将整个输入串归约成单个分组时，我们将剩下的这个符号称作开始符号。而解析器的整个操作是自下而上的。 向前看符号就像之前的理论部分，解析器向前看一个符号，来确定下一个动作是什么。如果我们写下一个阶乘代码 expr: term '+' expr | term ; term: '(' expr ')' | term '!' | \"number\" ; 现在假设输入上的 1+2 已被读入并移入栈 如果后面是 ) 那么栈顶的三个元素将被归约为 expr，这是唯一有效的操作，因为移入之后没有规则继续 如果后面是 ! 那么会移入符号，以便 2! 可以归约成一项。如果在移入之前归约，那么将没有规则可以继续 可以通过 yychar 查看向前看符号。 移入归约冲突经典的悬空 else 问题 if_stmt: \"if\" expr \"then\" stmt | \"if\" expr \"then\" stmt \"else\" stmt ; 当 else 成为向前看符号时，移入规则有效，归约规则也有效，这就产生了一个移入规约冲突。但是 Bison 解析器更喜欢采用移入规则解决这种冲突。 悬空 else 问题往往的解决方式是，通常原则是 else 匹配最近的 if，那么下面这两行代码等价 if x then if y then win; else lose; if x then do; if y then win; else lose; end; 如果选用归约规则，与通常原则将大相径庭。就是下面这两行例子 if x then if y then win; else lose; if x then do; if y then win; end; else lose; 既然移入/归约冲突都是移入优先，那用之前介绍的 %expect N 可以吗？ 警告 不建议使用 %expect N (除 %expect 0)，即使移入归约冲突的数量正确，不代表发生错误的原因是预期的 操作符优先级算数表达式中也经常出现移入归约冲突，但这里移入不总是首选。优先级则是处理这类问题的一种解决方法。 想想这段代码， expr: expr '+' expr | expr '*' expr | expr '\u003c' expr | '(' expr ')' | ... ; 遇到 1+2*5 和 1+1+1 时，移入归约冲突，此时应该怎么选择，这就是优先级和结合性的作用。至于定义已经在声明运算符优先级中介绍过了。 当然也可以使用优先级去解决悬空 else 问题。比如说，token ELSE 的优先级总是高于 token THEN，这样在悬空 else 问题上，每次都优先移入 else 而非归约。 %precedence THEN %precedence ELSE 有个很奇怪的地方就是，往往优先级是上下文相关的。最直接的例子是一元 ‘-’ (负号) 与二元 ‘-’ (减号)，比如 C 语言的定义中，符号的优先级为 2 (越小越优先)，而乘号为 3，减号为 4。但是减号与负号的区别在于上下文的不同。另一个问题，Bison 中的优先级声明只能一 token 一次，这时就需要 %prec 修饰符在规则中进行修饰。 %prec TERMINAL-SYMBOL 首先在规则中声明这个上下文相关的符号为一个不存在的 (虚构的) token type，在声明部分声明这个 token type 的优先级。 ... %left '+' '-' %left '*' %right UMINUS %% expr: ... | expr '-' expr | ... | '-' expr %prec UMINUS | ... ; 归约归约冲突如果有多个规则可以应用咋同一个输入上，会发生归约归约冲突，通常这是严重的语法错误。比如下面这个示例 sequence: %empty { printf (\"empty sequence\\n\"); } | maybeword | sequence word { printf (\"added word %s\\n\", $2); } ; maybeword: %empty { printf (\"empty maybeword\\n\"); } | word { printf (\"single word %s\\n\", $1); } ; 比如现在栈顶是 word，word 可以被归约为 maybeword，也可以被归约为 sequence。Bison 会选择首先出现在语法中的规则进行归约，但这可能超出编码预期，因此尽量不要依赖这种方式，而是选择消除所有的归约归约冲突。比如将 sequence 修改为 sequence: %empty { printf (\"empty sequence\\n\"); } | sequence word { printf (\"added word %s\\n\", $2); } ; 当然有可能有其他方式产生归约归约冲突，比如下面这个例子。虽然每个规则独立看是没有问题的，但三个规则放在一起将产生错误：空输入可以被无限多种方式解析。 sequence: %empty | sequence words | sequence redirects ; words: %empty | words word ; redirects: %empty | redirects redirect ; 稍加修改，你会得到一个看起来不错的方法，空输入再也不会产生冲突了。 sequence: %empty | sequence words | sequence redirects ; words: word | words word ; redirects: redirect | redirects redirect ; 但是，如果输入为 “word word”，很明显可以被归约 words words 或 words，这有一个二义性的移入归约冲突，第二个 word 是移入还是将栈中的 word 归约。 可以用优先级解决这个问题 %precedence \"word\" %precedence \"sequence\" %% sequence: %empty | sequence word %prec \"sequence\" | sequence redirect %prec \"sequence\" ; words: word | words \"word\" ; 当然结合性也能解决这个问题 %right \"word\" \"redirect\" %% sequence: %empty | sequence word %prec \"word\" | sequence redirect %prec \"redirect\" ; 神秘冲突有些归约归约冲突看起来根本没什么依据。这在 info 中称为 神秘 (Mysterious) 冲突。比如 def: param_spec return_spec ','; param_spec: type | name_list ':' type ; return_spec: type | name ':' type ; type: \"id\"; name: \"id\"; name_list: name | name ',' name_list ; 这个文法是一个 LR(1) 文法，从 param_spec 开始，如果 id 后面是一个 ‘:’ 或 ‘,’ 它将被归约为 name，如果后面是一个 id 则被归约为 type。但是问题在于这不是一个 LALR(1) 文法，看起来 param_spec 和 return_spec 太像了，以至于 Bison 无法处理。 对于许多语法 (特别是非 LR(1) 语法)，LALR(1) 的局限性造成了各种问题，因此最直接的解决方法是构造不那么高效的规范 LR(1) 分析表或 IELR(1) 分析表。 信息 从 GNU Bison 3.1 开始 LR(LR,LALR,IELR) 分析表不再是实验性功能 如果只用 LALR(1) 解决这个问题，你可以添加一些东西，让这两个生成体不那么相同。以此来解决这个问题。 return_spec: type | name ':' type | \"id\" \"bogus\" /* This rule is never used. */ ; 很明显，yylex 不能解析出一个 bogus 的 token。在这个示例中，更好的解决方法是 return_spec: type | \"id\" ':' type ; 调整 LR构造 LR 解析器是因为历史原因的选择，现代化的 Bison 给出了一些更好的选择。 LR 表构建 只要修改一下下声明，就可以从历史遗留的 LALR(1) 分析表切换到 IELR 或 canonical LR 分析表。当然，主要是由于 LALR(1) 文法不够强大，不能完全解析 LR(1) 文法。有些需要复杂重构的 LALR 文法，或许在切换到 LR(1) 文法后，就可以完全消除冲突。 先说说怎么指定不同的分析表。 %define lr.type TYPE 可选值如下 lalr (default) ielr canonical-lr 当然每种 LR 分析表都有其特点，在龙书中也详细讲了 Canonic","date":"05-01","objectID":"/2022/flex_and_bison/:2:4","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#调整-lr"},{"categories":["CompilerPrinciple"],"content":" main 函数示例 信息 本节学习自 北大编译实践 sysy 采用扩展 Backus 范式 (EBNF)，CompUnit 作为开始符号。当然除了函数定义还有变量声明，但是现在我们跟着北大编译实践来学习，目前只处理 main 函数 CompUnit ::= FuncDef; FuncDef ::= FuncType Ident ‘(’ ‘)’ Block; FuncType ::= int; Block ::= ‘{’ Stmt ‘}’; Stmt ::= return Number ‘;’ ; Number ::= INT_CONST; 根据这个我们先写一个 flex 词法分析文件 sysy.l %option noyywrap nodefault noinput nounput noyy_top_state stack %top{ #include \"sysy.tab.hh\" #include \u003cstdlib.h\u003e #include \u003cstring\u003e } WhiteSpace [[:space:]]* LineComment \"//\".*$ Identifier [[:alpha:]_][[:alnum:]_]* Decimal [1-9][[:digit:]]* Octal 0[0-7]* Hexadecimal 0[xX][[:xdigit:]]+ Binarydecimal 0[bB][01]+ %x COMMENT %% {LineComment} { /* ignore */ } \"/*\" { yy_push_state(COMMENT); } \u003cCOMMENT\u003e.|\\n { /* ignore */ } \u003cCOMMENT\u003e\"*/\" { yy_pop_state(); } {WhiteSpace} { /* ignore */ } \"int\" { return INT; } \"return\" { return RETURN; } {Identifier} { yylval.ident = new ::std::string(yytext); return IDENTIFIER; } {Decimal} { yylval.int_val = strtol(yytext, nullptr, 10); return INT_CONST; } {Octal} { yylval.int_val = strtol(yytext, nullptr, 8); return INT_CONST; } {Hexadecimal} { yylval.int_val = strtol(yytext, nullptr, 16); return INT_CONST; } {Binarydecimal} { yylval.int_val = strtol(yytext + 2, nullptr, 2); return INT_CONST; } . { return yytext[0]; } %% 简单解释一下，词法分析器将忽略所有的行注释、块注释以及空白符，并且会将得到的标识符名称存入 idnet 中，解析到的整型变量存入 int_val 中。 既然有了词法分析器，我们还需要在在语法分析器中描述 sysy 的语法，即 sysy.y %code requires { #include \u003cmemory\u003e #include \u003cstring\u003e } %code { #include \u003ciostream\u003e extern int yylex(void); void yyerror(::std::unique_ptr\u003c::std::string const\u003e \u0026ast, char const *s); } %parse-param { ::std::unique_ptr\u003c::std::string const\u003e \u0026ast } %union { ::std::string const *ident; int int_val; } %token INT RETURN %token \u003cident\u003e IDENTIFIER %token \u003cint_val\u003e INT_CONST %type \u003cident\u003e FuncDef FuncType Block Stmt Number CompUnit %% CompUnit: FuncDef { ast = ::std::unique_ptr\u003c::std::string const\u003e($1); } ; FuncDef: FuncType IDENTIFIER '(' ')' Block { auto type = ::std::unique_ptr\u003c::std::string const\u003e($1); auto ident = ::std::unique_ptr\u003c::std::string const\u003e($2); auto block = ::std::unique_ptr\u003c::std::string const\u003e($5); $$ = new ::std::string(*type + \" \" + *ident + \"()\" + *block); } ; FuncType: INT { $$ = new ::std::string(\"int\"); } ; Block: '{' Stmt '}' { auto stmt = ::std::unique_ptr\u003c::std::string const\u003e($2); $$ = new ::std::string(\"{ \" + *stmt + \" }\"); } ; Stmt: RETURN Number ';' { auto number = ::std::unique_ptr\u003c::std::string const\u003e($2); $$ = new ::std::string(\"return \" + *number + \";\"); } ; Number: INT_CONST { $$ = new ::std::string(::std::to_string($1)); } ; %% void yyerror(::std::unique_ptr\u003c::std::string const\u003e \u0026ast, const char *s) { ::std::cerr \u003c\u003c \"error: \" \u003c\u003c s \u003c\u003c ::std::endl; } 很明显语法分析器中，这个语法分析可以很好得解析我们在本节开始声明的 EBNF 文法，也就是 main 函数。需要注意的是，这里不能使用 ::std::make_unique，因为其只能移动或者从参数构建智能指针，但我们的非终结符语义值都是用 new 构建好的指针 (非 POD 类型不能作为 union 成员)，因此我们需要用构造函数来构造 ::std::unique_ptr，将之前构造的指针所有权移交给智能指针。 虽然词法分析与语法分析出来了，我们还要自己编写一个主文件 #include \u003cstdio.h\u003e #include \u003ciostream\u003e #include \u003cmemory\u003e #include \u003cstring\u003e extern FILE *yyin; extern int yylex_destroy(void); extern int yyparse(::std::unique_ptr\u003c::std::string\u003e \u0026ast); int main(int /* argc */, char const *argv[]) { auto input = argv[1]; yyin = fopen(input, \"r\"); ::std::unique_ptr\u003c::std::string\u003e ast; static_cast\u003cvoid\u003e(yyparse(ast)); ::std::cout \u003c\u003c *ast \u003c\u003c ::std::endl; fclose(yyin); yylex_destroy(); } 主文件需要注意，在最后关闭 yyin 是必要操作，不然会造成资源泄漏 (虽然程序立即结束，操作系统会回收资源)。还有就是 yylex_destroy 必须在 fclose 之后关闭，否则会造成 crash。如果有兴趣可以自己写一个 deferer，然后做 RAII，保证资源的正确释放。 最后，写一个 CMake 就大功告成 cmake_minimum_required(VERSION 3.5) project(sysyc LANGUAGES CXX) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) if(NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE \"Debug\") set(CMAKE_EXPORT_COMPILE_COMMANDS ON) endif() find_package(FLEX REQUIRED) find_package(BISON REQUIRED) file(GLOB_RECURSE L_SOURCES \"src/*.l\") file(GLOB_RECURSE Y_SOURCES \"src/*.y\") if(NOT (L_SOURCES STREQUAL \"\" AND Y_SOURCES STREQUAL \"\")) string(REGEX REPLACE \".*/(.*)\\\\.l\" \"${CMAKE_CUR","date":"05-01","objectID":"/2022/flex_and_bison/:3:0","series":null,"tags":["Applications","SyntacticAnalysis","LexicalAnalysis"],"title":"词法分析软件 Flex 及语法分析软件 Bison 的用法","uri":"/2022/flex_and_bison/#main-函数示例"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第四章 4.5 4.6 4.7 读书笔记","date":"04-20","objectID":"/2022/compilerprinciple_006/","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/"},{"categories":["CompilerPrinciple"],"content":"一个自底向上的语法分析过程对应于为输入字符串构造语法分析树的过程，它从叶节点开始开始逐渐向上构造。虽然大部分编译器前端不会显示构造语法分析树，而是直接翻译，但自底向上构建有些像构建语法分析树。 移入归约语法分析是自底向上语法分析的通用框架。LR 文法就是采用移入-归约语法分析的文法。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:0:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#"},{"categories":["CompilerPrinciple"],"content":" 移入-归约","date":"04-20","objectID":"/2022/compilerprinciple_006/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#移入-归约"},{"categories":["CompilerPrinciple"],"content":" 归约将语法分析过程，看作输入串 w 归约 (reduction) 为文法开始符号的过程， 在归约步骤中，一个与某产生式体相匹配的特定子串被替换为该产生式头部的非终结符。 自底向上语法分析过程中，最关键的是何时进行归约，以及应用哪个产生式进行归约。 当然归约是推导步骤的反向操作，不过可以是 最右 推导。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:1:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#归约"},{"categories":["CompilerPrinciple"],"content":" 句柄剪枝对输入进行从左向右扫描，并在扫描过程中进行自底向上语法分析，就可以反向构造出最右推导。简单地说，句柄 是和某个产生式体匹配的子串，对它的归约代表了相应最右推导中的一个反向步骤。 如果有 \\(\\textit{S}\\xRightarrow[rm]{*}\\alpha\\textit{A}w\\xRightarrow[rm]{}\\alpha\\beta{}w\\)，那么紧跟 \\(\\alpha\\) 的产生式 \\(\\textit{A}\\rightarrow\\beta\\) 是 \\(\\alpha\\beta{}w\\) 的一个 句柄 (handle)。换句话说，最右句型 \\(\\gamma\\) 的一个句柄是满足以下条件的产生式 \\(\\textit{A}\\rightarrow\\beta\\) 及串 \\(\\beta\\) 在 \\(\\gamma\\) 中出现的位置：将这个位置上的 \\(\\beta\\) 替换为 A 之后得到的串是 \\(\\gamma\\) 的某个最右推导序列中出现在位于 \\(\\gamma\\) 之前的最右句型。 句柄右边的串 w 一定只包含终结符，即产生式体 \\(\\beta\\) 称为一个句柄 (而不是 \\(\\textit{A}\\rightarrow\\beta\\))，如果文法有二义性时可能存在多个最右推导，但无二义性的文法有且仅有一个句柄。通过句柄剪枝可以得到一个反向的最右推导。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:1:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#句柄剪枝"},{"categories":["CompilerPrinciple"],"content":" 移入-规约语法分析技术该语法分析使用栈来保存符号，并用一个输入缓冲区来存放将要进行语法分析的其余符号。句柄在被识别之前，总是出现在栈顶的。 在栈中依然用 \\(\\$\\) 标记栈底位置，在从左到右扫描输入串时，语法分析器将零个或多个输入符号移动到栈顶，直到对栈顶的一个文法符号串 \\(\\beta\\) 进行归约为止。语法分析器将不断重复这个过程，直到检测到错误，或栈中包含了开始符号且输入缓冲区为空为止。此时宣告语法分析完成。 语法分析器主要由四个动作构成 移入 (shift) 将下一个输入符号移到栈顶 归约 (reduce) 被归约的符号串的右端必然是栈顶，语法分析器在栈中确定这个栈的左端，并决定用哪个非终结符来替换这个串 接受 (accept) 语法分析完成 报错 (error) 发现一个语法错误，调用错误恢复过程 使用栈主要是因为在语法分析过程中有个重要的性质：句柄总出现在栈顶，绝不会出现在栈中。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:1:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#移入-规约语法分析技术"},{"categories":["CompilerPrinciple"],"content":" 移入-归约语法分析中的冲突某些上下文无关文法无法使用移入-归约语法分析技术，对于这样的文法可能出现如下 configuration：虽然知道栈中的所有内容以及接下来的 k 个输入符号， 移入/归约冲突 无法判断应该进行移动还是归约 归约/归约冲突 无法在多个可能的归约方法中原则正确的归约 简单的来看一个有关过程调用和数组引用的文法 \\[ \\begin{align} stmt\u0026\\rightarrow\\textbf{id}\\ \\texttt{(}\\ parameter\\_list\\ \\texttt{)}\\\\ stmt\u0026\\rightarrow expr\\ \\texttt{::=}\\ expr\\\\ parameter\\_list\u0026\\rightarrow parameter\\_list \\ ,\\ parameter\\\\ parameter\\_list\u0026\\rightarrow parameter\\\\ parameter\u0026\\rightarrow \\textbf{id}\\\\ expr\u0026\\rightarrow \\textbf{id} \\ \\texttt{(}\\ expr\\_list \\ \\texttt{)}\\\\ expr\u0026\\rightarrow \\textbf{id}\\\\ expr\\_list\u0026\\rightarrow expr\\_list \\ ,\\ expr\\\\ expr\\_list\u0026\\rightarrow expr \\end{align}\\] 对一个以 \\(p(i, j)\\) 开头的语句以词法单元流 \\(\\textbf{id}(\\textbf{id}, \\textbf{id})\\) 的方式输入词法分析器。在处于如下 configuration 时， 栈 \\(\\cdots \\textbf{id} ( \\textbf{id}\\)，输入 \\(, \\textbf{id} )\\cdots\\) 此时应该归约栈顶的 id，但选用哪个产生式呢？如果： p 是一个过程，那么正确的选择是产生式 5 p 是一个数组，那么正确的选择是产生式 7 这将产生归约/归约冲突。 这必须在 p 的声明中来确定符号表中的信息。相对简单地方法是，将产生式 1 中的词法单元 id 改为 procid，使用更加复杂的词法分析器，在识别到过程名字的词素时返回词法单元名 procid。 可以发现，移入-归约语法分析技术可以使用栈中离栈顶较远的信息来引导语法分析过程。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:1:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#移入-归约语法分析中的冲突"},{"categories":["CompilerPrinciple"],"content":" 简单 LR 技术与 LL(1) 技术类似，LR(k) 技术即从左向右扫描的最右推导过程，语法分析决定最多向前看 k 个字符。 最简单的移入-规约语法分析方法被称为 SLR (简单 LR 技术)。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#简单-lr-技术"},{"categories":["CompilerPrinciple"],"content":" 为什么使用 LR 语法分析器LR 分析器是表格驱动的，与迭代 LL 语法分析器类似。并且只要存在从左到右的移入-归约语法分析器，它总能在某文法的最右句型的句柄出现在栈顶时识别出句柄，那么这个文法是 LR 的。 LR 语法分析技术的吸引力如下： 对几乎所有的程序设计语言都糟，只要能写出该构造的上下文无关文法，就能构造出识别该构造的 LR 语法分析器。确实存在非 LR 的上下文无关文法，但一般来说构造都可以避免这样的文法。 LR 语法分析方法是已知最通用的无回溯移入-归约方法，并且实现可以和其他更原始的移入-归约方法一样高效。 一个 LR 语法分析器可以在对输入进行从左到右扫描时，尽可能早地检测到错误。 可以使用 LR 方法进行语法分析的文法类是可以使用预测方法或 LL 方法进行语法分析的文法类的真超集。因此 LR 文法能够比 LL 文法描述更多的程序设计语言。 LR(k) 文法是，当在一个最右句型中看到某个产生式的右部时，再向前看 k 个符号就可以决定是否使用这个产生式进行归约 LL(k) 文法是，决定是否使用某个产生式时，只能向前看该产生式右部推导出的串的前 k 个符号 LR 方法的主要缺点是，为一个典型的程序设计语言文法手工构造 LR 分析器的工作量非常大。因此有很多通用的 LR 语法分析生成器的工具诞生，简化了 LR 分析器构造的工作量。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#为什么使用-lr-语法分析器"},{"categories":["CompilerPrinciple"],"content":" 项和 LR(0) 自动机那么一个语法分析器怎么知道何时移入、何时归约的呢？ 一个 IR 语法分析器通过维护一些状态，用这些状态来表明我们在语法分析过程中所处的位置，从而做出移入-归约决定。这些状态代表了项 (item) 的集合。一个文法 G 的一个 LR (0) 项是 G 的一个产生式再加上一个位于它的体中某处的点。因此，产生式 \\(\\textit{A}\\rightarrow\\textit{XYZ}\\) 产生了四个项： \\[\\begin{aligned} \\textit{A} \u0026\\rightarrow \\cdot\\textit{XYZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{X}\\cdot\\textit{YZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XY}\\cdot\\textit{Z}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XYZ}\\cdot \\end{aligned}\\] 产生式 \\(\\textit{A}\\rightarrow\\varepsilon\\) 只生成一个项 \\(\\textit{A}\\rightarrow\\cdot\\) 。 一个称为规范 LR(0) 项集族的一组项集提供了构建一个确定有穷自动机的基础，该自动机用于做出语法分析决定，这样的自动机被称为 LR(0) 自动机。这个自动机的每个状态代表了规范 LR(0) 项集族中的一个项集。 为了构造一个文法的规范 LR(0) 项集族，我们定义了一个增广文法和两个函数 CLOSURE 和 GOTO。如果 G 是以 S 为开始符号的文法，那么 G 的增广文法 \\(\\textit{G}^{’}\\) 就是在 G 中加上新的开始符号 \\(\\textit{S}^{’}\\) 和产生式 \\(\\textit{S}^{’}\\rightarrow\\textit{S}\\) 的文法。引入新的产生式的目的是告诉文法分析器何时应该停止语法分析并宣称接受输入符号串。 对于之前反复提到的示例 id + id * id，可以构造出如下自动机。 项集的闭包如果 I 是文法 G 的一个项集，那么 \\(\\texttt{CLOSURE}(\\textit{I})\\) 就是根据以下两个规则从 I 构造得到的： 将 I 中的各个项加入到 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中 如果 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\) 在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中， \\(\\textit{B}\\rightarrow\\gamma\\) 是一个产生式，并且项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中，就将这个项加入其中。不断应用这个规则，直到没有新项可以加入 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中为止。 closure 可以按照以下方式计算。函数 closure 可以添加一个 added 数组来方便实现，该数组下标是 G 的非终结符，当各个 B 的产生式 \\(\\textit{B}\\rightarrow\\gamma\\) 加入对应的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 时，added[B] 被设置为 true。 SetOfItems CLOSURE(I) { J = I; repeat for (J 中的每个项 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\)) for (G 的每个产生式 \\(\\textit{B}\\rightarrow\\gamma\\)) if (项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 G 中) 将 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 添加到 J 中; until 没有可以被加入到 J 中的项; return J; } 如果点在最左端的产生式 B 被加入 I 中，那么所有 B 的产生式都会被加入 I 的闭包中。因此在某些情况下，不需要真的将那些被 \\(\\texttt{CLOSURE}\\) 函数加入到 I 中的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 列出来，只需要列出这些被加入的产生式的左部非终结符就行。可以将各项分为两类： 内核项 包括初始化 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 以及点不在最左端的所有项 非内核项 除了 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 之外的所有点在最左端的所有项 感兴趣的项集是某个内核项集合的闭包，求闭包加入的项必然是非内核项。如果我们抛弃所有非内核项，就可以用很少的内存来表示真正感兴趣的项的集合，因为我们已知这些非内核项可以通过闭包运算重新生成。即之前构造出自动机的阴影部分。 GOTO 函数GOTO(I,X)，其中 I 是一个项集而 X 是一个文法符号。GOTO(I,X) 被定义为 I 中所有形如 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\textit{X}\\beta]\\) 的项所对应的项 \\([\\textit{A}\\rightarrow\\alpha\\textit{X}\\cdot\\beta]\\) 的集合的闭包。简单地说，就是自动机的状态转换。示例自动机，\\(\\texttt{GOTO}(\\textit{I}_{1}, +)\\) 的结果为项集 \\(\\textit{I}_{6}\\)。 现在我们可以构造出增广文法 \\(\\textit{G}^{’}\\) 的规范 LR(0) 项集族 C 的算法。 void items(\\(\\textit{G}^{’}\\)) { C = {\\(\\texttt{CLOSURE}\\)(\\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}]\\))}; repeat for (C 中的每个项集 I) for (每个文法符号 X) if (\\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 非空且不在 C 中) 将 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 加入 C 中; until 在某轮中没有新的项集加入到 C 中; } LR(0) 自动机的用法SLR 的中心思想是根据文法构造出 LR(0) 自动机。这个自动机的状态是规范 LR(0) 项集族中的元素，而它的转换由 \\(\\texttt{GOTO}\\) 函数给出。 状态 j 是指对应的与项集 \\(\\textit{I}_{j}\\) 的状态。LR(0) 自动机从开始状态 0 运行到某个状态 j，如果下一个输入符号为 a 且状态 j 有一个 a 上的转换，那么就移入 a，否则就进行归约。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#项和-lr--0--自动机"},{"categories":["CompilerPrinciple"],"content":" 项和 LR(0) 自动机那么一个语法分析器怎么知道何时移入、何时归约的呢？ 一个 IR 语法分析器通过维护一些状态，用这些状态来表明我们在语法分析过程中所处的位置，从而做出移入-归约决定。这些状态代表了项 (item) 的集合。一个文法 G 的一个 LR (0) 项是 G 的一个产生式再加上一个位于它的体中某处的点。因此，产生式 \\(\\textit{A}\\rightarrow\\textit{XYZ}\\) 产生了四个项： \\[\\begin{aligned} \\textit{A} \u0026\\rightarrow \\cdot\\textit{XYZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{X}\\cdot\\textit{YZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XY}\\cdot\\textit{Z}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XYZ}\\cdot \\end{aligned}\\] 产生式 \\(\\textit{A}\\rightarrow\\varepsilon\\) 只生成一个项 \\(\\textit{A}\\rightarrow\\cdot\\) 。 一个称为规范 LR(0) 项集族的一组项集提供了构建一个确定有穷自动机的基础，该自动机用于做出语法分析决定，这样的自动机被称为 LR(0) 自动机。这个自动机的每个状态代表了规范 LR(0) 项集族中的一个项集。 为了构造一个文法的规范 LR(0) 项集族，我们定义了一个增广文法和两个函数 CLOSURE 和 GOTO。如果 G 是以 S 为开始符号的文法，那么 G 的增广文法 \\(\\textit{G}^{’}\\) 就是在 G 中加上新的开始符号 \\(\\textit{S}^{’}\\) 和产生式 \\(\\textit{S}^{’}\\rightarrow\\textit{S}\\) 的文法。引入新的产生式的目的是告诉文法分析器何时应该停止语法分析并宣称接受输入符号串。 对于之前反复提到的示例 id + id * id，可以构造出如下自动机。 项集的闭包如果 I 是文法 G 的一个项集，那么 \\(\\texttt{CLOSURE}(\\textit{I})\\) 就是根据以下两个规则从 I 构造得到的： 将 I 中的各个项加入到 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中 如果 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\) 在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中， \\(\\textit{B}\\rightarrow\\gamma\\) 是一个产生式，并且项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中，就将这个项加入其中。不断应用这个规则，直到没有新项可以加入 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中为止。 closure 可以按照以下方式计算。函数 closure 可以添加一个 added 数组来方便实现，该数组下标是 G 的非终结符，当各个 B 的产生式 \\(\\textit{B}\\rightarrow\\gamma\\) 加入对应的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 时，added[B] 被设置为 true。 SetOfItems CLOSURE(I) { J = I; repeat for (J 中的每个项 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\)) for (G 的每个产生式 \\(\\textit{B}\\rightarrow\\gamma\\)) if (项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 G 中) 将 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 添加到 J 中; until 没有可以被加入到 J 中的项; return J; } 如果点在最左端的产生式 B 被加入 I 中，那么所有 B 的产生式都会被加入 I 的闭包中。因此在某些情况下，不需要真的将那些被 \\(\\texttt{CLOSURE}\\) 函数加入到 I 中的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 列出来，只需要列出这些被加入的产生式的左部非终结符就行。可以将各项分为两类： 内核项 包括初始化 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 以及点不在最左端的所有项 非内核项 除了 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 之外的所有点在最左端的所有项 感兴趣的项集是某个内核项集合的闭包，求闭包加入的项必然是非内核项。如果我们抛弃所有非内核项，就可以用很少的内存来表示真正感兴趣的项的集合，因为我们已知这些非内核项可以通过闭包运算重新生成。即之前构造出自动机的阴影部分。 GOTO 函数GOTO(I,X)，其中 I 是一个项集而 X 是一个文法符号。GOTO(I,X) 被定义为 I 中所有形如 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\textit{X}\\beta]\\) 的项所对应的项 \\([\\textit{A}\\rightarrow\\alpha\\textit{X}\\cdot\\beta]\\) 的集合的闭包。简单地说，就是自动机的状态转换。示例自动机，\\(\\texttt{GOTO}(\\textit{I}_{1}, +)\\) 的结果为项集 \\(\\textit{I}_{6}\\)。 现在我们可以构造出增广文法 \\(\\textit{G}^{’}\\) 的规范 LR(0) 项集族 C 的算法。 void items(\\(\\textit{G}^{’}\\)) { C = {\\(\\texttt{CLOSURE}\\)(\\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}]\\))}; repeat for (C 中的每个项集 I) for (每个文法符号 X) if (\\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 非空且不在 C 中) 将 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 加入 C 中; until 在某轮中没有新的项集加入到 C 中; } LR(0) 自动机的用法SLR 的中心思想是根据文法构造出 LR(0) 自动机。这个自动机的状态是规范 LR(0) 项集族中的元素，而它的转换由 \\(\\texttt{GOTO}\\) 函数给出。 状态 j 是指对应的与项集 \\(\\textit{I}_{j}\\) 的状态。LR(0) 自动机从开始状态 0 运行到某个状态 j，如果下一个输入符号为 a 且状态 j 有一个 a 上的转换，那么就移入 a，否则就进行归约。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#项集的闭包"},{"categories":["CompilerPrinciple"],"content":" 项和 LR(0) 自动机那么一个语法分析器怎么知道何时移入、何时归约的呢？ 一个 IR 语法分析器通过维护一些状态，用这些状态来表明我们在语法分析过程中所处的位置，从而做出移入-归约决定。这些状态代表了项 (item) 的集合。一个文法 G 的一个 LR (0) 项是 G 的一个产生式再加上一个位于它的体中某处的点。因此，产生式 \\(\\textit{A}\\rightarrow\\textit{XYZ}\\) 产生了四个项： \\[\\begin{aligned} \\textit{A} \u0026\\rightarrow \\cdot\\textit{XYZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{X}\\cdot\\textit{YZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XY}\\cdot\\textit{Z}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XYZ}\\cdot \\end{aligned}\\] 产生式 \\(\\textit{A}\\rightarrow\\varepsilon\\) 只生成一个项 \\(\\textit{A}\\rightarrow\\cdot\\) 。 一个称为规范 LR(0) 项集族的一组项集提供了构建一个确定有穷自动机的基础，该自动机用于做出语法分析决定，这样的自动机被称为 LR(0) 自动机。这个自动机的每个状态代表了规范 LR(0) 项集族中的一个项集。 为了构造一个文法的规范 LR(0) 项集族，我们定义了一个增广文法和两个函数 CLOSURE 和 GOTO。如果 G 是以 S 为开始符号的文法，那么 G 的增广文法 \\(\\textit{G}^{’}\\) 就是在 G 中加上新的开始符号 \\(\\textit{S}^{’}\\) 和产生式 \\(\\textit{S}^{’}\\rightarrow\\textit{S}\\) 的文法。引入新的产生式的目的是告诉文法分析器何时应该停止语法分析并宣称接受输入符号串。 对于之前反复提到的示例 id + id * id，可以构造出如下自动机。 项集的闭包如果 I 是文法 G 的一个项集，那么 \\(\\texttt{CLOSURE}(\\textit{I})\\) 就是根据以下两个规则从 I 构造得到的： 将 I 中的各个项加入到 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中 如果 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\) 在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中， \\(\\textit{B}\\rightarrow\\gamma\\) 是一个产生式，并且项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中，就将这个项加入其中。不断应用这个规则，直到没有新项可以加入 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中为止。 closure 可以按照以下方式计算。函数 closure 可以添加一个 added 数组来方便实现，该数组下标是 G 的非终结符，当各个 B 的产生式 \\(\\textit{B}\\rightarrow\\gamma\\) 加入对应的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 时，added[B] 被设置为 true。 SetOfItems CLOSURE(I) { J = I; repeat for (J 中的每个项 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\)) for (G 的每个产生式 \\(\\textit{B}\\rightarrow\\gamma\\)) if (项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 G 中) 将 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 添加到 J 中; until 没有可以被加入到 J 中的项; return J; } 如果点在最左端的产生式 B 被加入 I 中，那么所有 B 的产生式都会被加入 I 的闭包中。因此在某些情况下，不需要真的将那些被 \\(\\texttt{CLOSURE}\\) 函数加入到 I 中的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 列出来，只需要列出这些被加入的产生式的左部非终结符就行。可以将各项分为两类： 内核项 包括初始化 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 以及点不在最左端的所有项 非内核项 除了 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 之外的所有点在最左端的所有项 感兴趣的项集是某个内核项集合的闭包，求闭包加入的项必然是非内核项。如果我们抛弃所有非内核项，就可以用很少的内存来表示真正感兴趣的项的集合，因为我们已知这些非内核项可以通过闭包运算重新生成。即之前构造出自动机的阴影部分。 GOTO 函数GOTO(I,X)，其中 I 是一个项集而 X 是一个文法符号。GOTO(I,X) 被定义为 I 中所有形如 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\textit{X}\\beta]\\) 的项所对应的项 \\([\\textit{A}\\rightarrow\\alpha\\textit{X}\\cdot\\beta]\\) 的集合的闭包。简单地说，就是自动机的状态转换。示例自动机，\\(\\texttt{GOTO}(\\textit{I}_{1}, +)\\) 的结果为项集 \\(\\textit{I}_{6}\\)。 现在我们可以构造出增广文法 \\(\\textit{G}^{’}\\) 的规范 LR(0) 项集族 C 的算法。 void items(\\(\\textit{G}^{’}\\)) { C = {\\(\\texttt{CLOSURE}\\)(\\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}]\\))}; repeat for (C 中的每个项集 I) for (每个文法符号 X) if (\\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 非空且不在 C 中) 将 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 加入 C 中; until 在某轮中没有新的项集加入到 C 中; } LR(0) 自动机的用法SLR 的中心思想是根据文法构造出 LR(0) 自动机。这个自动机的状态是规范 LR(0) 项集族中的元素，而它的转换由 \\(\\texttt{GOTO}\\) 函数给出。 状态 j 是指对应的与项集 \\(\\textit{I}_{j}\\) 的状态。LR(0) 自动机从开始状态 0 运行到某个状态 j，如果下一个输入符号为 a 且状态 j 有一个 a 上的转换，那么就移入 a，否则就进行归约。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#goto-函数"},{"categories":["CompilerPrinciple"],"content":" 项和 LR(0) 自动机那么一个语法分析器怎么知道何时移入、何时归约的呢？ 一个 IR 语法分析器通过维护一些状态，用这些状态来表明我们在语法分析过程中所处的位置，从而做出移入-归约决定。这些状态代表了项 (item) 的集合。一个文法 G 的一个 LR (0) 项是 G 的一个产生式再加上一个位于它的体中某处的点。因此，产生式 \\(\\textit{A}\\rightarrow\\textit{XYZ}\\) 产生了四个项： \\[\\begin{aligned} \\textit{A} \u0026\\rightarrow \\cdot\\textit{XYZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{X}\\cdot\\textit{YZ}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XY}\\cdot\\textit{Z}\\\\ \\textit{A} \u0026\\rightarrow \\textit{XYZ}\\cdot \\end{aligned}\\] 产生式 \\(\\textit{A}\\rightarrow\\varepsilon\\) 只生成一个项 \\(\\textit{A}\\rightarrow\\cdot\\) 。 一个称为规范 LR(0) 项集族的一组项集提供了构建一个确定有穷自动机的基础，该自动机用于做出语法分析决定，这样的自动机被称为 LR(0) 自动机。这个自动机的每个状态代表了规范 LR(0) 项集族中的一个项集。 为了构造一个文法的规范 LR(0) 项集族，我们定义了一个增广文法和两个函数 CLOSURE 和 GOTO。如果 G 是以 S 为开始符号的文法，那么 G 的增广文法 \\(\\textit{G}^{’}\\) 就是在 G 中加上新的开始符号 \\(\\textit{S}^{’}\\) 和产生式 \\(\\textit{S}^{’}\\rightarrow\\textit{S}\\) 的文法。引入新的产生式的目的是告诉文法分析器何时应该停止语法分析并宣称接受输入符号串。 对于之前反复提到的示例 id + id * id，可以构造出如下自动机。 项集的闭包如果 I 是文法 G 的一个项集，那么 \\(\\texttt{CLOSURE}(\\textit{I})\\) 就是根据以下两个规则从 I 构造得到的： 将 I 中的各个项加入到 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中 如果 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\) 在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中， \\(\\textit{B}\\rightarrow\\gamma\\) 是一个产生式，并且项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中，就将这个项加入其中。不断应用这个规则，直到没有新项可以加入 \\(\\texttt{CLOSURE}(\\textit{I})\\) 中为止。 closure 可以按照以下方式计算。函数 closure 可以添加一个 added 数组来方便实现，该数组下标是 G 的非终结符，当各个 B 的产生式 \\(\\textit{B}\\rightarrow\\gamma\\) 加入对应的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 时，added[B] 被设置为 true。 SetOfItems CLOSURE(I) { J = I; repeat for (J 中的每个项 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta\\)) for (G 的每个产生式 \\(\\textit{B}\\rightarrow\\gamma\\)) if (项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 不在 G 中) 将 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 添加到 J 中; until 没有可以被加入到 J 中的项; return J; } 如果点在最左端的产生式 B 被加入 I 中，那么所有 B 的产生式都会被加入 I 的闭包中。因此在某些情况下，不需要真的将那些被 \\(\\texttt{CLOSURE}\\) 函数加入到 I 中的项 \\(\\textit{B}\\rightarrow\\cdot\\gamma\\) 列出来，只需要列出这些被加入的产生式的左部非终结符就行。可以将各项分为两类： 内核项 包括初始化 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 以及点不在最左端的所有项 非内核项 除了 \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) 之外的所有点在最左端的所有项 感兴趣的项集是某个内核项集合的闭包，求闭包加入的项必然是非内核项。如果我们抛弃所有非内核项，就可以用很少的内存来表示真正感兴趣的项的集合，因为我们已知这些非内核项可以通过闭包运算重新生成。即之前构造出自动机的阴影部分。 GOTO 函数GOTO(I,X)，其中 I 是一个项集而 X 是一个文法符号。GOTO(I,X) 被定义为 I 中所有形如 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\textit{X}\\beta]\\) 的项所对应的项 \\([\\textit{A}\\rightarrow\\alpha\\textit{X}\\cdot\\beta]\\) 的集合的闭包。简单地说，就是自动机的状态转换。示例自动机，\\(\\texttt{GOTO}(\\textit{I}_{1}, +)\\) 的结果为项集 \\(\\textit{I}_{6}\\)。 现在我们可以构造出增广文法 \\(\\textit{G}^{’}\\) 的规范 LR(0) 项集族 C 的算法。 void items(\\(\\textit{G}^{’}\\)) { C = {\\(\\texttt{CLOSURE}\\)(\\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}]\\))}; repeat for (C 中的每个项集 I) for (每个文法符号 X) if (\\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 非空且不在 C 中) 将 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 加入 C 中; until 在某轮中没有新的项集加入到 C 中; } LR(0) 自动机的用法SLR 的中心思想是根据文法构造出 LR(0) 自动机。这个自动机的状态是规范 LR(0) 项集族中的元素，而它的转换由 \\(\\texttt{GOTO}\\) 函数给出。 状态 j 是指对应的与项集 \\(\\textit{I}_{j}\\) 的状态。LR(0) 自动机从开始状态 0 运行到某个状态 j，如果下一个输入符号为 a 且状态 j 有一个 a 上的转换，那么就移入 a，否则就进行归约。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#lr--0--自动机的用法"},{"categories":["CompilerPrinciple"],"content":" LR 语法分析算法还记得 LL 语法分析的非递归预测分析中提到的分析表驱动的语法分析器吗，我们的 LR 语法分析器与它很像。 所有 LR 语法分析器的驱动程序都是相同的，而语法分析表是根据语法分析器的不同而变化的。每个状态都有一个对应的文法符号，各个状态都和每个项集对应，并有从状态 i 到状态 j 的转换 \\(\\texttt{GOTO}(\\textit{I}_{i}, \\textit{X}) = \\textit{I}_{j}\\)。所有到达状态 j 的转换一定对应于同一个文法符号 \\(\\textit{X}\\)。因此，除了开始状态 0 之外每个状态都和唯一的文法符号项关联。 LR 语法分析表的结构语法分析表由语法分析动作函数 ACTION 和转换函数 GOTO 组成。 \\(\\texttt{ACTION}[i, a]\\) 取值有四种形式: 移入状态 j。语法分析器将输入符号 a 高效地移入栈中，并使用 j 来代表 a 归约 \\(\\textit{A}\\rightarrow\\beta\\)。语法分析器将栈顶的 \\(\\beta\\) 高效地归约为产生式头 A 接受。语法分析器接受输入并完成分析过程 报错。发现语法分析错误并执行纠正动作 \\(\\texttt{GOTO}[\\textit{I}_{i}, \\textit{A}] = \\textit{I}_{j}\\)，将状态 i 与非终结符 A 映射到状态 j LR 语法分析器的行为语法分析器根据 configuration 决定下一个动作时，首先读入当前输入符号 \\(a_{i}\\) 和栈顶状态 \\(s_{m}\\)，在分析动作表中查询条目 \\(\\texttt{ACTION}[s_{m}, a_{i}]\\)。对于每个 ACTION 的形式结束后格局如下： 如果 ACTION 为移入 s，那么语法分析器将下一个状态 s 移入栈中，而输入符号 \\(a_{i}\\) 不需要存放在栈中 如果 ACTION 为归约 \\(\\textit{A}\\rightarrow\\beta\\)，那么语法分析器进行一次归约操作。语法分析器会先从栈中弹出 r 个状态 (r 是 \\(\\beta\\) 的长度)，之后将状态 s (\\(\\texttt{GOTO}[s_{m-r}, \\textit{A}]\\) 的值) 压入栈。而归约动作中，当前输入符号不会改变。 所有的 LR 语法分析器都会按照以下的算法执行，两个 LR 语法分析器之间唯一的区别即 ACTION 和 GOTO 包含的信息不同。 输入：一个输入串 w，一个 LR 语法分析表 (GOTO 和 ACTION) 输出：如果 w 在 L(G) 中，输出 w 的自底向上语法分析过程的归约步骤，否则报错 方法：语法分析器栈中最初为 \\(s_{0}\\)，输入缓冲区的内容为 \\(w\\$\\)，然后执行以下算法 令 a 为 \\(w\\$\\) 的第一个符号; while (true) { 令 s 是栈顶状态; if (\\(\\texttt{ACTION}[s, a] =\\) 移入 t) { 将 t 压入栈; 令 a 为下一个符号; } else if (\\(\\texttt{ACTION}[s, a] =\\) 归约 \\(\\textit{A}\\rightarrow\\beta\\)) { 从栈中弹出 \\(\\texttt{len}(\\beta)\\) 个符号; 令 t 为当前栈顶的符号; 将 \\(\\texttt{GOTO}[t, \\textit{A}]\\) 压入栈; 输出产生式 \\(\\textit{A}\\rightarrow\\beta\\); } else if (\\(\\texttt{ACTION}[s, a] =\\) 接受) break; else 调用错误恢复例程; } 我们尝试构造一下老朋友 id + id * id 的语法分析表，首先对产生式进行编号： \\[ \\begin{align} \\textit{E} \u0026\\rightarrow \\textit{E} + \\textit{T}\\tag{1}\\\\ \\textit{E} \u0026\\rightarrow \\textit{T}\\tag{2}\\\\ \\textit{T} \u0026\\rightarrow \\textit{T} * \\textit{F}\\tag{3}\\\\ \\textit{T} \u0026\\rightarrow \\textit{F}\\tag{4}\\\\ \\textit{F} \u0026\\rightarrow (\\textit{E})\\tag{5}\\\\ \\textit{F} \u0026\\rightarrow \\textbf{id}\\tag{6} \\end{align}\\] 我们由如下规定 si 表示移入并将状态 i 压入栈 rj 表示将编号为 j 的产生式进行归约 acc 表示接受 空白表示报错 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#lr-语法分析算法"},{"categories":["CompilerPrinciple"],"content":" LR 语法分析算法还记得 LL 语法分析的非递归预测分析中提到的分析表驱动的语法分析器吗，我们的 LR 语法分析器与它很像。 所有 LR 语法分析器的驱动程序都是相同的，而语法分析表是根据语法分析器的不同而变化的。每个状态都有一个对应的文法符号，各个状态都和每个项集对应，并有从状态 i 到状态 j 的转换 \\(\\texttt{GOTO}(\\textit{I}_{i}, \\textit{X}) = \\textit{I}_{j}\\)。所有到达状态 j 的转换一定对应于同一个文法符号 \\(\\textit{X}\\)。因此，除了开始状态 0 之外每个状态都和唯一的文法符号项关联。 LR 语法分析表的结构语法分析表由语法分析动作函数 ACTION 和转换函数 GOTO 组成。 \\(\\texttt{ACTION}[i, a]\\) 取值有四种形式: 移入状态 j。语法分析器将输入符号 a 高效地移入栈中，并使用 j 来代表 a 归约 \\(\\textit{A}\\rightarrow\\beta\\)。语法分析器将栈顶的 \\(\\beta\\) 高效地归约为产生式头 A 接受。语法分析器接受输入并完成分析过程 报错。发现语法分析错误并执行纠正动作 \\(\\texttt{GOTO}[\\textit{I}_{i}, \\textit{A}] = \\textit{I}_{j}\\)，将状态 i 与非终结符 A 映射到状态 j LR 语法分析器的行为语法分析器根据 configuration 决定下一个动作时，首先读入当前输入符号 \\(a_{i}\\) 和栈顶状态 \\(s_{m}\\)，在分析动作表中查询条目 \\(\\texttt{ACTION}[s_{m}, a_{i}]\\)。对于每个 ACTION 的形式结束后格局如下： 如果 ACTION 为移入 s，那么语法分析器将下一个状态 s 移入栈中，而输入符号 \\(a_{i}\\) 不需要存放在栈中 如果 ACTION 为归约 \\(\\textit{A}\\rightarrow\\beta\\)，那么语法分析器进行一次归约操作。语法分析器会先从栈中弹出 r 个状态 (r 是 \\(\\beta\\) 的长度)，之后将状态 s (\\(\\texttt{GOTO}[s_{m-r}, \\textit{A}]\\) 的值) 压入栈。而归约动作中，当前输入符号不会改变。 所有的 LR 语法分析器都会按照以下的算法执行，两个 LR 语法分析器之间唯一的区别即 ACTION 和 GOTO 包含的信息不同。 输入：一个输入串 w，一个 LR 语法分析表 (GOTO 和 ACTION) 输出：如果 w 在 L(G) 中，输出 w 的自底向上语法分析过程的归约步骤，否则报错 方法：语法分析器栈中最初为 \\(s_{0}\\)，输入缓冲区的内容为 \\(w\\$\\)，然后执行以下算法 令 a 为 \\(w\\$\\) 的第一个符号; while (true) { 令 s 是栈顶状态; if (\\(\\texttt{ACTION}[s, a] =\\) 移入 t) { 将 t 压入栈; 令 a 为下一个符号; } else if (\\(\\texttt{ACTION}[s, a] =\\) 归约 \\(\\textit{A}\\rightarrow\\beta\\)) { 从栈中弹出 \\(\\texttt{len}(\\beta)\\) 个符号; 令 t 为当前栈顶的符号; 将 \\(\\texttt{GOTO}[t, \\textit{A}]\\) 压入栈; 输出产生式 \\(\\textit{A}\\rightarrow\\beta\\); } else if (\\(\\texttt{ACTION}[s, a] =\\) 接受) break; else 调用错误恢复例程; } 我们尝试构造一下老朋友 id + id * id 的语法分析表，首先对产生式进行编号： \\[ \\begin{align} \\textit{E} \u0026\\rightarrow \\textit{E} + \\textit{T}\\tag{1}\\\\ \\textit{E} \u0026\\rightarrow \\textit{T}\\tag{2}\\\\ \\textit{T} \u0026\\rightarrow \\textit{T} * \\textit{F}\\tag{3}\\\\ \\textit{T} \u0026\\rightarrow \\textit{F}\\tag{4}\\\\ \\textit{F} \u0026\\rightarrow (\\textit{E})\\tag{5}\\\\ \\textit{F} \u0026\\rightarrow \\textbf{id}\\tag{6} \\end{align}\\] 我们由如下规定 si 表示移入并将状态 i 压入栈 rj 表示将编号为 j 的产生式进行归约 acc 表示接受 空白表示报错 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#lr-语法分析表的结构"},{"categories":["CompilerPrinciple"],"content":" LR 语法分析算法还记得 LL 语法分析的非递归预测分析中提到的分析表驱动的语法分析器吗，我们的 LR 语法分析器与它很像。 所有 LR 语法分析器的驱动程序都是相同的，而语法分析表是根据语法分析器的不同而变化的。每个状态都有一个对应的文法符号，各个状态都和每个项集对应，并有从状态 i 到状态 j 的转换 \\(\\texttt{GOTO}(\\textit{I}_{i}, \\textit{X}) = \\textit{I}_{j}\\)。所有到达状态 j 的转换一定对应于同一个文法符号 \\(\\textit{X}\\)。因此，除了开始状态 0 之外每个状态都和唯一的文法符号项关联。 LR 语法分析表的结构语法分析表由语法分析动作函数 ACTION 和转换函数 GOTO 组成。 \\(\\texttt{ACTION}[i, a]\\) 取值有四种形式: 移入状态 j。语法分析器将输入符号 a 高效地移入栈中，并使用 j 来代表 a 归约 \\(\\textit{A}\\rightarrow\\beta\\)。语法分析器将栈顶的 \\(\\beta\\) 高效地归约为产生式头 A 接受。语法分析器接受输入并完成分析过程 报错。发现语法分析错误并执行纠正动作 \\(\\texttt{GOTO}[\\textit{I}_{i}, \\textit{A}] = \\textit{I}_{j}\\)，将状态 i 与非终结符 A 映射到状态 j LR 语法分析器的行为语法分析器根据 configuration 决定下一个动作时，首先读入当前输入符号 \\(a_{i}\\) 和栈顶状态 \\(s_{m}\\)，在分析动作表中查询条目 \\(\\texttt{ACTION}[s_{m}, a_{i}]\\)。对于每个 ACTION 的形式结束后格局如下： 如果 ACTION 为移入 s，那么语法分析器将下一个状态 s 移入栈中，而输入符号 \\(a_{i}\\) 不需要存放在栈中 如果 ACTION 为归约 \\(\\textit{A}\\rightarrow\\beta\\)，那么语法分析器进行一次归约操作。语法分析器会先从栈中弹出 r 个状态 (r 是 \\(\\beta\\) 的长度)，之后将状态 s (\\(\\texttt{GOTO}[s_{m-r}, \\textit{A}]\\) 的值) 压入栈。而归约动作中，当前输入符号不会改变。 所有的 LR 语法分析器都会按照以下的算法执行，两个 LR 语法分析器之间唯一的区别即 ACTION 和 GOTO 包含的信息不同。 输入：一个输入串 w，一个 LR 语法分析表 (GOTO 和 ACTION) 输出：如果 w 在 L(G) 中，输出 w 的自底向上语法分析过程的归约步骤，否则报错 方法：语法分析器栈中最初为 \\(s_{0}\\)，输入缓冲区的内容为 \\(w\\$\\)，然后执行以下算法 令 a 为 \\(w\\$\\) 的第一个符号; while (true) { 令 s 是栈顶状态; if (\\(\\texttt{ACTION}[s, a] =\\) 移入 t) { 将 t 压入栈; 令 a 为下一个符号; } else if (\\(\\texttt{ACTION}[s, a] =\\) 归约 \\(\\textit{A}\\rightarrow\\beta\\)) { 从栈中弹出 \\(\\texttt{len}(\\beta)\\) 个符号; 令 t 为当前栈顶的符号; 将 \\(\\texttt{GOTO}[t, \\textit{A}]\\) 压入栈; 输出产生式 \\(\\textit{A}\\rightarrow\\beta\\); } else if (\\(\\texttt{ACTION}[s, a] =\\) 接受) break; else 调用错误恢复例程; } 我们尝试构造一下老朋友 id + id * id 的语法分析表，首先对产生式进行编号： \\[ \\begin{align} \\textit{E} \u0026\\rightarrow \\textit{E} + \\textit{T}\\tag{1}\\\\ \\textit{E} \u0026\\rightarrow \\textit{T}\\tag{2}\\\\ \\textit{T} \u0026\\rightarrow \\textit{T} * \\textit{F}\\tag{3}\\\\ \\textit{T} \u0026\\rightarrow \\textit{F}\\tag{4}\\\\ \\textit{F} \u0026\\rightarrow (\\textit{E})\\tag{5}\\\\ \\textit{F} \u0026\\rightarrow \\textbf{id}\\tag{6} \\end{align}\\] 我们由如下规定 si 表示移入并将状态 i 压入栈 rj 表示将编号为 j 的产生式进行归约 acc 表示接受 空白表示报错 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#lr-语法分析器的行为"},{"categories":["CompilerPrinciple"],"content":" 构造 SLR 语法分析表构造增广文法 \\(\\textit{G}^{’}\\) 的语法分析表方法如下： 构造 \\(\\textit{G}^{’}\\) 的规范 LR(0) 项集族 \\(C = \\{\\textit{I}_{0}, \\textit{I}_{1}, \\cdots, \\textit{I}_{n}\\}\\) 根据 \\(\\textit{I}_{i}\\) 构造出状态 i,状态 i 的语法分析动作按照下面的方法决定。如果这些规则生成了任何冲突动作，那么文法就不是 SLR(1) 的，也就无法生成语法分析器。 如果 \\([\\textit{A}\\rightarrow\\alpha\\cdot{}a\\beta]\\) 在 \\(\\textit{I}_{i}\\) 中且 \\(\\texttt{GOTO}(\\textit{I}_{i}, a) = \\textit{I}_{j}\\)，那么将 \\(\\texttt{ACTION}[i, a]\\) 设置为移入 j，其中 a 必须是一个终结符 如果 \\([\\textit{A}\\rightarrow\\alpha\\cdot]\\) 在 \\(\\textit{I}_{i}\\) 中，那么对于 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中的所有 a，将 \\(\\texttt{ACTION}[i, a]\\) 设置为归约 \\(\\textit{A}\\rightarrow\\alpha\\)，这里 \\(\\textit{A}\\) 不为 \\(\\textit{S}^{’}\\) 如果 \\([\\textit{S}^{’}\\rightarrow\\textit{S}\\cdot]\\) 在 \\(\\textit{I}_{i}\\) 中，那么将 \\(\\texttt{ACTION}[i, \\$]\\) 设置为接受 状态 i 对于各个非终结符 A 的 \\(\\texttt{GOTO}\\) 转换使用下面的规则构造：如果 \\(\\texttt{GOTO}(\\textit{I}_{i}, \\textit{A}) = \\textit{I}_{j}\\)，那么 \\(\\textit{GOTO}[i, \\textit{A}] = j\\)。 规则 2 与规则 3 没有定义的所有条目都是报错。 语法分析器的初始状态根据 \\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}]\\) 所在的项集构造得到的状态。 每个 SLR 文法都是无二义性的，但还是存在一些非 SLR 的无二义性文法，如： \\[\\begin{aligned} \\textit{S} \u0026\\rightarrow \\textit{L} = \\textit{R} \\ |\\ \\textit{R}\\\\ \\textit{L} \u0026\\rightarrow * \\textit{R} \\ |\\ \\textbf{id}\\\\ \\textit{R} \u0026\\rightarrow \\textit{L} \\end{aligned}.\\] 其对应的规范 LR(0) 项集为： \\(\\textit{I}_{0}\\) \\[\\begin{aligned} \\textit{S}^{’} \u0026\\rightarrow \\cdot\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot\\textit{L} = \\textit{R}\\\\ \\textit{S} \u0026\\rightarrow \\cdot\\textit{R}\\\\ \\textit{L} \u0026\\rightarrow \\cdot*\\textit{R}\\\\ \\textit{L} \u0026\\rightarrow \\cdot\\textbf{id}\\\\ \\textit{R} \u0026\\rightarrow \\cdot\\textit{L} \\end{aligned}\\] \\(\\textit{I}_{1}\\) \\[\\textit{S}^{’} \\rightarrow \\textit{S}\\cdot\\] \\(\\textit{I}_{2}\\) \\[\\begin{aligned} \\textit{S} \u0026\\rightarrow \\textit{L}\\cdot = \\textit{R}\\\\ \\textit{R} \u0026\\rightarrow \\textit{L}\\cdot \\end{aligned}\\] \\(\\textit{I}_{3}\\) \\[\\textit{S} \\rightarrow \\textit{R}\\cdot\\] \\(\\textit{I}_{4}\\) \\[\\begin{aligned} \\textit{L} \u0026\\rightarrow *\\cdot\\textit{R}\\\\ \\textit{R} \u0026\\rightarrow \\cdot\\textit{L}\\\\ \\textit{L} \u0026\\rightarrow \\cdot*\\textit{R}\\\\ \\textit{L} \u0026\\rightarrow \\cdot\\textbf{id} \\end{aligned}\\] \\(\\textit{I}_{5}\\) \\[\\textit{L} \\rightarrow \\textbf{id}\\cdot\\] \\(\\textit{I}_{6}\\) \\[\\begin{aligned} \\textit{S} \u0026\\rightarrow \\textit{L} = \\cdot\\textit{R}\\\\ \\textit{R} \u0026\\rightarrow \\cdot\\textit{L}\\\\ \\textit{L} \u0026\\rightarrow \\cdot*\\textit{R}\\\\ \\textit{L} \u0026\\rightarrow \\cdot\\textbf{id} \\end{aligned}\\] \\(\\textit{I}_{7}\\) \\[\\textit{L}\\rightarrow*\\textit{R}\\cdot\\] \\(\\textit{I}_{8}\\) \\[\\textit{R}\\rightarrow\\textit{L}\\cdot\\] \\(\\textit{I}_{9}\\) \\[\\textit{S}\\rightarrow\\textit{L}=\\textit{R}\\cdot\\] 项集 \\(\\textit{I}_{2}\\) 告诉我们 \\(\\texttt{ACTION}[2, =]\\) 是移入 6，而 \\(\\texttt{FOLLOW}(\\textit{R})\\) 告诉我们 \\(\\texttt{ACTION}[2, =]\\) 是归约 \\(\\textit{R}\\rightarrow\\textit{L}\\)，因此这回导致移入 / 归约冲突。产生的原因是 SLR 不够强大，之后更强大的 LR 语法分析可以成功处理更大的文法类型。当然有些无论什么 LR 方法都产生冲突的文法，在设计时都会避免使用。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:2:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#构造-slr-语法分析表"},{"categories":["CompilerPrinciple"],"content":" 更强大的 LR 语法分析器扩展 LR(0) 语法分析技术，在输入中向前看一个符号，有两种方法： 规范 LR，采用很大的 LR(1) 项集，充分利用向前符号 向前看 LR 或 LALR，基于 LR(0) 项集族，但比 LR(1) 拥有更少的状态。可以构造出更强的文法，同时分析表与 SLR 差不多大。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:3:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#更强大的-lr-语法分析器"},{"categories":["CompilerPrinciple"],"content":" 规范 LR(1) 项回顾一下在 构造 SLR 语法分析表 中提到的那个无二义性文法，\\(\\textit{I}_{2}\\) 要求按照 \\(\\textit{R}\\rightarrow\\textit{L}\\) 归约，同时要求 \\(\\textit{S}\\rightarrow\\textit{L}\\cdot=\\textit{R}\\) 移入，很明显 \\(\\textit{I}_{2}\\) 没有 \\(\\textit{R}=\\cdots\\) 开头的最右句型，因此只能进行移入操作。 如果我们在状态中添加额外的信息，在必要时分裂某些状态，设法让 LR 语法分析器的每个状态精确地指明哪些输入符号可以跟在句柄只有，从而使句柄被正确归约。 这个额外的信息将项变为了 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\beta,{}a]\\)，其中 \\(\\textit{A}\\rightarrow\\alpha\\beta\\) 是产生式，a 是终结符或结束标记。我们称这样的对象为 LR(1) 项，第二个分量称为向前看符号。在形如 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\beta,{}a]\\) 且 \\(\\beta\\ne\\varepsilon\\) 的项中向前看符没有任何用；但 \\([\\textit{A}\\rightarrow\\alpha\\cdot,{}a]\\) 的项只有在下一个输入符号等于 a 时才进行归约。通常 a 的集合是 \\(\\texttt{FOLLOW}(\\textit{A})\\) 的子集。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:3:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#规范-lr--1--项"},{"categories":["CompilerPrinciple"],"content":" 构造 LR(1) 项集构造 LR(1) 项集只需要改写 CLOSURE 和 GOTO 方法。 SetOfItems \\(\\texttt{CLOSURE}\\)(I) { repeat for (I 中的每个项 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\textit{B}\\beta,{}a]\\)) for (\\(\\textit{G}^{’}\\) 中的每个产生式 \\(\\textit{B}\\rightarrow\\gamma\\)) for (\\(\\texttt{FIRST}(\\beta{}a)\\) 中的每个终结符 b) 将 \\([\\textit{B}\\rightarrow\\cdot\\gamma,{}b]\\) 加入到集合 I 中; until 不能向 I 中加入更多的项; return I; } SetOfItems \\(\\texttt{GOTO}\\)(I, X) { 将 J 初始化为空集; for (I 中的每个项 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\textit{X}\\beta,{}a]\\)) 将\\([\\textit{A}\\rightarrow\\alpha\\textit{X}\\cdot\\beta,{}a]\\) 加入集合 J 中; return \\(\\texttt{CLOSURE}(\\textit{J})\\); } void items(\\(\\textit{G}^{’}\\)) { 将 C 初始化为 \\(\\{\\texttt{CLOSURE}\\}(\\{[\\textit{S}^{’}\\rightarrow\\cdot\\textit{S},\\$]\\})\\); repeat for (C 中的每个项集 I) for (每个文法符号 X) if (\\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 非空且不在 C 中) 将 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 加入 C 中; until 不再有新的项集加入到 C 中; } 我们可以针对增广文法构造自动机 \\[\\begin{aligned} \\textit{S}^{’} \u0026\\rightarrow \\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\textit{C}\\ \\textit{C}\\\\ \\textit{C} \u0026\\rightarrow c\\ \\textit{C} \\ |\\ d \\end{aligned}\\] ","date":"04-20","objectID":"/2022/compilerprinciple_006/:3:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#构造-lr--1--项集"},{"categories":["CompilerPrinciple"],"content":" 规范 LR(1) 语法分析表 构造 \\(\\textit{G}^{’}\\) 的规范 LR(1) 项集族 \\(C^{’} = \\{\\textit{I}_{0}, \\textit{I}_{1}, \\cdots, \\textit{I}_{n}\\}\\) 根据 \\(\\textit{I}_{i}\\) 构造出状态 i，状态 i 的语法分析动作按照下面的方法决定。如果这些规则生成了任何冲突动作，那么文法就不是 LR(1) 的，也就无法生成语法分析器。 如果 \\([\\textit{A}\\rightarrow\\alpha\\cdot{}a\\beta, b]\\) 在 \\(\\textit{I}_{i}\\) 中且 \\(\\texttt{GOTO}(\\textit{I}_{i}, a) = \\textit{I}_{j}\\)，那么将 \\(\\texttt{ACTION}[i, a]\\) 设置为移入 j，其中 a 必须是一个终结符 如果 \\([\\textit{A}\\rightarrow\\alpha\\cdot, a]\\) 在 \\(\\textit{I}_{i}\\) 中且 \\(\\textit{A}\\ne\\textit{S}^{’}\\)，那么对于将 \\(\\texttt{ACTION}[i, a]\\) 设置为归约 \\(\\textit{A}\\rightarrow\\alpha\\) 如果 \\([\\textit{S}^{’}\\rightarrow\\textit{S}\\cdot,\\$]\\) 在 \\(\\textit{I}_{i}\\) 中，那么将 \\(\\texttt{ACTION}[i, \\$]\\) 设置为接受 状态 i 对于各个非终结符 A 的 \\(\\texttt{GOTO}\\) 转换使用下面的规则构造：如果 \\(\\texttt{GOTO}(\\textit{I}_{i}, \\textit{A}) = \\textit{I}_{j}\\)，那么 \\(\\textit{GOTO}[i, \\textit{A}] = j\\)。 规则 2 与规则 3 没有定义的所有条目都是报错。 语法分析器的初始状态根据 \\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S},\\$]\\) 所在的项集构造得到的状态。 每个 SLR 文法都是规范 LR(1) 文法，但对同一文法 SLR 的状态比 LR(1) 少。上一节提到的文法，SLR 只需要七个状态，而 LR(1) 文法需要 10 个状态。 \\[ \\begin{align} \\textit{S} \u0026\\rightarrow \\textit{C}\\ \\textit{C}\\tag{1}\\\\ \\textit{C} \u0026\\rightarrow c\\ \\textit{C}\\tag{2}\\\\ \\textit{C} \u0026\\rightarrow d\\tag{3} \\end{align}\\] 其语法分析表如下： ","date":"04-20","objectID":"/2022/compilerprinciple_006/:3:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#规范-lr--1--语法分析表"},{"categories":["CompilerPrinciple"],"content":" 构造 LALR 语法分析表LALR 语法分析技术是实践中常用的分析技术，因为其分析表比 LR 分析表小的多，且大部分常见的程序设计语言都可以方便构造 LALR 文法表示。SLR 与 LALR 总是有相同数量的状态。比如 C 语言 SLR 可能有几百个状态，但规范 LR(1) 可能达到上千个状态。 考虑规范 LR(1) 文法中的示例，考虑状态 4 与状态 7 的区别，正则表达式 c*dc*d，第一组 c*d 会进入状态 4，而第二组 c*d 会进入状态 7 \\(\\textit{I}_{4}\\): \\(\\textit{C}\\rightarrow{}d\\cdot,c/d\\) \\(\\textit{I}_{7}\\): \\(\\textit{C}\\rightarrow{}d\\cdot,\\$\\) 基本没什么区别，可以将状态 4 和状态 7 作为并集替换为 \\(\\textit{I}_{47}\\)，这个项集包含了 \\([\\textit{C}\\rightarrow{}d\\cdot,c/d/\\$]\\)。 我们通常将具有相同核心 (core) 的 LR(1) 项集合并为第一分量的集合，一个核心就是当前正处理的文法的 LR(0) 项集，LR(1) 文法可能产生多个具有相同核心的项集。 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 的核心只有 I 的核心决定，一组被合并的项集的 \\(\\texttt{GOTO}\\) 的目标也可以被合并。因此我们可以相应地修改 \\(\\texttt{GOTO}\\) 函数和动作函数。 但是如果无脑合并，可能会产生冲突，比如以下这个文法 \\[\\begin{aligned} \\textit{S}^{’} \u0026\\rightarrow \\textit{S}\\\\ \\textit{S} \u0026\\rightarrow a\\textit{A}d\\,|\\,b\\textit{B}d\\,|\\,a\\textit{A}e\\,|\\,b\\textit{A}e\\\\ \\textit{A} \u0026\\rightarrow c\\\\ \\textit{B} \u0026\\rightarrow c \\end{aligned}\\] 这是一个 LR(1) 文法，其中有两个项集 core 相同：\\(\\{[\\textit{A}\\rightarrow{}c\\cdot,d], [\\textit{B}\\rightarrow{}c\\cdot,e]\\}\\) 和 \\(\\{[\\textit{A}\\rightarrow{}c\\cdot,e], [\\textit{B}\\rightarrow{}c\\cdot,d]\\}\\)。但是它们的并集 \\(\\{[\\textit{A}\\rightarrow{}c\\cdot,d/e], [\\textit{B}\\rightarrow{}c\\cdot,d/e]\\}\\) 将会造成归约/ 归约冲突。 那么可以给出定义 LALR(1) 文法的语法分析表构建方法，其核心思想就是构造出 LR(1) 项集，将没有冲突且相同核心的项集合并。 构造 LR(1) 项集族 \\(\\textit{C}=\\{\\textit{I}_{0}, \\textit{I}_{1}, \\cdots, \\textit{I}_{n}\\}\\) 对于 LR(1) 项集中的每个核心，找出具有相同和新的项集，用并集替换它们 令 \\(\\textit{C}^{’}=\\{\\textit{J}_{0}, \\textit{J}_{1}, \\cdots, \\textit{J}_{m}\\}\\) 是得到的 LR(1) 项集族 GOTO 表的构造方法如下：如果 J 是一个或多个 LR(1) 项集的并集 (\\(\\textit{J}=\\textit{I}_{1}\\cup\\textit{I}_{2}\\cup\\cdots\\cup\\textit{I}_{k}\\))，那么 \\(\\texttt{GOTO}(\\textit{I}_{1}, \\textit{X})\\)、 \\(\\texttt{GOTO}(\\textit{I}_{2}, \\textit{X})\\)、\\(\\cdots\\)、 \\(\\texttt{GOTO}(\\textit{I}_{k}, \\textit{X})\\) 的核心相同，令 K 是所有和 \\(\\texttt{GOTO}(\\textit{I}_{1}, \\textit{X})\\) 具有相同核心的项集的并集，那么 \\(\\texttt{GOTO}(\\textit{J}, \\textit{X}) = \\textit{K}\\)。 如果没有冲突，那么将其称为 LALR(1) 文法，第三步的项集族被称为 LALR(1) 项集族。 我们可以针对之前 LR(1) 文法示例的增广文法构造自动机，方便对比两个文法 \\[\\begin{aligned} \\textit{S}^{’} \u0026\\rightarrow \\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\textit{C}\\ \\textit{C}\\\\ \\textit{C} \u0026\\rightarrow c\\ \\textit{C} \\ |\\ d \\end{aligned}\\] 其分析表也很简单。 在处理正确的输入时，LR 语法分析器和 LALR 语法分析器可以相互模拟；在处理错误的输入时，LALR 语法分析器可能在 LR 语法分析器报错之后继续执行一些归约动作，但绝不会在 LR 语法分析器报错之后移入任何符号。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:3:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#构造-lalr-语法分析表"},{"categories":["CompilerPrinciple"],"content":" 高效构造 LALR 语法分析表构建 LALR(1) 文法时，实际上我们不需要先构建完整的规范 LR(1) 项集族，这样效率太低了，也不是实际应用的构建方式。因此可以优化其构造。 首先只用用内核项来表示任意的 LR(0) 或 LR(1) 项集。就是只使用初始项 \\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}]\\) 或 \\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S},\\$]\\) 以及那些点不在产生体左端的项来表示项集 我们可以使用一个传播和自发生成的过程生成向前看符号，根据 LR(0) 项的内核来生成 LALR(1) 项的内核 如果有了 LALR(1) 内核，对各个内核求 \\(\\texttt{CLOSURE}\\)，再把 LALR(1) 项集当作规范 LR(1) 项集族，计算分析表，从而得到 LALR(1) 语法分析表 还是一样的用一个示例构造 LALR(1) 语法分析表。 \\[\\begin{aligned} \\textit{S}^{’} \u0026\\rightarrow \\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\textit{L} = \\textit{R}\\ |\\ \\textit{R}\\\\ \\textit{L} \u0026\\rightarrow * \\textit{R} \\ |\\ \\textbf{id}\\\\ \\textit{R} \u0026\\rightarrow \\textit{L} \\end{aligned}\\] 状态内核如下 \\(\\textit{I}_{0}\\): \\(\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}\\) \\(\\textit{I}_{1}\\): \\(\\textit{S}^{’}\\rightarrow\\textit{S}\\cdot\\) \\(\\textit{I}_{2}\\): \\[\\begin{aligned} \\textit{S}\u0026\\rightarrow\\textit{L}\\cdot=\\textit{R}\\\\ \\textit{R}\u0026\\rightarrow\\textit{L}\\cdot\\end{aligned}\\] \\(\\textit{I}_{3}\\): \\(\\textit{S}\\rightarrow\\textit{R}\\cdot\\) \\(\\textit{I}_{4}\\): \\(\\textit{L}\\rightarrow*\\cdot\\textit{R}\\) \\(\\textit{I}_{5}\\): \\(\\textit{L}\\rightarrow\\textbf{id}\\cdot\\) \\(\\textit{I}_{6}\\): \\(\\textit{S}\\rightarrow\\textit{L}=\\cdot\\textit{R}\\) \\(\\textit{I}_{7}\\): \\(\\textit{L}\\rightarrow*\\textit{R}\\cdot\\) \\(\\textit{I}_{8}\\): \\(\\textit{R}\\rightarrow\\textit{L}\\cdot\\) \\(\\textit{I}_{9}\\): \\(\\textit{S}\\rightarrow\\textit{L}=\\textit{R}\\cdot\\) 在这些内核上加上正确的向前看符号，创建出 LALR(1) 项集的内核。在两种情况下向前看符号 b 可以添加到某个 LALR(1) 项集 J 中的 LR(0) 项 \\(\\textit{B}\\rightarrow\\gamma\\cdot\\delta\\) 上 存在一个包含内核项 \\([\\textit{A}\\rightarrow\\alpha\\cdot\\beta, a]\\) 的项集 I 且 \\(\\textit{J}=\\texttt{GOTO}(\\textit{I},\\textit{X})\\)。不管 a 为何值，构造 \\(\\texttt{GOTO}(\\texttt{CLOSURE}(\\{[\\textit{A}\\rightarrow\\alpha\\cdot\\beta, a]\\}), X)\\) 时得到的结果总是包含 \\([\\textit{B}\\rightarrow\\gamma\\cdot\\delta, b]\\)。对于 \\(\\textit{B}\\rightarrow\\gamma\\cdot\\delta\\) 而言，这个向前看符号 b 称为自发生成的。符号 \\(\\$\\) 对于初始项 \\([\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}]\\) 而言总是自发生成的。 其余条件与上一个条件相同，但是 \\(a=b\\)，且计算 \\(\\texttt{GOTO}(\\texttt{CLOSURE}(\\{[\\textit{A}\\rightarrow\\alpha\\cdot\\beta, b]\\}), \\textit{X})\\) 得到的结果中包含 \\([\\textit{B}\\rightarrow\\gamma\\cdot\\delta, b]\\) 的原因是项 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\beta\\) 有一个向前看符号 b。这种情况称为向前看符号从 I 的内核中的 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\beta\\) 传播到了 J 的内核中的 \\(\\textit{B}\\rightarrow\\gamma\\cdot\\delta\\) 上。 需要确定每个 LR(0)　项集中自发生成的向前看符号，同时也要确定向前看符号从哪些项传播到了哪些项。 这个检测实际上很简单。令 \\(\\#\\) 为一个不在当前文法中的符号。令 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\beta\\)　为项集 I 中的一个内核 LR(0) 项。对每个 X 计算 \\(\\textit{J}=\\texttt{GOTO}(\\texttt{CLOSURE}(\\{[\\textit{A}\\rightarrow\\alpha\\cdot\\beta, \\#]\\}), \\textit{X})\\)。检查 J 中的每个内核项的向前看符号集合，如果 \\(\\#\\) 是它的向前看符号，那么向前看符号就从 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\beta\\) 传播到这个项。所有其他的向前看符号都是自发生成的。这个算法还用到了一个性质：J 中的所有内核项中点的左边都是 X，即它们必然是形如 \\(\\textit{B}\\rightarrow\\gamma\\textit{X}\\cdot\\delta\\) 的项。 有 LR(0) 项集 I 的内核 K 构造向前看符号的算法如下： for (K 中的每个项 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\beta\\)) { J := \\(\\texttt{CLOSURE}(\\{[\\textit{A}\\rightarrow\\alpha\\cdot\\beta, \\#]\\})\\); if (\\([\\textit{B}\\rightarrow\\gamma\\cdot\\textit{X}\\delta, a]\\) 在 J 中，且 \\(a\\ne\\#\\)) { 断定 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 中的项 \\(\\textit{B}\\rightarrow\\gamma\\textit{X}\\cdot\\delta\\) 的向前看符号 a 是自发生成的; } if (\\([\\textit{B}\\rightarrow\\gamma\\cdot\\textit{X}\\delta, \\#]\\) 在 J 中) { 断定向前看符号从 I 中的项 \\(\\textit{A}\\rightarrow\\alpha\\cdot\\beta\\) 传播到了 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 中的项 \\(\\textit{B}\\rightarrow\\gamma\\textit{X}\\cdot\\delta\\) 上; } } 现在我们就可以高效地构建 LALR(1) 项集族内核了。 构造 G 的 LR(0) 项集族的内核，保留各个项集的内核项，并计算一个项集 I 的 GOTO 之前先计算 I 的闭包 对每个 LR(0) 项集的内核和每个文法符号 X 应用上面介绍的构造向前看符号的算法，确定 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 中各内核项的哪些向前看符号是自发生成的，并确定向前看符号从 I 中的哪个项被传播 \\(\\texttt{GOTO}(\\textit{I}, \\textit{X})\\) 中的内核上 初始化一个表格，表中给出了每个项集中的每个内核项相关的向前看符号。最初每个项的向前看符号只包括那些我们确定的自发生成的符号 不断扫描所有项集的内核项。当我们访问一个项 i 时，使用表中符号以及自发生成的符号，确定 i 将它的向前看符号传播到了哪些内核项中。项 i 的qq当前向前看符号集合被加到和这些被传播的内核项相关连的向前看符号集中。直到没有新的向前看符号被传播为止 本节前文提到了 LALR(1) 的一个示例，对这个内核，我们先将计算向前看符号的算法应用到项集 \\(\\textit{I}_{0}\\) 的内核上，计算 \\(\\texttt{CLOSURE}(\\{[\\textit{S}^{’}\\rightarrow\\cdot\\textit{S}, \\#]\\})\\) 可以得到 \\[\\b","date":"04-20","objectID":"/2022/compilerprinciple_006/:3:5","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#高效构造-lalr-语法分析表"},{"categories":["CompilerPrinciple"],"content":" 使用二义性文法每个二义性文法都不是 LR 的，但某些类型的二义性文法在语言的归约和实现中很有用。像表达式这样的语言构造，二义性文法能提供比任何等价的无二义性文法更短、更自然的归约。二义性文法的另一个用途是隔离经常出现的语法构造，以对其进行特殊优化。 虽然使用的文法有二义性，但在所有情况下给出消除二义性的规则，使得每个句子只有一颗语法分析树。通过这个方法，语言的归约在整体上是无二义性的，有时还可以构造出遵循这个二义性解决方法的 LR 语法分析器。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:4:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#使用二义性文法"},{"categories":["CompilerPrinciple"],"content":" 用优先级和结合性解决冲突回想之前一直提及的示例 id + id * id，其无二义性文法 \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow\\textit{E}+\\textit{T}\\\\ \\textit{E}\u0026\\rightarrow\\textit{T}\\\\ \\textit{T}\u0026\\rightarrow\\textit{T}*\\textit{F}\\\\ \\textit{T}\u0026\\rightarrow\\textit{F}\\\\ \\textit{F}\u0026\\rightarrow(\\textit{E})\\\\ \\textit{F}\u0026\\rightarrow\\textbf{id}\\end{aligned}.\\] 但其二义性文法 \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow\\textit{E}+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}*\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow(\\textit{E})\\\\ \\textit{E}\u0026\\rightarrow\\textbf{id}\\end{aligned}.\\] 无二义性的版本指定了运算符 + 和 * 的优先级和结合性。但二义性文法没有给出这样的信息，可以轻易改变运算符的优先级和结合性，且不用修改文法产生式，也不用修改语法分析器的状态数目。 首先给出二义性文法的 LR(0) 项集 \\(\\textit{I}_{0}\\): \\[\\begin{aligned} \\textit{E}^{’}\u0026\\rightarrow\\cdot\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}*\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot(\\textit{E})\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textbf{id} \\end{aligned}\\] \\(\\textit{I}_{1}\\): \\[\\begin{aligned} \\textit{E}^{’}\u0026\\rightarrow\\textit{E}\\cdot\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot*\\textit{E} \\end{aligned}\\] \\(\\textit{I}_{2}\\): \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow(\\cdot\\textit{E})\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}*\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot(\\textit{E})\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textbf{id} \\end{aligned}\\] \\(\\textit{I}_{3}\\): \\(\\textit{E}\\rightarrow\\textbf{id}\\cdot\\) \\(\\textit{I}_{4}\\): \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow\\textit{E}+\\cdot\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}*\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot(\\textit{E})\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textbf{id} \\end{aligned}\\] \\(\\textit{I}_{5}\\): \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow\\textit{E}*\\cdot\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textit{E}*\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\cdot(\\textit{E})\\\\ \\textit{E}\u0026\\rightarrow\\cdot\\textbf{id} \\end{aligned}\\] \\(\\textit{I}_{6}\\): \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow(\\textit{E}\\cdot)\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot*\\textit{E} \\end{aligned}\\] \\(\\textit{I}_{7}\\): \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow\\textit{E}+\\textit{E}\\cdot\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot*\\textit{E} \\end{aligned}\\] \\(\\textit{I}_{8}\\): \\[\\begin{aligned} \\textit{E}\u0026\\rightarrow\\textit{E}*\\textit{E}\\cdot\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot+\\textit{E}\\\\ \\textit{E}\u0026\\rightarrow\\textit{E}\\cdot*\\textit{E} \\end{aligned}\\] \\(\\textit{I}_{9}\\): \\(\\textit{E}\\rightarrow(\\textit{E})\\cdot\\) 现在假设语法分析器处理完了 id + id，栈中的状态为 \\(0, 1, 4, 7\\)，剩下的输入为 * id，这会产生移入/归约冲突 如果 * 的优先级高于 +，语法分析器将移入 * 如果 + 的优先级高于 *，语法分析器将归约 \\(\\textit{E}\\rightarrow\\textit{E}+\\textit{E}\\) + id，这会产生移入/归约冲突。如果 + 是左结合的，那么按照 \\(\\textit{E}\\rightarrow\\textit{E}+\\textit{E}\\) 归约 因此利用优先级和结合性，可以得到一个与 SLR 近似的语法动作表。 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:4:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#用优先级和结合性解决冲突"},{"categories":["CompilerPrinciple"],"content":" 悬空-else 的二义性再说说悬空 else 的文法 \\[\\begin{aligned} smtm \u0026\\rightarrow\\ \\textbf{if}\\ expr\\ \\textbf{then}\\ stmt\\ \\textbf{else}\\ stmt\\\\ \u0026|\\ \\textbf{if}\\ expr\\ \\textbf{then}\\ stmt\\\\ \u0026|\\ \\textbf{other} \\end{aligned}\\] 将其简写为如下这个增广文法 \\[\\begin{aligned} \\textit{S}^{’} \u0026\\rightarrow \\textit{S}\\\\ \\textit{S} \u0026\\rightarrow i\\textit{S}e\\textit{S} \\ |\\ i\\textit{S} \\ |\\ a \\end{aligned}\\] 它有如下 LR(0) 状态 \\(\\textit{I}_{0}\\): \\[\\begin{aligned} \\textit{S}^{’} \u0026\\rightarrow \\cdot\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}i\\textit{S}e\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}i\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}a \\end{aligned}\\] \\(\\textit{I}_{1}\\): \\(\\textit{S}^{’} \\rightarrow \\cdot\\textit{S}\\) \\(\\textit{I}_{2}\\): \\[\\begin{aligned} \\textit{S} \u0026\\rightarrow i\\cdot\\textit{S}e\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow i\\cdot\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}i\\textit{S}e\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}i\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}a \\end{aligned}\\] \\(\\textit{I}_{3}\\): \\(\\textit{S}\\rightarrow{}a\\cdot\\) \\(\\textit{I}_{4}\\): \\[\\begin{aligned} \\textit{S} \u0026\\rightarrow i\\textit{S}\\cdot{}e\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow i\\textit{S}\\cdot \\end{aligned}\\] \\(\\textit{I}_{5}\\): \\[\\begin{aligned} \\textit{S} \u0026\\rightarrow i\\textit{S}e\\cdot\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}i\\textit{S}e\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}i\\textit{S}\\\\ \\textit{S} \u0026\\rightarrow \\cdot{}a \\end{aligned}\\] \\(\\textit{I}_{6}\\): \\(\\textit{S}\\rightarrow{}i\\textit{S}e\\textit{S}\\cdot\\) 在 \\(\\textit{I}_{4}\\) 上有一个移入/归约冲突。项 \\(\\textit{S}\\rightarrow{}i\\textit{S}\\cdot{}e\\textit{S}\\) 要求移入 e，但 \\(\\texttt{FOLLOW}(S) = \\{e, \\$\\}\\)，项 \\(\\textit{S}\\rightarrow{}i\\textit{S}\\cdot\\) 在输入为 e 时进行归约。我们可以要求在输入 e 时执行移入操作，可以得到一个近似无二义性的 LR 分析表。 当然如此解决悬空 else 问题后，我们可以为 iiaea 语法产生正确的语法分析动作。 编号 栈 符号 输入 动作 1 0 \\(iiaea\\$\\) 移入 2 02 \\(i\\) \\(iaea\\$\\) 移入 3 022 \\(ii\\) \\(aea\\$\\) 移入 4 0223 \\(iia\\) \\(ea\\$\\) 归约 \\(\\textit{S}\\rightarrow{}a\\) 5 0224 \\(ii\\textit{S}\\) \\(ea\\$\\) 移入 6 02245 \\(ii\\textit{S}e\\) \\(a\\$\\) 移入 7 022453 \\(ii\\textit{S}ea\\) \\(\\$\\) 归约 \\(\\textit{S}\\rightarrow{}a\\) 8 022456 \\(ii\\textit{S}e\\textit{S}\\) \\(\\$\\) 归约 \\(\\textit{S}\\rightarrow{}i\\textit{S}e\\textit{S}\\) 9 024 \\(i\\textit{S}\\) \\(\\$\\) 归约 \\(\\textit{S}\\rightarrow{}i\\textit{S}\\) 10 01 \\(\\textit{S}\\) \\(\\$\\) 接受 ","date":"04-20","objectID":"/2022/compilerprinciple_006/:4:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 3","uri":"/2022/compilerprinciple_006/#悬空-else-的二义性"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第四章 4.4 读书笔记","date":"04-12","objectID":"/2022/compilerprinciple_005/","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/"},{"categories":["CompilerPrinciple"],"content":"自顶向下语法可以被看作输入串构造语法分析树的问题，从语法分析树的根结点开始，深度优先创建这棵树的各个结点。 对于输入 id + id * id，可以根据最左推导序列产生语法分析树序列： \\[\\begin{aligned} E \u0026\\rightarrow TE^{’}\\\\ E^{’} \u0026\\rightarrow +\\ T\\ E^{’}\\,|\\,\\varepsilon\\\\ T \u0026\\rightarrow FT^{’}\\\\ T^{’} \u0026\\rightarrow *\\ F\\ T^{’}\\,|\\,\\varepsilon\\\\ F \u0026\\rightarrow (\\ E\\ )\\,|\\,\\textbf{id}\\\\ \\end{aligned}\\] 根据左推导有如下自顶向下分析过程 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:0:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#"},{"categories":["CompilerPrinciple"],"content":" 递归下降的语法分析递归下降的语法分析由一组过程组成，每个非终结符有一个对应的过程，程序的执行从开始符号对应的过程开始，如果这个过程扫描了整个输入串，就停止执行并宣布语法分析完成。 通用递归下降分析技术可能需要回溯，即重复扫描输入串。在 PL 构造进行语法分析时很少回溯，因此需要回溯的语法分析器并不常见。自然语言分析的场合，回溯也不高效，因此更加倾向基于表格的语法分析方法，如 DP 或 Earley 方法。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#递归下降的语法分析"},{"categories":["CompilerPrinciple"],"content":" 一个简单的示例考虑文法 \\[\\begin{aligned} S \u0026\\rightarrow c\\,A\\,d\\\\ A \u0026\\rightarrow a\\,b \\ |\\ a\\\\ \\end{aligned}\\] 自顶向下构造串 \\(w=cad\\)，初始结点指向 w 的第一个字符，即标号为 S 的结点指向 c，将会得到图 1 (a) 中的树，字符c 匹配。 Figure 1: 自顶向下语法分析器过程 将 A 用 \\(\\textit{A} \\rightarrow ab\\) 展开得到图 1 (b) 的树，第二个字符 a 匹配，此时指针推进到第三个字符 d。由于 b 与 d 不匹配，将报告失败，并回到 A 开始尝试之前未进行的且有可能匹配的其他产生式，图 1 (c) 所示。 左递归的文法会让递归下降语法分析器陷入死循环，即使有回溯也是如此。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:1:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#一个简单的示例"},{"categories":["CompilerPrinciple"],"content":" FIRST 与 FOLLOW自顶向下的语法分析器构造都可以采用 FIRST 和 FOLLOW 来实现。这两个函数让我们可以根据下一个输入符来选择如何应用产生式。而 painc 恢复可以由 FOLLOW 产生的词法单元合集作为同步词法单元。 \\(\\texttt{FIRST}(\\alpha)\\) 被定义为可以从 \\(\\alpha\\) 推导得到的串的首符号集合。如果 \\(\\alpha \\xRightarrow{*}\\varepsilon\\)，那么 \\(\\varepsilon\\) 也在 \\(\\texttt{FIRST}(\\alpha)\\) 中。在之前的示例中，就是 \\(\\textit{A} \\xRightarrow{*} c\\gamma\\)，因此 \\(c\\) 在 \\(\\texttt{FIRST}(\\alpha)\\) 中，如图 2 所示。 Figure 2: 终结符 c 在 FIRST(A) 中且 a 在 FOLLOW(A) 中 考虑两个 A 的产生式 \\(\\textit{A} \\rightarrow \\alpha\\,|\\,\\beta\\)，其中 \\(\\texttt{FIRST}(\\alpha)\\) 和 \\(\\texttt{FIRST}(\\beta)\\) 是不相交的集合。那么只需要查看下一个输入符号 a 就可以选择产生式。因为 a 只能出现在 \\(\\texttt{FIRST}(\\alpha)\\) 或 \\(\\texttt{FIRST}(\\beta)\\) 中，但不能同时出现在两个集合中。 对于非终结符 A，\\(\\texttt{FOLLOW}(\\textit{A})\\) 被定义为可能在某些句型中紧跟在 A 右边的终结符的集合。如图 2 所示 \\(\\textit{S}\\xRightarrow{*}\\alpha\\textit{A}a\\beta\\) 的推导，\\(\\alpha\\) 和 \\(\\beta\\) 是文法符号串，终结符 a 在 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:1:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#first-与-follow"},{"categories":["CompilerPrinciple"],"content":" 计算 FIRST计算各个文法符号 X 的 \\(\\texttt{FRIST}(\\textit{X})\\) 时，不断应用以下规则，直到没有新的终结符或 \\(\\varepsilon\\) 可以被加入。 如果 X 是一个终结符，那么 \\(\\texttt{FIRST}(\\textit{X})=\\textit{X}\\) 如果 X 是一个非终结符，且 \\(\\textit{X} \\rightarrow \\textit{Y}_{1}\\textit{Y}_{2}\\cdots\\textit{Y}_{k} (k \\ge 1)\\) 是一个产生式 如果对于某个 i，a 在\\(\\texttt{FIRST}(\\textit{Y}_{i})\\) 中且 \\(\\varepsilon\\) 在所有的 \\(\\texttt{FIRST}(\\textit{Y}_{1})\\)、 \\(\\texttt{FIRST}(\\textit{Y}_{2})\\)、\\(\\cdots\\)、 \\(\\texttt{FIRST}(\\textit{Y}_{i-1})\\) 中，那么就把 a 加入到 \\(\\texttt{FIRST}(\\textit{X})\\) 中，即 \\(\\textit{Y}_{1}\\cdots\\textit{Y}_{i-1}\\xRightarrow{*}\\varepsilon\\) 如果对于所有的 \\(j=1, 2, \\cdots, k\\)，\\(\\varepsilon\\) 在 \\(\\texttt{FIRST}(\\textit{Y}_{j})\\) 中，那么将 \\(\\varepsilon\\) 加入到 \\(\\texttt{FIRST}(\\textit{X})\\) 中。 如果 \\(\\textit{Y}_{1}\\) 不能推导出 \\(\\varepsilon\\)，那么我们就不再向 \\(\\texttt{FIRST}(\\textit{X})\\) 中加入任何符号，但如果 \\(\\textit{Y}_{1}\\xRightarrow{*}\\varepsilon\\)，就加上 \\(\\texttt{FIRST}(\\textit{Y}_{2})\\)，依此类推。 如果 \\(\\textit{X}\\rightarrow\\varepsilon\\) 是一个产生式，那么将 \\(\\varepsilon\\) 加入到 \\(\\texttt{FIRST}(\\textit{X})\\) 中 简单地说，计算任何串 \\(\\textit{X}_{1}\\textit{X}_{2}\\cdots\\textit{X}_{n}\\) 的 FIRST 集合：向 \\(\\texttt{FIRST}(\\textit{X}_{1}\\textit{X}_{2}\\cdots\\textit{X}_{n})\\) 加入 \\(\\texttt{FIRST}(\\textit{X}_{1})\\) 的所有非 \\(\\varepsilon\\) 运算符，如果 \\(\\varepsilon\\) 在 \\(\\texttt{FIRST}(\\textit{X}_{1})\\) 中，那么加入 \\(\\texttt{FIRST}(\\textit{X}_{2})\\) 的所有非 \\(\\varepsilon\\) 运算符，如果 \\(\\varepsilon\\) 在 \\(\\texttt{FIRST}(\\textit{X}_{1})\\) 和 \\(\\texttt{FIRST}(\\textit{X}_{2})\\) 中则加入 \\(\\texttt{FIRST}(\\textit{X}_{3})\\) 的所有非 \\(\\varepsilon\\) 运算符，依此类推。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:1:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#计算-first"},{"categories":["CompilerPrinciple"],"content":" 计算 FOLLOW计算各个文法符号 X 的 \\(\\texttt{FRIST}(\\textit{X})\\) 时，不断应用以下规则，直到没有新的终结符可以被加入。 将 $ 放到 \\(\\texttt{FOLLOW}(\\textit{S})\\) 中，其中 S 是开始符号，而 $ 是输入右端的结束符号 如果存在一个产生式 \\(\\textit{A}\\rightarrow\\alpha\\textit{B}\\beta\\)，那么 \\(\\texttt{FIRST}(\\beta)\\) 中除 \\(\\varepsilon\\) 外的所有符号都在 \\(\\texttt{FOLLOW}(\\textit{B})\\) 中 如果存在一个产生式 \\(\\textit{A}\\rightarrow\\alpha\\textit{B}\\)，或存在产生式 \\(\\textit{A}\\rightarrow\\alpha\\textit{B}\\beta\\) 且 \\(\\texttt{FIRST}(\\beta)\\) 包含 \\(\\varepsilon\\)，那么 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中的所有符号都在 \\(\\texttt{FOLLOW}(\\textit{B})\\) 中。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:1:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#计算-follow"},{"categories":["CompilerPrinciple"],"content":" 示例的 FIRST 和 FOLLOW 计算对于示例 id + id * id，我们可以如此计算 FIRST 与 FOLLOW。 \\(\\texttt{FIRST}(\\textit{F})\\ =\\ \\texttt{FIRST}(\\textit{T})\\ =\\ \\texttt{FIRST}(\\textit{E})\\ =\\ \\{(,\\textbf{id}\\}\\)。F 的产生式以终结符 id 和左括号开头；T 只有 F 开头的产生式，F 不能推出 \\(\\varepsilon\\)，因此 \\(\\texttt{FIRST}(\\textit{T})\\) 与 \\(\\texttt{FIRST}(\\textit{F})\\) 必然相同；对 \\(\\texttt{FIRST}(\\textit{E})\\) 论据相同。 \\(\\texttt{FIRST}(\\textit{E}^{’}) = \\{+, \\varepsilon\\}\\)。\\(\\textit{E}^{’}\\) 的两个产生式中，一个以 \\(+\\) 开头，而另一个为 \\(\\varepsilon\\)。只要有一个 \\(\\varepsilon\\) 我们就将其加入 FIRST 中。 \\(\\texttt{FIRST}(\\textit{T}^{’}) = \\{*, \\varepsilon\\}\\)。论据同 2。 \\(\\texttt{FOLLOW}(\\textit{E}) = \\texttt{FOLLOW}(\\textit{E}^{’}) = \\{), \\$\\}\\)。因为 E 是开始符号，因此 \\(\\texttt{FOLLOW(\\textit{E})}\\) 一定包含 \\(\\$\\)。产生式 (E) 说明了右括号为什么在 FOLLOW 当中。对于 \\(\\textit{E}^{’}\\)，非终结符只出现在了 E 产生式的尾部，因此 \\(\\texttt{FOLLOW}(\\textit{E})\\) 必然与 \\(\\texttt{FOLLOW}(\\textit{E}^{’})\\) 相同。 \\(\\texttt{FOLLOW}(\\textit{T}) = \\texttt{FOLLOW}(\\textit{T}^{’}) = \\{+, ), \\$\\}\\)。在所有产生式体中 T 只有 \\(\\textit{E}^{’}\\) 跟在后面，因此 \\(\\texttt{FIRST}(\\textit{E}^{’})\\) 中除 \\(\\varepsilon\\) 外所有符号都在 \\(\\texttt{FOLLOW}(\\textit{T})\\) 中。另外 \\(\\texttt{FIRST}(\\textit{E}^{’})\\) 包含 \\(\\varepsilon\\) (即 \\(\\textit{E}^{’} \\xRightarrow[lm]{} \\varepsilon\\))，且 E 产生体中 \\(\\textit{E}^{’}\\) 跟在 T 之后，因此 \\(\\texttt{FOLLOW}(\\textit{E})\\) 中的所有符号出现在 \\(\\texttt{FOLLOW}(\\textit{T})\\) 中。而 \\(\\textit{T}^{’}\\) 只出现在 T 产生式的末尾，因此 \\(\\texttt{FOLLOW}(\\textit{T}) = \\texttt{FOLLOW}(\\textit{T}^{’})\\) \\(\\texttt{FOLLOW}(\\textit{F}) = \\{+, *, ), \\$\\}\\)。论据同 5。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:1:5","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#示例的-first-和-follow-计算"},{"categories":["CompilerPrinciple"],"content":" LL(1) 文法LL(1) 文法，表示从左向右扫描输入，产生最左推导的文法，而 1 表示每一步只需要向前看一个输入符号来决定语法分析动作。 LL(1) 文法我们可以构造除预测分析器，即不需要回溯的递归下降语法。但是需要注意的是，左递归文法和二义性文法都不是 LL(1) 的。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#ll--1--文法"},{"categories":["CompilerPrinciple"],"content":" 预测分析器的转换图转换图有助于将预测分析器可视化，构造一个文法的转换图，首先要消除左递归，然后对文法提取左公因子，之后对每个非终结符 A 进行： 创建一个初始状态和结束状态 对每个产生式 \\(\\textit{A} \\rightarrow \\textit{X}_{1}\\textit{X}_{2}\\cdots\\textit{X}_{n}\\) 创建一个从初始状态到结束状态的路经 预测分析器的转换图与词法分析器的转换图是不同的：分析器对每个非终结符都有图，边的标号可以是词法单元也可以是非终结符；词法分析器表示下一个输入符号输入时应该执行的转换。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:2:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#预测分析器的转换图"},{"categories":["CompilerPrinciple"],"content":" LL(1) 文法判断当且仅当文法 G 的任意两个不同的表达式 \\(\\textit{A} \\rightarrow \\alpha\\,|\\,\\beta\\) 满足以下条件时，G 是 LL(1) 文法： 不存在终结符 a 使得 \\(\\alpha\\) 和 \\(\\beta\\) 都能够推导出以 a 开头的串 \\(\\alpha\\) 和 \\(\\beta\\) 中最多只有一个可以推导出空 如果 \\(\\beta \\xRightarrow{*} \\varepsilon\\)，那么 \\(\\alpha\\) 不能推导出任何以 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中某个终结符开头的串。反之亦然。 前两个条件说明 \\(\\texttt{FIRST}(\\alpha)\\) 和 \\(\\texttt{FIRST}(\\beta)\\) 是不相交的集合。第三个条件说明，如果 \\(\\varepsilon\\) 在 \\(\\texttt{FIRST}(\\beta)\\) 中，那么 \\(\\texttt{FIRST}(\\alpha)\\) 与 \\(\\texttt{FOLLOW}(\\textit{A})\\) 是不相交的集合。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#ll--1--文法判断"},{"categories":["CompilerPrinciple"],"content":" 构造 LL(1) 文法预测分析表预测分析表是构造 LL(1) 文法预测分析器的重要方法。也需要用到 \\(\\texttt{FIRST}\\) 和 \\(\\texttt{FOLLOW}\\) 函数。 输入：文法 G 输出：预测分析表 M 方法：对于文法 G 每个产生式 \\(\\textit{A} \\rightarrow \\alpha\\) 有 1. 对于 \\(\\texttt{FIRST}(\\alpha)\\) 中的每个终结符 a，将 \\(\\textit{A} \\rightarrow \\alpha\\) 加入到 \\(\\textit{M}[\\textit{A}, a]\\) 中 2. 如果 \\(\\varepsilon\\) 在 \\(\\texttt{FIRST}(\\alpha)\\) 中，那么对于 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中的每个终结符 b，将 \\(\\textit{A} \\rightarrow \\alpha\\) 加入到 \\(\\textit{M}[\\textit{A}, b]\\) 中。如果 \\(\\$\\) 也在 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中，那也将 \\(\\textit{A} \\rightarrow \\alpha\\) 加入到 \\(\\textit{M}[\\textit{A}, \\$]\\) 中 对于示例 id + id * id 可以构造出如下分析表 非终结符 id + * ( ) $ E \\(\\textit{E} \\rightarrow \\textit{TE}^{’}\\) \\(\\textit{E} \\rightarrow \\textit{TE}^{’}\\) \\(\\textit{E}^{’}\\) \\(\\textit{E}^{’} \\rightarrow + \\textit{TE}^{’}\\) \\(\\textit{E}^{’} \\rightarrow \\varepsilon\\) \\(\\textit{E}^{’} \\rightarrow \\varepsilon\\) T \\(\\textit{T} \\rightarrow \\textit{FT}^{’}\\) \\(\\textit{T} \\rightarrow \\textit{FT}^{’}\\) \\(\\textit{T}^{’}\\) \\(\\textit{T}^{’} \\rightarrow \\varepsilon\\) \\(\\textit{T}^{’} \\rightarrow * \\textit{FT}^{’}\\) \\(\\textit{T}^{’} \\rightarrow \\varepsilon\\) \\(\\textit{T}^{’} \\rightarrow \\varepsilon\\) F \\(\\textit{F} \\rightarrow \\textbf{id}\\) \\(\\textit{F} \\rightarrow (\\textit{E})\\) 如果一个文法是左递归或有二义性的，那么 M 中至少会包含一个多重定义的条目。比如悬空-else 文法 \\[\\begin{aligned} \\textit{S} \u0026\\rightarrow i\\textit{E}t\\textit{SS}^{’}\\ |\\ a\\\\ \\textit{S}^{’} \u0026\\rightarrow e\\textit{S}\\ |\\ \\varepsilon\\\\ \\textit{E} \u0026\\rightarrow b\\\\ \\end{aligned}\\] 它的分析表如下 非终结符 a b e i t \\(\\$\\) S \\(\\textit{S} \\rightarrow a\\) \\(\\textit{S} \\rightarrow i\\textit{E}t\\textit{SS}^{’}\\) \\(\\textit{S}^{’}\\) \\(\\textit{S}^{’} \\rightarrow \\varepsilon\\), \\(\\textit{S}^{’} \\rightarrow e\\textit{S}\\) \\(\\textit{S}^{’} \\rightarrow \\varepsilon\\) E \\(\\textit{E} \\rightarrow b\\) 可以看到 \\(\\textit{M}[\\textit{S}^{’}, e]\\) 不止一个条目，LL(1) 文法预测分析表会披露出文法中存在的二义性。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#构造-ll--1--文法预测分析表"},{"categories":["CompilerPrinciple"],"content":" 非递归的预测分析构造递归的预测分析是隐式使用栈，可以采用迭代的方式显示使用栈，如果 w 是已匹配完成的输入部分，那么栈中保存的文法符号序列 \\(\\alpha\\) 满足 \\[S \\xRightarrow[lm]{*}w\\alpha.\\] 由分析表驱动的语法分析器有一个输入缓冲区，一个包含了文法符号序列的栈，一个预测分析表，以及一个输出流。 它的输入缓冲区中包含要进行语法分析的串，串后面跟着结束标记\\(\\$\\)。我们复用符号 \\(\\$\\)来标记栈底，在开始时刻栈中\\(\\$\\)的上方的开始符号S。 预测分析程序考虑栈顶符号 X 和当前输入符号 a，如果 X 是非终结符，查询分析表 \\(\\textit{M}[\\textit{X}, a]\\) 来选择一个 X 的产生式；否则检查终结符 X 和当前输入符 a 是否匹配。语法分析器的行为可以用配置 (configuration) 来描述， configuration 描述了栈中的内容和余下的输入。 输入：一个串 w，文法 G 的预测分析表 M 输出：如果 w 在 \\(\\textit{L}(\\textit{G})\\) 中，输出 w 的最左推导，否则输出错误提示 方法：最初输入缓冲区中是 \\(w\\$\\)，而 G 开始符号 S 位于栈顶，它下面是 \\(\\$\\)。 算法如下： #+begin_verse 设置 ip (输入指针) 使其指向 w 的第一个符号; 令 X 为栈顶符号; while (\\(\\textit{X} \\ne \\$\\)) { if (X == ip 指向的符号 a) { 执行出栈操作; 将 ip 向前移动一个位置; } else if (X 是一个终结符 or \\(\\textit{M}[\\textit{X}, a]\\) 是一个报错条目) { error(); } else if (\\(\\textit{M}[\\textit{X}, a] = \\textit{X} \\rightarrow Y_{1}Y_{2}\\cdots Y_{k}\\)) { 输出产生式 \\(\\textit{X} \\rightarrow Y_{1}Y_{2}\\cdots Y_{k}\\); 执行出栈操作; 将 \\(Y_{1}Y_{2}\\cdots Y_{k}\\) 入栈，其中 \\(Y_{1}\\) 位于栈顶; } 令 X 为栈顶符号; } #+end_vers 在之前已经有了示例 id + id * id 的预测分析表了，现在展示以下其执行步骤 已匹配 栈 输入 操作 \\(\\textit{E}\\$\\) \\(\\textbf{id}+\\textbf{id}*\\textbf{id}\\$\\) \\(\\textit{TE}^{’}\\$\\) \\(\\textbf{id}+\\textbf{id}*\\textbf{id}\\$\\) 输出 \\(\\textit{E}\\rightarrow\\textit{TE}^{’}\\) \\(\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}+\\textbf{id}*\\textbf{id}\\$\\) 输出 \\(\\textit{T}\\rightarrow\\textit{FT}^{’}\\) \\(\\textbf{id}\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}+\\textbf{id}*\\textbf{id}\\$\\) 输出 \\(\\textit{F}\\rightarrow\\textbf{id}\\) id \\(\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(+\\textbf{id}*\\textbf{id}\\$\\) 匹配 id id \\(\\textit{E}^{’}\\$\\) \\(+\\textbf{id}*\\textbf{id}\\$\\) 输出 \\(\\textit{T}^{’}\\rightarrow\\varepsilon\\) id \\(+\\textit{TE}^{’}\\$\\) \\(+\\textbf{id}*\\textbf{id}\\$\\) 输出 \\(\\textit{E}^{’}\\rightarrow+\\textit{TE}^{’}\\) \\(\\textbf{id}+\\) \\(\\textit{TE}^{’}\\$\\) \\(\\textbf{id}*\\textbf{id}\\$\\) 匹配 \\(+\\) \\(\\textbf{id}+\\) \\(\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}*\\textbf{id}\\$\\) 输出 \\(\\textit{T}\\rightarrow\\textit{FT}^{’}\\) \\(\\textbf{id}+\\) \\(\\textbf{id}\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}*\\textbf{id}\\$\\) 输出 \\(\\textit{F}\\rightarrow\\textbf{id}\\) \\(\\textbf{id}+\\textbf{id}\\) \\(\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(*\\textbf{id}\\$\\) 匹配 id \\(\\textbf{id}+\\textbf{id}\\) \\(*\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(*\\textbf{id}\\$\\) 输出 \\(\\textit{T}^{’}\\rightarrow*\\textit{FT}^{’}\\) \\(\\textbf{id}+\\textbf{id}*\\) \\(\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}\\$\\) 匹配 \\(*\\) \\(\\textbf{id}+\\textbf{id}*\\) \\(\\textbf{id}\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}\\$\\) 输出 \\(\\textit{F}\\rightarrow\\textbf{id}\\) \\(\\textbf{id}+\\textbf{id}*\\textbf{id}\\) \\(\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(\\$\\) 匹配 id \\(\\textbf{id}+\\textbf{id}*\\textbf{id}\\) \\(\\textit{E}^{’}\\$\\) \\(\\$\\) 输出 \\(\\textit{T}^{’}\\rightarrow\\varepsilon\\) \\(\\textbf{id}+\\textbf{id}*\\textbf{id}\\) \\(\\$\\) \\(\\$\\) 输出 \\(\\textit{E}^{’}\\rightarrow\\varepsilon\\) ","date":"04-12","objectID":"/2022/compilerprinciple_005/:3:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#非递归的预测分析"},{"categories":["CompilerPrinciple"],"content":" 预测分析中的错误恢复当栈顶的终结符和下一个输入不匹配时，或当前非终结符 A 处于栈顶但 \\(\\textit{M}[\\textit{A}, a]\\) 为 error 时，预测分析器就会检测到语法错误。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:4:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#预测分析中的错误恢复"},{"categories":["CompilerPrinciple"],"content":" panic 模式panic 模式的错误恢复是基于以下思想：语法分析器忽略输入中的一些符号，直到输入中出现同步词法单元。这依赖于同步集合的选取。选取原则是使得语法分析器能够从实践中可能遇到的错误中快速恢复。有一些启发式的规则： 将 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中的所有符号都放在非终结符 A 的同步集合中。如果不断忽略一些词法单元，直到碰到了 \\(\\texttt{FOLLOW}(\\textit{A})\\) 中的某个元素，再将 A 从栈中弹出，可能语法分析器就能继续进行了。 只用 \\(\\texttt{FOLLOW}(\\textit{A})\\) 是不够的。如 C 语言采用分号结尾，而遗漏分号可能导致语法分析器忽略下一个语句开头的关键字。我们可以将较高层次的开始符加入到较低层次的同步集合中。 如果将 \\(\\texttt{FIRST}(\\textit{A})\\) 加入到非终结符 A 的同步集合，那么当其中符号出现在输入中，可能可以根据 A 继续进行语法分析 如果一个非终结符可以产生空串，那么可以将空串作为默认值使用。这么做可能会推迟错误检测，但不会遗漏。并且减少需要考虑的非终结符数量。 如果栈顶的一个终结符不能和输入匹配，最简单的方法是将其弹出栈，发出消息称已经插入这个终结符，并继续语法分析。这个方法是将其他词法单元的合集作为同步集合。 因此我们可以将 FOLLOW 作为同步词法单元，重绘预测分析表，示例 id + id * id 的预测分析表如下 非终结符 id + * ( ) $ E \\(\\textit{E} \\rightarrow \\textit{TE}^{’}\\) \\(\\textit{E} \\rightarrow \\textit{TE}^{’}\\) synch synch \\(\\textit{E}^{’}\\) \\(\\textit{E}^{’} \\rightarrow + \\textit{TE}^{’}\\) \\(\\textit{E}^{’} \\rightarrow \\varepsilon\\) \\(\\textit{E}^{’} \\rightarrow \\varepsilon\\) T \\(\\textit{T} \\rightarrow \\textit{FT}^{’}\\) synch \\(\\textit{T} \\rightarrow \\textit{FT}^{’}\\) synch synch \\(\\textit{T}^{’}\\) \\(\\textit{T}^{’} \\rightarrow \\varepsilon\\) \\(\\textit{T}^{’} \\rightarrow * \\textit{FT}^{’}\\) \\(\\textit{T}^{’} \\rightarrow \\varepsilon\\) \\(\\textit{T}^{’} \\rightarrow \\varepsilon\\) F \\(\\textit{F} \\rightarrow \\textbf{id}\\) synch synch \\(\\textit{F} \\rightarrow (\\textit{E})\\) synch synch 如果输入串是 \\(+\\textbf{id}*+\\textbf{id}\\) 这样的错误输入，可以得到如此语法分析过程 已匹配 栈 输入 说明 \\(\\textit{E}\\$\\) \\(+\\textbf{id}*+\\textbf{id}\\$\\) \\(\\textit{E}\\$\\) \\(\\textbf{id}*+\\textbf{id}\\$\\) 错误，\\(+\\)略过 \\(\\textit{TE}^{’}\\$\\) \\(\\textbf{id}*+\\textbf{id}\\$\\) 输出 \\(\\textit{E}\\rightarrow\\textit{TE}^{’}\\) \\(\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}*+\\textbf{id}\\$\\) 输出 \\(\\textit{T}\\rightarrow\\textit{FT}^{’}\\) \\(\\textbf{id}\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}*+\\textbf{id}\\$\\) 输出 \\(\\textit{F}\\rightarrow\\textbf{id}\\) id \\(\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(*+\\textbf{id}\\$\\) 匹配 id id \\(*\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(*+\\textbf{id}\\$\\) 输出 \\(\\textit{T}^{’}\\rightarrow*\\textit{FT}^{’}\\) \\(\\textbf{id}*\\) \\(\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(+\\textbf{id}\\$\\) 匹配 * \\(\\textbf{id}*\\) \\(\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(+\\textbf{id}\\$\\) \\(\\textit{M}[\\textit{F},+]\\), 弹出 F \\(\\textbf{id}*\\) \\(\\textit{E}^{’}\\$\\) \\(+\\textbf{id}\\$\\) 输出 \\(\\textit{T}^{’}\\rightarrow\\varepsilon\\) \\(\\textbf{id}*\\) \\(+\\textit{TE}^{’}\\$\\) \\(+\\textbf{id}\\$\\) 输出 \\(\\textit{E}^{’}\\rightarrow+\\textit{TE}^{’}\\) \\(\\textbf{id}*+\\) \\(\\textit{TE}^{’}\\$\\) \\(\\textbf{id}\\$\\) 匹配 \\(+\\) \\(\\textbf{id}*+\\) \\(\\textit{FT}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}\\$\\) 输出 \\(\\textit{T}\\rightarrow\\textit{FT}^{’}\\) \\(\\textbf{id}*+\\) \\(\\textbf{id}\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(\\textbf{id}\\$\\) 输出 \\(\\textit{F}\\rightarrow\\textbf{id}\\) \\(\\textbf{id}*+\\textbf{id}\\) \\(\\textit{T}^{’}\\textit{E}^{’}\\$\\) \\(\\$\\) 匹配 id \\(\\textbf{id}*+\\textbf{id}\\) \\(\\textit{E}^{’}\\$\\) \\(\\$\\) 输出 \\(\\textit{T}^{’}\\rightarrow\\varepsilon\\) \\(\\textbf{id}*+\\textbf{id}\\) \\(\\$\\) \\(\\$\\) 输出 \\(\\textit{E}^{’}\\rightarrow\\varepsilon\\) ","date":"04-12","objectID":"/2022/compilerprinciple_005/:4:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#panic-模式"},{"categories":["CompilerPrinciple"],"content":" 短语层次恢复在预测分析表的空白条目中填入过程指针，即可用过程改变、插入或删除输入的符号，并发出适当的错误消息，或执行一些栈操作 (改变栈符号或压入新符号)。 ","date":"04-12","objectID":"/2022/compilerprinciple_005/:4:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 2","uri":"/2022/compilerprinciple_005/#短语层次恢复"},{"categories":["Emacs"],"content":"GinShio | GinShio's Doom Emacs Configuration","date":"04-08","objectID":"/2022/doom_emacs_configuration/","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/"},{"categories":["Emacs"],"content":" 信息 本篇是基于 tecosaur 的 Emacs 配置 大幅缩减版本，如果你对 Org Mode 感兴趣可以看他制作的 This Month in Org。 本篇标题和副标题均采用原标题的中译 Title: Doom Emacs Configuration Subtitle: The Methods, Management, and Menagerie of Madness — in meticulous detail 让我们改变对程序构建的传统态度：与其想象我们的主要任务是指导计算机做什么，不如专注于向人们解释我们想让计算机做什么。 — 高德纳 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:0:0","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#"},{"categories":["Emacs"],"content":" 引言与其说不喜欢 IDE，倒不如说现在太菜了用不上 IDE，况且 Linux 下除了 JetBrains 家的也没其他能用的了。 不过这都是后话了，最初用的是 Dev-C++ 和 VS 2017 ，说实话 VS 好看但太大了，对我这种写 Hello World 的人来说太浪费了。其实简单说就是，VS 太大不会用，JetBrains 太贵买不起，虽然是学生可以申请免费全家桶，但 CLion 不像 Intellij IDEA 或者 PyCharm 它没免费版啊。也就尝试着 Notepad++ 了，当时并没什么政治倾向。也就当一个自带高亮的记事本， Dev-C++ 改用还得用啊。 在用编辑器时听闻了 Vim 、 Linux 等软件，爷投 *nix 了！所以 Notepad++ 也用不上了，选来选去选了个 Emacs ，多少开始离谱起来了，因为我觉得 Vim 的且模式好麻烦。不过好在这种玩意一招鲜吃遍天，你可以为其添加任意语言的支持，什么 Intellij IDEA 、 PyCharm 还要分，太麻烦了，就问你能加 Erlang 、 Elixir 、 Haskell 等支持吗 (这不巧了吗，还都能加)。况且用 shell 时默认的快捷键也是和 Emacs 一致的。 怎么不用 Vim，因为我不喜欢切换模式啊。所以我现在也不用 evil 或 meow ，不过很推荐尝试一下 meow。怎么不用 VSCode ，其实当时也没什么 VSCode， Atom 用不用啊， Sublime 用不用啊。那当然是不用啊，不想再要一个 Chromium 啊，也不想买授权。hhhh 至于 Lisp 会不会啊，那肯定也是不会的啊，就只学会了 New Jersey style 和 The MIT approach 两个名词，干什么用，可以装 B 可以嘲讽你说有用吗。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:1:0","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#引言"},{"categories":["Emacs"],"content":" Why Emacs网上对于 Emacs 的推荐的文章很多，我就不说了，我就一菜鸡。 这里看看 @tecosaur 对于几个编辑器的比较 Editor Extensibility Ecosystem Ease of Use Comfort Completion Performance IDLE 1 1 3 1 1 2 VSCode 3 3 4 3.5 4 3 Brackets 2.5 2 3 3 2.5 2 Emacs 4 4 2 4 3.5 3 Komodo Edit 2 1 3 2 2 2 这种表有很强的主观印象，不过对于编辑器，还是推荐 VSCode、Emacs 和 Vim。我只是喜欢编辑器在编辑方面极强的扩展优势，对于 Everything (比如 mu4e, EAF)，我的兴趣并不高，或许因为它们又麻烦又没有 FF / Thunderbird 好用？ 最后还是要说一下，Emacs 的优势， 递归编辑 普遍的文档字符串，以及完全的内省性 在可变环境中的増量修改 无需特定应用启用功能 允许客户端、服务器分离，启动守护进程，提供几乎无感知的启动时间 尤其是 Org-mode ，全球独此一家！ ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:1:1","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#why-emacs"},{"categories":["Emacs"],"content":" Issues Emacs 有一些令人厌烦的奇怪操作 某些方面很暴露年龄 (名称约定、API 等) Emacs 几乎 是单线程的，这意味着一些不当的操作将阻塞整个应用 一些其他方面的干扰…… 说实话，我是个 Emacs 「低手」，并不是高手。所以 ELisp 有点痛苦，不过还好吧，起码会算 (+ 1 1)。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:1:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#issues"},{"categories":["Emacs"],"content":" 基础配置创建词法绑定可以 (稍微) 加速配置文件的运行。 (更多内容可以查看 这篇博客) ;;; config.el -*- lexical-binding: t; -*- 配置有用且基础的个人信息 (setq user-full-name \"GinShio\" user-mail-address \"ginshio78@gmail.com\" user-gpg-key \"9E2949D214995C7E\" wakatime-api-key \"cb5cccd0-e5a0-4922-abfd-748a42a96cae\" org-directory \"~/org\") 显然这可以被 GPG 或其他程序使用。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:0","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#基础配置"},{"categories":["Emacs"],"content":" 设置默认值尝试一下别人的默认值，比如 angrybacon/dotemacs (setq-default delete-by-moving-to-trash t ; 将文件删除到回收站 window-combination-resize t ; 从其他窗口获取新窗口的大小 x-stretch-cursor t ; 将光标拉伸到字形宽度 ) (setq! undo-limit 104857600 ; 重置撤销限制到 100 MiB auto-save-default t ; 没有人喜欢丢失工作，我也是如此 truncate-string-ellipsis \"…\" ; Unicode 省略号相比 ascii 更好 ; 同时节省 /宝贵的/ 空间 password-cache-expiry nil ; 我能信任我的电脑 ... 或不能? ; scroll-preserve-screen-position 'always ; 不要让 `点' (光标) 跳来跳去 scroll-margin 2 ; 适当保持一点点边距 gc-cons-threshold 1073741824 read-process-output-max 1048576 ) (remove-hook 'text-mode-hook #'visual-line-mode) (add-hook 'text-mode-hook #'auto-fill-mode) (add-hook! 'window-setup-hook #'toggle-frame-fullscreen) ; 设置最大化启动 ;;(display-time-mode t) ; 开启时间状态栏 (require 'battery) (when (and battery-status-function (not (string-match-p \"N/A\" (battery-format \"%B\" (funcall battery-status-function))))) (display-battery-mode 1)) ; 知道还剩多少 ⚡️ 很重要 (global-subword-mode 1) ; 识别驼峰，而不是傻瓜前进 (global-unset-key (kbd \"C-z\")) ; 关闭 \"C-z\" 最小化 (define-key! global-map \"C-s\" #'+default/search-buffer) (map! (:leader (:desc \"load a saved workspace\" :g \"wr\" #'+workspace/load))) ;; workspace load keybind (when IS-WINDOWS (setq-default buffer-file-coding-system 'utf-8-unix) (set-default-coding-systems 'utf-8-unix) (prefer-coding-system 'utf-8-unix)) ; 将 Windows 上的编码改为 UTF-8 Unix 换行 (custom-set-variables '(delete-selection-mode t) ;; delete when you select region and modify '(delete-by-moving-to-trash t) ;; delete \u0026\u0026 move to transh '(inhibit-compacting-font-caches t) ;; don’t compact font caches during GC. '(gc-cons-percentage 1)) (add-hook 'prog-mode-hook (lambda () (setq show-trailing-whitespace 1))) ; 编程模式下让结尾的空白符亮起 定义一个自己的 key leader，或许没什么用 (after! general (general-create-definer ginshio/leader :prefix \"s-y\")) 默认情况下通过自定义界面所做的修改会被添加到 init.el 中。不过正常的方法是将它们放在 .custom.el 中。 (setq-default custom-file (expand-file-name \".custom.el\" doom-private-dir)) (when (file-exists-p custom-file) (load custom-file)) 设置一个方便的在 window 之间进行切换的快捷键。 (map! :map ctl-x-map \"\u003cleft\u003e\" #'windmove-left \"\u003cdown\u003e\" #'windmove-down \"\u003cup\u003e\" #'windmove-up \"\u003cright\u003e\" #'windmove-right ) 如果是 evil 用户可以改为下面这种 (map! :map evil-window-map \"SPC\" #'rotate-layout ;; 方向 \"\u003cleft\u003e\" #'evil-window-left \"\u003cdown\u003e\" #'evil-window-down \"\u003cup\u003e\" #'evil-window-up \"\u003cright\u003e\" #'evil-window-right ;; 交换窗口 \"C-\u003cleft\u003e\" #'+evil/window-move-left \"C-\u003cdown\u003e\" #'+evil/window-move-down \"C-\u003cup\u003e\" #'+evil/window-move-up \"C-\u003cright\u003e\" #'+evil/window-move-right ) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:1","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#设置默认值"},{"categories":["Emacs"],"content":" Doom 配置拉取 doom-emacs 仓库的分支 git commit: aed2972d74 doom-version: 3.0.0-dev doom-modules-version: 21.12.0-dev 模组 ;;; init.el -*- lexical-binding: t; -*- ;; This file controls what Doom modules are enabled and what order they load ;; in. Remember to run 'doom sync' after modifying it! ;; NOTE Press 'SPC h d h' (or 'C-h d h' for non-vim users) to access Doom's ;; documentation. There you'll find a link to Doom's Module Index where all ;; of our modules are listed, including what flags they support. ;; NOTE Move your cursor over a module's name (or its flags) and press 'K' (or ;; 'C-c c k' for non-vim users) to view its documentation. This works on ;; flags as well (those symbols that start with a plus). ;; ;; Alternatively, press 'gd' (or 'C-c c d') on a module to browse its ;; directory (for easy access to its source code). (doom! :input \u003c\u003cdoom-input\u003e\u003e :completion \u003c\u003cdoom-completion\u003e\u003e :ui \u003c\u003cdoom-ui\u003e\u003e :editor \u003c\u003cdoom-editor\u003e\u003e :emacs \u003c\u003cdoom-emacs\u003e\u003e :term \u003c\u003cdoom-term\u003e\u003e :checkers \u003c\u003cdoom-checkers\u003e\u003e :tools \u003c\u003cdoom-tools\u003e\u003e :os \u003c\u003cdoom-os\u003e\u003e :lang \u003c\u003cdoom-lang\u003e\u003e :email \u003c\u003cdoom-email\u003e\u003e :app \u003c\u003cdoom-app\u003e\u003e :config \u003c\u003cdoom-config\u003e\u003e ) 结构 这是一篇文学编程，同时也是 Doom Emacs 的配置文件。Doom 对其支持良好，更多详情可以通过 literate (文学) 模块了解。 literate (default +bindings +smartparens) 接口 可以做很多事来增强 Emacs 的功能，。 输入 : 中日文输入与键盘布局，我主要依赖系统输入法 fcitx 且不输入法语，因此不开此选项 emacs-lisp ;;chinese ;;japanese ;;layout ; auie,ctsrnm is the superior home row 补全 : 或许叫补全有点不合适，不过也就这样了。另外说一下， helm 、 ido 、 ivy 以及 vertico 是功能一致的，生态不同的四个包 emacs-lisp company ; the ultimate code completion backend ;;helm ; the *other* search engine for love and life ;;ido ; the other *other* search engine... ;;(ivy ; a search engine for love and life ;; +icons ; ... icons are nice ;; +prescient) ; ... I know what I want(ed) (vertico +icons) ; the search engine of the future UI : 好不好看就看你这么配置了 emacs-lisp ;;deft ; notational velocity for Emacs doom ; what makes DOOM look the way it does doom-dashboard ; a nifty splash screen for Emacs doom-quit ; DOOM quit-message prompts when you quit Emacs (emoji +unicode +github) ; 🙂 hl-todo ; highlight TODO/FIXME/NOTE/DEPRECATED/HACK/REVIEW ;;hydra ;;indent-guides ; highlighted indent columns (ligatures +extra) ; ligatures and symbols to make your code pretty again ;;minimap ; show a map of the code on the side modeline ; snazzy, Atom-inspired modeline, plus API nav-flash ; blink cursor line after big motions ;;neotree ; a project drawer, like NERDTree for vim ophints ; highlight the region an operation acts on (popup ; tame sudden yet inevitable temporary windows +all ; catch all popups that start with an asterix +defaults) ; default popup rules ;;tabs ; a tab bar for Emacs treemacs ; a project drawer, like neotree but cooler ;;unicode ; extended unicode support for various languages vc-gutter ; vcs diff in the fringe vi-tilde-fringe ; fringe tildes to mark beyond EOB (window-select ; visually switch windows +numbers) workspaces ; tab emulation, persistence \u0026 separate workspaces zen ; distraction-free coding or writing 编辑器 : VI VI VI Editor of the Beast emacs-lisp ;;(evil +everywhere); come to the dark side, we have cookies file-templates ; auto-snippets for empty files fold ; (nigh) universal code folding (format +onsave) ; automated prettiness ;;god ; run Emacs commands without modifier keys ;;lispy ; vim for lisp, for people who don't like vim multiple-cursors ; editing in many places at once ;;objed ; text object editing for the innocent ;;parinfer ; turn lisp into python, sort of rotate-text ; cycle region at point between text candidates snippets ; my elves. They type so I don't have to ;;word-wrap ; soft wrapping with language-aware indent Emacs : 增强一下吧，不然真的是笔记本了 (其实不是 emacs-lisp (dired +icons) ; making dired pretty [functional] electric ; smarter, keyword-based electric-indent (ibuffer +icons) ; interactive buffer management undo ; persistent, smarter undo for your inevitable mistakes vc ; version-control and Emacs, sitting in a tree 终端 : 也许我应该卸载掉我的 Konsole emacs-lisp ;;eshell ; the elisp sh","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#doom-配置"},{"categories":["Emacs"],"content":" Doom 配置拉取 doom-emacs 仓库的分支 git commit: aed2972d74 doom-version: 3.0.0-dev doom-modules-version: 21.12.0-dev 模组 ;;; init.el -*- lexical-binding: t; -*- ;; This file controls what Doom modules are enabled and what order they load ;; in. Remember to run 'doom sync' after modifying it! ;; NOTE Press 'SPC h d h' (or 'C-h d h' for non-vim users) to access Doom's ;; documentation. There you'll find a link to Doom's Module Index where all ;; of our modules are listed, including what flags they support. ;; NOTE Move your cursor over a module's name (or its flags) and press 'K' (or ;; 'C-c c k' for non-vim users) to view its documentation. This works on ;; flags as well (those symbols that start with a plus). ;; ;; Alternatively, press 'gd' (or 'C-c c d') on a module to browse its ;; directory (for easy access to its source code). (doom! :input \u003c\u003e :completion \u003c\u003e :ui \u003c\u003e :editor \u003c\u003e :emacs \u003c\u003e :term \u003c\u003e :checkers \u003c\u003e :tools \u003c\u003e :os \u003c\u003e :lang \u003c\u003e :email \u003c\u003e :app \u003c\u003e :config \u003c\u003e ) 结构 这是一篇文学编程，同时也是 Doom Emacs 的配置文件。Doom 对其支持良好，更多详情可以通过 literate (文学) 模块了解。 literate (default +bindings +smartparens) 接口 可以做很多事来增强 Emacs 的功能，。 输入 : 中日文输入与键盘布局，我主要依赖系统输入法 fcitx 且不输入法语，因此不开此选项 emacs-lisp ;;chinese ;;japanese ;;layout ; auie,ctsrnm is the superior home row 补全 : 或许叫补全有点不合适，不过也就这样了。另外说一下， helm 、 ido 、 ivy 以及 vertico 是功能一致的，生态不同的四个包 emacs-lisp company ; the ultimate code completion backend ;;helm ; the *other* search engine for love and life ;;ido ; the other *other* search engine... ;;(ivy ; a search engine for love and life ;; +icons ; ... icons are nice ;; +prescient) ; ... I know what I want(ed) (vertico +icons) ; the search engine of the future UI : 好不好看就看你这么配置了 emacs-lisp ;;deft ; notational velocity for Emacs doom ; what makes DOOM look the way it does doom-dashboard ; a nifty splash screen for Emacs doom-quit ; DOOM quit-message prompts when you quit Emacs (emoji +unicode +github) ; 🙂 hl-todo ; highlight TODO/FIXME/NOTE/DEPRECATED/HACK/REVIEW ;;hydra ;;indent-guides ; highlighted indent columns (ligatures +extra) ; ligatures and symbols to make your code pretty again ;;minimap ; show a map of the code on the side modeline ; snazzy, Atom-inspired modeline, plus API nav-flash ; blink cursor line after big motions ;;neotree ; a project drawer, like NERDTree for vim ophints ; highlight the region an operation acts on (popup ; tame sudden yet inevitable temporary windows +all ; catch all popups that start with an asterix +defaults) ; default popup rules ;;tabs ; a tab bar for Emacs treemacs ; a project drawer, like neotree but cooler ;;unicode ; extended unicode support for various languages vc-gutter ; vcs diff in the fringe vi-tilde-fringe ; fringe tildes to mark beyond EOB (window-select ; visually switch windows +numbers) workspaces ; tab emulation, persistence \u0026 separate workspaces zen ; distraction-free coding or writing 编辑器 : VI VI VI Editor of the Beast emacs-lisp ;;(evil +everywhere); come to the dark side, we have cookies file-templates ; auto-snippets for empty files fold ; (nigh) universal code folding (format +onsave) ; automated prettiness ;;god ; run Emacs commands without modifier keys ;;lispy ; vim for lisp, for people who don't like vim multiple-cursors ; editing in many places at once ;;objed ; text object editing for the innocent ;;parinfer ; turn lisp into python, sort of rotate-text ; cycle region at point between text candidates snippets ; my elves. They type so I don't have to ;;word-wrap ; soft wrapping with language-aware indent Emacs : 增强一下吧，不然真的是笔记本了 (其实不是 emacs-lisp (dired +icons) ; making dired pretty [functional] electric ; smarter, keyword-based electric-indent (ibuffer +icons) ; interactive buffer management undo ; persistent, smarter undo for your inevitable mistakes vc ; version-control and Emacs, sitting in a tree 终端 : 也许我应该卸载掉我的 Konsole emacs-lisp ;;eshell ; the elisp sh","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#模组"},{"categories":["Emacs"],"content":" Doom 配置拉取 doom-emacs 仓库的分支 git commit: aed2972d74 doom-version: 3.0.0-dev doom-modules-version: 21.12.0-dev 模组 ;;; init.el -*- lexical-binding: t; -*- ;; This file controls what Doom modules are enabled and what order they load ;; in. Remember to run 'doom sync' after modifying it! ;; NOTE Press 'SPC h d h' (or 'C-h d h' for non-vim users) to access Doom's ;; documentation. There you'll find a link to Doom's Module Index where all ;; of our modules are listed, including what flags they support. ;; NOTE Move your cursor over a module's name (or its flags) and press 'K' (or ;; 'C-c c k' for non-vim users) to view its documentation. This works on ;; flags as well (those symbols that start with a plus). ;; ;; Alternatively, press 'gd' (or 'C-c c d') on a module to browse its ;; directory (for easy access to its source code). (doom! :input \u003c\u003e :completion \u003c\u003e :ui \u003c\u003e :editor \u003c\u003e :emacs \u003c\u003e :term \u003c\u003e :checkers \u003c\u003e :tools \u003c\u003e :os \u003c\u003e :lang \u003c\u003e :email \u003c\u003e :app \u003c\u003e :config \u003c\u003e ) 结构 这是一篇文学编程，同时也是 Doom Emacs 的配置文件。Doom 对其支持良好，更多详情可以通过 literate (文学) 模块了解。 literate (default +bindings +smartparens) 接口 可以做很多事来增强 Emacs 的功能，。 输入 : 中日文输入与键盘布局，我主要依赖系统输入法 fcitx 且不输入法语，因此不开此选项 emacs-lisp ;;chinese ;;japanese ;;layout ; auie,ctsrnm is the superior home row 补全 : 或许叫补全有点不合适，不过也就这样了。另外说一下， helm 、 ido 、 ivy 以及 vertico 是功能一致的，生态不同的四个包 emacs-lisp company ; the ultimate code completion backend ;;helm ; the *other* search engine for love and life ;;ido ; the other *other* search engine... ;;(ivy ; a search engine for love and life ;; +icons ; ... icons are nice ;; +prescient) ; ... I know what I want(ed) (vertico +icons) ; the search engine of the future UI : 好不好看就看你这么配置了 emacs-lisp ;;deft ; notational velocity for Emacs doom ; what makes DOOM look the way it does doom-dashboard ; a nifty splash screen for Emacs doom-quit ; DOOM quit-message prompts when you quit Emacs (emoji +unicode +github) ; 🙂 hl-todo ; highlight TODO/FIXME/NOTE/DEPRECATED/HACK/REVIEW ;;hydra ;;indent-guides ; highlighted indent columns (ligatures +extra) ; ligatures and symbols to make your code pretty again ;;minimap ; show a map of the code on the side modeline ; snazzy, Atom-inspired modeline, plus API nav-flash ; blink cursor line after big motions ;;neotree ; a project drawer, like NERDTree for vim ophints ; highlight the region an operation acts on (popup ; tame sudden yet inevitable temporary windows +all ; catch all popups that start with an asterix +defaults) ; default popup rules ;;tabs ; a tab bar for Emacs treemacs ; a project drawer, like neotree but cooler ;;unicode ; extended unicode support for various languages vc-gutter ; vcs diff in the fringe vi-tilde-fringe ; fringe tildes to mark beyond EOB (window-select ; visually switch windows +numbers) workspaces ; tab emulation, persistence \u0026 separate workspaces zen ; distraction-free coding or writing 编辑器 : VI VI VI Editor of the Beast emacs-lisp ;;(evil +everywhere); come to the dark side, we have cookies file-templates ; auto-snippets for empty files fold ; (nigh) universal code folding (format +onsave) ; automated prettiness ;;god ; run Emacs commands without modifier keys ;;lispy ; vim for lisp, for people who don't like vim multiple-cursors ; editing in many places at once ;;objed ; text object editing for the innocent ;;parinfer ; turn lisp into python, sort of rotate-text ; cycle region at point between text candidates snippets ; my elves. They type so I don't have to ;;word-wrap ; soft wrapping with language-aware indent Emacs : 增强一下吧，不然真的是笔记本了 (其实不是 emacs-lisp (dired +icons) ; making dired pretty [functional] electric ; smarter, keyword-based electric-indent (ibuffer +icons) ; interactive buffer management undo ; persistent, smarter undo for your inevitable mistakes vc ; version-control and Emacs, sitting in a tree 终端 : 也许我应该卸载掉我的 Konsole emacs-lisp ;;eshell ; the elisp sh","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#视觉设置"},{"categories":["Emacs"],"content":" Doom 配置拉取 doom-emacs 仓库的分支 git commit: aed2972d74 doom-version: 3.0.0-dev doom-modules-version: 21.12.0-dev 模组 ;;; init.el -*- lexical-binding: t; -*- ;; This file controls what Doom modules are enabled and what order they load ;; in. Remember to run 'doom sync' after modifying it! ;; NOTE Press 'SPC h d h' (or 'C-h d h' for non-vim users) to access Doom's ;; documentation. There you'll find a link to Doom's Module Index where all ;; of our modules are listed, including what flags they support. ;; NOTE Move your cursor over a module's name (or its flags) and press 'K' (or ;; 'C-c c k' for non-vim users) to view its documentation. This works on ;; flags as well (those symbols that start with a plus). ;; ;; Alternatively, press 'gd' (or 'C-c c d') on a module to browse its ;; directory (for easy access to its source code). (doom! :input \u003c\u003e :completion \u003c\u003e :ui \u003c\u003e :editor \u003c\u003e :emacs \u003c\u003e :term \u003c\u003e :checkers \u003c\u003e :tools \u003c\u003e :os \u003c\u003e :lang \u003c\u003e :email \u003c\u003e :app \u003c\u003e :config \u003c\u003e ) 结构 这是一篇文学编程，同时也是 Doom Emacs 的配置文件。Doom 对其支持良好，更多详情可以通过 literate (文学) 模块了解。 literate (default +bindings +smartparens) 接口 可以做很多事来增强 Emacs 的功能，。 输入 : 中日文输入与键盘布局，我主要依赖系统输入法 fcitx 且不输入法语，因此不开此选项 emacs-lisp ;;chinese ;;japanese ;;layout ; auie,ctsrnm is the superior home row 补全 : 或许叫补全有点不合适，不过也就这样了。另外说一下， helm 、 ido 、 ivy 以及 vertico 是功能一致的，生态不同的四个包 emacs-lisp company ; the ultimate code completion backend ;;helm ; the *other* search engine for love and life ;;ido ; the other *other* search engine... ;;(ivy ; a search engine for love and life ;; +icons ; ... icons are nice ;; +prescient) ; ... I know what I want(ed) (vertico +icons) ; the search engine of the future UI : 好不好看就看你这么配置了 emacs-lisp ;;deft ; notational velocity for Emacs doom ; what makes DOOM look the way it does doom-dashboard ; a nifty splash screen for Emacs doom-quit ; DOOM quit-message prompts when you quit Emacs (emoji +unicode +github) ; 🙂 hl-todo ; highlight TODO/FIXME/NOTE/DEPRECATED/HACK/REVIEW ;;hydra ;;indent-guides ; highlighted indent columns (ligatures +extra) ; ligatures and symbols to make your code pretty again ;;minimap ; show a map of the code on the side modeline ; snazzy, Atom-inspired modeline, plus API nav-flash ; blink cursor line after big motions ;;neotree ; a project drawer, like NERDTree for vim ophints ; highlight the region an operation acts on (popup ; tame sudden yet inevitable temporary windows +all ; catch all popups that start with an asterix +defaults) ; default popup rules ;;tabs ; a tab bar for Emacs treemacs ; a project drawer, like neotree but cooler ;;unicode ; extended unicode support for various languages vc-gutter ; vcs diff in the fringe vi-tilde-fringe ; fringe tildes to mark beyond EOB (window-select ; visually switch windows +numbers) workspaces ; tab emulation, persistence \u0026 separate workspaces zen ; distraction-free coding or writing 编辑器 : VI VI VI Editor of the Beast emacs-lisp ;;(evil +everywhere); come to the dark side, we have cookies file-templates ; auto-snippets for empty files fold ; (nigh) universal code folding (format +onsave) ; automated prettiness ;;god ; run Emacs commands without modifier keys ;;lispy ; vim for lisp, for people who don't like vim multiple-cursors ; editing in many places at once ;;objed ; text object editing for the innocent ;;parinfer ; turn lisp into python, sort of rotate-text ; cycle region at point between text candidates snippets ; my elves. They type so I don't have to ;;word-wrap ; soft wrapping with language-aware indent Emacs : 增强一下吧，不然真的是笔记本了 (其实不是 emacs-lisp (dired +icons) ; making dired pretty [functional] electric ; smarter, keyword-based electric-indent (ibuffer +icons) ; interactive buffer management undo ; persistent, smarter undo for your inevitable mistakes vc ; version-control and Emacs, sitting in a tree 终端 : 也许我应该卸载掉我的 Konsole emacs-lisp ;;eshell ; the elisp sh","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#辅助宏"},{"categories":["Emacs"],"content":" Doom 配置拉取 doom-emacs 仓库的分支 git commit: aed2972d74 doom-version: 3.0.0-dev doom-modules-version: 21.12.0-dev 模组 ;;; init.el -*- lexical-binding: t; -*- ;; This file controls what Doom modules are enabled and what order they load ;; in. Remember to run 'doom sync' after modifying it! ;; NOTE Press 'SPC h d h' (or 'C-h d h' for non-vim users) to access Doom's ;; documentation. There you'll find a link to Doom's Module Index where all ;; of our modules are listed, including what flags they support. ;; NOTE Move your cursor over a module's name (or its flags) and press 'K' (or ;; 'C-c c k' for non-vim users) to view its documentation. This works on ;; flags as well (those symbols that start with a plus). ;; ;; Alternatively, press 'gd' (or 'C-c c d') on a module to browse its ;; directory (for easy access to its source code). (doom! :input \u003c\u003e :completion \u003c\u003e :ui \u003c\u003e :editor \u003c\u003e :emacs \u003c\u003e :term \u003c\u003e :checkers \u003c\u003e :tools \u003c\u003e :os \u003c\u003e :lang \u003c\u003e :email \u003c\u003e :app \u003c\u003e :config \u003c\u003e ) 结构 这是一篇文学编程，同时也是 Doom Emacs 的配置文件。Doom 对其支持良好，更多详情可以通过 literate (文学) 模块了解。 literate (default +bindings +smartparens) 接口 可以做很多事来增强 Emacs 的功能，。 输入 : 中日文输入与键盘布局，我主要依赖系统输入法 fcitx 且不输入法语，因此不开此选项 emacs-lisp ;;chinese ;;japanese ;;layout ; auie,ctsrnm is the superior home row 补全 : 或许叫补全有点不合适，不过也就这样了。另外说一下， helm 、 ido 、 ivy 以及 vertico 是功能一致的，生态不同的四个包 emacs-lisp company ; the ultimate code completion backend ;;helm ; the *other* search engine for love and life ;;ido ; the other *other* search engine... ;;(ivy ; a search engine for love and life ;; +icons ; ... icons are nice ;; +prescient) ; ... I know what I want(ed) (vertico +icons) ; the search engine of the future UI : 好不好看就看你这么配置了 emacs-lisp ;;deft ; notational velocity for Emacs doom ; what makes DOOM look the way it does doom-dashboard ; a nifty splash screen for Emacs doom-quit ; DOOM quit-message prompts when you quit Emacs (emoji +unicode +github) ; 🙂 hl-todo ; highlight TODO/FIXME/NOTE/DEPRECATED/HACK/REVIEW ;;hydra ;;indent-guides ; highlighted indent columns (ligatures +extra) ; ligatures and symbols to make your code pretty again ;;minimap ; show a map of the code on the side modeline ; snazzy, Atom-inspired modeline, plus API nav-flash ; blink cursor line after big motions ;;neotree ; a project drawer, like NERDTree for vim ophints ; highlight the region an operation acts on (popup ; tame sudden yet inevitable temporary windows +all ; catch all popups that start with an asterix +defaults) ; default popup rules ;;tabs ; a tab bar for Emacs treemacs ; a project drawer, like neotree but cooler ;;unicode ; extended unicode support for various languages vc-gutter ; vcs diff in the fringe vi-tilde-fringe ; fringe tildes to mark beyond EOB (window-select ; visually switch windows +numbers) workspaces ; tab emulation, persistence \u0026 separate workspaces zen ; distraction-free coding or writing 编辑器 : VI VI VI Editor of the Beast emacs-lisp ;;(evil +everywhere); come to the dark side, we have cookies file-templates ; auto-snippets for empty files fold ; (nigh) universal code folding (format +onsave) ; automated prettiness ;;god ; run Emacs commands without modifier keys ;;lispy ; vim for lisp, for people who don't like vim multiple-cursors ; editing in many places at once ;;objed ; text object editing for the innocent ;;parinfer ; turn lisp into python, sort of rotate-text ; cycle region at point between text candidates snippets ; my elves. They type so I don't have to ;;word-wrap ; soft wrapping with language-aware indent Emacs : 增强一下吧，不然真的是笔记本了 (其实不是 emacs-lisp (dired +icons) ; making dired pretty [functional] electric ; smarter, keyword-based electric-indent (ibuffer +icons) ; interactive buffer management undo ; persistent, smarter undo for your inevitable mistakes vc ; version-control and Emacs, sitting in a tree 终端 : 也许我应该卸载掉我的 Konsole emacs-lisp ;;eshell ; the elisp sh","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#允许-cli-运行-org-babel-程序"},{"categories":["Emacs"],"content":" Doom 配置拉取 doom-emacs 仓库的分支 git commit: aed2972d74 doom-version: 3.0.0-dev doom-modules-version: 21.12.0-dev 模组 ;;; init.el -*- lexical-binding: t; -*- ;; This file controls what Doom modules are enabled and what order they load ;; in. Remember to run 'doom sync' after modifying it! ;; NOTE Press 'SPC h d h' (or 'C-h d h' for non-vim users) to access Doom's ;; documentation. There you'll find a link to Doom's Module Index where all ;; of our modules are listed, including what flags they support. ;; NOTE Move your cursor over a module's name (or its flags) and press 'K' (or ;; 'C-c c k' for non-vim users) to view its documentation. This works on ;; flags as well (those symbols that start with a plus). ;; ;; Alternatively, press 'gd' (or 'C-c c d') on a module to browse its ;; directory (for easy access to its source code). (doom! :input \u003c\u003e :completion \u003c\u003e :ui \u003c\u003e :editor \u003c\u003e :emacs \u003c\u003e :term \u003c\u003e :checkers \u003c\u003e :tools \u003c\u003e :os \u003c\u003e :lang \u003c\u003e :email \u003c\u003e :app \u003c\u003e :config \u003c\u003e ) 结构 这是一篇文学编程，同时也是 Doom Emacs 的配置文件。Doom 对其支持良好，更多详情可以通过 literate (文学) 模块了解。 literate (default +bindings +smartparens) 接口 可以做很多事来增强 Emacs 的功能，。 输入 : 中日文输入与键盘布局，我主要依赖系统输入法 fcitx 且不输入法语，因此不开此选项 emacs-lisp ;;chinese ;;japanese ;;layout ; auie,ctsrnm is the superior home row 补全 : 或许叫补全有点不合适，不过也就这样了。另外说一下， helm 、 ido 、 ivy 以及 vertico 是功能一致的，生态不同的四个包 emacs-lisp company ; the ultimate code completion backend ;;helm ; the *other* search engine for love and life ;;ido ; the other *other* search engine... ;;(ivy ; a search engine for love and life ;; +icons ; ... icons are nice ;; +prescient) ; ... I know what I want(ed) (vertico +icons) ; the search engine of the future UI : 好不好看就看你这么配置了 emacs-lisp ;;deft ; notational velocity for Emacs doom ; what makes DOOM look the way it does doom-dashboard ; a nifty splash screen for Emacs doom-quit ; DOOM quit-message prompts when you quit Emacs (emoji +unicode +github) ; 🙂 hl-todo ; highlight TODO/FIXME/NOTE/DEPRECATED/HACK/REVIEW ;;hydra ;;indent-guides ; highlighted indent columns (ligatures +extra) ; ligatures and symbols to make your code pretty again ;;minimap ; show a map of the code on the side modeline ; snazzy, Atom-inspired modeline, plus API nav-flash ; blink cursor line after big motions ;;neotree ; a project drawer, like NERDTree for vim ophints ; highlight the region an operation acts on (popup ; tame sudden yet inevitable temporary windows +all ; catch all popups that start with an asterix +defaults) ; default popup rules ;;tabs ; a tab bar for Emacs treemacs ; a project drawer, like neotree but cooler ;;unicode ; extended unicode support for various languages vc-gutter ; vcs diff in the fringe vi-tilde-fringe ; fringe tildes to mark beyond EOB (window-select ; visually switch windows +numbers) workspaces ; tab emulation, persistence \u0026 separate workspaces zen ; distraction-free coding or writing 编辑器 : VI VI VI Editor of the Beast emacs-lisp ;;(evil +everywhere); come to the dark side, we have cookies file-templates ; auto-snippets for empty files fold ; (nigh) universal code folding (format +onsave) ; automated prettiness ;;god ; run Emacs commands without modifier keys ;;lispy ; vim for lisp, for people who don't like vim multiple-cursors ; editing in many places at once ;;objed ; text object editing for the innocent ;;parinfer ; turn lisp into python, sort of rotate-text ; cycle region at point between text candidates snippets ; my elves. They type so I don't have to ;;word-wrap ; soft wrapping with language-aware indent Emacs : 增强一下吧，不然真的是笔记本了 (其实不是 emacs-lisp (dired +icons) ; making dired pretty [functional] electric ; smarter, keyword-based electric-indent (ibuffer +icons) ; interactive buffer management undo ; persistent, smarter undo for your inevitable mistakes vc ; version-control and Emacs, sitting in a tree 终端 : 也许我应该卸载掉我的 Konsole emacs-lisp ;;eshell ; the elisp sh","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#dashboard"},{"categories":["Emacs"],"content":" 其他设置 窗口标题我更喜欢窗口展示缓冲区的名字，然后是项目文件夹 (如果可用)。 (setq! frame-title-format '(\"%b – Doom Emacs\" (:eval (let ((project-name (projectile-project-name))) (unless (string= \"-\" project-name) (format \" - [%s]\" project-name)))))) 启动界面@tecosaur 做了一个相当棒的启动画面，心动！但是太复杂了。我只是想简单的在每次重启时更换 banner，仅此而已。 (setq! fancy-splash-image (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".png\" eos)))) (elt banners (random (length banners))))) 当然，不要忘记 ASCII banner (setq! ginshio/+doom-dashbord-ascii-banner (split-string (with-output-to-string (call-process \"cat\" nil standard-output nil (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".txt\" eos)))) (elt banners (random (length banners)))))) \"\\n\" t)) (setq! +doom-dashboard-ascii-banner-fn #'(lambda () (mapc (lambda (line) (insert (propertize (+doom-dashboard--center +doom-dashboard--width line) 'face 'doom-dashboard-banner) \" \") (insert \"\\n\")) ginshio/+doom-dashbord-ascii-banner))) 守护进程守护进程是个好东西，可惜我用的是 WSL，没有 systemd ，不过 EmacsWiki 中还是列出了各种 Daemon 的方法。 Systemd (Not WSL) ~/.config/systemd/user/emacs.service [Unit] Description=Emacs text editor Documentation=info:emacs man:emacs(1) https://gnu.org/software/emacs/ [Service] Type=forking ExecStart=/usr/bin/emacs --daemon ExecStop=/usr/bin/emacsclient --no-wait --eval \"(progn (setq kill-emacs-hook 'nil) (kill-emacs))\" Restart=on-failure [Install] WantedBy=default.target Launchd (MacOS) /Library/LaunchAgents/gnu.emacs.daemon.plist \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e \u003cplist version=\"1.0\"\u003e \u003cdict\u003e \u003ckey\u003eLabel\u003c/key\u003e \u003cstring\u003egnu.emacs.daemon\u003c/string\u003e \u003ckey\u003eProgramArguments\u003c/key\u003e \u003carray\u003e \u003cstring\u003e/Applications/Emacs.app/Contents/MacOS/Emacs\u003c/string\u003e \u003cstring\u003e--daemon\u003c/string\u003e \u003c/array\u003e \u003ckey\u003eRunAtLoad\u003c/key\u003e \u003ctrue/\u003e \u003ckey\u003eServiceDescription\u003c/key\u003e \u003cstring\u003eGnu Emacs Daemon\u003c/string\u003e \u003c/dict\u003e \u003c/plist\u003e rc.d (FreeBSD) 这来自 FreeBSD 论坛的讨论 /etc/init.d/emacsd #!/bin/sh # # PROVIDE: emacsd # REQUIRE: LOGIN # KEYWORD: shutdown . /etc/rc.subr name=\"emacsd\" rcvar=\"${name}_enable\" start_precmd=\"${name}_prestart\" load_rc_config $name : \"${emacsd_enable:=\"NO\"}\" : \"${emacsd_user:=\"nobody\"}\" piddir=\"/var/run/${emacsd_user}\" pidfile=\"${piddir}/${name}_daemon.pid\" pidfile_child=\"${piddir}/${name}_emacs.pid\" user_id=\"$(id -u ${emacsd_user}\" emacsd_env=\"$(env -iL ${user_id} XDG_RUNTIME_DIR=/var/run/user/${user_id} HOME=/home/${emacsd_user} | tr '\\n' ' ')\" proc_cmd=\"/usr/local/bin/emacs\" proc_args=\"--fg-daemon -u ${emacsd_user}\" command=\"/usr/sbin/daemon\" command_args=\"-f -P ${pidfile} -p ${pidfile_child} -r ${proc_cmd} ${proc_args}\" emacsd_prestart() { [ -z \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 XDG_RUNTIME_DIR=\"/var/run/user/$(id -u ${emacsd_user})\" ! [ -d \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -m 0700 \"${XDG_RUNTIME_DIR}\" - ! [ -d \"${piddir}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -g \"$(id -u ${emacsd_user})\" \"${piddir}\" - return 0 } run_rc_command \"$1\" rc.d (Linux, Maybe WSL) 这是 EmacsWiki 上为 GNU/Debian 创建的 rc.d 脚本，或许 WSL 也能用， /etc/init.d/emacsd #!/bin/sh ### BEGIN INIT INFO # Provides: emacsd # Required-Start: $remote_fs $syslog # Required-Stop: $remote_fs $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start emacsd at boot time # Description: Enable service provided by daemon. ### END INIT INFO # Local Settings PATH=/usr/local/bin:/usr/bin:/bin # emacs location. emacs=\"emacs\" emacsclient=\"emacsclient\" # EE code EE_EMACS_NOT_FOUND=1 EE_INVALID_OPTION=2 EE_EMACS_FAIL_TO_START=3 EE_EMACS_FAIL_TO_STOP=4 # Real code begins here if [ -z `which emacs` ] then log_daemon_msg \"Error: Emacs not found. emacsd will now exit.\" exit $EE_EMACS_NOT_FOUND fi # TODO Start emacs as normal user \"emacsd\" or \"daemon\" #emacs run under this uid emacsd_uid=\"1000\" socket_file=\"/tmp/emacs${emacsd_uid}/server\" case \"$1\" in start) #check whether already started if [ -e \"$socket_file\" ] then echo \"Erro","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#其他设置"},{"categories":["Emacs"],"content":" 其他设置 窗口标题我更喜欢窗口展示缓冲区的名字，然后是项目文件夹 (如果可用)。 (setq! frame-title-format '(\"%b – Doom Emacs\" (:eval (let ((project-name (projectile-project-name))) (unless (string= \"-\" project-name) (format \" - [%s]\" project-name)))))) 启动界面@tecosaur 做了一个相当棒的启动画面，心动！但是太复杂了。我只是想简单的在每次重启时更换 banner，仅此而已。 (setq! fancy-splash-image (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".png\" eos)))) (elt banners (random (length banners))))) 当然，不要忘记 ASCII banner (setq! ginshio/+doom-dashbord-ascii-banner (split-string (with-output-to-string (call-process \"cat\" nil standard-output nil (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".txt\" eos)))) (elt banners (random (length banners)))))) \"\\n\" t)) (setq! +doom-dashboard-ascii-banner-fn #'(lambda () (mapc (lambda (line) (insert (propertize (+doom-dashboard--center +doom-dashboard--width line) 'face 'doom-dashboard-banner) \" \") (insert \"\\n\")) ginshio/+doom-dashbord-ascii-banner))) 守护进程守护进程是个好东西，可惜我用的是 WSL，没有 systemd ，不过 EmacsWiki 中还是列出了各种 Daemon 的方法。 Systemd (Not WSL) ~/.config/systemd/user/emacs.service [Unit] Description=Emacs text editor Documentation=info:emacs man:emacs(1) https://gnu.org/software/emacs/ [Service] Type=forking ExecStart=/usr/bin/emacs --daemon ExecStop=/usr/bin/emacsclient --no-wait --eval \"(progn (setq kill-emacs-hook 'nil) (kill-emacs))\" Restart=on-failure [Install] WantedBy=default.target Launchd (MacOS) /Library/LaunchAgents/gnu.emacs.daemon.plist \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e Label gnu.emacs.daemon ProgramArguments /Applications/Emacs.app/Contents/MacOS/Emacs --daemon RunAtLoad ServiceDescription Gnu Emacs Daemon rc.d (FreeBSD) 这来自 FreeBSD 论坛的讨论 /etc/init.d/emacsd #!/bin/sh # # PROVIDE: emacsd # REQUIRE: LOGIN # KEYWORD: shutdown . /etc/rc.subr name=\"emacsd\" rcvar=\"${name}_enable\" start_precmd=\"${name}_prestart\" load_rc_config $name : \"${emacsd_enable:=\"NO\"}\" : \"${emacsd_user:=\"nobody\"}\" piddir=\"/var/run/${emacsd_user}\" pidfile=\"${piddir}/${name}_daemon.pid\" pidfile_child=\"${piddir}/${name}_emacs.pid\" user_id=\"$(id -u ${emacsd_user}\" emacsd_env=\"$(env -iL ${user_id} XDG_RUNTIME_DIR=/var/run/user/${user_id} HOME=/home/${emacsd_user} | tr '\\n' ' ')\" proc_cmd=\"/usr/local/bin/emacs\" proc_args=\"--fg-daemon -u ${emacsd_user}\" command=\"/usr/sbin/daemon\" command_args=\"-f -P ${pidfile} -p ${pidfile_child} -r ${proc_cmd} ${proc_args}\" emacsd_prestart() { [ -z \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 XDG_RUNTIME_DIR=\"/var/run/user/$(id -u ${emacsd_user})\" ! [ -d \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -m 0700 \"${XDG_RUNTIME_DIR}\" - ! [ -d \"${piddir}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -g \"$(id -u ${emacsd_user})\" \"${piddir}\" - return 0 } run_rc_command \"$1\" rc.d (Linux, Maybe WSL) 这是 EmacsWiki 上为 GNU/Debian 创建的 rc.d 脚本，或许 WSL 也能用， /etc/init.d/emacsd #!/bin/sh ### BEGIN INIT INFO # Provides: emacsd # Required-Start: $remote_fs $syslog # Required-Stop: $remote_fs $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start emacsd at boot time # Description: Enable service provided by daemon. ### END INIT INFO # Local Settings PATH=/usr/local/bin:/usr/bin:/bin # emacs location. emacs=\"emacs\" emacsclient=\"emacsclient\" # EE code EE_EMACS_NOT_FOUND=1 EE_INVALID_OPTION=2 EE_EMACS_FAIL_TO_START=3 EE_EMACS_FAIL_TO_STOP=4 # Real code begins here if [ -z `which emacs` ] then log_daemon_msg \"Error: Emacs not found. emacsd will now exit.\" exit $EE_EMACS_NOT_FOUND fi # TODO Start emacs as normal user \"emacsd\" or \"daemon\" #emacs run under this uid emacsd_uid=\"1000\" socket_file=\"/tmp/emacs${emacsd_uid}/server\" case \"$1\" in start) #check whether already started if [ -e \"$socket_file\" ] then echo \"Erro","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#窗口标题"},{"categories":["Emacs"],"content":" 其他设置 窗口标题我更喜欢窗口展示缓冲区的名字，然后是项目文件夹 (如果可用)。 (setq! frame-title-format '(\"%b – Doom Emacs\" (:eval (let ((project-name (projectile-project-name))) (unless (string= \"-\" project-name) (format \" - [%s]\" project-name)))))) 启动界面@tecosaur 做了一个相当棒的启动画面，心动！但是太复杂了。我只是想简单的在每次重启时更换 banner，仅此而已。 (setq! fancy-splash-image (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".png\" eos)))) (elt banners (random (length banners))))) 当然，不要忘记 ASCII banner (setq! ginshio/+doom-dashbord-ascii-banner (split-string (with-output-to-string (call-process \"cat\" nil standard-output nil (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".txt\" eos)))) (elt banners (random (length banners)))))) \"\\n\" t)) (setq! +doom-dashboard-ascii-banner-fn #'(lambda () (mapc (lambda (line) (insert (propertize (+doom-dashboard--center +doom-dashboard--width line) 'face 'doom-dashboard-banner) \" \") (insert \"\\n\")) ginshio/+doom-dashbord-ascii-banner))) 守护进程守护进程是个好东西，可惜我用的是 WSL，没有 systemd ，不过 EmacsWiki 中还是列出了各种 Daemon 的方法。 Systemd (Not WSL) ~/.config/systemd/user/emacs.service [Unit] Description=Emacs text editor Documentation=info:emacs man:emacs(1) https://gnu.org/software/emacs/ [Service] Type=forking ExecStart=/usr/bin/emacs --daemon ExecStop=/usr/bin/emacsclient --no-wait --eval \"(progn (setq kill-emacs-hook 'nil) (kill-emacs))\" Restart=on-failure [Install] WantedBy=default.target Launchd (MacOS) /Library/LaunchAgents/gnu.emacs.daemon.plist \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e Label gnu.emacs.daemon ProgramArguments /Applications/Emacs.app/Contents/MacOS/Emacs --daemon RunAtLoad ServiceDescription Gnu Emacs Daemon rc.d (FreeBSD) 这来自 FreeBSD 论坛的讨论 /etc/init.d/emacsd #!/bin/sh # # PROVIDE: emacsd # REQUIRE: LOGIN # KEYWORD: shutdown . /etc/rc.subr name=\"emacsd\" rcvar=\"${name}_enable\" start_precmd=\"${name}_prestart\" load_rc_config $name : \"${emacsd_enable:=\"NO\"}\" : \"${emacsd_user:=\"nobody\"}\" piddir=\"/var/run/${emacsd_user}\" pidfile=\"${piddir}/${name}_daemon.pid\" pidfile_child=\"${piddir}/${name}_emacs.pid\" user_id=\"$(id -u ${emacsd_user}\" emacsd_env=\"$(env -iL ${user_id} XDG_RUNTIME_DIR=/var/run/user/${user_id} HOME=/home/${emacsd_user} | tr '\\n' ' ')\" proc_cmd=\"/usr/local/bin/emacs\" proc_args=\"--fg-daemon -u ${emacsd_user}\" command=\"/usr/sbin/daemon\" command_args=\"-f -P ${pidfile} -p ${pidfile_child} -r ${proc_cmd} ${proc_args}\" emacsd_prestart() { [ -z \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 XDG_RUNTIME_DIR=\"/var/run/user/$(id -u ${emacsd_user})\" ! [ -d \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -m 0700 \"${XDG_RUNTIME_DIR}\" - ! [ -d \"${piddir}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -g \"$(id -u ${emacsd_user})\" \"${piddir}\" - return 0 } run_rc_command \"$1\" rc.d (Linux, Maybe WSL) 这是 EmacsWiki 上为 GNU/Debian 创建的 rc.d 脚本，或许 WSL 也能用， /etc/init.d/emacsd #!/bin/sh ### BEGIN INIT INFO # Provides: emacsd # Required-Start: $remote_fs $syslog # Required-Stop: $remote_fs $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start emacsd at boot time # Description: Enable service provided by daemon. ### END INIT INFO # Local Settings PATH=/usr/local/bin:/usr/bin:/bin # emacs location. emacs=\"emacs\" emacsclient=\"emacsclient\" # EE code EE_EMACS_NOT_FOUND=1 EE_INVALID_OPTION=2 EE_EMACS_FAIL_TO_START=3 EE_EMACS_FAIL_TO_STOP=4 # Real code begins here if [ -z `which emacs` ] then log_daemon_msg \"Error: Emacs not found. emacsd will now exit.\" exit $EE_EMACS_NOT_FOUND fi # TODO Start emacs as normal user \"emacsd\" or \"daemon\" #emacs run under this uid emacsd_uid=\"1000\" socket_file=\"/tmp/emacs${emacsd_uid}/server\" case \"$1\" in start) #check whether already started if [ -e \"$socket_file\" ] then echo \"Erro","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#启动界面"},{"categories":["Emacs"],"content":" 其他设置 窗口标题我更喜欢窗口展示缓冲区的名字，然后是项目文件夹 (如果可用)。 (setq! frame-title-format '(\"%b – Doom Emacs\" (:eval (let ((project-name (projectile-project-name))) (unless (string= \"-\" project-name) (format \" - [%s]\" project-name)))))) 启动界面@tecosaur 做了一个相当棒的启动画面，心动！但是太复杂了。我只是想简单的在每次重启时更换 banner，仅此而已。 (setq! fancy-splash-image (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".png\" eos)))) (elt banners (random (length banners))))) 当然，不要忘记 ASCII banner (setq! ginshio/+doom-dashbord-ascii-banner (split-string (with-output-to-string (call-process \"cat\" nil standard-output nil (let ((banners (directory-files (expand-file-name \"banners\" doom-private-dir) 'full (rx \".txt\" eos)))) (elt banners (random (length banners)))))) \"\\n\" t)) (setq! +doom-dashboard-ascii-banner-fn #'(lambda () (mapc (lambda (line) (insert (propertize (+doom-dashboard--center +doom-dashboard--width line) 'face 'doom-dashboard-banner) \" \") (insert \"\\n\")) ginshio/+doom-dashbord-ascii-banner))) 守护进程守护进程是个好东西，可惜我用的是 WSL，没有 systemd ，不过 EmacsWiki 中还是列出了各种 Daemon 的方法。 Systemd (Not WSL) ~/.config/systemd/user/emacs.service [Unit] Description=Emacs text editor Documentation=info:emacs man:emacs(1) https://gnu.org/software/emacs/ [Service] Type=forking ExecStart=/usr/bin/emacs --daemon ExecStop=/usr/bin/emacsclient --no-wait --eval \"(progn (setq kill-emacs-hook 'nil) (kill-emacs))\" Restart=on-failure [Install] WantedBy=default.target Launchd (MacOS) /Library/LaunchAgents/gnu.emacs.daemon.plist \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e Label gnu.emacs.daemon ProgramArguments /Applications/Emacs.app/Contents/MacOS/Emacs --daemon RunAtLoad ServiceDescription Gnu Emacs Daemon rc.d (FreeBSD) 这来自 FreeBSD 论坛的讨论 /etc/init.d/emacsd #!/bin/sh # # PROVIDE: emacsd # REQUIRE: LOGIN # KEYWORD: shutdown . /etc/rc.subr name=\"emacsd\" rcvar=\"${name}_enable\" start_precmd=\"${name}_prestart\" load_rc_config $name : \"${emacsd_enable:=\"NO\"}\" : \"${emacsd_user:=\"nobody\"}\" piddir=\"/var/run/${emacsd_user}\" pidfile=\"${piddir}/${name}_daemon.pid\" pidfile_child=\"${piddir}/${name}_emacs.pid\" user_id=\"$(id -u ${emacsd_user}\" emacsd_env=\"$(env -iL ${user_id} XDG_RUNTIME_DIR=/var/run/user/${user_id} HOME=/home/${emacsd_user} | tr '\\n' ' ')\" proc_cmd=\"/usr/local/bin/emacs\" proc_args=\"--fg-daemon -u ${emacsd_user}\" command=\"/usr/sbin/daemon\" command_args=\"-f -P ${pidfile} -p ${pidfile_child} -r ${proc_cmd} ${proc_args}\" emacsd_prestart() { [ -z \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 XDG_RUNTIME_DIR=\"/var/run/user/$(id -u ${emacsd_user})\" ! [ -d \"${XDG_RUNTIME_DIR}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -m 0700 \"${XDG_RUNTIME_DIR}\" - ! [ -d \"${piddir}\" ] \u0026\u0026 \\ install -d -o \"$(id -u ${emacsd_user})\" -g \"$(id -u ${emacsd_user})\" \"${piddir}\" - return 0 } run_rc_command \"$1\" rc.d (Linux, Maybe WSL) 这是 EmacsWiki 上为 GNU/Debian 创建的 rc.d 脚本，或许 WSL 也能用， /etc/init.d/emacsd #!/bin/sh ### BEGIN INIT INFO # Provides: emacsd # Required-Start: $remote_fs $syslog # Required-Stop: $remote_fs $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start emacsd at boot time # Description: Enable service provided by daemon. ### END INIT INFO # Local Settings PATH=/usr/local/bin:/usr/bin:/bin # emacs location. emacs=\"emacs\" emacsclient=\"emacsclient\" # EE code EE_EMACS_NOT_FOUND=1 EE_INVALID_OPTION=2 EE_EMACS_FAIL_TO_START=3 EE_EMACS_FAIL_TO_STOP=4 # Real code begins here if [ -z `which emacs` ] then log_daemon_msg \"Error: Emacs not found. emacsd will now exit.\" exit $EE_EMACS_NOT_FOUND fi # TODO Start emacs as normal user \"emacsd\" or \"daemon\" #emacs run under this uid emacsd_uid=\"1000\" socket_file=\"/tmp/emacs${emacsd_uid}/server\" case \"$1\" in start) #check whether already started if [ -e \"$socket_file\" ] then echo \"Erro","date":"04-08","objectID":"/2022/doom_emacs_configuration/:2:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#守护进程"},{"categories":["Emacs"],"content":" 包","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:0","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#包"},{"categories":["Emacs"],"content":" 加载结构doom 通过 packages.el 来安装包，非常简单，只需要 package! 就可以安装。需要注意，不应该将该文件编译为字节码。 ;; -*- no-byte-compile: t; -*- ;;; $DOOMDIR/packages.el 警告: 不要禁用 ~/.emacs.d/core/packages.el 中列出的包。Doom 依赖这些，禁用它们可能出现严重问题。 从官方的源 MELPA / GNU ELPA / emacsmirror 安装 (package! some-package) 关闭某些包 (package! some-package :disable t) 从 Git Repo 安装 ;; github (package! github-package :recipe (:host github :repo \"username/repo\")) ;; gitlab (package! gitlab-package :recipe (:host gitlab :repo \"username/repo\")) ;; other (package! other-package :recipe (:host nil :repo \"https://example.com/repo\")) 如果 repo 仅中只有某个 / 某些文件是你需要的 (package! some-package :recipe (:host github :repo \"username/repo\" :files (\"some-file.el\" \"src/elisp/*.el\"))) 如果需要指定某个 commit 或某个 branch ;; commit (package! some-package :pin \"abcdefghijk\") ;; branch (package! some-package :recipe (:branch \"stable\")) 使用本地的 repo (package! some-package :recipe (:local-repo \"/path/to/repo\")) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:1","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#加载结构"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\u003cmouse-2\u003e\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#工具"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#缩写"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#hungry-delete"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#dired"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#magit"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#补全"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#consult"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#marginalia"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#lsp"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#拼写检查"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#string-inflection"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#tramp"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#vterm"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#yasnippet"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#screenshot"},{"categories":["Emacs"],"content":" 工具 缩写Emacs Stack Exchange 上的在多种模式下使用统一的缩写表，是一个很好的思路 (add-hook 'doom-first-buffer-hook (defun +abbrev-file-name () (setq-default abbrev-mode t) (setq abbrev-file-name (expand-file-name \"abbrev.el\" doom-private-dir)))) hungry delete一次 backspace 吃掉所有空白符 (当前光标限定) (package! hungry-delete :recipe (:host github :repo \"nflath/hungry-delete\")) (use-package! hungry-delete :config (setq-default hungry-delete-chars-to-skip \" \\t\\v\") (add-hook! 'after-init-hook #'global-hungry-delete-mode)) DIREDemacs 自带的强大文件管理器，和之后提到的 Magit、TRAMP 都是 Emacs 的杀手级应用。还出现了很多增强性的包来增加其能力，不过对我来说，稍微修改一下也就够了。 (after! dired (require 'dired-async) (define-key! dired-mode-map \"RET\" #'dired-find-alternate-file) (define-key! dired-mode-map \"C\" #'dired-async-do-copy) (define-key! dired-mode-map \"H\" #'dired-async-do-hardlink) (define-key! dired-mode-map \"R\" #'dired-async-do-rename) (define-key! dired-mode-map \"S\" #'dired-async-do-symlink) (define-key! dired-mode-map \"n\" #'dired-next-marked-file) (define-key! dired-mode-map \"p\" #'dired-prev-marked-file) (define-key! dired-mode-map \"=\" #'ginshio/dired-ediff-files) (define-key! dired-mode-map \"\" #'dired-mouse-find-file) (defun ginshio/dired-ediff-files () \"Mark files and ediff in dired mode, you can mark 1, 2 or 3 files and diff. see: https://oremacs.com/2017/03/18/dired-ediff/\" (let ((files (dired-get-marked-files))) (cond ((= (length files) 0)) ((= (length files) 1) (let ((file1 (nth 0 files)) (file2 (read-file-name \"file: \" (dired-dwim-target-directory)))) (ediff-files file1 file2))) ((= (length files) 2) (let ((file1 (nth 0 files)) (file2 (nth 1 files))) (ediff-files file1 file2))) ((= (length files) 3) (let ((file1 (car files)) (file2 (nth 1 files)) (file3 (nth 2 files))) (ediff-files3 file1 file2 file3))) (t (error \"no more than 3 files should be marked\"))))) (define-advice dired-do-print (:override (\u0026optional _)) \"show/hide dotfiles in current dired see: https://www.emacswiki.org/emacs/DiredOmitMode\" (cond ((or (not (boundp 'dired-dotfiles-show-p)) dired-dotfiles-show-p) (setq-local dired-dotfiles-show-p nil) (dired-mark-files-regexp \"^\\\\.\") (dired-do-kill-lines)) (t (revert-buffer) (setq-local dired-dotfiles-show-p t)))) (define-advice dired-up-directory (:override (\u0026optional _)) \"goto up directory in this buffer\" (find-alternate-file \"..\")) (define-advice dired-do-compress-to (:override (\u0026optional _)) \"Compress selected files and directories to an archive.\" (let* ((output (read-file-name \"Compress to: \")) (command-assoc (assoc output dired-compress-files-alist 'string-match)) (files-str (mapconcat 'identity (dired-get-marked-files t) \" \"))) (when (and command-assoc (not (string= \"\" files-str))) (let ((command (format-spec (cdr command-assoc) `((?o . ,output) (?i . ,files-str))))) (async-start (lambda () (shell-command command)) nil)))))) Magit 这应该是 Emacs 的杀手应用之一了，感谢 Jonas 及其他贡献者。 提交模板 现在并没有完成这部分，处于一种完全不会写提交的状态，以后大概率会增加一个提交模板。 Delta Delta 是用 rust 实现的 git diff 语法高亮的工具。该作者还将其挂接到了 magit 的 diff 视图上 (默认不会有语法高亮)。不过这需要 delta 二进制文件，在 cargo 安装显得简单些，不过你也可以选择 GitHub Release。 cargo install git-delta 简单地配置它就行 (package! magit-delta :recipe (:host github :repo \"dandavison/magit-delta\")) (use-package! magit-delta :after magit :hook (magit-mode . magit-delta-mode)) 冲突 在 Emacs 中处理冲突也是不错的体验，和或许可以尝试自己制造一点 (defun smerge-repeatedly () \"Perform smerge actions again and again\" (interactive) (smerge-mode 1) (smerge-transient)) (after! transient (transient-define-prefix smerge-transient () [[\"Move\" (\"n\" \"next\" (lambda () (interactive) (ignore-errors (smerge-next)) (smerge-repeatedly))) (\"p\" \"previous\" (lambda () (interactive) (ignore-errors (smerge-prev)) (smerge-repeatedly)))] [\"Keep\" (\"b\" \"base\" (lambda () (interactive) (ignore-errors (smerge-keep-base)) (smerge-repeatedly))) (\"u\" \"upper\" (lambda () (interactive) (ignore-errors (smerge-keep-upper)) (smerge-repeatedly))) (\"l\" \"lower\" (lambda () (interactive) (ignore-errors (smerge-keep-lower)) (smerge-repeatedly))) (\"a\" \"all\" (lambda () (interactive) (ignore-errors (smerge-keep-all))","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#ebooks"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#ui"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#nyan"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#eros"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#theme-magic"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#emojify"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#tabs"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#treemacs"},{"categories":["Emacs"],"content":" UI Nyan首先添加一下彩虹猫，这不能忘！ (package! nyan-mode :recipe (:host github :repo \"TeMPOraL/nyan-mode\")) (use-package! nyan-mode :config (setq nyan-animate-nyancat t nyan-wavy-trail t nyan-cat-face-number 4 nyan-bar-length 16 nyan-minimum-window-width 64) (add-hook! 'doom-modeline-hook #'nyan-mode)) Eros这个包可以修改 emacs lisp 内联执行的视觉效果，让这个结果的前缀更好看一点。 (setq eros-eval-result-prefix \"⟹ \") ; default =\u003e 你可以用 C-x C-e 来对比一下前后变化 (+ 1 1 (* 2 2) 1) return 2 ** 4 Theme Magic非常神奇的是你可以在 Emacs 中用现有的 Theme，改变终端的 Theme，且 GUI 和 TUI 都可用！作者说 Linux 和 Mac 可用， Windows Terminal + WSL 同样适用，压力来到了纯 Windows 下的 Emacs。 (package! theme-magic) 这个操作使用 pywal ，你可以通过仓库安装它，不过最简单的方式就是 pip 。 sudo python3 -m pip install pywal Theme Magic 提供了一个数字界面，尝试从饱和度、色彩差异来有效的选取八个颜色。然而，它可能会为 light 选择相同的颜色，并不总能够选取最佳颜色。我们可以用 Doom themes 提供的色彩工具来轻松获取合理的配色来生成 light 版本 — 现在就开始！ (use-package! theme-magic :defer t :after +doom-dashboard :config (defadvice! theme-magic--auto-extract-16-doom-colors () :override #'theme-magic--auto-extract-16-colors (list (face-attribute 'default :background) (doom-color 'error) (doom-color 'success) (doom-color 'type) (doom-color 'keywords) (doom-color 'constants) (doom-color 'functions) (face-attribute 'default :foreground) (face-attribute 'shadow :foreground) (doom-blend 'base8 'error 0.1) (doom-blend 'base8 'success 0.1) (doom-blend 'base8 'type 0.1) (doom-blend 'base8 'keywords 0.1) (doom-blend 'base8 'constants 0.1) (doom-blend 'base8 'functions 0.1) (face-attribute 'default :foreground)))) EmojifyOOTB 的 emoji 模块！麻烦的一点是设置的有些默认字符，可能会显示为 emoji。直接从 emoji 表中删除它们 (除了有点暴力) (defvar emojify-disabled-emojis '(;; Org \"◼\" \"☑\" \"☸\" \"⚙\" \"⏩\" \"⏪\" ;; Org Heading \"✙\" \"♱\" \"♰\" \"☥\" \"✞\" \"✟\" \"✝\" \"†\" \"☰\" \"☱\" \"☲\" \"☳\" \"☴\" \"☵\" \"☶\" \"☷\" \"☿\" \"♀\" \"♁\" \"♂\" \"♃\" \"♄\" \"♅\" \"♆\" \"♇\" \"☽\" \"☾\" \"♈\" \"♉\" \"♊\" \"♋\" \"♌\" \"♍\" \"♎\" \"♏\" \"♐\" \"♑\" \"♒\" \"♓\" \"♔\" \"♕\" \"♖\" \"♗\" \"♘\" \"♙\" \"♚\" \"♛\" \"♜\" \"♝\" \"♞\" \"♟\" ;; Org Agenda \"⚡\" \"↑\" \"↓\" \"☕\" \"❓\" ;; I just want to see this as text \"©\" \"™\") \"Characters that should never be affected by `emojify-mode'.\") (defadvice! emojify-delete-from-data () \"Ensure `emojify-disabled-emojis' don't appear in `emojify-emojis'.\" :after #'emojify-set-emoji-data (dolist (emoji emojify-disabled-emojis) (remhash emoji emojify-emojis))) Tabs如果你想像现代编辑器一样拥有 tabs，或许你可以考虑一下 (after! centaur-tabs (centaur-tabs-group-by-projectile-project) (centaur-tabs-mode t) (setq! centaur-tabs-style \"bar\" centaur-tabs-set-icons t centaur-tabs-set-modified-marker t centaur-tabs-show-navigation-buttons t centaur-tabs-gray-out-icons 'buffer)) 如果想要 tabs 底下显示 bar ，需要开启 x-underline-at-descent-line，但是它在 (after! ...) 中不起作用。 (setq! centaur-tabs-set-bar 'under x-underline-at-descent-line t) Treemacs开启 git-mode 、 follow-mode 和 indent-guide-mode ，体验还是不错 (after! treemacs (setq! treemacs-indent-guide-mode t treemacs-show-hidden-files t doom-themes-treemacs-theme \"doom-colors\" treemacs-file-event-delay 1000 treemacs-file-follow-delay 0.1) (treemacs-follow-mode t) ;; (treemacs-filewatch-mode t) (treemacs-git-mode 'deferred) (treemacs-hide-gitignored-files-mode t)) 可能是我的用法不对？ filewatch-mode 总是有问题，在外部修改 (不在 treemacs 中重命名、添加、删除等) 就会有问题，没法更新 tree。需要自己手动折叠一下。 hl todohl-todo 允许你设置一些关键字，这些关键字将高亮并且便于查找。往往用于代码注释中强调某些内容。 (custom-set-variables '(hl-todo-keyword-faces '((\"NOTE\" font-lock-builtin-face bold) ;; needs discussion or further investigation. (\"REVIEW\" font-lock-keyword-face bold) ;; review was conducted. (\"HACK\" font-lock-variable-name-face bold) ;; workaround a known problem. (\"DEPRECATED\" region bold) ;; why it was deprecated and to suggest an alternative. (\"XXX+\" font-lock-constant-face bold) ;; warn other programmers of problematic or misguiding code. (\"TODO\" font-lock-function-name-face bold) ;; tasks/features to be done. (\"FIXME\" font-lock-warning-face bold) ;; problematic or ugly code needing refactoring or cleanup. (\"KLUDGE\" font-lock-preprocessor-face bold ) (\"BUG\" error bold) ;; a known bug that should be corrected. ))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#hl-todo"},{"categories":["Emacs"],"content":" 有趣的生活 xkcd也许你看过，但是现在让你看个够。 (package! xkcd) (after! xkcd (setq xkcd-cache-dir (expand-file-name \"xkcd/\" doom-cache-dir) xkcd-cache-latest (concat xkcd-cache-dir \"latest\")) (unless (file-exists-p xkcd-cache-dir) (make-directory xkcd-cache-dir))) 现在让 Org-Mode 可以支持 xkcd (after! org (org-link-set-parameters \"xkcd\" :image-data-fun #'+org-xkcd-image-fn :follow #'+org-xkcd-open-fn :export #'+org-xkcd-export)) ;;;###autoload (defun ginshio/xkcd-file-info (\u0026optional num) \"Get xkcd image info\" (require 'xkcd) (let* ((url (format \"https://xkcd.com/%d/info.0.json\" num)) (json-assoc (json-read-from-string (xkcd-get-json url num)))) `(:img ,(cdr (assoc 'img json-assoc)) :alt ,(cdr (assoc 'alt json-assoc)) :title ,(cdr (assoc 'title json-assoc))))) ;;;###autoload (defun +org-xkcd-open-fn (link) (+org-xkcd-image-fn nil link nil)) ;;;###autoload (defun +org-xkcd-image-fn (protocol link description) \"Get image data for xkcd num LINK\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number link))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt))) (+org-image-file-data-fn protocol (xkcd-download img (string-to-number link)) description))) ;;;###autoload (defun +org-xkcd-export (num desc backend _com) \"Convert xkcd to html/LaTeX/Markdown form\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number num))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt)) (title (plist-get xkcd-info :title)) (file (xkcd-download img (string-to-number num)))) (cond ((org-export-derived-backend-p backend 'hugo) (format \" \" img (subst-char-in-string ?\\\" ?“ alt))) ((org-export-derived-backend-p backend 'html) (format \"\u003cimg class='invertible' src='%s' title=\\\"%s\\\" alt='%s'\u003e\" img (subst-char-in-string ?\\\" ?“ alt) title)) ((org-export-derived-backend-p backend 'latex) (format \"\\\\begin{figure}[!htb] \\\\centering \\\\includegraphics[scale=0.4]{%s}%s \\\\end{figure}\" file (if (equal desc (format \"xkcd:%s\" num)) \"\" (format \"\\n \\\\caption*{\\\\label{xkcd:%s} %s}\" num (or desc (format \"\\\\textbf{%s} %s\" title alt)))))) ((org-export-derived-backend-p backend 'markdown) (format \"![%s](https://xkcd.com/%s)\" (subst-char-in-string ?\\\" ?“ alt) num)) (t (format \"https://xkcd.com/%s\" num))))) 打字机或许你想听听打字机的声音，或者让身边人听听。 (package! selectric-mode) (use-package! selectric-mode :defer t) 当然这是需要 aplay 的，如果你没有，可以找一下有没有 alsa 相关内容。但是我的 WSL ？没声？没声！ wakatimeWakatime 可以帮助你记录在编程语言、编辑器、项目以及操作系统上所用的时间，并在 GitHub / GitLab 同名仓库下展示。 (package! wakatime-mode :recipe (:host github :repo \"wakatime/wakatime-mode\")) (global-wakatime-mode) Elcord除非你不断告诉身边的每个人，不然你使用 Emacs 的意义是什么？ ;; (package! elcord) ;; (use-package! elcord ;; :commands elcord-mode ;; :config ;; (setq elcord-use-major-mode-as-main-icon t)) 但是国内好像用不上，到时候看看 steam。 IRCcirce 是 Emacs 的 IRC 客户端 (这个名字+缩写简直太棒了)，还是将人变为怪物的希腊女神 — 喀耳刻。 还是用前面说的意思吧，用它与那些在线 隐身 的人聊天！ 或许我们暂时用不上它，先这样吧 字典我用了很久也不知道 doom 提供了字典，采用 define-word 和 wordnut 提供服务，不过离线字典可能更符合本地的要求。 GoldenDict CLI 在 GitHub 上是 相当好评的问题，开发者已经将其加入 1.5 的待做事项，但 1.5 依旧难产，也许你可以考虑贡献一下。 我的配置也等 GoldenDict 吧。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#有趣的生活"},{"categories":["Emacs"],"content":" 有趣的生活 xkcd也许你看过，但是现在让你看个够。 (package! xkcd) (after! xkcd (setq xkcd-cache-dir (expand-file-name \"xkcd/\" doom-cache-dir) xkcd-cache-latest (concat xkcd-cache-dir \"latest\")) (unless (file-exists-p xkcd-cache-dir) (make-directory xkcd-cache-dir))) 现在让 Org-Mode 可以支持 xkcd (after! org (org-link-set-parameters \"xkcd\" :image-data-fun #'+org-xkcd-image-fn :follow #'+org-xkcd-open-fn :export #'+org-xkcd-export)) ;;;###autoload (defun ginshio/xkcd-file-info (\u0026optional num) \"Get xkcd image info\" (require 'xkcd) (let* ((url (format \"https://xkcd.com/%d/info.0.json\" num)) (json-assoc (json-read-from-string (xkcd-get-json url num)))) `(:img ,(cdr (assoc 'img json-assoc)) :alt ,(cdr (assoc 'alt json-assoc)) :title ,(cdr (assoc 'title json-assoc))))) ;;;###autoload (defun +org-xkcd-open-fn (link) (+org-xkcd-image-fn nil link nil)) ;;;###autoload (defun +org-xkcd-image-fn (protocol link description) \"Get image data for xkcd num LINK\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number link))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt))) (+org-image-file-data-fn protocol (xkcd-download img (string-to-number link)) description))) ;;;###autoload (defun +org-xkcd-export (num desc backend _com) \"Convert xkcd to html/LaTeX/Markdown form\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number num))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt)) (title (plist-get xkcd-info :title)) (file (xkcd-download img (string-to-number num)))) (cond ((org-export-derived-backend-p backend 'hugo) (format \" \" img (subst-char-in-string ?\\\" ?“ alt))) ((org-export-derived-backend-p backend 'html) (format \"\" img (subst-char-in-string ?\\\" ?“ alt) title)) ((org-export-derived-backend-p backend 'latex) (format \"\\\\begin{figure}[!htb] \\\\centering \\\\includegraphics[scale=0.4]{%s}%s \\\\end{figure}\" file (if (equal desc (format \"xkcd:%s\" num)) \"\" (format \"\\n \\\\caption*{\\\\label{xkcd:%s} %s}\" num (or desc (format \"\\\\textbf{%s} %s\" title alt)))))) ((org-export-derived-backend-p backend 'markdown) (format \"![%s](https://xkcd.com/%s)\" (subst-char-in-string ?\\\" ?“ alt) num)) (t (format \"https://xkcd.com/%s\" num))))) 打字机或许你想听听打字机的声音，或者让身边人听听。 (package! selectric-mode) (use-package! selectric-mode :defer t) 当然这是需要 aplay 的，如果你没有，可以找一下有没有 alsa 相关内容。但是我的 WSL ？没声？没声！ wakatimeWakatime 可以帮助你记录在编程语言、编辑器、项目以及操作系统上所用的时间，并在 GitHub / GitLab 同名仓库下展示。 (package! wakatime-mode :recipe (:host github :repo \"wakatime/wakatime-mode\")) (global-wakatime-mode) Elcord除非你不断告诉身边的每个人，不然你使用 Emacs 的意义是什么？ ;; (package! elcord) ;; (use-package! elcord ;; :commands elcord-mode ;; :config ;; (setq elcord-use-major-mode-as-main-icon t)) 但是国内好像用不上，到时候看看 steam。 IRCcirce 是 Emacs 的 IRC 客户端 (这个名字+缩写简直太棒了)，还是将人变为怪物的希腊女神 — 喀耳刻。 还是用前面说的意思吧，用它与那些在线 隐身 的人聊天！ 或许我们暂时用不上它，先这样吧 字典我用了很久也不知道 doom 提供了字典，采用 define-word 和 wordnut 提供服务，不过离线字典可能更符合本地的要求。 GoldenDict CLI 在 GitHub 上是 相当好评的问题，开发者已经将其加入 1.5 的待做事项，但 1.5 依旧难产，也许你可以考虑贡献一下。 我的配置也等 GoldenDict 吧。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#xkcd"},{"categories":["Emacs"],"content":" 有趣的生活 xkcd也许你看过，但是现在让你看个够。 (package! xkcd) (after! xkcd (setq xkcd-cache-dir (expand-file-name \"xkcd/\" doom-cache-dir) xkcd-cache-latest (concat xkcd-cache-dir \"latest\")) (unless (file-exists-p xkcd-cache-dir) (make-directory xkcd-cache-dir))) 现在让 Org-Mode 可以支持 xkcd (after! org (org-link-set-parameters \"xkcd\" :image-data-fun #'+org-xkcd-image-fn :follow #'+org-xkcd-open-fn :export #'+org-xkcd-export)) ;;;###autoload (defun ginshio/xkcd-file-info (\u0026optional num) \"Get xkcd image info\" (require 'xkcd) (let* ((url (format \"https://xkcd.com/%d/info.0.json\" num)) (json-assoc (json-read-from-string (xkcd-get-json url num)))) `(:img ,(cdr (assoc 'img json-assoc)) :alt ,(cdr (assoc 'alt json-assoc)) :title ,(cdr (assoc 'title json-assoc))))) ;;;###autoload (defun +org-xkcd-open-fn (link) (+org-xkcd-image-fn nil link nil)) ;;;###autoload (defun +org-xkcd-image-fn (protocol link description) \"Get image data for xkcd num LINK\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number link))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt))) (+org-image-file-data-fn protocol (xkcd-download img (string-to-number link)) description))) ;;;###autoload (defun +org-xkcd-export (num desc backend _com) \"Convert xkcd to html/LaTeX/Markdown form\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number num))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt)) (title (plist-get xkcd-info :title)) (file (xkcd-download img (string-to-number num)))) (cond ((org-export-derived-backend-p backend 'hugo) (format \" \" img (subst-char-in-string ?\\\" ?“ alt))) ((org-export-derived-backend-p backend 'html) (format \"\" img (subst-char-in-string ?\\\" ?“ alt) title)) ((org-export-derived-backend-p backend 'latex) (format \"\\\\begin{figure}[!htb] \\\\centering \\\\includegraphics[scale=0.4]{%s}%s \\\\end{figure}\" file (if (equal desc (format \"xkcd:%s\" num)) \"\" (format \"\\n \\\\caption*{\\\\label{xkcd:%s} %s}\" num (or desc (format \"\\\\textbf{%s} %s\" title alt)))))) ((org-export-derived-backend-p backend 'markdown) (format \"![%s](https://xkcd.com/%s)\" (subst-char-in-string ?\\\" ?“ alt) num)) (t (format \"https://xkcd.com/%s\" num))))) 打字机或许你想听听打字机的声音，或者让身边人听听。 (package! selectric-mode) (use-package! selectric-mode :defer t) 当然这是需要 aplay 的，如果你没有，可以找一下有没有 alsa 相关内容。但是我的 WSL ？没声？没声！ wakatimeWakatime 可以帮助你记录在编程语言、编辑器、项目以及操作系统上所用的时间，并在 GitHub / GitLab 同名仓库下展示。 (package! wakatime-mode :recipe (:host github :repo \"wakatime/wakatime-mode\")) (global-wakatime-mode) Elcord除非你不断告诉身边的每个人，不然你使用 Emacs 的意义是什么？ ;; (package! elcord) ;; (use-package! elcord ;; :commands elcord-mode ;; :config ;; (setq elcord-use-major-mode-as-main-icon t)) 但是国内好像用不上，到时候看看 steam。 IRCcirce 是 Emacs 的 IRC 客户端 (这个名字+缩写简直太棒了)，还是将人变为怪物的希腊女神 — 喀耳刻。 还是用前面说的意思吧，用它与那些在线 隐身 的人聊天！ 或许我们暂时用不上它，先这样吧 字典我用了很久也不知道 doom 提供了字典，采用 define-word 和 wordnut 提供服务，不过离线字典可能更符合本地的要求。 GoldenDict CLI 在 GitHub 上是 相当好评的问题，开发者已经将其加入 1.5 的待做事项，但 1.5 依旧难产，也许你可以考虑贡献一下。 我的配置也等 GoldenDict 吧。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#打字机"},{"categories":["Emacs"],"content":" 有趣的生活 xkcd也许你看过，但是现在让你看个够。 (package! xkcd) (after! xkcd (setq xkcd-cache-dir (expand-file-name \"xkcd/\" doom-cache-dir) xkcd-cache-latest (concat xkcd-cache-dir \"latest\")) (unless (file-exists-p xkcd-cache-dir) (make-directory xkcd-cache-dir))) 现在让 Org-Mode 可以支持 xkcd (after! org (org-link-set-parameters \"xkcd\" :image-data-fun #'+org-xkcd-image-fn :follow #'+org-xkcd-open-fn :export #'+org-xkcd-export)) ;;;###autoload (defun ginshio/xkcd-file-info (\u0026optional num) \"Get xkcd image info\" (require 'xkcd) (let* ((url (format \"https://xkcd.com/%d/info.0.json\" num)) (json-assoc (json-read-from-string (xkcd-get-json url num)))) `(:img ,(cdr (assoc 'img json-assoc)) :alt ,(cdr (assoc 'alt json-assoc)) :title ,(cdr (assoc 'title json-assoc))))) ;;;###autoload (defun +org-xkcd-open-fn (link) (+org-xkcd-image-fn nil link nil)) ;;;###autoload (defun +org-xkcd-image-fn (protocol link description) \"Get image data for xkcd num LINK\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number link))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt))) (+org-image-file-data-fn protocol (xkcd-download img (string-to-number link)) description))) ;;;###autoload (defun +org-xkcd-export (num desc backend _com) \"Convert xkcd to html/LaTeX/Markdown form\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number num))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt)) (title (plist-get xkcd-info :title)) (file (xkcd-download img (string-to-number num)))) (cond ((org-export-derived-backend-p backend 'hugo) (format \" \" img (subst-char-in-string ?\\\" ?“ alt))) ((org-export-derived-backend-p backend 'html) (format \"\" img (subst-char-in-string ?\\\" ?“ alt) title)) ((org-export-derived-backend-p backend 'latex) (format \"\\\\begin{figure}[!htb] \\\\centering \\\\includegraphics[scale=0.4]{%s}%s \\\\end{figure}\" file (if (equal desc (format \"xkcd:%s\" num)) \"\" (format \"\\n \\\\caption*{\\\\label{xkcd:%s} %s}\" num (or desc (format \"\\\\textbf{%s} %s\" title alt)))))) ((org-export-derived-backend-p backend 'markdown) (format \"![%s](https://xkcd.com/%s)\" (subst-char-in-string ?\\\" ?“ alt) num)) (t (format \"https://xkcd.com/%s\" num))))) 打字机或许你想听听打字机的声音，或者让身边人听听。 (package! selectric-mode) (use-package! selectric-mode :defer t) 当然这是需要 aplay 的，如果你没有，可以找一下有没有 alsa 相关内容。但是我的 WSL ？没声？没声！ wakatimeWakatime 可以帮助你记录在编程语言、编辑器、项目以及操作系统上所用的时间，并在 GitHub / GitLab 同名仓库下展示。 (package! wakatime-mode :recipe (:host github :repo \"wakatime/wakatime-mode\")) (global-wakatime-mode) Elcord除非你不断告诉身边的每个人，不然你使用 Emacs 的意义是什么？ ;; (package! elcord) ;; (use-package! elcord ;; :commands elcord-mode ;; :config ;; (setq elcord-use-major-mode-as-main-icon t)) 但是国内好像用不上，到时候看看 steam。 IRCcirce 是 Emacs 的 IRC 客户端 (这个名字+缩写简直太棒了)，还是将人变为怪物的希腊女神 — 喀耳刻。 还是用前面说的意思吧，用它与那些在线 隐身 的人聊天！ 或许我们暂时用不上它，先这样吧 字典我用了很久也不知道 doom 提供了字典，采用 define-word 和 wordnut 提供服务，不过离线字典可能更符合本地的要求。 GoldenDict CLI 在 GitHub 上是 相当好评的问题，开发者已经将其加入 1.5 的待做事项，但 1.5 依旧难产，也许你可以考虑贡献一下。 我的配置也等 GoldenDict 吧。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#wakatime"},{"categories":["Emacs"],"content":" 有趣的生活 xkcd也许你看过，但是现在让你看个够。 (package! xkcd) (after! xkcd (setq xkcd-cache-dir (expand-file-name \"xkcd/\" doom-cache-dir) xkcd-cache-latest (concat xkcd-cache-dir \"latest\")) (unless (file-exists-p xkcd-cache-dir) (make-directory xkcd-cache-dir))) 现在让 Org-Mode 可以支持 xkcd (after! org (org-link-set-parameters \"xkcd\" :image-data-fun #'+org-xkcd-image-fn :follow #'+org-xkcd-open-fn :export #'+org-xkcd-export)) ;;;###autoload (defun ginshio/xkcd-file-info (\u0026optional num) \"Get xkcd image info\" (require 'xkcd) (let* ((url (format \"https://xkcd.com/%d/info.0.json\" num)) (json-assoc (json-read-from-string (xkcd-get-json url num)))) `(:img ,(cdr (assoc 'img json-assoc)) :alt ,(cdr (assoc 'alt json-assoc)) :title ,(cdr (assoc 'title json-assoc))))) ;;;###autoload (defun +org-xkcd-open-fn (link) (+org-xkcd-image-fn nil link nil)) ;;;###autoload (defun +org-xkcd-image-fn (protocol link description) \"Get image data for xkcd num LINK\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number link))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt))) (+org-image-file-data-fn protocol (xkcd-download img (string-to-number link)) description))) ;;;###autoload (defun +org-xkcd-export (num desc backend _com) \"Convert xkcd to html/LaTeX/Markdown form\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number num))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt)) (title (plist-get xkcd-info :title)) (file (xkcd-download img (string-to-number num)))) (cond ((org-export-derived-backend-p backend 'hugo) (format \" \" img (subst-char-in-string ?\\\" ?“ alt))) ((org-export-derived-backend-p backend 'html) (format \"\" img (subst-char-in-string ?\\\" ?“ alt) title)) ((org-export-derived-backend-p backend 'latex) (format \"\\\\begin{figure}[!htb] \\\\centering \\\\includegraphics[scale=0.4]{%s}%s \\\\end{figure}\" file (if (equal desc (format \"xkcd:%s\" num)) \"\" (format \"\\n \\\\caption*{\\\\label{xkcd:%s} %s}\" num (or desc (format \"\\\\textbf{%s} %s\" title alt)))))) ((org-export-derived-backend-p backend 'markdown) (format \"![%s](https://xkcd.com/%s)\" (subst-char-in-string ?\\\" ?“ alt) num)) (t (format \"https://xkcd.com/%s\" num))))) 打字机或许你想听听打字机的声音，或者让身边人听听。 (package! selectric-mode) (use-package! selectric-mode :defer t) 当然这是需要 aplay 的，如果你没有，可以找一下有没有 alsa 相关内容。但是我的 WSL ？没声？没声！ wakatimeWakatime 可以帮助你记录在编程语言、编辑器、项目以及操作系统上所用的时间，并在 GitHub / GitLab 同名仓库下展示。 (package! wakatime-mode :recipe (:host github :repo \"wakatime/wakatime-mode\")) (global-wakatime-mode) Elcord除非你不断告诉身边的每个人，不然你使用 Emacs 的意义是什么？ ;; (package! elcord) ;; (use-package! elcord ;; :commands elcord-mode ;; :config ;; (setq elcord-use-major-mode-as-main-icon t)) 但是国内好像用不上，到时候看看 steam。 IRCcirce 是 Emacs 的 IRC 客户端 (这个名字+缩写简直太棒了)，还是将人变为怪物的希腊女神 — 喀耳刻。 还是用前面说的意思吧，用它与那些在线 隐身 的人聊天！ 或许我们暂时用不上它，先这样吧 字典我用了很久也不知道 doom 提供了字典，采用 define-word 和 wordnut 提供服务，不过离线字典可能更符合本地的要求。 GoldenDict CLI 在 GitHub 上是 相当好评的问题，开发者已经将其加入 1.5 的待做事项，但 1.5 依旧难产，也许你可以考虑贡献一下。 我的配置也等 GoldenDict 吧。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#elcord"},{"categories":["Emacs"],"content":" 有趣的生活 xkcd也许你看过，但是现在让你看个够。 (package! xkcd) (after! xkcd (setq xkcd-cache-dir (expand-file-name \"xkcd/\" doom-cache-dir) xkcd-cache-latest (concat xkcd-cache-dir \"latest\")) (unless (file-exists-p xkcd-cache-dir) (make-directory xkcd-cache-dir))) 现在让 Org-Mode 可以支持 xkcd (after! org (org-link-set-parameters \"xkcd\" :image-data-fun #'+org-xkcd-image-fn :follow #'+org-xkcd-open-fn :export #'+org-xkcd-export)) ;;;###autoload (defun ginshio/xkcd-file-info (\u0026optional num) \"Get xkcd image info\" (require 'xkcd) (let* ((url (format \"https://xkcd.com/%d/info.0.json\" num)) (json-assoc (json-read-from-string (xkcd-get-json url num)))) `(:img ,(cdr (assoc 'img json-assoc)) :alt ,(cdr (assoc 'alt json-assoc)) :title ,(cdr (assoc 'title json-assoc))))) ;;;###autoload (defun +org-xkcd-open-fn (link) (+org-xkcd-image-fn nil link nil)) ;;;###autoload (defun +org-xkcd-image-fn (protocol link description) \"Get image data for xkcd num LINK\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number link))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt))) (+org-image-file-data-fn protocol (xkcd-download img (string-to-number link)) description))) ;;;###autoload (defun +org-xkcd-export (num desc backend _com) \"Convert xkcd to html/LaTeX/Markdown form\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number num))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt)) (title (plist-get xkcd-info :title)) (file (xkcd-download img (string-to-number num)))) (cond ((org-export-derived-backend-p backend 'hugo) (format \" \" img (subst-char-in-string ?\\\" ?“ alt))) ((org-export-derived-backend-p backend 'html) (format \"\" img (subst-char-in-string ?\\\" ?“ alt) title)) ((org-export-derived-backend-p backend 'latex) (format \"\\\\begin{figure}[!htb] \\\\centering \\\\includegraphics[scale=0.4]{%s}%s \\\\end{figure}\" file (if (equal desc (format \"xkcd:%s\" num)) \"\" (format \"\\n \\\\caption*{\\\\label{xkcd:%s} %s}\" num (or desc (format \"\\\\textbf{%s} %s\" title alt)))))) ((org-export-derived-backend-p backend 'markdown) (format \"![%s](https://xkcd.com/%s)\" (subst-char-in-string ?\\\" ?“ alt) num)) (t (format \"https://xkcd.com/%s\" num))))) 打字机或许你想听听打字机的声音，或者让身边人听听。 (package! selectric-mode) (use-package! selectric-mode :defer t) 当然这是需要 aplay 的，如果你没有，可以找一下有没有 alsa 相关内容。但是我的 WSL ？没声？没声！ wakatimeWakatime 可以帮助你记录在编程语言、编辑器、项目以及操作系统上所用的时间，并在 GitHub / GitLab 同名仓库下展示。 (package! wakatime-mode :recipe (:host github :repo \"wakatime/wakatime-mode\")) (global-wakatime-mode) Elcord除非你不断告诉身边的每个人，不然你使用 Emacs 的意义是什么？ ;; (package! elcord) ;; (use-package! elcord ;; :commands elcord-mode ;; :config ;; (setq elcord-use-major-mode-as-main-icon t)) 但是国内好像用不上，到时候看看 steam。 IRCcirce 是 Emacs 的 IRC 客户端 (这个名字+缩写简直太棒了)，还是将人变为怪物的希腊女神 — 喀耳刻。 还是用前面说的意思吧，用它与那些在线 隐身 的人聊天！ 或许我们暂时用不上它，先这样吧 字典我用了很久也不知道 doom 提供了字典，采用 define-word 和 wordnut 提供服务，不过离线字典可能更符合本地的要求。 GoldenDict CLI 在 GitHub 上是 相当好评的问题，开发者已经将其加入 1.5 的待做事项，但 1.5 依旧难产，也许你可以考虑贡献一下。 我的配置也等 GoldenDict 吧。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#irc"},{"categories":["Emacs"],"content":" 有趣的生活 xkcd也许你看过，但是现在让你看个够。 (package! xkcd) (after! xkcd (setq xkcd-cache-dir (expand-file-name \"xkcd/\" doom-cache-dir) xkcd-cache-latest (concat xkcd-cache-dir \"latest\")) (unless (file-exists-p xkcd-cache-dir) (make-directory xkcd-cache-dir))) 现在让 Org-Mode 可以支持 xkcd (after! org (org-link-set-parameters \"xkcd\" :image-data-fun #'+org-xkcd-image-fn :follow #'+org-xkcd-open-fn :export #'+org-xkcd-export)) ;;;###autoload (defun ginshio/xkcd-file-info (\u0026optional num) \"Get xkcd image info\" (require 'xkcd) (let* ((url (format \"https://xkcd.com/%d/info.0.json\" num)) (json-assoc (json-read-from-string (xkcd-get-json url num)))) `(:img ,(cdr (assoc 'img json-assoc)) :alt ,(cdr (assoc 'alt json-assoc)) :title ,(cdr (assoc 'title json-assoc))))) ;;;###autoload (defun +org-xkcd-open-fn (link) (+org-xkcd-image-fn nil link nil)) ;;;###autoload (defun +org-xkcd-image-fn (protocol link description) \"Get image data for xkcd num LINK\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number link))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt))) (+org-image-file-data-fn protocol (xkcd-download img (string-to-number link)) description))) ;;;###autoload (defun +org-xkcd-export (num desc backend _com) \"Convert xkcd to html/LaTeX/Markdown form\" (let* ((xkcd-info (ginshio/xkcd-file-info (string-to-number num))) (img (plist-get xkcd-info :img)) (alt (plist-get xkcd-info :alt)) (title (plist-get xkcd-info :title)) (file (xkcd-download img (string-to-number num)))) (cond ((org-export-derived-backend-p backend 'hugo) (format \" \" img (subst-char-in-string ?\\\" ?“ alt))) ((org-export-derived-backend-p backend 'html) (format \"\" img (subst-char-in-string ?\\\" ?“ alt) title)) ((org-export-derived-backend-p backend 'latex) (format \"\\\\begin{figure}[!htb] \\\\centering \\\\includegraphics[scale=0.4]{%s}%s \\\\end{figure}\" file (if (equal desc (format \"xkcd:%s\" num)) \"\" (format \"\\n \\\\caption*{\\\\label{xkcd:%s} %s}\" num (or desc (format \"\\\\textbf{%s} %s\" title alt)))))) ((org-export-derived-backend-p backend 'markdown) (format \"![%s](https://xkcd.com/%s)\" (subst-char-in-string ?\\\" ?“ alt) num)) (t (format \"https://xkcd.com/%s\" num))))) 打字机或许你想听听打字机的声音，或者让身边人听听。 (package! selectric-mode) (use-package! selectric-mode :defer t) 当然这是需要 aplay 的，如果你没有，可以找一下有没有 alsa 相关内容。但是我的 WSL ？没声？没声！ wakatimeWakatime 可以帮助你记录在编程语言、编辑器、项目以及操作系统上所用的时间，并在 GitHub / GitLab 同名仓库下展示。 (package! wakatime-mode :recipe (:host github :repo \"wakatime/wakatime-mode\")) (global-wakatime-mode) Elcord除非你不断告诉身边的每个人，不然你使用 Emacs 的意义是什么？ ;; (package! elcord) ;; (use-package! elcord ;; :commands elcord-mode ;; :config ;; (setq elcord-use-major-mode-as-main-icon t)) 但是国内好像用不上，到时候看看 steam。 IRCcirce 是 Emacs 的 IRC 客户端 (这个名字+缩写简直太棒了)，还是将人变为怪物的希腊女神 — 喀耳刻。 还是用前面说的意思吧，用它与那些在线 隐身 的人聊天！ 或许我们暂时用不上它，先这样吧 字典我用了很久也不知道 doom 提供了字典，采用 define-word 和 wordnut 提供服务，不过离线字典可能更符合本地的要求。 GoldenDict CLI 在 GitHub 上是 相当好评的问题，开发者已经将其加入 1.5 的待做事项，但 1.5 依旧难产，也许你可以考虑贡献一下。 我的配置也等 GoldenDict 吧。 ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:3:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#字典"},{"categories":["Emacs"],"content":" 编程语言配置","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:0","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#编程语言配置"},{"categories":["Emacs"],"content":" 纯文本Emacs 可以显示 ANSI 颜色代码。然而，在 Emacs 28 之前，如果不修改缓冲区是不可能做到这一点的，所以让我们以此为基础设置这个块。 (after! text-mode (add-hook! 'text-mode-hook ;; Apply ANSI color codes (with-silent-modifications (ansi-color-apply-on-region (point-min) (point-max) t)))) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:1","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#纯文本"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003corg-conf\u003e\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 \u003cmime-info xmlns='http://www.freedesktop.org/standards/shared-mime-info'\u003e \u003cmime-type type=\"text/org\"\u003e \u003ccomment\u003eEmacs Org-mode File\u003c/comment\u003e \u003cglob pattern=\"*.org\"/\u003e \u003calias type=\"text/org\"/\u003e \u003c/mime-type\u003e \u003c/mime-info\u003e Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#org"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#系统设置"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#功能增强"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#视觉"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#导出通用设置"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#html-导出"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#latex-导出"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#幻灯片导出"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#ascii-导出"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#markdown-导出"},{"categories":["Emacs"],"content":" Org ModeOrg Mode 无疑是 Emacs 的杀手级应用，其扩展能力以及 Emacs 的契合，让它吊打一众标记语言和富文本格式。当然 LaTeX 除外。 格式 细粒度控制 上手易用性 语法简单 编辑器支持 集成度 易于参考 多功能性 Word 2 4 4 2 3 2 2 LaTeX 4 1 1 3 2 4 3 Org Mode 4 2 3.5 1 4 4 4 Markdown 1 3 3 4 3 3 1 Markdown + Pandoc 2.5 2.5 2.5 3 3 3 2 除了标记语言 (Markup Language) 的优雅外，Emacs 集成了一场丰富的有趣的功能，比如说对当前任何技术都支持的文学编程。 ╭─╴Code╶─╮ ╭─╴Raw Code╶─▶ Computer Ideas╺┥ ┝━▶ Org Mode╺┥ ╰─╴Text╶─╯ ╰─╴Document╶─▶ People 在 .org 文件可以包含代码块 (不支持 noweb 模板)，这些代码块可以与专用源代码文件纠缠在一起，并通过各种 (可扩展的) 方法编译成文档 (报告、文档、演示文稿等)。这些源块甚至可以创建要包含在文档中的图像或其他内容，或者生成源代码。 ╭───────────────────────────────────▶ .pdf ⎫ pdfLaTeX ▶╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╮ ⎪ ╿ ╿ ┊ ⎪ │ ┊ ┊ ⎪ .tex ┊ ┊ ⎪ ╿ ┊ ┊ ⎪ ╭──┴╌╌╮ ┊ ┊ style.scss ⎬ Weaving graphc.png ─╮ │ embedded TeX ┊ ╽ ⎪ (Documents) image.jpeg ─┤ filters ╿ ┊ .css ⎪ ╎ ╿ ┊ ┊ ▾╎ ⎪ figure.png╶─╧─▶ PROJECT.ORG ▶───╴filters╶───╧──────╪──▶ .html ⎪ ╿ ╿┊ ║ │ ╰╌╌╌▷╌╌ embedded html ▶╌╌╌╌╯ ⎪ ├╌╌╌╌╌╌╌▷╌╌╌╯┊ ║ │ ⎪ result╶╌╌╌╌╌╮ ┊ ║ ├──────╴filters╶────────────────▶ .txt ⎪ ┊▴ ┊ ┊ ║ │ ⎪ execution ┊ ┊ ║ ╰──────╴filters╶────────────────▶ .md ⎭ ┊▴ ┊ ┊ ║ code blocks◀╯ ┊ ╟─────────────────────────────────▶ .c ⎫ ╰╌╌╌╌◁╌╌╌╌╌╌╌╯ ╟─────────────────────────────────▶ .sh ⎬ Tangling ╟─────────────────────────────────▶ .hs ⎪ (Code) ╙─────────────────────────────────▶ .el ⎭ 因为这部分初始化时相当费时，我们需要将其放在 (after! ...) 中。 (after! org \u003c\u003e ) 系统设置 Mime 类型 默认情况下，Org 模式不会被识别为它自己的 MIME 类型，但可以使用以下文件轻松更改。 对于系统范围的更改，请尝试 /usr/share/mime/packages/org.xml 。 Emacs Org-mode File Papirus 有一个 text/org 的图标，只需刷新 mime 数据库。 update-mime-database ~/.local/share/mime 现在设置 emacs 为默认的编辑器。 xdg-mime default emacs.desktop text/org Git diffs Protesilaos 写了一篇 非常有用的文章，他在其中解释了如何将 git diff 块标题更改为比大块上方的直接行更有用的东西 — 就像父标题一样。 这可以通过首先在 ~/.config/git/attributes 中为 git 添加新的差异模式来实现。 *.org diff=org 然后为它添加一个正则表达式到 ~/.config/git/config 。 [diff \"org\"] xfuncname = \"^(\\\\*+ +.*)$\" 功能增强 标题结构 说起标题行，我注意到了一个非常棒的包，它可以浏览并管理标题结构。 (package! org-ol-tree :recipe (:host github :repo \"Townk/org-ol-tree\")) (use-package! org-ol-tree :defer t :after org :config (map! :map org-mode-map :localleader :desc \"Outline\" \"O\" #'org-ol-tree)) Agenda (defvar org-agenda-dir (concat org-directory \"/\" \"agenda\")) (defvar org-agenda-todo-file (expand-file-name \"todo.org\" org-agenda-dir)) (defvar org-agenda-project-file (expand-file-name \"project.org\" org-agenda-dir)) (after! org-agenda ;;urgancy|soon|as soon as possible|at some point|eventually ;; (setq! org-agenda-files `(,org-agenda-todo-file ,org-agenda-project-file) org-agenda-skip-scheduled-if-done t org-agenda-skip-deadline-if-done t org-agenda-include-deadlines t org-agenda-block-separator nil org-agenda-tags-column 100 ;; from testing this seems to be a good value org-agenda-compact-blocks t)) Capture 开始设置 Org-capture 模板吧，快速记录！ (after! org-capture (defun ginshio/find-project-tree(priority) \"find or create project headline https://www.zmonster.me/2018/02/28/org-mode-capture.html\" (let* ((hl (let ((headlines (org-element-map (org-element-parse-buffer 'headline) 'headline (lambda (hl) (and (= (org-element-property :level hl) 1) (org-element-property :title hl)))))) (completing-read \"Project Name: \" headlines)))) (goto-char (point-min)) (if (re-search-forward (format org-complex-heading-regexp-format (regexp-quote hl)) nil t) (goto-char (point-at-bol)) (progn (or (bolp) (insert \"\\n\")) (if (/= (point) (point-min)) (org-end-of-subtree)) (insert (format \"* %s :project:%s:\\n:properties:\\n:homepage: %s\\n:repo: \\ %s\\n🔚\\n\\n** urgancy :urgancy:\\n\\n** soon 🔜\\n\\n** as soon as\\ possible :asap:\\n\\n** at some point :asp:\\n\\n** eventually :eventually:\\n\" hl hl (read-string \"homepage: \") (read-string \"repo: \"))) (beginning-of-line 0) (org-up-heading-safe)))) (re-search-forward (format org-complex-heading-regexp-format (regexp-quote priority)) (save-excursion (org-end-of-subtree t t)) t) (org-end-of-subtree)) (setq! org-capt","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:2","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#hugo"},{"categories":["Emacs"],"content":" LaTeX (setq! LaTeX-biblatex-use-Biber t) (custom-set-variables '(LaTeX-section-label '((\"part\" . \"part:\") (\"chapter\" . \"chap:\") (\"section\" . \"sec:\") (\"subsection\" . \"subsec:\") (\"subsubsection\" . \"subsubsec:\"))) '(TeX-auto-local \"auto\") '(TeX-command-extra-options \"-shell-escape\")) (after! latex \u003c\u003ctex-conf\u003e\u003e ) 编译 (setq! TeX-save-query nil TeX-show-compilation t LaTeX-clean-intermediate-suffixes (append TeX-clean-default-intermediate-suffixes '(\"\\\\.acn\" \"\\\\.acr\" \"\\\\.alg\" \"\\\\.glg\" \"\\\\.ist\" \"\\\\.listing\" \"\\\\.fdb_latexmk\"))) (add-to-list 'TeX-command-list '(\"LatexMk (XeLaTeX)\" \"latexmk -pdf -xelatex -8bit %S%(mode) %(file-line-error) %(extraopts) %t\" TeX-run-latexmk nil (plain-tex-mode latex-mode doctex-mode) :help \"Run LatexMk (XeLaTeX)\")) 引用如果 bib 中有你不想要的某些字段，可以通过一下方法去除 (导言区) \\AtEveryBibitem{ \\clearfield{note} \\ifentrytype{book}{ \\clearfield{url} \\clearfield{isbn} }{} \\ifentrytype{article}{ \\clearfield{url} }{} \\ifentrytype{thesis}{ \\clearfield{url} }{} } 视觉增强一点点视觉效果 (setcar (assoc \"⋆\" LaTeX-fold-math-spec-list) \"★\") (setq! TeX-fold-math-spec-list `(;; missing/better symbols (\"≤\" (\"le\")) (\"≥\" (\"ge\")) (\"≠\" (\"ne\")) ;; convenience shorts -- these don't work nicely ATM ;; (\"‹\" (\"left\")) ;; (\"›\" (\"right\")) ;; private macros (\"ℝ\" (\"RR\")) (\"ℕ\" (\"NN\")) (\"ℤ\" (\"ZZ\")) (\"ℚ\" (\"QQ\")) (\"ℂ\" (\"CC\")) (\"ℙ\" (\"PP\")) (\"ℍ\" (\"HH\")) (\"𝔼\" (\"EE\")) (\"𝑑\" (\"dd\")) ;; known commands (\"\" (\"phantom\")) (,(lambda (num den) (if (and (TeX-string-single-token-p num) (TeX-string-single-token-p den)) (concat num \"／\" den) (concat \"❪\" num \"／\" den \"❫\"))) (\"frac\")) (,(lambda (arg) (concat \"√\" (TeX-fold-parenthesize-as-necessary arg))) (\"sqrt\")) (,(lambda (arg) (concat \"⭡\" (TeX-fold-parenthesize-as-necessary arg))) (\"vec\")) (\"‘{1}’\" (\"text\")) ;; private commands (\"|{1}|\" (\"abs\")) (\"‖{1}‖\" (\"norm\")) (\"⌊{1}⌋\" (\"floor\")) (\"⌈{1}⌉\" (\"ceil\")) (\"⌊{1}⌉\" (\"round\")) (\"𝑑{1}/𝑑{2}\" (\"dv\")) (\"∂{1}/∂{2}\" (\"pdv\")) ;; fancification (\"{1}\" (\"mathrm\")) (,(lambda (word) (string-offset-roman-chars 119743 word)) (\"mathbf\")) (,(lambda (word) (string-offset-roman-chars 119951 word)) (\"mathcal\")) (,(lambda (word) (string-offset-roman-chars 120003 word)) (\"mathfrak\")) (,(lambda (word) (string-offset-roman-chars 120055 word)) (\"mathbb\")) (,(lambda (word) (string-offset-roman-chars 120159 word)) (\"mathsf\")) (,(lambda (word) (string-offset-roman-chars 120367 word)) (\"mathtt\")) ) TeX-fold-macro-spec-list '(;; as the defaults (\"[f]\" (\"footnote\" \"marginpar\")) (\"[c]\" (\"cite\")) (\"[l]\" (\"label\")) (\"[r]\" (\"ref\" \"pageref\" \"eqref\")) (\"[i]\" (\"index\" \"glossary\")) (\"...\" (\"dots\")) (\"{1}\" (\"emph\" \"textit\" \"textsl\" \"textmd\" \"textrm\" \"textsf\" \"texttt\" \"textbf\" \"textsc\" \"textup\")) ;; tweaked defaults (\"©\" (\"copyright\")) (\"®\" (\"textregistered\")) (\"™\" (\"texttrademark\")) (\"[1]:||►\" (\"item\")) (\"❡❡ {1}\" (\"part\" \"part*\")) (\"❡ {1}\" (\"chapter\" \"chapter*\")) (\"§ {1}\" (\"section\" \"section*\")) (\"§§ {1}\" (\"subsection\" \"subsection*\")) (\"§§§ {1}\" (\"subsubsection\" \"subsubsection*\")) (\"¶ {1}\" (\"paragraph\" \"paragraph*\")) (\"¶¶ {1}\" (\"subparagraph\" \"subparagraph*\")) ;; extra (\"⬖ {1}\" (\"begin\")) (\"⬗ {1}\" (\"end\")) )) (defun string-offset-roman-chars (offset word) \"Shift the codepoint of each character in WORD by OFFSET with an extra -6 shift if the letter is lowercase\" (apply 'string (mapcar (lambda (c) (string-offset-apply-roman-char-exceptions (+ (if (\u003e= c 97) (- c 6) c) offset))) word))) (defvar string-offset-roman-char-exceptions '(;; lowercase serif (119892 . 8462) ; ℎ ;; lowercase caligraphic (119994 . 8495) ; ℯ (119996 . 8458) ; ℊ (120004 . 8500) ; ℴ ;; caligraphic (119965 . 8492) ; ℬ (119968 . 8496) ; ℰ (119969 . 8497) ; ℱ (119971 . 8459) ; ℋ (119972 . 8464) ; ℐ (119975 . 8466) ; ℒ (119976 . 8499) ; ℳ (119981 . 8475) ; ℛ ;; fraktur (120070 . 8493) ; ℭ (120075 . 8460) ; ℌ (120076 . 8465) ; ℑ (120085 . 8476) ; ℜ (120092 . 8488) ; ℨ ;; blackboard (120122 . 8450) ; ℂ (120127 . 8461) ; ℍ (120133 . 8469) ; ℕ (120135 . 8473) ; ℙ (120136 . 8474) ; ℚ (120137 . 8477) ; ℝ (120145 . 8484) ; ℤ ) \"An alist of deceptive codepoints, and then where t","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#LaTeX"},{"categories":["Emacs"],"content":" LaTeX (setq! LaTeX-biblatex-use-Biber t) (custom-set-variables '(LaTeX-section-label '((\"part\" . \"part:\") (\"chapter\" . \"chap:\") (\"section\" . \"sec:\") (\"subsection\" . \"subsec:\") (\"subsubsection\" . \"subsubsec:\"))) '(TeX-auto-local \"auto\") '(TeX-command-extra-options \"-shell-escape\")) (after! latex \u003c\u003e ) 编译 (setq! TeX-save-query nil TeX-show-compilation t LaTeX-clean-intermediate-suffixes (append TeX-clean-default-intermediate-suffixes '(\"\\\\.acn\" \"\\\\.acr\" \"\\\\.alg\" \"\\\\.glg\" \"\\\\.ist\" \"\\\\.listing\" \"\\\\.fdb_latexmk\"))) (add-to-list 'TeX-command-list '(\"LatexMk (XeLaTeX)\" \"latexmk -pdf -xelatex -8bit %S%(mode) %(file-line-error) %(extraopts) %t\" TeX-run-latexmk nil (plain-tex-mode latex-mode doctex-mode) :help \"Run LatexMk (XeLaTeX)\")) 引用如果 bib 中有你不想要的某些字段，可以通过一下方法去除 (导言区) \\AtEveryBibitem{ \\clearfield{note} \\ifentrytype{book}{ \\clearfield{url} \\clearfield{isbn} }{} \\ifentrytype{article}{ \\clearfield{url} }{} \\ifentrytype{thesis}{ \\clearfield{url} }{} } 视觉增强一点点视觉效果 (setcar (assoc \"⋆\" LaTeX-fold-math-spec-list) \"★\") (setq! TeX-fold-math-spec-list `(;; missing/better symbols (\"≤\" (\"le\")) (\"≥\" (\"ge\")) (\"≠\" (\"ne\")) ;; convenience shorts -- these don't work nicely ATM ;; (\"‹\" (\"left\")) ;; (\"›\" (\"right\")) ;; private macros (\"ℝ\" (\"RR\")) (\"ℕ\" (\"NN\")) (\"ℤ\" (\"ZZ\")) (\"ℚ\" (\"QQ\")) (\"ℂ\" (\"CC\")) (\"ℙ\" (\"PP\")) (\"ℍ\" (\"HH\")) (\"𝔼\" (\"EE\")) (\"𝑑\" (\"dd\")) ;; known commands (\"\" (\"phantom\")) (,(lambda (num den) (if (and (TeX-string-single-token-p num) (TeX-string-single-token-p den)) (concat num \"／\" den) (concat \"❪\" num \"／\" den \"❫\"))) (\"frac\")) (,(lambda (arg) (concat \"√\" (TeX-fold-parenthesize-as-necessary arg))) (\"sqrt\")) (,(lambda (arg) (concat \"⭡\" (TeX-fold-parenthesize-as-necessary arg))) (\"vec\")) (\"‘{1}’\" (\"text\")) ;; private commands (\"|{1}|\" (\"abs\")) (\"‖{1}‖\" (\"norm\")) (\"⌊{1}⌋\" (\"floor\")) (\"⌈{1}⌉\" (\"ceil\")) (\"⌊{1}⌉\" (\"round\")) (\"𝑑{1}/𝑑{2}\" (\"dv\")) (\"∂{1}/∂{2}\" (\"pdv\")) ;; fancification (\"{1}\" (\"mathrm\")) (,(lambda (word) (string-offset-roman-chars 119743 word)) (\"mathbf\")) (,(lambda (word) (string-offset-roman-chars 119951 word)) (\"mathcal\")) (,(lambda (word) (string-offset-roman-chars 120003 word)) (\"mathfrak\")) (,(lambda (word) (string-offset-roman-chars 120055 word)) (\"mathbb\")) (,(lambda (word) (string-offset-roman-chars 120159 word)) (\"mathsf\")) (,(lambda (word) (string-offset-roman-chars 120367 word)) (\"mathtt\")) ) TeX-fold-macro-spec-list '(;; as the defaults (\"[f]\" (\"footnote\" \"marginpar\")) (\"[c]\" (\"cite\")) (\"[l]\" (\"label\")) (\"[r]\" (\"ref\" \"pageref\" \"eqref\")) (\"[i]\" (\"index\" \"glossary\")) (\"...\" (\"dots\")) (\"{1}\" (\"emph\" \"textit\" \"textsl\" \"textmd\" \"textrm\" \"textsf\" \"texttt\" \"textbf\" \"textsc\" \"textup\")) ;; tweaked defaults (\"©\" (\"copyright\")) (\"®\" (\"textregistered\")) (\"™\" (\"texttrademark\")) (\"[1]:||►\" (\"item\")) (\"❡❡ {1}\" (\"part\" \"part*\")) (\"❡ {1}\" (\"chapter\" \"chapter*\")) (\"§ {1}\" (\"section\" \"section*\")) (\"§§ {1}\" (\"subsection\" \"subsection*\")) (\"§§§ {1}\" (\"subsubsection\" \"subsubsection*\")) (\"¶ {1}\" (\"paragraph\" \"paragraph*\")) (\"¶¶ {1}\" (\"subparagraph\" \"subparagraph*\")) ;; extra (\"⬖ {1}\" (\"begin\")) (\"⬗ {1}\" (\"end\")) )) (defun string-offset-roman-chars (offset word) \"Shift the codepoint of each character in WORD by OFFSET with an extra -6 shift if the letter is lowercase\" (apply 'string (mapcar (lambda (c) (string-offset-apply-roman-char-exceptions (+ (if (\u003e= c 97) (- c 6) c) offset))) word))) (defvar string-offset-roman-char-exceptions '(;; lowercase serif (119892 . 8462) ; ℎ ;; lowercase caligraphic (119994 . 8495) ; ℯ (119996 . 8458) ; ℊ (120004 . 8500) ; ℴ ;; caligraphic (119965 . 8492) ; ℬ (119968 . 8496) ; ℰ (119969 . 8497) ; ℱ (119971 . 8459) ; ℋ (119972 . 8464) ; ℐ (119975 . 8466) ; ℒ (119976 . 8499) ; ℳ (119981 . 8475) ; ℛ ;; fraktur (120070 . 8493) ; ℭ (120075 . 8460) ; ℌ (120076 . 8465) ; ℑ (120085 . 8476) ; ℜ (120092 . 8488) ; ℨ ;; blackboard (120122 . 8450) ; ℂ (120127 . 8461) ; ℍ (120133 . 8469) ; ℕ (120135 . 8473) ; ℙ (120136 . 8474) ; ℚ (120137 . 8477) ; ℝ (120145 . 8484) ; ℤ ) \"An alist of deceptive codepoints, and then where t","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#编译"},{"categories":["Emacs"],"content":" LaTeX (setq! LaTeX-biblatex-use-Biber t) (custom-set-variables '(LaTeX-section-label '((\"part\" . \"part:\") (\"chapter\" . \"chap:\") (\"section\" . \"sec:\") (\"subsection\" . \"subsec:\") (\"subsubsection\" . \"subsubsec:\"))) '(TeX-auto-local \"auto\") '(TeX-command-extra-options \"-shell-escape\")) (after! latex \u003c\u003e ) 编译 (setq! TeX-save-query nil TeX-show-compilation t LaTeX-clean-intermediate-suffixes (append TeX-clean-default-intermediate-suffixes '(\"\\\\.acn\" \"\\\\.acr\" \"\\\\.alg\" \"\\\\.glg\" \"\\\\.ist\" \"\\\\.listing\" \"\\\\.fdb_latexmk\"))) (add-to-list 'TeX-command-list '(\"LatexMk (XeLaTeX)\" \"latexmk -pdf -xelatex -8bit %S%(mode) %(file-line-error) %(extraopts) %t\" TeX-run-latexmk nil (plain-tex-mode latex-mode doctex-mode) :help \"Run LatexMk (XeLaTeX)\")) 引用如果 bib 中有你不想要的某些字段，可以通过一下方法去除 (导言区) \\AtEveryBibitem{ \\clearfield{note} \\ifentrytype{book}{ \\clearfield{url} \\clearfield{isbn} }{} \\ifentrytype{article}{ \\clearfield{url} }{} \\ifentrytype{thesis}{ \\clearfield{url} }{} } 视觉增强一点点视觉效果 (setcar (assoc \"⋆\" LaTeX-fold-math-spec-list) \"★\") (setq! TeX-fold-math-spec-list `(;; missing/better symbols (\"≤\" (\"le\")) (\"≥\" (\"ge\")) (\"≠\" (\"ne\")) ;; convenience shorts -- these don't work nicely ATM ;; (\"‹\" (\"left\")) ;; (\"›\" (\"right\")) ;; private macros (\"ℝ\" (\"RR\")) (\"ℕ\" (\"NN\")) (\"ℤ\" (\"ZZ\")) (\"ℚ\" (\"QQ\")) (\"ℂ\" (\"CC\")) (\"ℙ\" (\"PP\")) (\"ℍ\" (\"HH\")) (\"𝔼\" (\"EE\")) (\"𝑑\" (\"dd\")) ;; known commands (\"\" (\"phantom\")) (,(lambda (num den) (if (and (TeX-string-single-token-p num) (TeX-string-single-token-p den)) (concat num \"／\" den) (concat \"❪\" num \"／\" den \"❫\"))) (\"frac\")) (,(lambda (arg) (concat \"√\" (TeX-fold-parenthesize-as-necessary arg))) (\"sqrt\")) (,(lambda (arg) (concat \"⭡\" (TeX-fold-parenthesize-as-necessary arg))) (\"vec\")) (\"‘{1}’\" (\"text\")) ;; private commands (\"|{1}|\" (\"abs\")) (\"‖{1}‖\" (\"norm\")) (\"⌊{1}⌋\" (\"floor\")) (\"⌈{1}⌉\" (\"ceil\")) (\"⌊{1}⌉\" (\"round\")) (\"𝑑{1}/𝑑{2}\" (\"dv\")) (\"∂{1}/∂{2}\" (\"pdv\")) ;; fancification (\"{1}\" (\"mathrm\")) (,(lambda (word) (string-offset-roman-chars 119743 word)) (\"mathbf\")) (,(lambda (word) (string-offset-roman-chars 119951 word)) (\"mathcal\")) (,(lambda (word) (string-offset-roman-chars 120003 word)) (\"mathfrak\")) (,(lambda (word) (string-offset-roman-chars 120055 word)) (\"mathbb\")) (,(lambda (word) (string-offset-roman-chars 120159 word)) (\"mathsf\")) (,(lambda (word) (string-offset-roman-chars 120367 word)) (\"mathtt\")) ) TeX-fold-macro-spec-list '(;; as the defaults (\"[f]\" (\"footnote\" \"marginpar\")) (\"[c]\" (\"cite\")) (\"[l]\" (\"label\")) (\"[r]\" (\"ref\" \"pageref\" \"eqref\")) (\"[i]\" (\"index\" \"glossary\")) (\"...\" (\"dots\")) (\"{1}\" (\"emph\" \"textit\" \"textsl\" \"textmd\" \"textrm\" \"textsf\" \"texttt\" \"textbf\" \"textsc\" \"textup\")) ;; tweaked defaults (\"©\" (\"copyright\")) (\"®\" (\"textregistered\")) (\"™\" (\"texttrademark\")) (\"[1]:||►\" (\"item\")) (\"❡❡ {1}\" (\"part\" \"part*\")) (\"❡ {1}\" (\"chapter\" \"chapter*\")) (\"§ {1}\" (\"section\" \"section*\")) (\"§§ {1}\" (\"subsection\" \"subsection*\")) (\"§§§ {1}\" (\"subsubsection\" \"subsubsection*\")) (\"¶ {1}\" (\"paragraph\" \"paragraph*\")) (\"¶¶ {1}\" (\"subparagraph\" \"subparagraph*\")) ;; extra (\"⬖ {1}\" (\"begin\")) (\"⬗ {1}\" (\"end\")) )) (defun string-offset-roman-chars (offset word) \"Shift the codepoint of each character in WORD by OFFSET with an extra -6 shift if the letter is lowercase\" (apply 'string (mapcar (lambda (c) (string-offset-apply-roman-char-exceptions (+ (if (\u003e= c 97) (- c 6) c) offset))) word))) (defvar string-offset-roman-char-exceptions '(;; lowercase serif (119892 . 8462) ; ℎ ;; lowercase caligraphic (119994 . 8495) ; ℯ (119996 . 8458) ; ℊ (120004 . 8500) ; ℴ ;; caligraphic (119965 . 8492) ; ℬ (119968 . 8496) ; ℰ (119969 . 8497) ; ℱ (119971 . 8459) ; ℋ (119972 . 8464) ; ℐ (119975 . 8466) ; ℒ (119976 . 8499) ; ℳ (119981 . 8475) ; ℛ ;; fraktur (120070 . 8493) ; ℭ (120075 . 8460) ; ℌ (120076 . 8465) ; ℑ (120085 . 8476) ; ℜ (120092 . 8488) ; ℨ ;; blackboard (120122 . 8450) ; ℂ (120127 . 8461) ; ℍ (120133 . 8469) ; ℕ (120135 . 8473) ; ℙ (120136 . 8474) ; ℚ (120137 . 8477) ; ℝ (120145 . 8484) ; ℤ ) \"An alist of deceptive codepoints, and then where t","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#引用"},{"categories":["Emacs"],"content":" LaTeX (setq! LaTeX-biblatex-use-Biber t) (custom-set-variables '(LaTeX-section-label '((\"part\" . \"part:\") (\"chapter\" . \"chap:\") (\"section\" . \"sec:\") (\"subsection\" . \"subsec:\") (\"subsubsection\" . \"subsubsec:\"))) '(TeX-auto-local \"auto\") '(TeX-command-extra-options \"-shell-escape\")) (after! latex \u003c\u003e ) 编译 (setq! TeX-save-query nil TeX-show-compilation t LaTeX-clean-intermediate-suffixes (append TeX-clean-default-intermediate-suffixes '(\"\\\\.acn\" \"\\\\.acr\" \"\\\\.alg\" \"\\\\.glg\" \"\\\\.ist\" \"\\\\.listing\" \"\\\\.fdb_latexmk\"))) (add-to-list 'TeX-command-list '(\"LatexMk (XeLaTeX)\" \"latexmk -pdf -xelatex -8bit %S%(mode) %(file-line-error) %(extraopts) %t\" TeX-run-latexmk nil (plain-tex-mode latex-mode doctex-mode) :help \"Run LatexMk (XeLaTeX)\")) 引用如果 bib 中有你不想要的某些字段，可以通过一下方法去除 (导言区) \\AtEveryBibitem{ \\clearfield{note} \\ifentrytype{book}{ \\clearfield{url} \\clearfield{isbn} }{} \\ifentrytype{article}{ \\clearfield{url} }{} \\ifentrytype{thesis}{ \\clearfield{url} }{} } 视觉增强一点点视觉效果 (setcar (assoc \"⋆\" LaTeX-fold-math-spec-list) \"★\") (setq! TeX-fold-math-spec-list `(;; missing/better symbols (\"≤\" (\"le\")) (\"≥\" (\"ge\")) (\"≠\" (\"ne\")) ;; convenience shorts -- these don't work nicely ATM ;; (\"‹\" (\"left\")) ;; (\"›\" (\"right\")) ;; private macros (\"ℝ\" (\"RR\")) (\"ℕ\" (\"NN\")) (\"ℤ\" (\"ZZ\")) (\"ℚ\" (\"QQ\")) (\"ℂ\" (\"CC\")) (\"ℙ\" (\"PP\")) (\"ℍ\" (\"HH\")) (\"𝔼\" (\"EE\")) (\"𝑑\" (\"dd\")) ;; known commands (\"\" (\"phantom\")) (,(lambda (num den) (if (and (TeX-string-single-token-p num) (TeX-string-single-token-p den)) (concat num \"／\" den) (concat \"❪\" num \"／\" den \"❫\"))) (\"frac\")) (,(lambda (arg) (concat \"√\" (TeX-fold-parenthesize-as-necessary arg))) (\"sqrt\")) (,(lambda (arg) (concat \"⭡\" (TeX-fold-parenthesize-as-necessary arg))) (\"vec\")) (\"‘{1}’\" (\"text\")) ;; private commands (\"|{1}|\" (\"abs\")) (\"‖{1}‖\" (\"norm\")) (\"⌊{1}⌋\" (\"floor\")) (\"⌈{1}⌉\" (\"ceil\")) (\"⌊{1}⌉\" (\"round\")) (\"𝑑{1}/𝑑{2}\" (\"dv\")) (\"∂{1}/∂{2}\" (\"pdv\")) ;; fancification (\"{1}\" (\"mathrm\")) (,(lambda (word) (string-offset-roman-chars 119743 word)) (\"mathbf\")) (,(lambda (word) (string-offset-roman-chars 119951 word)) (\"mathcal\")) (,(lambda (word) (string-offset-roman-chars 120003 word)) (\"mathfrak\")) (,(lambda (word) (string-offset-roman-chars 120055 word)) (\"mathbb\")) (,(lambda (word) (string-offset-roman-chars 120159 word)) (\"mathsf\")) (,(lambda (word) (string-offset-roman-chars 120367 word)) (\"mathtt\")) ) TeX-fold-macro-spec-list '(;; as the defaults (\"[f]\" (\"footnote\" \"marginpar\")) (\"[c]\" (\"cite\")) (\"[l]\" (\"label\")) (\"[r]\" (\"ref\" \"pageref\" \"eqref\")) (\"[i]\" (\"index\" \"glossary\")) (\"...\" (\"dots\")) (\"{1}\" (\"emph\" \"textit\" \"textsl\" \"textmd\" \"textrm\" \"textsf\" \"texttt\" \"textbf\" \"textsc\" \"textup\")) ;; tweaked defaults (\"©\" (\"copyright\")) (\"®\" (\"textregistered\")) (\"™\" (\"texttrademark\")) (\"[1]:||►\" (\"item\")) (\"❡❡ {1}\" (\"part\" \"part*\")) (\"❡ {1}\" (\"chapter\" \"chapter*\")) (\"§ {1}\" (\"section\" \"section*\")) (\"§§ {1}\" (\"subsection\" \"subsection*\")) (\"§§§ {1}\" (\"subsubsection\" \"subsubsection*\")) (\"¶ {1}\" (\"paragraph\" \"paragraph*\")) (\"¶¶ {1}\" (\"subparagraph\" \"subparagraph*\")) ;; extra (\"⬖ {1}\" (\"begin\")) (\"⬗ {1}\" (\"end\")) )) (defun string-offset-roman-chars (offset word) \"Shift the codepoint of each character in WORD by OFFSET with an extra -6 shift if the letter is lowercase\" (apply 'string (mapcar (lambda (c) (string-offset-apply-roman-char-exceptions (+ (if (\u003e= c 97) (- c 6) c) offset))) word))) (defvar string-offset-roman-char-exceptions '(;; lowercase serif (119892 . 8462) ; ℎ ;; lowercase caligraphic (119994 . 8495) ; ℯ (119996 . 8458) ; ℊ (120004 . 8500) ; ℴ ;; caligraphic (119965 . 8492) ; ℬ (119968 . 8496) ; ℰ (119969 . 8497) ; ℱ (119971 . 8459) ; ℋ (119972 . 8464) ; ℐ (119975 . 8466) ; ℒ (119976 . 8499) ; ℳ (119981 . 8475) ; ℛ ;; fraktur (120070 . 8493) ; ℭ (120075 . 8460) ; ℌ (120076 . 8465) ; ℑ (120085 . 8476) ; ℜ (120092 . 8488) ; ℨ ;; blackboard (120122 . 8450) ; ℂ (120127 . 8461) ; ℍ (120133 . 8469) ; ℕ (120135 . 8473) ; ℙ (120136 . 8474) ; ℚ (120137 . 8477) ; ℝ (120145 . 8484) ; ℤ ) \"An alist of deceptive codepoints, and then where t","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#视觉"},{"categories":["Emacs"],"content":" LaTeX (setq! LaTeX-biblatex-use-Biber t) (custom-set-variables '(LaTeX-section-label '((\"part\" . \"part:\") (\"chapter\" . \"chap:\") (\"section\" . \"sec:\") (\"subsection\" . \"subsec:\") (\"subsubsection\" . \"subsubsec:\"))) '(TeX-auto-local \"auto\") '(TeX-command-extra-options \"-shell-escape\")) (after! latex \u003c\u003e ) 编译 (setq! TeX-save-query nil TeX-show-compilation t LaTeX-clean-intermediate-suffixes (append TeX-clean-default-intermediate-suffixes '(\"\\\\.acn\" \"\\\\.acr\" \"\\\\.alg\" \"\\\\.glg\" \"\\\\.ist\" \"\\\\.listing\" \"\\\\.fdb_latexmk\"))) (add-to-list 'TeX-command-list '(\"LatexMk (XeLaTeX)\" \"latexmk -pdf -xelatex -8bit %S%(mode) %(file-line-error) %(extraopts) %t\" TeX-run-latexmk nil (plain-tex-mode latex-mode doctex-mode) :help \"Run LatexMk (XeLaTeX)\")) 引用如果 bib 中有你不想要的某些字段，可以通过一下方法去除 (导言区) \\AtEveryBibitem{ \\clearfield{note} \\ifentrytype{book}{ \\clearfield{url} \\clearfield{isbn} }{} \\ifentrytype{article}{ \\clearfield{url} }{} \\ifentrytype{thesis}{ \\clearfield{url} }{} } 视觉增强一点点视觉效果 (setcar (assoc \"⋆\" LaTeX-fold-math-spec-list) \"★\") (setq! TeX-fold-math-spec-list `(;; missing/better symbols (\"≤\" (\"le\")) (\"≥\" (\"ge\")) (\"≠\" (\"ne\")) ;; convenience shorts -- these don't work nicely ATM ;; (\"‹\" (\"left\")) ;; (\"›\" (\"right\")) ;; private macros (\"ℝ\" (\"RR\")) (\"ℕ\" (\"NN\")) (\"ℤ\" (\"ZZ\")) (\"ℚ\" (\"QQ\")) (\"ℂ\" (\"CC\")) (\"ℙ\" (\"PP\")) (\"ℍ\" (\"HH\")) (\"𝔼\" (\"EE\")) (\"𝑑\" (\"dd\")) ;; known commands (\"\" (\"phantom\")) (,(lambda (num den) (if (and (TeX-string-single-token-p num) (TeX-string-single-token-p den)) (concat num \"／\" den) (concat \"❪\" num \"／\" den \"❫\"))) (\"frac\")) (,(lambda (arg) (concat \"√\" (TeX-fold-parenthesize-as-necessary arg))) (\"sqrt\")) (,(lambda (arg) (concat \"⭡\" (TeX-fold-parenthesize-as-necessary arg))) (\"vec\")) (\"‘{1}’\" (\"text\")) ;; private commands (\"|{1}|\" (\"abs\")) (\"‖{1}‖\" (\"norm\")) (\"⌊{1}⌋\" (\"floor\")) (\"⌈{1}⌉\" (\"ceil\")) (\"⌊{1}⌉\" (\"round\")) (\"𝑑{1}/𝑑{2}\" (\"dv\")) (\"∂{1}/∂{2}\" (\"pdv\")) ;; fancification (\"{1}\" (\"mathrm\")) (,(lambda (word) (string-offset-roman-chars 119743 word)) (\"mathbf\")) (,(lambda (word) (string-offset-roman-chars 119951 word)) (\"mathcal\")) (,(lambda (word) (string-offset-roman-chars 120003 word)) (\"mathfrak\")) (,(lambda (word) (string-offset-roman-chars 120055 word)) (\"mathbb\")) (,(lambda (word) (string-offset-roman-chars 120159 word)) (\"mathsf\")) (,(lambda (word) (string-offset-roman-chars 120367 word)) (\"mathtt\")) ) TeX-fold-macro-spec-list '(;; as the defaults (\"[f]\" (\"footnote\" \"marginpar\")) (\"[c]\" (\"cite\")) (\"[l]\" (\"label\")) (\"[r]\" (\"ref\" \"pageref\" \"eqref\")) (\"[i]\" (\"index\" \"glossary\")) (\"...\" (\"dots\")) (\"{1}\" (\"emph\" \"textit\" \"textsl\" \"textmd\" \"textrm\" \"textsf\" \"texttt\" \"textbf\" \"textsc\" \"textup\")) ;; tweaked defaults (\"©\" (\"copyright\")) (\"®\" (\"textregistered\")) (\"™\" (\"texttrademark\")) (\"[1]:||►\" (\"item\")) (\"❡❡ {1}\" (\"part\" \"part*\")) (\"❡ {1}\" (\"chapter\" \"chapter*\")) (\"§ {1}\" (\"section\" \"section*\")) (\"§§ {1}\" (\"subsection\" \"subsection*\")) (\"§§§ {1}\" (\"subsubsection\" \"subsubsection*\")) (\"¶ {1}\" (\"paragraph\" \"paragraph*\")) (\"¶¶ {1}\" (\"subparagraph\" \"subparagraph*\")) ;; extra (\"⬖ {1}\" (\"begin\")) (\"⬗ {1}\" (\"end\")) )) (defun string-offset-roman-chars (offset word) \"Shift the codepoint of each character in WORD by OFFSET with an extra -6 shift if the letter is lowercase\" (apply 'string (mapcar (lambda (c) (string-offset-apply-roman-char-exceptions (+ (if (\u003e= c 97) (- c 6) c) offset))) word))) (defvar string-offset-roman-char-exceptions '(;; lowercase serif (119892 . 8462) ; ℎ ;; lowercase caligraphic (119994 . 8495) ; ℯ (119996 . 8458) ; ℊ (120004 . 8500) ; ℴ ;; caligraphic (119965 . 8492) ; ℬ (119968 . 8496) ; ℰ (119969 . 8497) ; ℱ (119971 . 8459) ; ℋ (119972 . 8464) ; ℐ (119975 . 8466) ; ℒ (119976 . 8499) ; ℳ (119981 . 8475) ; ℛ ;; fraktur (120070 . 8493) ; ℭ (120075 . 8460) ; ℌ (120076 . 8465) ; ℑ (120085 . 8476) ; ℜ (120092 . 8488) ; ℨ ;; blackboard (120122 . 8450) ; ℂ (120127 . 8461) ; ℍ (120133 . 8469) ; ℕ (120135 . 8473) ; ℙ (120136 . 8474) ; ℚ (120137 . 8477) ; ℝ (120145 . 8484) ; ℤ ) \"An alist of deceptive codepoints, and then where t","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:3","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#fixes"},{"categories":["Emacs"],"content":" MarkdownDoom 默认的 Markdown 是 GFM (GitHub Flavored Markdown)，不过有了 Emacs 和 Org Mode 谁还用 Markdown 。但是 toc-org-mode 依然可以使用。 可以采用如下方式支持 Markdown。 # TOC \u003c!-- :TOC: --\u003e ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:4","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#markdown"},{"categories":["Emacs"],"content":" C++tcc 也是 C++ (add-to-list 'auto-mode-alist '(\"\\\\.tcc\\\\'\" . c++-mode)) ","date":"04-08","objectID":"/2022/doom_emacs_configuration/:4:5","series":null,"tags":["Doom","OrgMode"],"title":"Doom Emacs 配置文件","uri":"/2022/doom_emacs_configuration/#c-plus-plus"},{"categories":["Applications"],"content":"GinShio | configure HP Printer on openSUSE","date":"02-27","objectID":"/2022/hp_printer_driver/","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/"},{"categories":["Applications"],"content":"正好家里买了打印机，HP 4800 系列，耗材是真便宜，喷墨是真慢啊。不过正好记录一下 Linux 下的 HP 打印机配置过程。 另外 HP 对开源的态度真不错，估计也是因为自家是开源大厂的缘故吧。 信息 本文主要是 openSUSE 配置 HP 打印机的过程 ","date":"02-27","objectID":"/2022/hp_printer_driver/:0:0","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#"},{"categories":["Applications"],"content":" 安装驱动Linux 下有 HP 官方的打印机驱动，称为 HPLIP (HP’s Linux Imaging and Printing software)，可以查看 HPLIP 文档 或者 下载。 对于 HPLIP 可以在以下 Linux 发行版进行自动安装 SUSE Linux (13.2, 42.1, 42.2, 42.3, 15.0, 15.1, 15.2, 15.3) Fedora (22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35) Linux Mint (18, 18.1, 18.2, 18.3, 19, 19.1, 19.2, 19.3, 20.04, 20.1, 20.2, 20.3) Red Hat Enterprise Linux (6.0, 7.0, 7.2, 7.6, 7.7, 7.8, 7.9, 8.0, 8.2, 8.3, 8.4, 8.5) Boss (5.0) Ubuntu (12.04, 14.04, 15.10, 16.xx, 17.xx, 18.xx, 19.xx, 20.xx, 21.xx) Debian (7.0 ~ 7.9, 8.0 ~ 8.8, 9.1 ~ 9.9, 10.0 ~ 10.9, 11) Manjaro Linux (17.1.4, 18.0, 18.0.4, 18.1.0, 18.1.2, 19.0, 20.0, 20.2, 21.0.7) Zorin (15, 16) MX Linux (21) Elementary OS (6, 6.1) 比如说 FreeBSD、Gentoo 等发行版，可以采用手动安装。 对 openSUSE 来说，也可以从源中直接安装 hplip zypper -y hplip hplip-sane hplip-scan-utils ","date":"02-27","objectID":"/2022/hp_printer_driver/:1:0","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#安装驱动"},{"categories":["Applications"],"content":" 自动安装本节着重讲解官方下载的自动安装方式，选择自己的发行版直接下载就行 (openSUSE 下载 SUSE Linux 即可)，如果没有自己的发行版选择 Others 手动安装。 下载完成后运行 .run 文件 (不需要 root 权限) ./hplip-3.22.2.run 第一个选项即选择自动安装 (a) 还是手动安装 (c)，选择自动即可 Please choose the installation mode (a=automatic*, c=custom, q=quit) : 之后确定发行版和输入 root 密码，直接回车允许 hplip 可以安装相关程序即可，当然别忘了同意 (y) hplip 的用户协议。如果想要手动安装可以查看 官方文档 来学习，如果想要查看依赖项可以查看 其他发行版的手动安装。 接下来的问题，重启电脑 (r) 后配置还是重新插拔 USB 线材 (p)，当然可以选择忽略 (i) Restart or re-plug in your printer (r=restart, p=re-plug in*, i=ignore/continue, q=quit) 最后直接回车即可出现配置的图形界面。此时插入打印机 USB 线材即可。 ","date":"02-27","objectID":"/2022/hp_printer_driver/:1:1","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#自动安装"},{"categories":["Applications"],"content":" 手动安装对于手动安装，第一步即卸载从源中安装的 hplip su -c \"zypper --non-interactive rm hplip hplip-devel hplip-hpijs hplip-sane hplip-scan-utils\" 之后需要的是从源中安装相关依赖 su -c \"zypper --non-interactive --no-gpg-checks in --auto-agree-with-licenses \\ gcc-c++ glibc ghostscript ghostscript-devel openssl make wget dbus-1-devel \\ cups cups-client cups-devel cups-ddk sane sane-backends sane-backends-devel xsane \\ python-pip leptonica-devel tesseract-ocr tesseract-ocr-devel tesseract-ocr-traineddata-* libzbar-devel \\ python2-Pillow python2-PyYAML python2-PyPDF2 python2-tesserocr python2-decorator python2-scipy python-gobject2 python2-opencv\" su -c \"pip2 install --upgrade pip\" su -c \"pip2 install setuptools\" su -c \"pip2 install reportlab==3.4.0 watchdog==0.10.7\" su -c \"pip2 install opencv-contrib-python zbar-py imutils scikit-image pypdfocr\" 依赖安装完成，下载你需要的 hplip 源码，并对其进行编译安装操作 su -c \"./configure --with-hpppddir=/usr/share/cups/model/HP --libdir=/usr/lib64 --prefix=/usr \\ --disable-qt4 --enable-qt5 --enable-doc-build \\ --disable-cups-ppd-install --disable-foomatic-drv-install --disable-libusb01_build \\ --disable-foomatic-ppd-install --disable-hpijs-install --disable-class-driver --disable-udev_sysfs_rules \\ --disable-policykit --enable-cups-drv-install --enable-hpcups-install --enable-network-build \\ --enable-dbus-build --enable-scan-build --enable-fax-build --enable-apparmor_build\" su -c \"make\" su -c \"make install\" ","date":"02-27","objectID":"/2022/hp_printer_driver/:1:2","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#手动安装"},{"categories":["Applications"],"content":" 插件安装将 HPLIP 安装完毕后，默认是不会安装 plug-in 的，如果有扫描需求，是需要自己安装插件的。否则会在使用 hp-scan 时出现 error: SANE: Error during device I/O (code=9) 插件安装比较简单，只需要命令行启动 hp-plugin 如果遇到以下错误 error: Python gobject/dbus may be not installed\\\\ error: Plug-in install failed. 那就更简单了 (openSUSE 自带了 apparmor)，如果是 debian 需要安装 apparmor-utils ，然后使用命令 su -c \"aa-disable /usr/share/hplip/plugin.py\" 重新安装插件即可 ","date":"02-27","objectID":"/2022/hp_printer_driver/:1:3","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#插件安装"},{"categories":["Applications"],"content":" 配置打印机","date":"02-27","objectID":"/2022/hp_printer_driver/:2:0","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#配置打印机"},{"categories":["Applications"],"content":" 临时连接打印机配置 WiFi如果不需要网络连接打印机进行无线打印，可以选择第一项 USB 连接，这不需要复杂的配置。我的做法是配置好其无线连接，也可以插 USB，方便家庭环境的使用。因此我的选择是第三项 Wireless/802.11。需要注意的是如果你已在手机上进行过 WiFi 配置，就无需这一步，请移步下一节。 第二步是选择连接的打印机，不过好像是 py 的原因，从第 2 步到第 3 步很慢，而且在配置无线时有可能出错，出错消息可以在刚刚的安装终端上看到。 Traceback (most recent call last): File \"/usr/share/hplip/ui5/wifisetupdialog.py\", line 713, in NextButton_clicked self.showNetworkPage() File \"/usr/share/hplip/ui5/wifisetupdialog.py\", line 296, in showNetworkPage self.performScan() File \"/usr/share/hplip/ui5/wifisetupdialog.py\", line 332, in performScan self.loadNetworksTable() File \"/usr/share/hplip/ui5/wifisetupdialog.py\", line 422, in loadNetworksTable i = QTableWidgetItem(str(name)) UnicodeEncodeError: 'ascii' codec can't encode character u'\\u674e' in position 0: ordinal not in range(128) error: hp-setup failed. Please run hp-setup manually. 这个错误是由于编码问题导致的，需要根据追踪栈，进入文件 /usr/share/hplip/ui5/wifisetupdialog.py 的第 422 行对其 name 进行修改，修改如下即可 i = QTableWidgetItem(str(name.encode('utf-8'))) 修改完成后重新使用命令 hp-setup 即可，重复之前的步骤等待进入第三步。第三步是打印机选择需要连接的 WiFi，第四步输入所选 WiFi 的密码。不过无论是前进、后退都相当的慢，尽量一次性输入正确，慢得让我看了一个傅正老师的视频hhhhh。最终配置完成时，会显示如下界面 ","date":"02-27","objectID":"/2022/hp_printer_driver/:2:1","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#临时连接打印机配置-wifi"},{"categories":["Applications"],"content":" 根据 IP 配置无线打印机上一节已经连接了 WiFi，获取到了无线打印机的 IP。这里就可以断开打印机的 USB 连接，当然如果你想要有线连接的方式控制打印机，也可以不断开连线。 在终端重新启动 hplip 配置，不过这次增加了参数 IP，其实这一步与 hp-setup 界面的第二个选项是一样的 hp-setup 192.168.0.112 之后就到了如下界面，输入打印机的 name (名称)、description (描述) 以及 location (位置)，其实只需要填写必要的 name 即可，另外 setup 会自动识别打印机型号并选择对应的驱动文件 (毕竟是 HP 自家驱动)。添加打印机就完成了！！！ 当连接完成之后，打开 HP Device Manager 即可查看打印机详细信息。 ","date":"02-27","objectID":"/2022/hp_printer_driver/:2:2","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#根据-ip-配置无线打印机"},{"categories":["Applications"],"content":" 打印与扫描成果尝试了一下配置好的机器，没 A4 纸了，用偏小的纸简单的试一试 打印的是 2021 东京电玩展时，steam 上的图片，打印完成后在通过 hplip 扫描到电脑上，效果如图 原图在下面 ","date":"02-27","objectID":"/2022/hp_printer_driver/:3:0","series":null,"tags":["Driver","Printer","openSUSE","Tool"],"title":"openSUSE 下 HP 打印机配置","uri":"/2022/hp_printer_driver/#打印与扫描成果"},{"categories":["API"],"content":"GinShio | Unix 网络编程：卷一 (3rd) 第二部分第六章：I/O 多路复用：select 与 poll","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/"},{"categories":["API"],"content":"在客户端阻塞在 read 等待用户输入时，服务器关闭会导致向客户端发送 FIN，这是客户端的另一个输入。但由于客户端阻塞在 read 从而无法立即接受这个输入，直到从套接字读入为止。这就需要进程提前告知内核，使得内核在进程指定的输入准备好后，或可以输出后，立即通知进程，这个能力被称为 I/O 复用 (I/O multiplexing)。 I/O 复用有多个 syscall 可以实现，Unix 古老的函数 select 与 poll，POSIX 有一个比较新的变种为 pselect，而 Linux 与 freeBSD 独立发展出了 epoll 与 kqueue。 I/O 复用典型适用于以下场合： 当客户处理多个描述符时 客户同时处理多个套接字时，不过这种场景比较少见 如果 TCP 服务器既要监听套接字，又要处理已连接的套接字 如果一个服务器既要处理 TCP 又要处理 UDP，或同时处理多个不同协议，或多个服务 需要注意的是，并非只有网络编程需要用到 I/O 复用，I/O 复用是对文件描述符状态的监听，因此其他场景下也有其适用的空间。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:0:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#"},{"categories":["API"],"content":" I/O 模型在 Unix 系统上，一个输入操作通常包含两个不同的阶段 数据准备完成 从内核向进程复制数据 在等待数据准备到复制数据的过程，根据行为的不同，I/O 模型主要分为以下几种 blocking I/O (阻塞式 I/O) 阻塞式 I/O 模型是最常见、最简单、最好理解的 I/O 模型，目前为止所有的套接字函数都是阻塞式 I/O，另外 C 标准库所提供的 I/O 也是阻塞式 I/O，这样的模型符合初学函数时提及的运行流程。 在 recvfrom 这个示例中，只有数据报到达且复制到进程缓冲区中或错误发生才返回，而这段时间内进程是被阻塞的，不再向下运行代码。 nonblocking I/O (非阻塞式 I/O) 非阻塞进程调用时，在数据未准备好时调用会直接返回一个错误，而非阻塞进程，在下次调用前，进程可以去处理其他数据。 在上图中，可以反复调用 syscall 查看数据是否已复制到缓冲区。如果我们有一系列非阻塞描述符时，可以轮流对其进行询问，这种方式称为轮询 (polling)，当有描述符可用时进行处理，否则询问下一个描述符。但是这样做会消耗大量的 CPU，因此需要尽可能少的使用该方法。 I/O multiplexing (I/O 复用, e.g. select, poll) I/O 复用是指对一组描述符进行监听，然后阻塞进程，直到这组事件中有某个或某些事件发生或等待超时。可以如此理解：将轮询操作从进程迁移进内核中，由内核对这些描述符进行监听，进程只需要等待内核通知某个描述符数据准备完毕。对于进程来说，将阻塞在 I/O 复用这个 syscall 上，而不会阻塞在真正的系统调用上。 I/O 复用类似多线程中的阻塞式 I/O，都可以同时监听多个描述符的 I/O 事件，区别就是 I/O 复用完成了一个线程监听多个描述符，而另一个实现方式是一个线程一个描述符，不过每个描述符都可以在不同线程上同时被处理。 另外需要注意的是，I/O 复用时会有 两次 syscall 的开销，第一次发生在 I/O 复用函数，这个函数将监听那些进程传递给内核的感兴趣的描述符集合，在某个或某些描述符准备完成后，将从 I/O 复用函数返回，进程再次调用真正的 I/O syscall 进行处理。 signal driven I/O (信号驱动型 I/O, e.g. SIGIO) 在描述符就绪时，让内核通过发送 SIGIO 信号通知进程的方式，被称为 signal driven I/O。 首先开启描述符的信号驱动 I/O 功能，再调用 sigaction 对 SIGIO 信号注册回调函数，sigaction 不会阻塞进程，会立即返回。当描述符准备完成后，内核将向进程发送 SIGIO 信号，随后可以在信号处理函数中对数据进行读写、操作等。 这种模型的优势在于等待数据期间，无需阻塞进程，可以在信号来临之前执行其他操作。 asynchronous I/O (异步 I/O, e.g. POSIX aio_xxx functions) 异步 I/O 告知内核启动某个操作，并让内核在整个操作完成后通知进程，进程可以不被阻塞继续执行。Asynchronous I/O 与 Signal-Driven I/O 最大的区别是：前者由内核通知 I/O 操作何时完成，后者通知何时可以启动一个 I/O 操作。 对于这 5 种 I/O 模型，前 4 种在第一阶段 (即等待数据阶段) 有所区别，但第二阶段 (将数据从内核复制到缓冲区中) 是一样的，都是调用实际的 syscall 并阻塞于此。而异步 I/O 需要处理这两个阶段，进程不被阻塞也无需关心，只需要在 I/O 完成后处理数据即可。所以称前四种 I/O 为 同步 (synchronous) I/O，这些 I/O 将导致请求的进程阻塞，直到 I/O 完成。 说一下特殊的几个描述符，每个 Unix 进程在开启时，都会打开这三个标准的 POSIX 文件描述符 整数值 名称 中文名称 \u003cunistd.h\u003e 符号常量 \u003cstdio.h\u003e 符号常量 0 Standard Input 标准输入流 STDIN_FILENO stdin 1 Standard Output 标准输出流 STDOUT_FILENO stdout 2 Standard Error 标准错误流 STDERR_FILENO stderr ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:1:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#i-o-模型"},{"categories":["API"],"content":" select 函数select 允许进程指示内核等待多个事件中任何一个发生，在只有一个或多个事件发生或超时时唤醒进程。任何描述符都可以使用 select 进行监听，不止套接字。其中监听事件包括准备读、准备写、异常条件处理，以及超时。 // sys/select.h // sys/time.h int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); // positive count of ready descriptors, 0 on timeout, -1 on error ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:2:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#select-函数"},{"categories":["API"],"content":" select 参数与返回值 参数 timeout 结构 timeval 的变量 timeout 指示超时时间，其结构如下 struct timeval { long tv_sec; // seconds long tv_usecs; // microseconds }; 该参数有三种可能 永远等待下去: 将参数设置为空指针 NULL 时，当集合中有描述符准备好后才返回 等待一段时间: 将参数设置非零值，当集合中有描述符在等待时间内准备好就返回，否则超时返回 不等待: 将参数设置为零值，检查描述符后立即返回，这种情况又被称为 轮询 (polling) 前两种情况下 select 通常会被捕获的信号终端，并从信号处理函数中返回错误 EINTR， Berkeley 内核绝不会自动重启被中断的 select，而 SVR4 可以重启指定了 SA_RESTART 标志的 select。因此为了可移植性，在捕获信号时，必须做好 select 返回 EINTR 错误的准备。 在绝大多数 Unix 系统上，采用符合 POSIX 的 const 修饰 timeout，也就是说 select 不会修改 timeout，如果想获取未睡眠的时间需要自行调用时间函数计算。但在 Linux 实现上，select 将修改 timeout 的值来反映未睡眠的时间。因此为了可移植性考虑，在每次调用 select 前，都要对其进行赋值操作。 如果 timeout 参数的 tv_sec 值超过 100'000'000 秒，有些系统会返回一个 EINVAL 错误，即不支持的 timeout。 描述符集合 readfds, writefds, exceptfds 描述符集合分别指定进程关心的读、写、异常描述符。通常在指定描述符集时，这是个整数数组，其中的每一 bit 对应一个描述符，比如第一位对应描述符 0，第 15 bit 对应描述符 15。用户无需关心为某一描述符如何读取或修改某一位，所有细节都隐藏在 fd_set 以及相关宏当中。当然使用 bit 只是常见实现方法，我们不应该关心或假想具体实现就是这样，而是将当作黑盒对 fd_set 进行处理。 void FD_ZERO(fd_set *fdset); // 清除所有 bit void FD_SET(int fd, fd_set *fdset); // 设置关心的描述符 void FD_CLR(int fd, fd_set *fdset); // 移除不关心的描述符 int FD_ISSET(int fd, fd_set *fdset); // 检查 fd 是否设置 如果对某一个种类的描述符集不感兴趣，可以置为空指针。如果将所有描述符集都置为空指针，将会得到一个比 sleep/1 更精确的休眠定时器。 nfds 参数 nfds 指定了待测试描述符的个数，其值是最大描述符 \\(+1\\)， FD_SETSIZE 是定义 fd_set 数据可描述的描述符大小的常值，通常是 1024。比如关心 1、4、5 描述符，那么 nfds 应该填 6。 最终看一下返回值，将表示有多少就绪的描述符，并将已就绪描述符在集合中置为 1，剩下的描述符都会被清除为 0。这时只要对描述符集中的关心的描述符依次调用 FD_ISSET 检查即可。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:2:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#select-参数与返回值"},{"categories":["API"],"content":" select 函数中描述符的就绪条件读、写、异常对描述符的状态是一个简单描述，尤其是普通文件描述符，但套接字的就绪条件，应该明确。 在满足下列四个条件之一时，套接字描述符可读 该套接字接收缓冲区中的数据字节数大于等于套接字接收缓冲区低水位标记的当前大小。对这样的套接字执行读操作不会阻塞且一定返回一个大于 0 的值。可以使用 SO_RCVLOWAT 套接字选项对修改标记的值，通常而言默认值为 1 该连接的读半部关闭 (接收了 FIN 的 TCP 连接)，这样的套接字读操作不会阻塞且返回 0 该套接字是一个监听套接字且已完成的连接数不为 0，对这样的套接字进行 accept 操作不会阻塞 有一个套接字错误待处理，这样的套接字读操作不会阻塞且返回 -1，同时将 errno 设置为确切的错误条件。可以通过指定 SO_ERROR 套接字选项调用 getsockopt 获取并清除 在满足下列四个条件之一时，套接字描述符可写 该套接字发送缓冲区中的可用空间字节数大于等于套接字发送缓冲区低水平位标记的当前大小，且套接字已连接或不需要连接，此时套接字就绪，写操作将不会阻塞并返回一个正值。可以使用 SO_SNDLOWAT 套接字选项来修改标记的值，通常而言默认值为 2048 该连接的写半部关闭，这样的套接字写操作将产生 SIGPIPE 信号 非阻塞式 connect 套接字已建立连接，或 connect 已失败告终 有一个套接字错误待处理，对这样的套接字写操作将不阻塞并返回 -1 以及 errno 错误。可以通过指定 SO_ERROR 套接字选项调用 getsockopt 获取并清除 如果一个套接字存在带外数据或仍处于带外标记，那么它有异常条件待处理 接收低水位标记与发送低水位标记的目的在于：允许进程控制在 select 返回可读或可写条件之前有多少数据可读或多大空间可写。举个例子，如果少于 64 byte 的数据对进程来说是无法处理的，可以将低水位标记设置为 64，防止少于 64 byte 数据准备好读时 select 唤醒进程。对于 UDP socket 来说，其发送低水位标记小于等于发送缓冲区大小时，总是可写的，因为 UDP socket 无需连接。 条件 可读 可写 异常 有数据可读 YES 关闭连接的读半部 YES 监听套接字准备好新连接 YES 有可用于写的空间 YES 关闭连接的写半部 YES 待处理错误 YES YES TCP 带外数据 YES ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:2:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#select-函数中描述符的就绪条件"},{"categories":["API"],"content":" select 示例","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:3:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#select-示例"},{"categories":["API"],"content":" 使用 select 实现 TCP echo 程序现在可以重写 TCP echo 客户端程序，可以在服务器终止之后，客户端马上得知，因此只需要在 str_cli 函数上做修改。新版本的 str_cli 将阻塞于 select，等待标准输入可读或 TCP 套接字可读。 套接字上的三个条件处理如下： 如果对端 TCP 发送数据，那么该套接字变为可读，并且 read 返回一个大于 0 的值 如果对端 TCP 发送一个 FIN (对端终止)，那么该套接字变为可读，并且 read 返回 0 (EOF) 如果对端 TCP 发送一个 RST (对端崩溃并重启)，那么该套接字变为可读，read 返回 -1 并设置 errno fileno 的作用是将标准 C 的 FILE* 结构转换为等价的 Unix 的 fd，而 fdopen 则是相反的操作。 void str_cli(FILE *fp, int sockfd) { char sendline[MAXLINE] = {0}, recvline[MAXLINE] = {0}; FILE *sockfd_fp = fdopen(sockfd, \"r+\"); fd_set rset; FD_ZERO(\u0026rset); while (true) { FD_SET(fileno(fp), \u0026rset); FD_SET(sockfd, \u0026rset); int nfds = max(fileno(fp), sockfd) + 1; select(nfds, \u0026rset, NULL, NULL, NULL); if (FD_ISSET(sockfd, \u0026rset)) { // socket is readable if (fgets(recvline, MAXLINE, sockfd_fp) == NULL) { err_quit(\"str_cli: server terminated prematurely\"); } fputs(recvline, stdout); bzero(recvline, MAXLINE); } if (FD_ISSET(fileno(fp), \u0026rset)) { // standard input if (fgets(sendline, MAXLINE, fp) == NULL) { return; } fputs(sendline, sockfd_fp); bzero(sendline, MAXLINE); } } } 服务端也可以使用 select 进行修改，从而减轻 fork 大量创建新进程所带来的开销。 通常从终端启动的进程都会打开 stdin、stdout 和 stderr，因此监听套接字 listenfd 通常是 fd = 3 的描述符，因此 select 第一个参数将为 4。当客户端建立连接时监听套接字可读，accept 返回连接套接字描述符 4，依次类推。当关闭其中一个连接时，需要一个描述最大描述符值的量，来快速确定 nfds。 在修改时，soket、listen、bind 等基本不变，主要是 while 无限循环中的结构变化。 #include \"unp.h\" int main(int argc, char **argv) { int listenfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_port = htons(7), .sin_addr.s_addr = htonl(INADDR_ANY), }; bind(listenfd, (struct sockaddr_in *) \u0026servaddr, sizeof(servaddr)); listen(listenfd, LISTENQ); int maxfd = listenfd, maxi = -1; const int addrlen = sizeof(servaddr); int client[FD_SETSIZE]; memset(client, -1, sizeof(client)); fd_set allset; FD_ZERO(\u0026allset); FD_SET(listenfd, \u0026allset); char buffer[MAXLINE] = {0}; while (true) { fd_set rset = allset; int nready = select(maxfd + 1, \u0026rset, NULL, NULL, NULL); if (FD_ISSET(listenfd, \u0026rset)) { // new client connection int i; struct sockaddr_in cliaddr; int connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026addrlen); for (i = 0; i \u003c FD_SETSIZE; i++) { if (client[i] \u003c 0) { client[i] = connfd; break; } } if (i == FD_SETSIZE) { err_quit(\"too many clients\"); } FD_SET(connfd, \u0026allset); if (connfd \u003e maxfd) { maxfd = connfd; } if (i \u003e maxi) { maxi = i; } if (--nready \u003c= 0) { continue; } } for (int i = 0; i \u003c= maxi; i++) { int n, sockfd; if ((sockfd = client[i]) \u003c 0) { continue; } if (FD_ISSET(sockfd, \u0026rset)) { if ((n = read(sockfd, buffer, MAXLINE)) == 0) { close(sockfd); FD_CLR(sockfd, \u0026allset); client[i] = -1; } else { write(sockfd, buffer, n); } if (--nready \u003c= 0) { break; } } } } } 但是需要注意但是，目前服务器程序最大的问题是 拒绝服务攻击 (Denial-of-Service Attacks)：如果客户端仅发送一个字节的数据，且这个字节不是换行符，然后客户端进入休眠状态。服务器将在调用 read 方法后阻塞于此。简单的说，服务器将被阻塞在这个用户的读操作上，不会处理其他用户数据或接收新连接，直到那个恶意客户终止或发送换行符为止。 这里有个基本概念：当服务器处理多个客户时，服务器 绝对不能 阻塞于单个客户相关的某个函数调用，否则导致服务器阻塞，拒绝为其他客户提供服务。这就是所谓的 拒绝服务攻击 。可能的解决方法主要是： Nonblocking I/O 每个客户单独一个进程 / 线程提供服务 对 I/O 操作设置超时时间，打破持续阻塞的窘境 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:3:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#使用-select-实现-tcp-echo-程序"},{"categories":["API"],"content":" shutdown 函数如果想终止网络通常是 close 函数，不过 close 函数有两个限制 close 将描述符引用计数 \\(-1\\) ，而非真正关闭描述符，只有描述符引用计数为 0 时才会真正关闭 close 终止读和写两个方向的数据传输，这是十分严重的，因为对端可能有数据继续写入 通常我们使用 shutdown 来处理，shutdown 可以忽略引用计数，直接向对端发送 FIN。 // sys/socket.h int shutdown(int sockfd, int howto); // return 0 if OK, -1 on error howto 参数可以控制 shutdown 的行为，取值如下 howto 取值 shutdown 行为 释义 SHUT_RD 关闭读半部 不再接收数据，缓冲区的数据全部被丢弃 SHUT_WR 关闭写半部 不再发送数据，连接进入半关闭状态 SHUT_RDWR 关闭读、写半部 等效于依次调用 SHUT_RD、SHUT_WR ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:3:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#shutdown-函数"},{"categories":["API"],"content":" 批量输入问题现在回头看上一篇中简单的 str_cli，这个函数实在太简单了，它可以完成我们预想的工作，除了不能即时反馈服务端已停止工作。它使用了停-等的工作方式，即发送文本服务器，然后等待响应。当然这段等待的时间，包含 RTT (Round-Trip Time, 往返时间) 以及服务器的处理时间。现在假设 RTT 为 8 个时间单位，请求在时刻 0 发出并在时刻 3 接收，响应在时刻 4 发出并在时刻 7 收到，且忽略服务器处理时间且请求、响应大小相同。 显然在全双工的 TCP 连接下，这将严重影响吞吐量。不过对于 Unix shell 环境下的交互式输入是合适的。 现在来看上一节使用 select 实现的 str_cli 函数，好像没什么问题了。我们现在简单的对输入输出进行重定向，即提前在文件中准备大量输入，将输入重定向到该文件，最终输出也重定向到一个文件中。 ./examplecli 127.0.0.1 \u003c in.txt \u003e out.txt 对于这样的实现来说，可以批量输入，也能极大提升网络吞吐量。假设在发出一个请求后立即发送另一个请求，且直接忽略服务器进行处理的时间。可以得到如下理想的数据传输情况，其实这里我们忽略了 TCP 的拥塞控制。 这里不得不提出一个问题，如果没有第 10 行，也就是请求结束，数据读入了 EOF。遗憾的是，我们将直接返回到 main 函数中，依然有数据在连接上，但我们已经不再处理数据。这也就是为什么每次重定向后，输出大小总是小于输入大小。 ls -la in.txt out.txt -rw-r--r-- 1 root root 7106 Feb 27 18:58 in.txt -rw-r--r-- 1 root root 4139 Feb 27 18:58 out.txt 这里我们需要一种关闭 TCP 写半部，即向服务器发送 FIN 告知数据已完毕，但仍保持套接字描述符打开已便读取响应。 另外，缓冲区的引入将导致程序复杂性的提升，比如说批量输入时，stdin 将之后的数据写入了输入缓冲区，等待读取这些缓冲区的数据，但 select 不这么想，它并不从 stdin 的缓冲区的角度出发。如果我们在自己写的函数中使用缓冲区，则需要考虑调用 select 之前，缓冲区中是否有等待消费的数据。 回到批量输入的问题，重新修改 str_cli 来解决这个问题，并使用 iseof 变量来判断是否读取到文件结尾，确定是否需要发送 FIN 进入连接的半关闭状态，而不是直接关闭连接从而丢失掉一部分数据。 void str_cli(FILE *fp, int sockfd) { fd_set rset; FD_ZERO(\u0026rset); bool iseof = false; int nfds = max(sockfd, fileno(fp)) + 1; int fpfd = fileno(fp); char buffer[MAXLINE] = {0}; while (true) { if (!iseof) { FD_SET(fpfd, \u0026rset); } FD_SET(sockfd, \u0026rset); select(nfds, \u0026rset, NULL, NULL, NULL); int n; if (FD_ISSET(sockfd, \u0026rset)) { // socket is readable if ((n = read(sockfd, buffer, MAXLINE)) == 0) { if (iseof) { return; // normal termination } else { err_quit(\"str_cli: server terminated prematurely\"); } } write(fileno(stdout), buffer, n); } if (FD_ISSET(fpfd, \u0026rset)) { // input is readable if ((n = read(fpfd, buffer, MAXLINE)) == 0) { iseof = true; shutdown(sockfd, SHUT_WR); FD_CLR(fpfd, \u0026rset); continue; } write(sockfd, buffer, n); } } } ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:3:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#批量输入问题"},{"categories":["API"],"content":" poll 函数poll 函数起源于 SVR3，最初限制在流描述符，SVR4 取消了限制，允许工作在任何描述符上。poll 与 select 类似，不过处理流描述符时能够提供额外信息。 // poll.h int poll(struct pollfd *fds, nfds_t nfds, int timeout); // return count of ready descriptors, 0 on timeout, -1 on error ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:4:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#poll-函数"},{"categories":["API"],"content":" poll 参数与返回值 参数 fds fds 是一个 pollfd 结构数组，是进程感兴趣的描述符集合，其中每一个元素表示的是感兴趣描述符以及其行为 struct pollfd { int fd; short events; // events of interest on fd short revents; // events that occurred on fd }; 测试条件由 events 成员指定，函数在相应的 revents 成员中返回相应的状态，另外这些值可以以逻辑组合的形式传递给 poll 或读取 值 events revents 说明 POLLIN YES YES 普通或优先级带数据可读 POLLRDNORM YES YES 普通数据可读 POLLRDBAND YES YES 优先级数据可读 POLLPRI YES YES 高优先级数据可读 POLLOUT YES YES 普通数据可写 POLLWRNORM YES YES 普通数据可写 POLLWRBAND YES YES 优先级数据可写 POLLERR NO YES 发生错误 POLLHUP NO YES 发生阻塞 POLLNVAL NO YES 描述符不是一个打开的文件 poll 可以识别并处理 普通 (normal)、优先级带 (priority band) 和 高优先级 (high priority) 数据 (术语出自基于流的实现)。另外 POLLIN 与 POLLOUT 自 SVR3 实现存在，早于 SVR4 中的优先级带，这两个值的出现是历史因素。 就 TCP 与 UDP 套接字而言，以下条件会引起 poll 返回特定的 revent 所有正规 TCP 数据和所有 UDP 数据都被认为是普通数据 TCP 的带外数据被认为是优先级带数据 TCP 连接读半关闭时 (收到对端 FIN)，被认为是普通数据，随后的读操作将返回 0 TCP 连接存在错误既可认为是普通数据也可认为是错误 (POLLERR)，无论那种情况，随后的读操作返回 -1 并设置 errno 监听套接字上有新连接即可认为是普通数据 (绝大多数实现)，也可认为是优先级数据 非阻塞式 connect 完成被认为是使相应的套接字可写 参数 nfds 表示数组的长度，即数组中元素个数 该参数在历史上常被定义为 unsigned long，有可能过分大了，Unix 98 为该参数定义为 nfds_t，该类型常常被定义为 unsigned int 参数 timeout 指定超时时间，单位为 ms (millisecond) timeout 说明 INFTIM 永远等待 0 立即返回，不阻塞进程 正值 等待指定的毫秒 另外需要说明的一点，INFTIM 常常被定义为一个负值，POSIX 规范要求 INFTIM 定义于 poll.h 中，而许多系统将其定义在 sys/stropts.h 中。 poll 返回值 与 select 返回一致，唯一的区别是，就绪的描述符将修改结构中的 revents 为非零值，告知其就绪状态。在不关心数组中的某个描述符时，可以将其 fd 设置为负值， poll 将忽略这个元素。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:4:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#poll-参数与返回值"},{"categories":["API"],"content":" poll 示例：TCP Echo 服务器现在想想使用 poll 来修改之前 select 的实现 #include \u003cstdbool.h\u003e // true #include \"unp.h\" int main(int argc, char **argv) { int listenfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_addr.s_addr = htonl(INADDR_ANY), .sin_port = htons(7), }; const socklen_t addrlen = sizeof(servaddr); bind(listenfd, (struct sockaddr *) \u0026servaddr, addrlen); listen(listenfd, LISTENQ); struct pollfd clients[LISTENQ]; memset(clients, 0, sizeof(clients)); clients[0].fd = listenfd; clients[0].events = POLLRDNORM; int maxi = 0; struct sockaddr_in cliaddr; char buffer[MAXLINE] = {0}; while (true) { int nready = poll(clients, maxi + 1, INFTIM); int connfd; if (clients[0].revents \u0026 POLLRDNORM) { connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026addrlen); int i; for (i = 1; i \u003c LISTENQ; i++) { if (clients[i].fd \u003c= 0) { clients[i].fd = connfd; clients[i].events = POLLRDNORM; break; } } if (i == LISTENQ) { err_quit(\"too many clients\"); } if (i \u003e maxi) { maxi = i; } if (--nready \u003c= 0) { continue; } } for (int i = 1; i \u003c= maxi; i++) { if ((connfd = clients[i].fd) \u003c= 0) { continue; } int n; if (clients[i].revents \u0026 (POLLRDNORM | POLLERR)) { if ((n = read(connfd, buffer, MAXLINE)) \u003c 0) { if (errno == ECONNRESET) { close(connfd); clients[i].fd = -1; } else { err_sys(\"read error\"); } } else if (n == 0) { close(connfd); clients[i].fd = -1; } else { write(connfd, buffer, n); } if (--nready \u003c= 0) { break; } } } } } ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:4:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#poll-示例-tcp-echo-服务器"},{"categories":["API"],"content":" epoll 函数epoll 是 Linux 内核在 2.5.44 扩展的 I/O 事件通知机制，主要为了取代 select 与 poll，在大量操作文件描述符时为发挥更优异的性能。select 与 poll 的时间复杂度为 \\(\\mathcal{O}(N)\\) ，而 epoll 由于使用红黑树结构，时间复杂度可以做到 \\(\\mathcal{O}(\\log N)\\) 。另外 select 与 poll 都是将整个描述符集在内核与进程之间拷贝，而 epoll 在这方面也有所改进。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-函数"},{"categories":["API"],"content":" epoll APIsepoll 将各个功能拆分到了不同 API 中，将描述符检测与传递监听描述符拆分为了讲个函数，以此减少每次都要在调用时传递描述符集的拷贝消耗。 epoll_create // sys/epoll.h int epoll_create(int size); // return a epoll file descriptor, -1 on error 该函数用于在内核中创建一个 epoll 实例并返回一个 epoll 描述符，这个描述符可以理解为实例的唯一地址，在之后需要使用到该集合时就要用到该描述符。 最初时参数 size 指示需要监听的描述符的数量，超过 size 时内核会自动扩容。如今 size 不再有此语义，但调用时必须传递大于 0 的 size 来保证向后兼容性。 该函数有一个变种 // sys/epoll.h int epoll_create1(int flag); // return a epoll file descriptor, -1 on error 如果 flag 为 0 则与 epoll_create 行为相同，只是删除了过时的参数 size。或者传入参数 EPOLL_CLOEXEC 来取得不同的行为：为新的文件描述符添加 close-on-exec (FD_CLOEXEC) 标记，该标记与 open 函数的标记 O_CLOEXEC 相同。 epoll_ctl // sys/epoll.h int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // return 0 if OK, -1 on error 该函数对内核 epoll 实例进行监听描述符的添加、修改、删除操作。 参数 op 对描述符的操作，可选值为 EPOLL_CTL_ADD (添加描述符)、 EPOLL_CTL_MOD (修改描述符) 以及 EPOLL_CTL_DEL (删除描述符) 参数 event 对描述符所作出的具体监听行为，与 pollfd 中的 event 类似，events 是针对文件描述符的事件掩码 typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; // Epoll events epoll_data_t data; // User data variable }; 在每次对描述符进行操作时，都需要设置 epoll_data_t 中的 fd 字段。epoll 中的事件可以取得如下值 值 内核版本 说明 返回 EPOLLIN 文件描述符可读 EPOLLOUT 文件描述符可写 EPOLLRDHUP 2.6.17 流套接字写半部关闭 EPOLLPRI 文件描述符存在异常，同 POLLPRI EPOLLERR 文件描述符发生错误 ALWAYS EPOLLHUP 文件描述符发生终止 ALWAYS EPOLLET 监听采用边缘触发模式 NEVER EPOLLONESHOT 2.6.2 文件描述符的一次性通知 NEVER EPOLLWAKEUP 3.5 直到下次监听期间，视为正在处理 NEVER EPOLLEXCLUSIVE 4.5 文件描述符采用独占唤醒模式 NEVER 这里单独说一下几个事件 EPOLLONESHOT: 即一次性通知，事件可用并通知后，将会被修改为禁止状态，也就是不再关心该描述符的事件，但 epoll 实例并没有删除该描述符，如果需要继续监听描述符需要以 EPOLL_CTL_MOD 来修改时间掩码 EPOLLEXCLUSIVE: 即独占通知，主要应用在多个 epoll 实例监听同一个描述符时，该描述符在多个 epoll 实例中必须都设置 EPOLLEXCLUSIVE 才能实现独占，没有设置该掩码的 epoll 实例依然可以监听并返回事件，而设置了该掩码的多个 epoll 实例至少有一个通知该描述符发生事件。 另外需要注意的是，该掩码可以和 EPOLLIN 、 EPOLLOUT 、 EPOLLWAKEUP 和 EPOLLET 一起使用，指定其他非 ALWAYS 值会出现 EINVAL 错误。且该掩码只能用于 EPOLL_CTL_ADD 操作，该描述符的后续修改操作同样会引起 EINVAL 错误。 EPOLLET 将在下面的 epoll 工作模式 中讲解，而 EPOLLWAKEUP 鄙人不是很理解。 epoll_wait // sys/epoll.h int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); // positive count of ready descriptors, 0 on timeout, -1 on error epoll_wait 是真正等待事件发生的 API，与之前的 poll 和 select 一样，timeout 是超时函数，但单位为毫秒 (millisecond)，0 ms 意味着立即返回，-1 则可以让 epoll 永远阻塞，直到以下三种情况之一发生 (当然第三种不会发生) 有监听的描述符发生了监听事件 信号中断，返回 EINTR 错误 超时 参数 maxevents 指示的是最多返回的已准备套接字的数量，也就是说返回值并不会大于 maxevents，且该参数必须大于 0，否则返回 EINVAL 错误。 参数 events，有点迷惑的是其 data 字段，其 events 字段表示了描述符相应因为何种事件准备就绪。因此该参数是一个 struct epoll_event 数组，用来承接描述符状态的。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-apis"},{"categories":["API"],"content":" epoll APIsepoll 将各个功能拆分到了不同 API 中，将描述符检测与传递监听描述符拆分为了讲个函数，以此减少每次都要在调用时传递描述符集的拷贝消耗。 epoll_create // sys/epoll.h int epoll_create(int size); // return a epoll file descriptor, -1 on error 该函数用于在内核中创建一个 epoll 实例并返回一个 epoll 描述符，这个描述符可以理解为实例的唯一地址，在之后需要使用到该集合时就要用到该描述符。 最初时参数 size 指示需要监听的描述符的数量，超过 size 时内核会自动扩容。如今 size 不再有此语义，但调用时必须传递大于 0 的 size 来保证向后兼容性。 该函数有一个变种 // sys/epoll.h int epoll_create1(int flag); // return a epoll file descriptor, -1 on error 如果 flag 为 0 则与 epoll_create 行为相同，只是删除了过时的参数 size。或者传入参数 EPOLL_CLOEXEC 来取得不同的行为：为新的文件描述符添加 close-on-exec (FD_CLOEXEC) 标记，该标记与 open 函数的标记 O_CLOEXEC 相同。 epoll_ctl // sys/epoll.h int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // return 0 if OK, -1 on error 该函数对内核 epoll 实例进行监听描述符的添加、修改、删除操作。 参数 op 对描述符的操作，可选值为 EPOLL_CTL_ADD (添加描述符)、 EPOLL_CTL_MOD (修改描述符) 以及 EPOLL_CTL_DEL (删除描述符) 参数 event 对描述符所作出的具体监听行为，与 pollfd 中的 event 类似，events 是针对文件描述符的事件掩码 typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; // Epoll events epoll_data_t data; // User data variable }; 在每次对描述符进行操作时，都需要设置 epoll_data_t 中的 fd 字段。epoll 中的事件可以取得如下值 值 内核版本 说明 返回 EPOLLIN 文件描述符可读 EPOLLOUT 文件描述符可写 EPOLLRDHUP 2.6.17 流套接字写半部关闭 EPOLLPRI 文件描述符存在异常，同 POLLPRI EPOLLERR 文件描述符发生错误 ALWAYS EPOLLHUP 文件描述符发生终止 ALWAYS EPOLLET 监听采用边缘触发模式 NEVER EPOLLONESHOT 2.6.2 文件描述符的一次性通知 NEVER EPOLLWAKEUP 3.5 直到下次监听期间，视为正在处理 NEVER EPOLLEXCLUSIVE 4.5 文件描述符采用独占唤醒模式 NEVER 这里单独说一下几个事件 EPOLLONESHOT: 即一次性通知，事件可用并通知后，将会被修改为禁止状态，也就是不再关心该描述符的事件，但 epoll 实例并没有删除该描述符，如果需要继续监听描述符需要以 EPOLL_CTL_MOD 来修改时间掩码 EPOLLEXCLUSIVE: 即独占通知，主要应用在多个 epoll 实例监听同一个描述符时，该描述符在多个 epoll 实例中必须都设置 EPOLLEXCLUSIVE 才能实现独占，没有设置该掩码的 epoll 实例依然可以监听并返回事件，而设置了该掩码的多个 epoll 实例至少有一个通知该描述符发生事件。 另外需要注意的是，该掩码可以和 EPOLLIN 、 EPOLLOUT 、 EPOLLWAKEUP 和 EPOLLET 一起使用，指定其他非 ALWAYS 值会出现 EINVAL 错误。且该掩码只能用于 EPOLL_CTL_ADD 操作，该描述符的后续修改操作同样会引起 EINVAL 错误。 EPOLLET 将在下面的 epoll 工作模式 中讲解，而 EPOLLWAKEUP 鄙人不是很理解。 epoll_wait // sys/epoll.h int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); // positive count of ready descriptors, 0 on timeout, -1 on error epoll_wait 是真正等待事件发生的 API，与之前的 poll 和 select 一样，timeout 是超时函数，但单位为毫秒 (millisecond)，0 ms 意味着立即返回，-1 则可以让 epoll 永远阻塞，直到以下三种情况之一发生 (当然第三种不会发生) 有监听的描述符发生了监听事件 信号中断，返回 EINTR 错误 超时 参数 maxevents 指示的是最多返回的已准备套接字的数量，也就是说返回值并不会大于 maxevents，且该参数必须大于 0，否则返回 EINVAL 错误。 参数 events，有点迷惑的是其 data 字段，其 events 字段表示了描述符相应因为何种事件准备就绪。因此该参数是一个 struct epoll_event 数组，用来承接描述符状态的。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-create"},{"categories":["API"],"content":" epoll APIsepoll 将各个功能拆分到了不同 API 中，将描述符检测与传递监听描述符拆分为了讲个函数，以此减少每次都要在调用时传递描述符集的拷贝消耗。 epoll_create // sys/epoll.h int epoll_create(int size); // return a epoll file descriptor, -1 on error 该函数用于在内核中创建一个 epoll 实例并返回一个 epoll 描述符，这个描述符可以理解为实例的唯一地址，在之后需要使用到该集合时就要用到该描述符。 最初时参数 size 指示需要监听的描述符的数量，超过 size 时内核会自动扩容。如今 size 不再有此语义，但调用时必须传递大于 0 的 size 来保证向后兼容性。 该函数有一个变种 // sys/epoll.h int epoll_create1(int flag); // return a epoll file descriptor, -1 on error 如果 flag 为 0 则与 epoll_create 行为相同，只是删除了过时的参数 size。或者传入参数 EPOLL_CLOEXEC 来取得不同的行为：为新的文件描述符添加 close-on-exec (FD_CLOEXEC) 标记，该标记与 open 函数的标记 O_CLOEXEC 相同。 epoll_ctl // sys/epoll.h int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // return 0 if OK, -1 on error 该函数对内核 epoll 实例进行监听描述符的添加、修改、删除操作。 参数 op 对描述符的操作，可选值为 EPOLL_CTL_ADD (添加描述符)、 EPOLL_CTL_MOD (修改描述符) 以及 EPOLL_CTL_DEL (删除描述符) 参数 event 对描述符所作出的具体监听行为，与 pollfd 中的 event 类似，events 是针对文件描述符的事件掩码 typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; // Epoll events epoll_data_t data; // User data variable }; 在每次对描述符进行操作时，都需要设置 epoll_data_t 中的 fd 字段。epoll 中的事件可以取得如下值 值 内核版本 说明 返回 EPOLLIN 文件描述符可读 EPOLLOUT 文件描述符可写 EPOLLRDHUP 2.6.17 流套接字写半部关闭 EPOLLPRI 文件描述符存在异常，同 POLLPRI EPOLLERR 文件描述符发生错误 ALWAYS EPOLLHUP 文件描述符发生终止 ALWAYS EPOLLET 监听采用边缘触发模式 NEVER EPOLLONESHOT 2.6.2 文件描述符的一次性通知 NEVER EPOLLWAKEUP 3.5 直到下次监听期间，视为正在处理 NEVER EPOLLEXCLUSIVE 4.5 文件描述符采用独占唤醒模式 NEVER 这里单独说一下几个事件 EPOLLONESHOT: 即一次性通知，事件可用并通知后，将会被修改为禁止状态，也就是不再关心该描述符的事件，但 epoll 实例并没有删除该描述符，如果需要继续监听描述符需要以 EPOLL_CTL_MOD 来修改时间掩码 EPOLLEXCLUSIVE: 即独占通知，主要应用在多个 epoll 实例监听同一个描述符时，该描述符在多个 epoll 实例中必须都设置 EPOLLEXCLUSIVE 才能实现独占，没有设置该掩码的 epoll 实例依然可以监听并返回事件，而设置了该掩码的多个 epoll 实例至少有一个通知该描述符发生事件。 另外需要注意的是，该掩码可以和 EPOLLIN 、 EPOLLOUT 、 EPOLLWAKEUP 和 EPOLLET 一起使用，指定其他非 ALWAYS 值会出现 EINVAL 错误。且该掩码只能用于 EPOLL_CTL_ADD 操作，该描述符的后续修改操作同样会引起 EINVAL 错误。 EPOLLET 将在下面的 epoll 工作模式 中讲解，而 EPOLLWAKEUP 鄙人不是很理解。 epoll_wait // sys/epoll.h int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); // positive count of ready descriptors, 0 on timeout, -1 on error epoll_wait 是真正等待事件发生的 API，与之前的 poll 和 select 一样，timeout 是超时函数，但单位为毫秒 (millisecond)，0 ms 意味着立即返回，-1 则可以让 epoll 永远阻塞，直到以下三种情况之一发生 (当然第三种不会发生) 有监听的描述符发生了监听事件 信号中断，返回 EINTR 错误 超时 参数 maxevents 指示的是最多返回的已准备套接字的数量，也就是说返回值并不会大于 maxevents，且该参数必须大于 0，否则返回 EINVAL 错误。 参数 events，有点迷惑的是其 data 字段，其 events 字段表示了描述符相应因为何种事件准备就绪。因此该参数是一个 struct epoll_event 数组，用来承接描述符状态的。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-ctl"},{"categories":["API"],"content":" epoll APIsepoll 将各个功能拆分到了不同 API 中，将描述符检测与传递监听描述符拆分为了讲个函数，以此减少每次都要在调用时传递描述符集的拷贝消耗。 epoll_create // sys/epoll.h int epoll_create(int size); // return a epoll file descriptor, -1 on error 该函数用于在内核中创建一个 epoll 实例并返回一个 epoll 描述符，这个描述符可以理解为实例的唯一地址，在之后需要使用到该集合时就要用到该描述符。 最初时参数 size 指示需要监听的描述符的数量，超过 size 时内核会自动扩容。如今 size 不再有此语义，但调用时必须传递大于 0 的 size 来保证向后兼容性。 该函数有一个变种 // sys/epoll.h int epoll_create1(int flag); // return a epoll file descriptor, -1 on error 如果 flag 为 0 则与 epoll_create 行为相同，只是删除了过时的参数 size。或者传入参数 EPOLL_CLOEXEC 来取得不同的行为：为新的文件描述符添加 close-on-exec (FD_CLOEXEC) 标记，该标记与 open 函数的标记 O_CLOEXEC 相同。 epoll_ctl // sys/epoll.h int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // return 0 if OK, -1 on error 该函数对内核 epoll 实例进行监听描述符的添加、修改、删除操作。 参数 op 对描述符的操作，可选值为 EPOLL_CTL_ADD (添加描述符)、 EPOLL_CTL_MOD (修改描述符) 以及 EPOLL_CTL_DEL (删除描述符) 参数 event 对描述符所作出的具体监听行为，与 pollfd 中的 event 类似，events 是针对文件描述符的事件掩码 typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; // Epoll events epoll_data_t data; // User data variable }; 在每次对描述符进行操作时，都需要设置 epoll_data_t 中的 fd 字段。epoll 中的事件可以取得如下值 值 内核版本 说明 返回 EPOLLIN 文件描述符可读 EPOLLOUT 文件描述符可写 EPOLLRDHUP 2.6.17 流套接字写半部关闭 EPOLLPRI 文件描述符存在异常，同 POLLPRI EPOLLERR 文件描述符发生错误 ALWAYS EPOLLHUP 文件描述符发生终止 ALWAYS EPOLLET 监听采用边缘触发模式 NEVER EPOLLONESHOT 2.6.2 文件描述符的一次性通知 NEVER EPOLLWAKEUP 3.5 直到下次监听期间，视为正在处理 NEVER EPOLLEXCLUSIVE 4.5 文件描述符采用独占唤醒模式 NEVER 这里单独说一下几个事件 EPOLLONESHOT: 即一次性通知，事件可用并通知后，将会被修改为禁止状态，也就是不再关心该描述符的事件，但 epoll 实例并没有删除该描述符，如果需要继续监听描述符需要以 EPOLL_CTL_MOD 来修改时间掩码 EPOLLEXCLUSIVE: 即独占通知，主要应用在多个 epoll 实例监听同一个描述符时，该描述符在多个 epoll 实例中必须都设置 EPOLLEXCLUSIVE 才能实现独占，没有设置该掩码的 epoll 实例依然可以监听并返回事件，而设置了该掩码的多个 epoll 实例至少有一个通知该描述符发生事件。 另外需要注意的是，该掩码可以和 EPOLLIN 、 EPOLLOUT 、 EPOLLWAKEUP 和 EPOLLET 一起使用，指定其他非 ALWAYS 值会出现 EINVAL 错误。且该掩码只能用于 EPOLL_CTL_ADD 操作，该描述符的后续修改操作同样会引起 EINVAL 错误。 EPOLLET 将在下面的 epoll 工作模式 中讲解，而 EPOLLWAKEUP 鄙人不是很理解。 epoll_wait // sys/epoll.h int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); // positive count of ready descriptors, 0 on timeout, -1 on error epoll_wait 是真正等待事件发生的 API，与之前的 poll 和 select 一样，timeout 是超时函数，但单位为毫秒 (millisecond)，0 ms 意味着立即返回，-1 则可以让 epoll 永远阻塞，直到以下三种情况之一发生 (当然第三种不会发生) 有监听的描述符发生了监听事件 信号中断，返回 EINTR 错误 超时 参数 maxevents 指示的是最多返回的已准备套接字的数量，也就是说返回值并不会大于 maxevents，且该参数必须大于 0，否则返回 EINVAL 错误。 参数 events，有点迷惑的是其 data 字段，其 events 字段表示了描述符相应因为何种事件准备就绪。因此该参数是一个 struct epoll_event 数组，用来承接描述符状态的。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-wait"},{"categories":["API"],"content":" epoll 工作模式epoll 提供了 边沿触发 (edge triggered, ET) 与 水平触发 (level triggered, LT) 两种模式，默认情况下是水平触发模式。 ET 模式下 epoll_wait 仅会在新的事件首次被加入 epoll 队列时返回；而 LT 模式下， epoll_wait 在事件状态未变更前将不断被触发。 以读事件为例，LT 模式下如果事件未被处理，该事件对应的读缓冲区非空，每次调用 epoll_wait 返回时都会包括该事件，直到事件缓冲区为空位置；ET 模式下，事件只会通知一次，不会反复通知 以写事件为例，LT 模式下只要写缓冲区未满，就会一直通知可写；ET 模式下，内核写缓冲区由满变为未满的情况下，只会通知一次可写事件 水平触发与边沿触发这两个术语来自中断。水平触发 也称状态触发，当设备希望发送中断信号时，驱动中断请求线路置相应电位，并在 CPU 发出强制停止命令或处理所请求的中断事件之前始终保持。持续保持中断也就对应了 LT 模式下事件发生就会一直返回，直到处理。边沿触发 系统中，中断设备向中断线路发送一个脉冲来表示其中断请求，脉冲可以为上升沿或下降沿，当发送完脉冲后立即释放中断线路。 ET 模式使得程序有可能在用户态缓存 I/O 状态，以下情况下推荐使用 ET 模式 read 或 write 系统调用返回了 EAGAIN 非阻塞的文件描述符 但是 ET 模式也有其缺陷 如果 I/O 缓冲区很大，需要很久才能将其一次读完，这可能导致饥饿。比如说，一个描述符上有大量输入，由于 ET 只会通知一次，因此程序往往希望一次将其读完，这样在源源不断的输入流上，其他描述符可能感到饥饿。可以采用就绪队列，将事件发生的描述符在就绪队列中标记，采用 Round-robin (循环 / 轮转) 处理就绪队列中的文件描述符。 如果 A 事件的发生让程序关闭了另一个描述符 B，那么 epoll 实例并不知道，需要手动删除描述符。 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-工作模式"},{"categories":["API"],"content":" epoll 示例 epoll: Manual Example在示例中，监听者是非阻塞套接字，函数 do_use_fd() 使用新的就绪文件描述符，直到 read / write 返回 EAGAIN 为止。事件驱动的状态机程序在 EAGAIN 发生后应记录当前状态，以便下次调用 do_use_fd 时从停止位置继续读写。 #define MAX_EVENTS 10 struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, conn_sock, nfds, epollfd; /* Code to set up listening socket, 'listen_sock', (socket(), bind(), listen()) omitted. */ epollfd = epoll_create1(0); if (epollfd == -1) { perror(\"epoll_create1\"); exit(EXIT_FAILURE); } ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026ev) == -1) { perror(\"epoll_ctl: listen_sock\"); exit(EXIT_FAILURE); } while (true) { nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); if (nfds == -1) { perror(\"epoll_wait\"); exit(EXIT_FAILURE); } for (n = 0; n \u003c nfds; ++n) { if (events[n].data.fd == listen_sock) { conn_sock = accept(listen_sock, (struct sockaddr *) \u0026addr, \u0026addrlen); if (conn_sock == -1) { perror(\"accept\"); exit(EXIT_FAILURE); } setnonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026ev) == -1) { perror(\"epoll_ctl: conn_sock\"); exit(EXIT_FAILURE); } } else { do_use_fd(events[n].data.fd); } } } epoll: TCP echo server又双叒叕重写 TCP echo server，本次使用 epoll LT 模式实现 #include \u003cstdbool.h\u003e #include \u003csys/epoll.h\u003e #include \"unp.h\" #define MAX_EVENTS 1024 int main(int argc, char **argv) { int listenfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_addr.s_addr = htonl(INADDR_ANY), .sin_port = htons(7), }; const socklen_t addrlen = sizeof(servaddr); bind(listenfd, (struct sockaddr *) \u0026servaddr, addrlen); listen(listenfd, LISTENQ); int epollfd = epoll_create1(0); struct epoll_event ev = { .events = EPOLLIN, .data = { .fd = listenfd, }, }; epoll_ctl(epollfd, EPOLL_CTL_ADD, listenfd, \u0026ev); struct epoll_event events[MAX_EVENTS]; struct sockaddr_in cliaddr; char buffer[MAXLINE] = {0}; while (true) { int nready = epoll_wait(epollfd, events, MAX_EVENTS, -1); for (int i = 0; i \u003c nready; i++) { if (events[i].data.fd == listenfd) { int connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026addrlen); ev.events = EPOLLIN; ev.data.fd = connfd; epoll_ctl(epollfd, EPOLL_CTL_ADD, connfd, \u0026ev); } else { int n; if ((n = read(events[i].data.fd, buffer, MAXLINE)) \u003c 0) { if (errno == ECONNRESET) { close(events[i].data.fd); epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, \u0026ev); } else { err_sys(\"read error\"); } } else if (n == 0) { close(events[i].data.fd); epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, \u0026ev); } else { write(events[i].data.fd, buffer, n); } } } } } ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-示例"},{"categories":["API"],"content":" epoll 示例 epoll: Manual Example在示例中，监听者是非阻塞套接字，函数 do_use_fd() 使用新的就绪文件描述符，直到 read / write 返回 EAGAIN 为止。事件驱动的状态机程序在 EAGAIN 发生后应记录当前状态，以便下次调用 do_use_fd 时从停止位置继续读写。 #define MAX_EVENTS 10 struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, conn_sock, nfds, epollfd; /* Code to set up listening socket, 'listen_sock', (socket(), bind(), listen()) omitted. */ epollfd = epoll_create1(0); if (epollfd == -1) { perror(\"epoll_create1\"); exit(EXIT_FAILURE); } ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026ev) == -1) { perror(\"epoll_ctl: listen_sock\"); exit(EXIT_FAILURE); } while (true) { nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); if (nfds == -1) { perror(\"epoll_wait\"); exit(EXIT_FAILURE); } for (n = 0; n \u003c nfds; ++n) { if (events[n].data.fd == listen_sock) { conn_sock = accept(listen_sock, (struct sockaddr *) \u0026addr, \u0026addrlen); if (conn_sock == -1) { perror(\"accept\"); exit(EXIT_FAILURE); } setnonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026ev) == -1) { perror(\"epoll_ctl: conn_sock\"); exit(EXIT_FAILURE); } } else { do_use_fd(events[n].data.fd); } } } epoll: TCP echo server又双叒叕重写 TCP echo server，本次使用 epoll LT 模式实现 #include #include #include \"unp.h\" #define MAX_EVENTS 1024 int main(int argc, char **argv) { int listenfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_addr.s_addr = htonl(INADDR_ANY), .sin_port = htons(7), }; const socklen_t addrlen = sizeof(servaddr); bind(listenfd, (struct sockaddr *) \u0026servaddr, addrlen); listen(listenfd, LISTENQ); int epollfd = epoll_create1(0); struct epoll_event ev = { .events = EPOLLIN, .data = { .fd = listenfd, }, }; epoll_ctl(epollfd, EPOLL_CTL_ADD, listenfd, \u0026ev); struct epoll_event events[MAX_EVENTS]; struct sockaddr_in cliaddr; char buffer[MAXLINE] = {0}; while (true) { int nready = epoll_wait(epollfd, events, MAX_EVENTS, -1); for (int i = 0; i \u003c nready; i++) { if (events[i].data.fd == listenfd) { int connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026addrlen); ev.events = EPOLLIN; ev.data.fd = connfd; epoll_ctl(epollfd, EPOLL_CTL_ADD, connfd, \u0026ev); } else { int n; if ((n = read(events[i].data.fd, buffer, MAXLINE)) \u003c 0) { if (errno == ECONNRESET) { close(events[i].data.fd); epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, \u0026ev); } else { err_sys(\"read error\"); } } else if (n == 0) { close(events[i].data.fd); epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, \u0026ev); } else { write(events[i].data.fd, buffer, n); } } } } } ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-manual-example"},{"categories":["API"],"content":" epoll 示例 epoll: Manual Example在示例中，监听者是非阻塞套接字，函数 do_use_fd() 使用新的就绪文件描述符，直到 read / write 返回 EAGAIN 为止。事件驱动的状态机程序在 EAGAIN 发生后应记录当前状态，以便下次调用 do_use_fd 时从停止位置继续读写。 #define MAX_EVENTS 10 struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, conn_sock, nfds, epollfd; /* Code to set up listening socket, 'listen_sock', (socket(), bind(), listen()) omitted. */ epollfd = epoll_create1(0); if (epollfd == -1) { perror(\"epoll_create1\"); exit(EXIT_FAILURE); } ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026ev) == -1) { perror(\"epoll_ctl: listen_sock\"); exit(EXIT_FAILURE); } while (true) { nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); if (nfds == -1) { perror(\"epoll_wait\"); exit(EXIT_FAILURE); } for (n = 0; n \u003c nfds; ++n) { if (events[n].data.fd == listen_sock) { conn_sock = accept(listen_sock, (struct sockaddr *) \u0026addr, \u0026addrlen); if (conn_sock == -1) { perror(\"accept\"); exit(EXIT_FAILURE); } setnonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET; ev.data.fd = conn_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026ev) == -1) { perror(\"epoll_ctl: conn_sock\"); exit(EXIT_FAILURE); } } else { do_use_fd(events[n].data.fd); } } } epoll: TCP echo server又双叒叕重写 TCP echo server，本次使用 epoll LT 模式实现 #include #include #include \"unp.h\" #define MAX_EVENTS 1024 int main(int argc, char **argv) { int listenfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_addr.s_addr = htonl(INADDR_ANY), .sin_port = htons(7), }; const socklen_t addrlen = sizeof(servaddr); bind(listenfd, (struct sockaddr *) \u0026servaddr, addrlen); listen(listenfd, LISTENQ); int epollfd = epoll_create1(0); struct epoll_event ev = { .events = EPOLLIN, .data = { .fd = listenfd, }, }; epoll_ctl(epollfd, EPOLL_CTL_ADD, listenfd, \u0026ev); struct epoll_event events[MAX_EVENTS]; struct sockaddr_in cliaddr; char buffer[MAXLINE] = {0}; while (true) { int nready = epoll_wait(epollfd, events, MAX_EVENTS, -1); for (int i = 0; i \u003c nready; i++) { if (events[i].data.fd == listenfd) { int connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026addrlen); ev.events = EPOLLIN; ev.data.fd = connfd; epoll_ctl(epollfd, EPOLL_CTL_ADD, connfd, \u0026ev); } else { int n; if ((n = read(events[i].data.fd, buffer, MAXLINE)) \u003c 0) { if (errno == ECONNRESET) { close(events[i].data.fd); epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, \u0026ev); } else { err_sys(\"read error\"); } } else if (n == 0) { close(events[i].data.fd); epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, \u0026ev); } else { write(events[i].data.fd, buffer, n); } } } } } ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:5:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#epoll-tcp-echo-server"},{"categories":["API"],"content":" POXIS 变种","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:6:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#poxis-变种"},{"categories":["API"],"content":" pselect 函数pselect 是 POSIX.1-2001 函数，原型如下 // sys/select.h // sys/time.h // sys/types.h // unistd.h int pselect(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, const struct timespec *timeout, const sigset_t *sigmask); // return count of ready descriptors, 0 on timeout, -1 on error pselect 与 select 函数有两处不同 将系统定时器结构 timeval 换成了 POSIX 定时器结构 timespec，timespec 使用了更精细的纳秒字段 tv_nsec 而非微秒字段 tv_usec。另外由于 timeout 使用 const 修饰，不会在返回时修改为剩余时间 struct timespec { time_t tv_sec; // seconds long tv_nsec; // nanoseconds }; 信号掩码 (sigmask)，将信号掩码保存并设置为指定的 sigmask，返回时恢复之前的信号掩码，信号掩码将屏蔽其中的信号。如果将其设置为 NULL，则信号方面与 select 行为一致 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:6:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#pselect-函数"},{"categories":["API"],"content":" ppoll 函数ppoll 是 poll 的仿 POSIX 变种，而非 POSIX 规范的函数，因此该函数需要得到系统的支持 FreeBSD 11.0 OpenBSD 5.4 Linux 2.6.16 glibc 2.4 函数原型如下 // sys/poll.h // sys/time.h // sys/types.h int ppoll(struct pollfd *fds, nfds_t nfds, const struct timespec *timeout, const sigset_t *sigmask); // return count of ready descriptors, 0 on timeout, -1 on error ppoll 参数 fds 与 nfds 与 poll 一致，参数 timeout 与 sigmask 与 pselect 一致 ","date":"02-26","objectID":"/2022/unixnetworkprogramming_005/:6:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","IuputOutput","Multiplexing"],"title":"I/O 复用","uri":"/2022/unixnetworkprogramming_005/#ppoll-函数"},{"categories":["API"],"content":"GinShio | Unix 网络编程：卷一 (3rd) 第二部分第四章：基础 TCP 套接字编程、第五章：TCP 客户端/服务器示例","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/"},{"categories":["API"],"content":" 基本 TCP 套接字函数 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:1:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#基本-tcp-套接字函数"},{"categories":["API"],"content":" socket 函数在网络编程中第一步往往调用 socket 函数，以指定通讯协议的详情。 // sys/socket.h int socket(int domain, int type, int protocol); // return socket fd, or -1 and set errno on error domain 指协议族，type 是套接字类型，protocol 参数应该设置为某个协议类型常量，或者为 0 表示对 domain 与 type 的系统默认值。 domain AF_INET: IPv4 协议 AF_INET6: IPv6 协议 AF_UNIX or AF_LOCAL: Unix Domain Socket AF_ROUTE: 路由套接字 AF_KEY: 密钥套接字 type SOCK_STREAM: 字节流套接字 SOCK_DGRAM: 数据报套接字 SOCK_SEQPACKET: 有序分组套接字 SOCK_RAW: 原始套接字 protocol (for IPv4 and IPv6) IPPROTO_TCP IPPROTO_UDP IPPROTO_SCTP 需要注意的是，不是所有的组合都是有效的，下表总结了有效的 socket 函数参数组合，空白意味着无效。 AF_INET AF_INET6 AF_LOCAL AF_ROUTE AF_KEY SOCK_STREAM TCP or SCTP TCP or SCTP YES SOCK_DGRAM UDP UDP YES SOCK_SEQPACKET SCTP SCTP YES SOCK_RAM IPv4 IPv6 YES YES 参数 domain 与 type 还有一些其他值，必须 4.4BSD 支持的 AF_NS (Xerox NS protocols or XNS) 和 AF_ISO (OSI protocols)，而 Linux 表述了 SOCK_PACKET 这样的 type 参数来表示 BPF 类似的协议。AF_KEY 采用内核中密钥表的接口来实现的加密的。 另外说一下 AF 是 Adress Family 的缩写，而 PF 是 protocol family 的缩写，由于历史原因：单个 PF 可以支持多个 AF，但这从未实现过，因此在一些实现中 PF_xxx 总与 AF_xxx 相等。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:1:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#socket-函数"},{"categories":["API"],"content":" connect 函数connect 函数被用于 TCP 客户端与服务端之间建立连接。 // sys/socket.h int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); // return 0 if OK, -1 on error sockfd 是由 socket 函数成功调用时返回的套接字文件描述符，addr 是上一篇讲解过的通用套接字地址结构，最后的参数 addrlen 则是对 addr 取 sizeof 所得的大小。 客户端无需在 connect 之前调用 bind 来绑定端口，在需要时 OS 会选择一个临时端口与服务端进行通信。对于 TCP socket 来说，connect 函数会初始化三次握手，在返回时连接是建立完成的，或建立失败。我们可以从 errno 中获取一些出错原因： TCP 客户端没有收到 SYN-ACK 响应，返回 ETIMEDOUT 错误。比如 4.4BSD 上的客户端发送 SYN 后，分别在无响应 6s、24s 后再发送一个 SYN 请求，总计 75s 仍无响应则返回该错误 TCP 服务端对 SYN 响应 RST (复位)，表明主机在端口上没有等待连接的进程，这是一个 硬错误 (hard error)，在收到 RST 后立即返回 ECONNREFUSED 错误 若 TCP 发送 SYN 请求时，链路上某个路由发生 destination unreachable (目的地址不可达) 的 ICMP 错误，则认为是 软错误 (soft error)。内核将保留消息并按第一种错误的时间间隔重新发送请求，仍未响应的情况下返回 EHOSTUNREACH 或 ENETUNREACH 错误 从 TCP 状态转换图来看，connect 函数将状态从 CLOSED 转移到 SYN_SENT，若成功则转移到 ESTABLISHED；失败时该套接字不可再次 connect，需要调用 close 函数关闭套接字文件描述符，然后重新调用 socket 创建新的套接字。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:1:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#connect-函数"},{"categories":["API"],"content":" bind 函数bind 函数将协议地址与一个套接字文件描述符进行绑定。bind 原型与 connect 类似。 // sys/socket.h int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); // return 0 if OK, -1 on error 对于 TCP 套接字，bind 可以指定端口号或 IP 地址，或两者都指定，也可以两者都不指定 TCP 如果没有经过 bind 就调用 connect 或 listen 时，内核会为其绑定一个临时端口 TCP 可以 bind 一个属于主机的网络接口之一的 IP 地址，对于客户端来说这是个源 IP 地址，而对服务端来说，这限定了只接收哪些目的地址的 IP。通常客户端不会绑定 IP，由内核根据外出网络接口决定源 IP 地址；服务器没有绑定 IP 时，内核会把客户发送 SYN 的目的地址作为服务器的源 IP 地址 下表总结了 bind 对于 ip 与 port 指定或不指定时的结果 指定 IP 地址 指定 port 结果 通配地址 0 内核选择 IP 与 port 通配地址 非 0 内核选择 IP，进程指定 port 本地 IP 地址 0 内核选择 port，进程指定 IP 本地 IP 地址 非 0 进程指定 IP 和 port 对于 IPv4 来说，通配地址通常使用 INADDR_ANY 来指定，其值一般为 0 (0.0.0.0)，而 IPv6 中使用结构变量 in6addr_any。 // IPv4 struct sockaddr_in addr4 = { .sin_addr = htonl(INADDR_ANY), }; // IPv6 struct sockaddr_in6 addr6 = { .sin6_addr = in6addr_any, }; 在不指定端口时，bind 并无法获取分配的临时端口，需要调用函数 getsockname 来获取。 bind 常见的错误是 EADDRINUSE (Address already in use)，这在以后再详细说明。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:1:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#bind-函数"},{"categories":["API"],"content":" listen 函数在 TCP 服务器中需要调用 listen 函数，这个函数会完成以下两个行为： 当套接字通过 socket 函数创建时，一般认为这是个主动连接套接字，也就是给客户端调用 connect 函数的。而 listen 可以将其转变为未连接的被动套接字，内核将连接进来的请求直接连接到这个套接字上。也就是说，在状态转换图上看，listen 将状态从 CLOSED 转换到 LISTEN 第二个参数往往指定内核开放的该套接字的连接队列的大小 // sys/socket.h int listen(int sockfd, int backlog); // return 0 if OK, -1 on error 通常情况下 listen 在 socket 和 bind 之后调用，在 accept 之前调用。为了明白参数 backlog，需要认识到内核会为 TCP 连接维护两个队列： 接入队列，或者称半连接队列，这是 TCP 服务器接收到 SYN 请求并发送 SYN-ACK 后等待第三次握手时，所建立的客户端套接字队列 完成队列，这个队列中包含了完成三次握手的客户端套接字，这些套接字都是 ESTABLISHED 状态 当请求接入后，系统将自动地创建新连接并将监听的套接字信息复制到连接中，整个过程是自动化的，无需 server 进程插手。在 server 进行 SYN-ACK 回复后状态变为 SYN_RCVD，将连接放入半连接队列等待客户端回应。如果客户端连接超时则会将其从半连接队列中删除，连接完成后进入 ESTABLISHED 状态，并将该连接从半连接队列移至完成队列的末尾，等待 accept 将其取出进行通信。 关于这两个队列，需要考虑以下几点： listen 的第二个参数 backlog 基于历史原因，是指定两个队列的总和的最大值。在 4.2BSD 帮助手册上定义其为 the maximum length the queue of pending connections may grow to (等待的连接队列的最大可增长长度)，不过没有定义什么是等待的连接，是 SYN_RCVD 还是 ESTABLISHED 或者两者都是 基于 Berkeley 的实现为 backlog 增添了模糊因子 (fudge factor)，最终结果为 backlog 乘以 1.5 不要将 backlog 设置为 0，在不同的实现上对此解释也不同，如果不想接收连接就直接关闭监听连接 在指定 backlog 时可以设置为比内核支持的最大值还要大的值，内核往往会将其改为自身支持的最大值而非返回错误 Linux 帮助手册的 NOTES 部分解释了 Linux 上 backlog 的实现行为，自 Linux 2.2 开始该参数指定的是完成队列的最大大小，即 ESTABLISHED 状态的连接队列。半连接状态队列大小可以通过 /proc/sys/net/ipv4/tcp_max_syn_backlog 进行修改，而 backlog 的最大值在 /proc/sys/net/core/somaxconn 中，通常为 128 当队列满时，一个 SYN 请求到达时 TCP 将会忽略该请求而非 RST。这是因为过满的情况是暂时的，重传 SYN 时期望可以找到可用空间，而返回 RST 会终止正常的 TCP 重传机制，还会让客户端无法区分错误 三次握手完成后，在服务器调用 accept 之前到达的数据由服务器 TCP 进行排队，最大数据量为相应已连接套接字的接收缓冲区大小 下表是 unp 给出的各个操作系统下，backlog 参数取不同值时已排队连接的实际数目。可以看到 AIX 与 MacOS 遵循传统的 Berkeley 算法，Solaris 也有类似的算法，而 FreeBSD 则是 backlog 值 \\(+1\\)。 backlog MaxOS 10.2.6 / AIX 5.1 Linux 2.4.7 HP-UX 11.11 FreeBSD 5.1 Solaris 2.9 0 1 3 1 1 1 1 2 4 1 2 2 2 4 5 3 3 4 3 5 6 4 4 5 4 7 7 6 5 6 5 8 8 7 6 8 6 10 9 9 7 10 7 11 10 10 8 11 8 13 11 12 9 13 9 14 12 13 10 14 10 16 13 15 11 16 11 17 14 16 12 17 12 19 15 18 13 19 13 20 16 19 14 20 14 22 17 21 15 22 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:1:4","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#listen-函数"},{"categories":["API"],"content":" accept 函数accept 是 TCP 服务端在 listen 之后的需要调用的函数，该函数返回一个完成队列中的连接，如果完成队列为空，则会阻塞服务器进程。 // sys/socket.h int accept(int sockfd, struct sockaddr *cliaddr, socklen_t *addrlen); // return non-negative descriptor if OK, -1 on error 参数 cliaddr 与 addrlen 是结果参数，调用时，将 addrlen 设置为 cliaddr 的套接字地址结构长度；返回时，该整数被内核设置为结构的确切字节值。如果对客户端的地址不感兴趣，可以将这两个参数在调用时设置为 NULL。成功时返回值是内核自动生成的一个套接字描述符，这是与其连接的客户端的描述符。 想想第一篇的时间获取客户端，这里给出该客户端对应的时间获取服务端，以这个程序作为例子讲解。 // 以下代码与 UNP intro/daytimetcpsrv1.c 等价 #include \u003cstdarg.h\u003e #include \u003cstdbool.h\u003e #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cstring.h\u003e #include \u003ctime.h\u003e // inet_pton/3, htons/1 #include \u003carpa/inet.h\u003e // struct sockaddr_in #include \u003cnetinet/in.h\u003e // errno #include \u003cerrno.h\u003e // socket/3, connect/3 #include \u003csys/socket.h\u003e #include \u003csys/types.h\u003e // read/3 #include \u003cunistd.h\u003e #define MAXLINE 4096 #define LISTENQ 1024 void err_sys(const char* fmt, ...) { va_list ap; va_start(ap, fmt); char buffer[MAXLINE + 1] = {0}; vsnprintf(buffer, MAXLINE, fmt, ap); int n = strlen(buffer); snprintf(buffer + n, MAXLINE - n, \":%s\", strerror(errno)); strcat(buffer, \"\\n\"); fflush(stdout); fputs(buffer, stderr); fflush(stderr); va_end(ap); exit(1); } void err_msg(const char *fmt, ...) { va_list ap; va_start(ap, fmt); char buffer[MAXLINE + 1] = {0}; vsnprintf(buffer, MAXLINE, fmt, ap); strcat(buffer, \"\\n\"); fflush(stdout); fputs(buffer, stderr); fflush(stderr); va_end(ap); } int main(int argc, char **argv) { int listenfd; if ((listenfd = socket(AF_INET, SOCK_STREAM, 0)) \u003c 0) { err_sys(\"socket error\"); } struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_addr.s_addr = htonl(INADDR_ANY), .sin_port = htons(13), }; if (bind(listenfd, (struct sockaddr *) \u0026servaddr, sizeof(servaddr)) \u003c 0) { err_sys(\"socket error\"); } if (listen(listenfd, LISTENQ) \u003c 0) { err_sys(\"socket error\"); } while (true) { struct sockaddr_in cliaddr; int len = sizeof(cliaddr); int connfd; if ((connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026len)) \u003c 0) { err_msg(\"accept error\"); continue; } char buffer[MAXLINE] = {0}; printf(\"connection from %s, port %d\\n\", inet_ntop(AF_INET, \u0026cliaddr.sin_addr, buffer, INET_ADDRSTRLEN), ntohs(cliaddr.sin_port)); time_t ticks = time(NULL); snprintf(buffer, sizeof(buffer), \"%.24s\\r\\n\", ctime(\u0026ticks)); if (write(connfd, buffer, strlen(buffer)) \u003c 0) { err_msg(\"write error\"); } close(connfd); } } 在我的本地，编译该文件，用 daytimetcpcli 请求时间，服务器输出如下 connection from 127.0.0.1, port 49736 connection from 192.168.0.105, port 53886 与之前的客户端程序很相似，需要注意的是，程序一次调用 socket、bind、listen，之后在一个无限循环中调用 accept 接收请求，并在每次请求完成后，关闭与客户端的连接，进行下一次请求。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:1:5","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#accept-函数"},{"categories":["API"],"content":" 并发服务器现在的服务端程序可以很好的运行，但是只能一次接受一个请求，如果请求很多且单次请求处理时间较长时，显然是不能满足及时响应客户请求的。于此，一个简单的方式诞生了，即创建一个新的进程，在这个新进程中处理请求，而老进程的任务变为接收请求并启动新进。这样每次有新请求时，都会开启一个新进程来处理，老进程可以继续无间断的接受新请求。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:2:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#并发服务器"},{"categories":["API"],"content":" fork在 Unix 操作系统中，有一个简单启动新进程的方式，即 fork // unistd.h pid_t fork(void); // return 0 in child, process ID of child in parent, -1 on error fork 是一个启动新进程的方式，该 syscall 会复制一份一模一样的进程环境作为新进程，新进程被称作子进程 (child process)，而老进程称为父进程 (parent process)。fork 在 parent 与 child 中都有返回值，child 中 fork 返回 0 表示调用成功，而 parent 中返回的是 child 的进程 ID (pid)，在不同的进程中不同的返回值可以让程序员知道当前身处哪个进程。fork 是比较特殊的函数，由于其创建新进程和两个不同的返回值的特性，需要特别注意。 首先介绍下 fork 的两个典型用法： 创建自身进程的副本，每个副本都可以执行不同的操作，即网络服务器的典型操作 一个进程想要执行另一个程序，先创建一个副本，再通过副本调用其他 syscall (后面讲到的 exec) 替换为新的程序，这是 shell 程序的典型用法 // example #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cstring.h\u003e #include \u003cunistd.h\u003e // fork, getpid, getppid #define MAXLINE 1024 int main (void) { pid_t ppid = getppid(); // 获取该进程的父进程 ID pid_t pid = getpid(); // 获取该进程的 ID char buffer[MAXLINE] = {0}; pid_t id = fork(); // 拷贝进程副本，即所有全局变量，与以上的所有变量、数据都被拷贝到新的进程 if (id == 0) { // 当前分支由子进程执行 // ppid 与 pid 为副本存在于子进程中，值目前为止没有改变 strcat(buffer, \"new process\"); printf(\"in %s, ppid: %d, pid: %d\\n\", buffer, ppid, pid); // 修改 ppid 与 pid 的值，不影响父进程中的结果 pid = getpid(); // 获取当前进程的 ID，即子进程的 ID ppid = getppid(); // 获取当前进程的父进程 ID，即父进程的 ID printf(\"in %s, ppid: %d, pid: %d\\n\", buffer, ppid, pid); } else if (id \u003e 0) { // 当前分支由父进程执行，id 值为子进程的值 strcat(buffer, \"old process\"); printf(\"in %s, ppid: %d, pid: %d\\n\", buffer, ppid, pid); // 由于在父进程中，重新获取后值应该不变 pid = getpid(); // 获取当前进程的 ID，即子进程的 ID ppid = getppid(); // 获取当前进程的父进程 ID，即父进程的 ID // 子进程 ID 是 fork 为父进程返回的值 printf(\"in %s, ppid: %d, pid: %d, spid: %d\\n\", buffer, ppid, pid, id); } else { perror(\"fork error\\n\"); exit(EXIT_FAILURE); } // 因为分支结束，这里是所有分支都会执行的代码，即子进程、父进程都会执行 printf(\"in %s, end\\n\", buffer); return 0; } 上述程序可能的输出 in old process, ppid: 17081, pid: 17975 in old process, ppid: 17081, pid: 17975, spid: 17976 in old process, end in new process, ppid: 17081, pid: 17975 in new process, ppid: 17975, pid: 17976 in new process, end 或 in old process, ppid: 17081, pid: 17961 in old process, ppid: 17081, pid: 17961, spid: 17962 in old process, end in new process, ppid: 17081, pid: 17961 in new process, ppid: 1, pid: 17962 in new process, end 可以看到可能的输出中，子进程可能的父进程 ID 变为了 1，这是由于父进程在子进程之前结束生命周期，导致子进程成为孤儿进程，该进程由 init 进程 (id: 1) 收养所导致的子进程父进程变为 1。如果不希望这种事情发生，可以在父进程中使用 wait 或 waitpid 等待子进程结束，这在以后的 APUE 笔记中介绍。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:2:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#fork"},{"categories":["API"],"content":" exec存放在硬盘中的可执行文件能够被 Unix 执行的唯一方法是：由一个现有进程调用 syscall exec 系列函数中的一个 (共 6 个，这些函数被统称为 exec)，exec 可以将当前进程映像替换为新的进程文件，从新进程的 main 函数开始执行，而进程的 ID 不会改变。通常称调用 exec 的进程为 调用进程 (calling process)，而新执行的程序称为 新程序 (new program)。 6 个 exec 函数分为三种 待执行的程序文件是由文件名 (filename) 还是路径名 (pathname) 指定 新程序的参数是一一列出还是指针数组引用 调用进程的环境进行传递还是指定新环境 // unistd.h int execlp(const char *file, const char *arg, ... /* (char *) NULL */); int execl(const char *path, const char *arg, ... /* (char *) NULL */); int execvp(const char *file, char *const argv[]); int execv(const char *path, char *const argv[]); int execle(const char *path, const char *arg, ... /*, (char *) NULL, char * const envp[] */); int execve(const char *file, char *const argv[], char *const envp[]); // return -1 on error, no return on success 这些函数只有错误时才返回到调用者，否则将从新程序的起始点 (通常为 main) 开始。一般 execve 是 syscall，而其他 5 个是调用 execve 的库函数，glibc 扩展了一个与 execve 的库函数 execvpe，检测宏为 _GNU_SOURCE。 需要注意几点： execl、execlp、execle 三个参数将程序的每个字符串参数作为独立的参数传递给 exec，并以 NULL 作为程序参数结束的标志。而 execvp、execv、execve 三个参数将程序的字符串参数作为参数数组 argv 的一部分进行传递，由于没有传递该数组的长度，因此约定 argv 的末尾必须含有空指针 NULL 来标记结尾。 最左侧的 execlp 与 execvp 两个函数指定的是 file，exec 函数将当前的环境变量 PATH 作为查找程序的依据。但如果 file 参数字符串中存在 /，则在当前程序的工作目录 (workpath) 中查找程序，而非 PATH 环境变量中。 execl、execlp、execv、execvp 四个函数均不指定环境变量，因此使用外部变量 environ (man 7) 作为环境变量列表。execle 与 execve 使用用户指定的环境变量列表，同 argv 一样，需要用户传递的 envp 也以 NULL 结尾。 通常进程打开的所有文件描述符，在 exec 切换程序后都会保留，继续打开。可以通过 fcntl 设置 FD_CLOEXEC 来禁止该默认行为。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:2:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#exec"},{"categories":["API"],"content":" getsockname 和 getpeername这两个函数与某个套接字关联的本端协议地址 (getsockname) 或对端协议地址 (getpeername) 相关的操作。 // sys/socket.h int getsockname(int sockfd, struct sockaddr *addr, socklen_t *addrlen); int getpeername(int sockfd, struct sockaddr *addr, socklen_t *addrlen); // return 0 if OK, -1 on error 简单的说就是用来获取已知套接字描述符，但不知道地址结构的套接字，具体用法如下： 在 TCP 客户端 connect 成功返回后，使用 getsockname 获取内核赋予的本地 IP 地址与本地端口号 在以端口号为 0 或通配 IP 地址 (INADDR_ANY) 的 bind 调用，使用 getsockname 获取内核赋予的端口号或 IP (查看 IP 时需要使用 accept 返回的 connfd) getsockname 可以获取某个套接字的协议族 (AF) 在子进程中执行了 exec 操作时，仅可知已连接的客户端的套接字描述符 (其依然保持打开状态)，需要获取客户端 IP 与端口需要使用 getpeername 在最后一个用法中，需要注意 exec 之后的程序映像，需要获取 connfd 的值，而不是凭空出现 connfd 的值。常用的方式是作为程序的字符串参数进行传递，或约定特定描述符的 ID，也可以修改环境变量传递。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:2:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#getsockname-和-getpeername"},{"categories":["API"],"content":" 时间获取服务的并发示例在上面 accept 函数中给出了一个时间获取服务器的代码，这个服务器的实现是一连接一处理的方式，通常称其为 迭代服务器 (iterative server)。缺点也说过了，对于处理时间较长且请求较多的场景下，是无法接受的，希望服务器可以同时服务更多用户。因此 Unix 环境下最简单的方式就是 fork 和 exec syscall，在子进程中处理请求，父进程只做监听、接收请求的操作。这种模型也就是 并发服务器 (concurrent server)。 // function main in daytimetcpsrv1.c int listenfd = socket(AF_INET, SOCK_STREAM, 0); struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_addr.s_addr = htonl(INADDR_ANY), .sin_port = htons(13), }; bind(listenfd, (struct sockaddr *) \u0026servaddr, sizeof(servaddr)); listen(listenfd, LISTENQ); while (true) { struct sockaddr_in cliaddr; int len = sizeof(cliaddr); int connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026len); char buffer[MAXLINE] = {0}; pid_t pid; if ((pid = fork()) == 0) { // in child process close(listenfd); // child closes listening socket dosomething(); // process the request close(connfd); // done with this client exit(EXIT_SUCCESS); // child terminates } close(connfd); // parent closes connected socket } 当连接建立时，accept 返回，此时服务器调用 fork 来创建新进程，listenfd (服务器的监听套接字) 和 connfd (客户端的请求套接字) 都会以副本的形式保留在新进程中，子进程不应该继续打开 listenfd，而父进程应该关闭 connfd。父进程就可以监听 listenfd 从而等待下一个客户端请求的到来，子进程只需要专心为以获取到的 connfd 工作。这就是一个简易的并发服务器模型。 这里有一个问题，close 套接字描述符时不是会导致该连接关闭，为什么子进程还可以正确处理客户端的请求？ 每个文件描述符都是引用计数的，系统会维护一个打开的描述符列表，打开文件时会将对应的描述符引用计数 \\(+1\\)，而关闭时会将引用计数 \\(-1\\)，只有引用计数为 \\(0\\) 时系统才会真正的关闭这个文件。换到这里，accept 导致 connfd \\(+1\\)，而 fork 拷贝副本会导致 listenfd 与 connfd 再次 \\(+1\\) 从而值为 2，父进程关闭 connfd 不会使其引用计数为 0，这就是不会导致提前回收 connfd 的原因。真正回收 connfd 是在子进程调用 close 或结束时。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:2:4","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#时间获取服务的并发示例"},{"categories":["API"],"content":" TCP Echo 服务Echo 服务器是一种简单且基础的 TCP 服务，默认服务端口 7，支持 TCP 与 UDP 服务。 Echo 服务会将客户端发送的数据完全返回，即请求数据就是响应数据。不过 echo 服务有着正常网络应用该有的一切，如果可以在其基础上，将它修改为需要的网络服务应用。echo 与之前介绍的 daytime 服务不同，daytime 服务由服务器主动断开，而 echo 服务由客户端断开，服务端一直保持连接，客户端主动断开而断开连接。 在以后的代码中不会出现诸如 err_sys 之类的错误处理函数的原型，而是用 unp.h 替代。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:3:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#tcp-echo-服务"},{"categories":["API"],"content":" TCP Echo 服务器这里直接展示一个 Echo 服务器的程序代码，相对于以前的代码来说，并没有太大的改动。这里将 str_echo 修改为其他行为就可以作为其他网络服务器使用。 #include \"unp.h\" int main(void) { int listenfd; if ((listenfd = socket(AF_INET, SOCK_STREAM, 0)) \u003c 0) { err_sys(\"socket error\"); } struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_addr.s_addr = htonl(INADDR_ANY), .sin_port = htons(7), }; if (bind(listenfd, (struct sockaddr *) \u0026servaddr, sizeof(servaddr)) \u003c 0) { err_sys(\"socket error\"); } if (listen(listenfd, LISTENQ) \u003c 0) { err_sys(\"socket error\"); } while (true) { struct sockaddr_in cliaddr; int cliaddrlen = sizeof(cliaddr); int connfd; if ((connfd = accept(listenfd, (struct sockaddr *) \u0026cliaddr, \u0026cliaddrlen)) \u003c 0) { err_msg(\"accept error\"); continue; } pid_t pid; if ((pid = fork()) == 0) { close(listenfd); str_echo(connfd); exit(EXIT_SUCCESS); } close(connfd); } } 上面的模板没什么看得，下来好好说一下 echo 服务中的 str_echo，str_echo 只会做一个简单的事：读出客户端的数据并将其重新写回客户端。简单的方式就是，用 read 函数读出数据，再用 write 函数写回即可。但是需要注意的是，这里服务器不会主动断开，而是一直接受客户端的请求并回射，直到被动断开。 void str_echo(int connfd) { size_t cnt; char buffer[MAXLINE] = {0}; while ((cnt = read(connfd, buffer, MAXLINE)) \u003e 0) { write(connfd, buffer, cnt); } if (cnt \u003c 0 \u0026\u0026 errno == EINTR) { return str_echo(connfd); } else if (cnt \u003c 0) { err_sys(\"str_echo: read error\"); } } ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:3:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#tcp-echo-服务器"},{"categories":["API"],"content":" TCP Echo 客户端对于客户端来说，main 函数一样是模板 #include \"unp.h\" int main(int argc, char **argv) { if (argc != 2) { err_quit(\"usage: tcpcli \u003cIPaddress\u003e\"); } int sockfd; if ((sockfd = socket(AF_INET, SOCK_STREAM, NULL)) \u003c 0) { err_sys(\"socket error\"); } struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_port = htons(7), }; if (inet_pton(AF_INET, argv[1], \u0026servaddr.sin_addr) \u003c 0) { err_sys(\"inet_pton error\"); } if (connect(sockfd, (struct sockaddr *) \u0026servaddr, sizeof(servaddr)) \u003c 0) { err_sys(\"connect error\"); } str_cli(stdin, sockfd); } str_cli 可以理解为 dosomething 函数，这里是做所有请求的函数。该函数只做了一件事，循环从标准输入读入一行文本，写入到服务器，等待服务器回射响应，再将结果写入标准输出。 void str_cli(FILE *fp, int sockfd) { char sendline[MAXLINE] = {0}, recvline[MAXLINE] = {0}; FILE *sockfd_fp = fdopen(sockfd, \"r+\"); while (fgets(sendline, MAXLINE, fp) != NULL) { if (fputs(sendline, sockfd_fp) == EOF) { err_quit(\"str_cli: stop input\"); } if (fgets(recvline, MAXLINE, sockfd_fp) == NULL) { err_quit(\"str_cli: server terminated prematurely\"); } fputs(recvline, stdout); bzero(sendline, MAXLINE); bzero(recvline, MAXLINE); } } ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:3:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#tcp-echo-客户端"},{"categories":["API"],"content":" echo 服务端的启动与终止 启动对于一般程序而言，在命令行中输入程序名称即可运行程序，但对于服务端这样的程序，需要一直运行，但当前终端我们可能需要做其他一些事情，不能一直让服务端占据，可以使用后台启动的方式 (即 fork 到子进程中启动) 运行。 ./examplesrv \u0026 服务器启动后，调用 socket、bind、listen 和 accept，并阻塞于 accept。使用 lsof 命令可以看到 7 号端口的使用信息 lsof -i :7 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME examplesr 20246 root 3u IPv4 802379 0t0 TCP *:echo (LISTEN) 当然也可以使用 netstat 检查服务器监听套接字的状态 netstat -a Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN * 或 0.0.0.0 来表示通配地址，netstat 中 :* 表示了为 0 的端口号。这时候启动客户端并指定服务器地址为 127.0.0.1 ./examplecli 127.0.0.1 客户端启动后通过 socket、connect 建立起连接，服务器上 accept 返回，客户端上 connect 返回，连接建立完成，客户端进入 fgets，等待用户输入，服务器子进程被 read 阻塞等待客户输入，父进程则会再次进入 accept 阻塞等待新的连接到来。 此时启动了一个客户端一个服务端，再次通过 netstat 查看网络信息 netstat -a Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 可以清楚的看到，由父进程进行的 LISTEN 状态的 sockfd，子进程与客户端在 echo (7) 和 32996 端口建立起了连接，其中 32996 是客户端由系统自动分配的端口。这是再开启一个通过 wlan0 (无线网卡) 连接的客户端，可以得到如下的输出。可以看到有一个地址 192.168.0.0/24 的地址建立起了连接，这两个不同的连接可以同时工作，当然，还可以添加不同的客户端。 Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 192.168.0.105:46956 192.168.0.105:echo ESTABLISHED tcp 0 0 192.168.0.105:echo 192.168.0.105:46956 ESTABLISHED tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 还可以通过 ps 命令来查看进程的状态与关系。我这里查看到服务端在 pts/4 上启动，而本地客户端在 pts/5 上，wlan0 客户端在 pts/6 上，通过以下 ps 命令查看 # pts/4 ps -t pts/4 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19195 19144 pts/4 Ss+ /bin/zsh - 20235 19195 pts/4 S sudo ./examplesrv - 20246 20235 pts/4 S ./examplesrv - 20254 20246 pts/4 S ./examplesrv - 20256 20246 pts/4 S ./examplesrv - # pts/5 ps -t pts/5 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19236 19144 pts/5 Ss /bin/zsh - 20253 19236 pts/5 S+ ./examplecli 127.0.0.1 - # pts/6 ps -t pts/6 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19277 19144 pts/6 Ss /bin/zsh - 20255 19277 pts/6 S+ ./examplecli 192.168.0.105 - 可以看到所有的进程的 STAT 都是 S，表明进程因等待某些资源而阻塞。 终止客户端程序在处理时，使用 fgets 读入标准输入的数据，当标准输入中输入 EOF (end-of-file) 字符时 fgets 将返回 NULL，由此可以终止客户端的输入，从而终止客户端程序。在 Unix 系统终端上，Control-D (^D) 即输入 EOF 字符。 终止客户端时，可能在 netstat 看到如下输出 Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 192.168.0.105:46956 192.168.0.105:echo TIME_WAIT tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 客户端在结束输入之后关闭套接字描述符，这导致 TCP 客户端向服务端发送一个 FIN，处于 FIN_WAIT_2 状态，服务端响应 ACK，处于 CLOSE_WAIT 状态。服务端从 str_echo 返回子进程的主函数，通过 exit 终止，打开的套接字描述符关闭，从而发送 FIN 到客户端，并接收客户端发送的 ACK，连接终止，客户端套接字进入 TIME_WAIT 状态。 另外进程终止时，会向父进程发送一个 SIGCHLD 信号，服务端代码并没有捕获该代码进行处理，也没有使用 wait 进行处理，从而父进程默认忽略该信号。由于父进程的忽略，子进程进入僵尸状态，在 ps 上显示状态为 Z。 ps -t pts/4 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19195 19144 pts/4 Ss+ /bin/zsh - 20235 19195 pts/4 S sudo ./examplesrv - 20246 20235 pts/4 S ./examplesrv - 20254 20246 pts/4 S ./examplesrv - 20256 20246 pts/4 Z [examplesrv] \u003cdefunct\u003e - 在 Unix 系统上，这种父进程没有处理回收的进程就是僵尸进程，系统不会释放其占用的资源。当僵尸进程过多时，系统就会出现问题，如进程号不足、内存不足等问题。因此需要及时清理，另外当父进程死亡时，僵尸子进程被过继到 init 进程，此时 init 进程会将负责僵尸进程的资源回收工作。 如果想主动终止服务端进程，可以使用 kill 命令对进程发送相应的信号，以此来终止进程。此时服务器就会作为连接的主动关闭方。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:3:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#echo-服务端的启动与终止"},{"categories":["API"],"content":" echo 服务端的启动与终止 启动对于一般程序而言，在命令行中输入程序名称即可运行程序，但对于服务端这样的程序，需要一直运行，但当前终端我们可能需要做其他一些事情，不能一直让服务端占据，可以使用后台启动的方式 (即 fork 到子进程中启动) 运行。 ./examplesrv \u0026 服务器启动后，调用 socket、bind、listen 和 accept，并阻塞于 accept。使用 lsof 命令可以看到 7 号端口的使用信息 lsof -i :7 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME examplesr 20246 root 3u IPv4 802379 0t0 TCP *:echo (LISTEN) 当然也可以使用 netstat 检查服务器监听套接字的状态 netstat -a Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN * 或 0.0.0.0 来表示通配地址，netstat 中 :* 表示了为 0 的端口号。这时候启动客户端并指定服务器地址为 127.0.0.1 ./examplecli 127.0.0.1 客户端启动后通过 socket、connect 建立起连接，服务器上 accept 返回，客户端上 connect 返回，连接建立完成，客户端进入 fgets，等待用户输入，服务器子进程被 read 阻塞等待客户输入，父进程则会再次进入 accept 阻塞等待新的连接到来。 此时启动了一个客户端一个服务端，再次通过 netstat 查看网络信息 netstat -a Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 可以清楚的看到，由父进程进行的 LISTEN 状态的 sockfd，子进程与客户端在 echo (7) 和 32996 端口建立起了连接，其中 32996 是客户端由系统自动分配的端口。这是再开启一个通过 wlan0 (无线网卡) 连接的客户端，可以得到如下的输出。可以看到有一个地址 192.168.0.0/24 的地址建立起了连接，这两个不同的连接可以同时工作，当然，还可以添加不同的客户端。 Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 192.168.0.105:46956 192.168.0.105:echo ESTABLISHED tcp 0 0 192.168.0.105:echo 192.168.0.105:46956 ESTABLISHED tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 还可以通过 ps 命令来查看进程的状态与关系。我这里查看到服务端在 pts/4 上启动，而本地客户端在 pts/5 上，wlan0 客户端在 pts/6 上，通过以下 ps 命令查看 # pts/4 ps -t pts/4 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19195 19144 pts/4 Ss+ /bin/zsh - 20235 19195 pts/4 S sudo ./examplesrv - 20246 20235 pts/4 S ./examplesrv - 20254 20246 pts/4 S ./examplesrv - 20256 20246 pts/4 S ./examplesrv - # pts/5 ps -t pts/5 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19236 19144 pts/5 Ss /bin/zsh - 20253 19236 pts/5 S+ ./examplecli 127.0.0.1 - # pts/6 ps -t pts/6 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19277 19144 pts/6 Ss /bin/zsh - 20255 19277 pts/6 S+ ./examplecli 192.168.0.105 - 可以看到所有的进程的 STAT 都是 S，表明进程因等待某些资源而阻塞。 终止客户端程序在处理时，使用 fgets 读入标准输入的数据，当标准输入中输入 EOF (end-of-file) 字符时 fgets 将返回 NULL，由此可以终止客户端的输入，从而终止客户端程序。在 Unix 系统终端上，Control-D (^D) 即输入 EOF 字符。 终止客户端时，可能在 netstat 看到如下输出 Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 192.168.0.105:46956 192.168.0.105:echo TIME_WAIT tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 客户端在结束输入之后关闭套接字描述符，这导致 TCP 客户端向服务端发送一个 FIN，处于 FIN_WAIT_2 状态，服务端响应 ACK，处于 CLOSE_WAIT 状态。服务端从 str_echo 返回子进程的主函数，通过 exit 终止，打开的套接字描述符关闭，从而发送 FIN 到客户端，并接收客户端发送的 ACK，连接终止，客户端套接字进入 TIME_WAIT 状态。 另外进程终止时，会向父进程发送一个 SIGCHLD 信号，服务端代码并没有捕获该代码进行处理，也没有使用 wait 进行处理，从而父进程默认忽略该信号。由于父进程的忽略，子进程进入僵尸状态，在 ps 上显示状态为 Z。 ps -t pts/4 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19195 19144 pts/4 Ss+ /bin/zsh - 20235 19195 pts/4 S sudo ./examplesrv - 20246 20235 pts/4 S ./examplesrv - 20254 20246 pts/4 S ./examplesrv - 20256 20246 pts/4 Z [examplesrv] - 在 Unix 系统上，这种父进程没有处理回收的进程就是僵尸进程，系统不会释放其占用的资源。当僵尸进程过多时，系统就会出现问题，如进程号不足、内存不足等问题。因此需要及时清理，另外当父进程死亡时，僵尸子进程被过继到 init 进程，此时 init 进程会将负责僵尸进程的资源回收工作。 如果想主动终止服务端进程，可以使用 kill 命令对进程发送相应的信号，以此来终止进程。此时服务器就会作为连接的主动关闭方。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:3:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#启动"},{"categories":["API"],"content":" echo 服务端的启动与终止 启动对于一般程序而言，在命令行中输入程序名称即可运行程序，但对于服务端这样的程序，需要一直运行，但当前终端我们可能需要做其他一些事情，不能一直让服务端占据，可以使用后台启动的方式 (即 fork 到子进程中启动) 运行。 ./examplesrv \u0026 服务器启动后，调用 socket、bind、listen 和 accept，并阻塞于 accept。使用 lsof 命令可以看到 7 号端口的使用信息 lsof -i :7 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME examplesr 20246 root 3u IPv4 802379 0t0 TCP *:echo (LISTEN) 当然也可以使用 netstat 检查服务器监听套接字的状态 netstat -a Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN * 或 0.0.0.0 来表示通配地址，netstat 中 :* 表示了为 0 的端口号。这时候启动客户端并指定服务器地址为 127.0.0.1 ./examplecli 127.0.0.1 客户端启动后通过 socket、connect 建立起连接，服务器上 accept 返回，客户端上 connect 返回，连接建立完成，客户端进入 fgets，等待用户输入，服务器子进程被 read 阻塞等待客户输入，父进程则会再次进入 accept 阻塞等待新的连接到来。 此时启动了一个客户端一个服务端，再次通过 netstat 查看网络信息 netstat -a Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 可以清楚的看到，由父进程进行的 LISTEN 状态的 sockfd，子进程与客户端在 echo (7) 和 32996 端口建立起了连接，其中 32996 是客户端由系统自动分配的端口。这是再开启一个通过 wlan0 (无线网卡) 连接的客户端，可以得到如下的输出。可以看到有一个地址 192.168.0.0/24 的地址建立起了连接，这两个不同的连接可以同时工作，当然，还可以添加不同的客户端。 Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 192.168.0.105:46956 192.168.0.105:echo ESTABLISHED tcp 0 0 192.168.0.105:echo 192.168.0.105:46956 ESTABLISHED tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 还可以通过 ps 命令来查看进程的状态与关系。我这里查看到服务端在 pts/4 上启动，而本地客户端在 pts/5 上，wlan0 客户端在 pts/6 上，通过以下 ps 命令查看 # pts/4 ps -t pts/4 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19195 19144 pts/4 Ss+ /bin/zsh - 20235 19195 pts/4 S sudo ./examplesrv - 20246 20235 pts/4 S ./examplesrv - 20254 20246 pts/4 S ./examplesrv - 20256 20246 pts/4 S ./examplesrv - # pts/5 ps -t pts/5 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19236 19144 pts/5 Ss /bin/zsh - 20253 19236 pts/5 S+ ./examplecli 127.0.0.1 - # pts/6 ps -t pts/6 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19277 19144 pts/6 Ss /bin/zsh - 20255 19277 pts/6 S+ ./examplecli 192.168.0.105 - 可以看到所有的进程的 STAT 都是 S，表明进程因等待某些资源而阻塞。 终止客户端程序在处理时，使用 fgets 读入标准输入的数据，当标准输入中输入 EOF (end-of-file) 字符时 fgets 将返回 NULL，由此可以终止客户端的输入，从而终止客户端程序。在 Unix 系统终端上，Control-D (^D) 即输入 EOF 字符。 终止客户端时，可能在 netstat 看到如下输出 Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:echo 0.0.0.0:* LISTEN tcp 0 0 192.168.0.105:46956 192.168.0.105:echo TIME_WAIT tcp 0 0 GinShio:echo GinShio:32996 ESTABLISHED tcp 0 0 GinShio:32996 GinShio:echo ESTABLISHED 客户端在结束输入之后关闭套接字描述符，这导致 TCP 客户端向服务端发送一个 FIN，处于 FIN_WAIT_2 状态，服务端响应 ACK，处于 CLOSE_WAIT 状态。服务端从 str_echo 返回子进程的主函数，通过 exit 终止，打开的套接字描述符关闭，从而发送 FIN 到客户端，并接收客户端发送的 ACK，连接终止，客户端套接字进入 TIME_WAIT 状态。 另外进程终止时，会向父进程发送一个 SIGCHLD 信号，服务端代码并没有捕获该代码进行处理，也没有使用 wait 进行处理，从而父进程默认忽略该信号。由于父进程的忽略，子进程进入僵尸状态，在 ps 上显示状态为 Z。 ps -t pts/4 -o pid,ppid,tty,stat,args,wchan PID PPID TT STAT COMMAND WCHAN 19195 19144 pts/4 Ss+ /bin/zsh - 20235 19195 pts/4 S sudo ./examplesrv - 20246 20235 pts/4 S ./examplesrv - 20254 20246 pts/4 S ./examplesrv - 20256 20246 pts/4 Z [examplesrv] - 在 Unix 系统上，这种父进程没有处理回收的进程就是僵尸进程，系统不会释放其占用的资源。当僵尸进程过多时，系统就会出现问题，如进程号不足、内存不足等问题。因此需要及时清理，另外当父进程死亡时，僵尸子进程被过继到 init 进程，此时 init 进程会将负责僵尸进程的资源回收工作。 如果想主动终止服务端进程，可以使用 kill 命令对进程发送相应的信号，以此来终止进程。此时服务器就会作为连接的主动关闭方。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:3:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#终止"},{"categories":["API"],"content":" POSIX 信号处理信号 (signal) 就是告知某个进程发生某事的通知，或称为 软件中断 (software interrupt)，signal 发生通常是 异步 的，信号由内核发送或一个进程向另一个进程发送。每个信号都有一个与之关联的 处置 (disposition) 或称为 行为 (action)，处理 sigaction 来设定一个信号的处理，并有三种选择 提供回调函数，在特定信号发生时进行回调。这个函数被称为 信号处理函数 (signal handler)，这种行为也被称为 捕获 (catching) 信号。其中信号 SIGKILL 与 SIGSTOP 不能被捕获。signal handler 原型如下 void handler(int signo); 将信号设置为 SIG_IGN 对信号进行忽略，当然 SIGKILL 与 SIGSTOP 不能被忽略 将信号设置为 SIG_DFL 进行默认处理 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:4:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#posix-信号处理"},{"categories":["API"],"content":" 信号信号的默认行为首先有以下几个大类： Term (终止) 信号发生时终止进程 Ign (忽略) 信号发生时进程忽略该信号 Core (内存映像) 信号发生时终止进程并生成内存映像 Stop (停止) 信号发生时停止进程 Cont (继续) 如果进程已停止，信号发生时继续进程 现在说说都有哪些 POSIX 信号吧 Signal Standard Value Action Comment SIGHUP POSIX.1-1990 1 Term 终端结束时，通知进程不再与终端关联 SIGINT POSIX.1-1990 2 Term 通过热键终止进程 (C-c) SIGQUIT POSIX.1-1990 3 Core 通过热键终止进程并产生内存映像 (C-\\) SIGILL POSIX.1-1990 4 Core 执行了非法指令 SIGTRAP POSIX.1-2001 5 Core 追踪 / 断点陷阱 SIGABRT POSIX.1-1990 6 Core 调用 abort(3) 产生的信号 SIGFPE POSIX.1-1990 8 Core 浮点数异常 SIGKILL POSIX.1-1990 9 Term 终结进程 SIGBUS POSIX.1-2001 10,7,10 Core 总线错误 (内存错误) SIGSEGV POSIX.1-1990 11 Core 无效内存引用 SIGSYS POSIX.1-2001 12,31,12 Core 错误系统调用，见 seccomp(2) SIGPIPE POSIX.1-1990 13 Term 管道破裂：写入无读者管道，见 pipe(7) SIGALRM POSIX.1-1990 14 Term 时钟信号，见 alarm(2) SIGTERM POSIX.1-1990 15 Term 可捕获终止信号，要求程序自己正常退出 SIGURG POSIX.1-2001 16,23,21 Ign 套接字紧急情况 SIGSTOP POSIX.1-1990 17,19,23 Stop 停止进程，不可被忽略或处理 SIGTSTP POSIX.1-1990 18,20,24 Stop 停止进程，可以被忽略或处理 SIGCONT POSIX.1-1990 19,18,25 Cont 停止时继续进程 SIGCHLD POSIX.1-1990 20,17,18 Ign 子进程停止或终止 SIGTTIN POSIX.1-1990 21,21,26 Stop 后台进程等待用户从终端输入 SIGTTOU POSIX.1-1990 22,22,27 Stop 后台进程等待写入终端 SIGXCPU POSIX.1-2001 24,24,30 Core 超过 CPU 时间限制, 见 setrlimit(2) SIGXFSZ POSIX.1-2001 25,25,21 Core 超过文件大小限制, 见 setrlimit(2) SIGVTALRM POSIX.1-2001 26,26,28 Term 虚拟计时器时钟 SIGPROF POSIX.1-2001 27,27,29 Term 分析计时器过期 SIGUSR1 POSIX.1-1990 30,10,16 Term 用户自定义 1 号信号 SIGUSR2 POSIX.1-1990 31,12,17 Term 用户自定义 2 号信号 SIGPOLL POSIX.1-2001 Term 可轮询事件，等价于 SIGIO 有些信号的值可能有多个，这是由于不同架构对于信号的定义不同产生的，通常来说，第一列是 Alpha / SPARC，第二列是 x86 / ARM 或其他架构，第三列是 MIPS 架构。对应的值为 - 时表示该架构下没有此信号。 当然除了 POSIX 信号，在 Linux 的 signal(7) 用户手册中还可以找到以下信号 Signal Value Action Comment SIGIOT 6 Core IOT 陷阱，等价于 SIGABRT SIGEMT 7,-,7 Term 模拟器陷阱 (Emulator trap) SIGSTKFLT -,16,- Term 协处理器上的堆栈错误 (unused) SIGIO 23,29,22 Term 当前 IO 可用 SIGCLD -,-,18 Ign 等价于 SIGCHLD SIGPWR 29,30,19 Term 断电信号 SIGINFO 29,-,- 等同于 SIGPWR SIGLOST -,-,- Term 文件锁丢失 (unused) SIGWINCH 28,28,20 Ign 窗口缩放信号 SIGUNUSED -,31,- Core 等同于 SIGSYS ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:4:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#信号"},{"categories":["API"],"content":" 信号处理对信号处理的方式相对简单，即调用 POSIX 方法 sigaction，但是相对复杂的是需要分配并填写相关结构。有一个相对简单的方式即 signal 函数，第一个参数是信号名，第二个参数就是指向回调函数的指针，或者宏定义 SIG_IGN 或 SIG_DFL。 signal 函数的原型很复杂，不简化时是这个样子 void (*signal(int signum, void (*handler)(int))) (int); 首先其参数为信号名 signo 与回调函数 handler，这个函数类型为无返回值的单参数为 int 的函数指针，signal 函数最终也返回这样的一个函数指针。简化如下 // signal.h typedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); 捕获信号成功时将返回该处理函数，而失败时会返回常量 SIG_ERR。 现在回过头来看一看 POSIX 函数 sigaction // signal.h int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact); // return 0 if OK, -1 on error 说实话 sigaction 虽然麻烦但是函数原型好看很多。act 即将要修改的信号的新行为， oldact 会将旧行为用参数 oldact 返回给用户。 再来看看 sigaction 核心的 struct sigaction struct sigaction { void (*sa_handler)(int); void (*sa_sigaction)(int, siginfo_t *, void *); sigset_t sa_mask; int sa_flags; void (*sa_restorer)(void); }; sa_handler 即信号处理函数，用法同 signal 中的 handler sa_sigaction 同样也是信号处理函数 sa_mask 指定需要阻塞的信号集，即捕获发生时将其进行屏蔽 sa_flags 指定修改信号行为的选项集合，不过其很复杂，常用的一些选项如下 SA_NODEFER: 在自己的信号处理内不对该信号做屏蔽 SA_RESETHAND: 执行信号处理函数后，将信号操作恢复默认值 SA_RESTART: 通过使某些系统调用可跨信号重新启动，提供与 BSD 信号语义兼容的行为。简单解释就是由信号中断的系统调用由内核自动重启 SA_INTERRUPT: 与 SA_RESTART 互补的操作，信号中断的系统调用不会自动重启 SA_SIGINFO: 提供附加信息，信号捕获行为由 sa_handler 改为 sa_sigaction。 struct siginfo_t 是一个复杂的结构，提供了大量字段来描述相关信息，过于复杂暂时不做讨论 这里简单的使用 signal 处理一下之前提到的子进程资源回收的问题。在父进程中，我们应该实现针对 SIGCHLD 的回调函数，在该回调事件中，信号 SIGCHLD 发生时应在父进程内调用 wait 函数用以等待子进程结束并清理资源。 void sig_chld(int signo) { int stat; pid_t pid = wait(\u0026stat); printf(\"child %d terminated\", pid); } 对于这个回调函数非常简单，也能达到我们的目的：清理终止的子进程所占用的资源。当然需要一个合适的位置来调用 signal 注册这个行为，只要在 main 函数的 while 循环之前注册都行，只需要简单的添加一句 signal(SIGCHLD, sig_chld); 即可。 当然你会发现这是不行的，因为这里有一个 慢系统调用 (slow system call)，即系统调用阻塞进程后不保证返回，服务端中典型的慢系统调用是 accept，当无客户连接时，主进程将永远阻塞在 accept。当进程阻塞在慢系统调用中，捕获信号并在相应行为返回后，这个系统调用可能返回一个 EINTR 错误，或者有些实现中会自动重启被中断的系统调用。因此为了方便在 POSIX 系统之间移植，对慢系统调用的 EINTR 错误处理很重要。 可以将符合 POSIX 的系统上的信号处理总结为以下几点 一旦注册了信号处理函数，该行为将一直存在于进程中 在一个信号处理函数运行期间，正在被捕获的信号是阻塞的，sigaction 中的 sa_mask 信号集在此时也是阻塞的 如果信号在阻塞期间产生了一次或多次，那么该信号在唤醒后仅被提交一次，也就是说 Unix 信号默认是不排队的 sigprocmask 函数可以选择性地阻塞或唤醒一组信号 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:4:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#信号处理"},{"categories":["API"],"content":" 意外情况下的程序终止 accept 函数返回前连接终止 即服务器准备从内核取出连接并处理，但连接中断，收到客户端发送的 RST 请求。这种情况依赖于实现，Berkeley 实现完全在内核中处理中止连接，服务器进程无法看到； SVR4 实现大多返回错误给进程，一些 SVR4 实现返回 EPROTO (protocol error)，但 POSIX 支持必须返回 ECONNABORTED 错误，因为在某些流子系统中发生某些致命的协议相关事件时也会返回 EPROTO，服务器可能无法分辨这些错误。因此为了让服务器可以忽略该非致命性错误，从而继续调用 accept。 SIGPIPE 信号 当客户端 read 返回错误时，客户不理会而是继续写入更多数据，会发生什么？这是内核默认发送一个 SIGPIPE 信号，无论是否捕获或忽略该信号，read 都将返回一个 EPIPE 错误。 将客户端 str_cli 稍加修改，就可以观察到 SIGPIPE 的行为：改为两次调用 write，第一次将文本数据第一个字节写入，引发 RST，暂停 1s 后进行第二次写入，将产生 SIGPIPE。 void str_cli(FILE *fp, int sockfd) { char sendline[MAXLINE] = {0}, recvline[MAXLINE] = {0}; while (fgets(sendline, MAXLINE, fp) != NULL) { write(sockfd, sendline, 1); sleep(1); write(sockfd, sendline + 1, strlen(sendline) - 1); if (fgets(recvline, MAXLINE, sockfd_fp) == NULL) { err_quit(\"str_cli: server terminated prematurely\"); } fputs(recvline, stdout); bzero(sendline, MAXLINE); bzero(recvline, MAXLINE); } } 处理 SIGPIPE 信号取决于发生时进程想做什么，如果没有特殊的事则直接忽略，并在后续的操作中检查 EPIPE 错误并终止。 服务器主机崩溃 在服务器主机崩溃时，已有的连接上无法发送数据。客户端为连接写入数据，并阻塞于 read 操作等待服务器响应，服务器不会对任何请求进行响应，从而 TCP 连接请求超时，客户端会收到 ETIMEDOUT 错误；如果在某个中间路由上检测到服务器不可达，则会返回 “destination unreachable” (目的地不可达) 的 ICMP 消息，并返回 EHOSTUNREACH 或 ENETUNREACH 错误。 服务器主机崩溃后重启 服务器主机崩溃，上一点简单的描述了崩溃没有恢复的情况，现在讨论一下服务器主机恢复的情况。 此时客户端发送请求到服务器上，由于已崩溃重启，客户端并不知道服务器有重启，但服务器并没有客户端的连接相关数据，此时客户端 TCP 收到 RST，read 调用返回 ECONNRESET 错误。 服务器主机关机 当 Unix 系统关机时，init 进程会给所有进程发送 SIGTERM 信号，并等待一段时间 (一般是 5 ~ 20 秒)，然后对仍在运行的进程发送 SIGKILL 信号。这么做是为了让进程得知将要关机，而捕获 SIGTERM 信号做相关的数据保存工作，相应的 SIGKILL 则是强制所有进程结束，进入关机状态。之后的情况与服务端主动断开类似。 ","date":"02-19","objectID":"/2022/unixnetworkprogramming_004/:5:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix","TCP"],"title":"基本 TCP 编程","uri":"/2022/unixnetworkprogramming_004/#意外情况下的程序终止"},{"categories":["API"],"content":"GinShio | Unix 网络编程：卷一 (3rd) 第二部分第三章：套接字简介","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/"},{"categories":["API"],"content":" 套接字地址数据结构套接字函数基本都需要一个指向套接字地址结构的指针作为参数，每个协议族都有自己的套接字定义，均以 sockaddr_ 开头，并有协议族的唯一后缀。 ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:1:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#套接字地址数据结构"},{"categories":["API"],"content":" IPv4 套接字地址结构IPv4 套接字地址结构通常称之为 互联网套接字结构 (Internet socket address structure)，结构体 sockaddr_in，定义于 \u003cnetinet/in.h\u003e 中 (POSIX)。 struct in_addr { in_addr_t s_addr; // 32 bit IPv4 地址 (网络序) }; struct sockaddr_in { uint8_t sin_len; // 结构体大小 sa_family_t sin_family; // AF_INET in_port_t sin_port; // 16 bit 传输层端口号 (网络序) struct in_addr sin_addr; char sin_zero[8]; // unused }; 需要注意几点： 长度字段 sin_len 是为了增加对 OSI 协议的支持而在 4.3BSD-Reno 添加的，但该字段并不是 POSIX 规范要求 (linux 实现并没有该字段)。数据类型 uint8_t 是典型符合 POSIX 系统提供的数据类型。 数据类型 说明 头文件 int8_t 有符号 8 bit 整型 sys/types.h uint8_t 无符号 8 bit 整型 sys/types.h int16_t 有符号 16 bit 整型 sys/types.h uint16_t 无符号 16 bit 整型 sys/types.h int32_t 有符号 32 bit 整型 sys/types.h uint32_t 无符号 32 bit 整型 sys/types.h sa_family_t 套接字地址结构的地址族 sys/socket.h socklen_t 套接字地址结构的长度 (一般 uint32_t) sys/socket.h in_addr_t IPv4 地址 (一般 uint32_t) netinet/in.h in_port_t 端口号 (一般 uint16_t) netinet/in.h 除非使用路由 socket，一般情况下无需检查或设置长度字段。 在 socket 函数中套接字地址结构总是被引用，为了增强对不同协议族的兼容性，定义了一个通用套接字地址结构来接受不同的协议族地址。当然现在可以使用 C 所提供的强制转换到 void* 来实现。 // in \u003csys/socket.h\u003e struct sockaddr { uint8_t sa_len; sa_family_t sa_family; // 协议族: AF_xxx char sa_data[14]; // Address }; ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:1:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#ipv4-套接字地址结构"},{"categories":["API"],"content":" 比较套接字地址结构下图展示了几种常见的套接字地址结构的对比，假设长度与协议族字段都是一字节大小。 可以看到 IPv4 与 IPv6 都是固定长度的结构体，而 Unix Domain 与 Datalink 都是可变长度的结构。为了处理这种可变长的结构体，在传递结构时通常会将其长度作为参数一同传递。 ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:1:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#比较套接字地址结构"},{"categories":["API"],"content":" 值-结果参数在使用套接字地址结构时，往往函数会传递结构长度作为参数，不过传递方式取决于该结构的传递方向： 从进程向内核传递 (bind, connect 以及 sendto)，它们一个参数接受结构一个参数接受结构大小 从内核向进程传递 (accept, recvfrom, getsockname 以及 getpeername)，它们将为参数中的 len 赋上对应的结构大小 在向内核传递时，进程告诉内核结构中的数据大小，这是一个值，防止内核越界；而内核向进程传递时，这是一个结果，告诉进程在结构中存储了多少信息。这种类型的参数被成为 值-结果 (value-result) 参数。 一般来讲，套接字地址结构是进程与内核之间的桥梁，比如 4.4BSD 系统就是如此实现的，而 SystemV 实现上套接字函数与普通的库函数无异，函数与协议栈之间如何实现并不影响我们的使用。 对于固定长度的套接字地址结构，长度始终是一个固定值 (IPv4: 14 byte, IPv6: 28 byte)，无论方向如何。对于可变长度的结构 (e.g. sockaddr_un) 改值始终小于结构的最大大小。 在网络变成中，还有很多 value-result 参数的应用： select 函数的中间三个参数 getsockopt 的长度参数 recvmsg 函数的 msghdr 结构的 msg_namelen 和 msg_controllen 字段 ifconf 结构中的 ifc_len 字段 sysctl 函数的两个长度参数中的第一个参数 ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:2:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#值-结果参数"},{"categories":["API"],"content":" 字节函数","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:3:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#字节函数"},{"categories":["API"],"content":" 字节序函数首先思考一个例子，一个 16-bit 整数由两个字节构成，这两个字节是如何存储这个整数的，或者说，高 8-bit 存储在哪个字节中。 大端 (big-endian) 字节序指的是高位到低位从数据起始位置开始存储，小端 (little-endian) 字节序值低位字节从数据起始位置开始存储，高位在数据结束的地方，也就是按照内存增大方向生长。 这两种表示并没有什么标准可言，且在不同系统中都有使用，我们又将其称为主机字节序，与网络字节序相区分。 // intro/byteorder.c #include \"unp.h\" int main(int argc, char** argv) { union { short s; char c[sizeof(short)]; } un; un.s = 0x0102; printf(\"%s: \", CPU_VENDOR_OS); if (sizeof(short) == 2) { puts(un.c[0] == 1 \u0026\u0026 un.c[1] == 2 ? \"big-endian\" : (un.c[0] == 2 \u0026\u0026 un.c[1] == 1 ? \"little-endian\" : \"unknown\")); } else { printf(\"sizeof(short) = %d\\n\", sizeof(short)); } return 0; } UNP 上给出了不同系统与处理器的不同输出 i386-unknown-freebsd4.8: little-endian powerpc-apple-darwin6.6: big-endian sparc64-unkown-freebsd5.1: big-endian powerpc-ibm-aix5.1.0.0: big-endian hppa1.1-hp-hpux11.11: big-endian x86_64-unknown-linux-gnu: little-endian sparc-sun-solaris2.9: big-endian 不同的处理器、系统它们所用的主机字节序有可能不同，因此网络传输中需要一个统一的网络字节序来进行数据的传输 (实际上是大端字节序)，因此就有了 hton 这样一系列函数，也就是第一篇中提到的四个函数。 uint32_t htonl(uint32_t hostlong); // 将 unsigned int 类型 host to network uint16_t htons(uint16_t hostshort); // 将 unsigned short 类型 host to network uint32_t ntohl(uint32_t netlong); // 将 unsigned int 类型 network to host uint16_t ntohs(uint16_t netshort); // 将 unsigned short 类型 network to host 其中 h 表示 host (主机)，n 表示 network (网络)，s 表示 short，l 表示 long，这种命名来自 4.2BSD 的 Digital VAX 实现，实际上可以将 s 看作 16-bit integer，而 l 看作 32-bit integer。 在使用这些函数时，我们无需关心主机序或网络序到底是大端还是小端，这是跨平台的 API 调用。另外注意一点，虽然现在使用的字节都是 8-bit 的定义，但是以前有些机器的字节使用 10-bit 之类的，因此在 RFC 定义上，通常使用位序来定义这些协议，比如说 RFC791 中的 IPv4 头定义。 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 这表示按照顺序来为协议栈写入 4 个字节，最左端的是最高有效位，编号为 0 的位是最高位。 ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:3:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#字节序函数"},{"categories":["API"],"content":" 字节操作函数在 C 语言标准库中关于字符串的操作函数集中于 string.h，而 C 语言中 char 类型与 int8_t 类型无大的差别，定义的 string 可以认为是 int8_t 的数组，也可以粗略的看作是一个字节数组。 首先第一祖字节操作函数是 C 语言标准库中的 mem 系列函数，这一系列函数主要是针对 memory 而言的。 void* memchr(const void *ptr, int ch, size_t count); // 内存查找 void* memset(void *dest, int ch, size_t count); // 内存设置 void* memcpy(void *dest, const void *src, size_t count); // 内存复制 int memcmp(const void *lhs, const void *rhs, size_t count); // 内存比较 memchr 将在内存 ptr 中查找 count 个字节，查找是否有字节值为 ch。memset 是将这块内存中的每个字节都设置为 C，设置长度 count byte。memcpy 是将 src 的内容复制到 dest 中，这里要求 src 与 dest 没有交集，如果可能有交集请使用类似的 memmove。memcmp 将两块内存 lhs 与 rhs 逐字节进行比较，负数表示 lhs 字典序小于 rhs，零表示两块内存相等。 另一组函数在网络编程中经常遇到，这是一组 POSIX 函数，派生自 4.2BSD，定义于 \u003cstrings.h\u003e 中。这些函数以 b 开头 (byte)。 void bzero(void *dest, size_t count); // 字节设置为 0 void bcopy(const void *src, void *dest, size_t count); // 字节复制 int bcmp(const void *lhs, const void *rhs, size_t count); // 字节比较 这些函数与标准 C 函数类似，不同的是它们不会返回指针结果，但这些函数在 POSIX 系统上还是可以随意使用的。 ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:3:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#字节操作函数"},{"categories":["API"],"content":" 地址转换函数","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:4:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#地址转换函数"},{"categories":["API"],"content":" POSIX 标准函数我们常用的 IP 地址往往是用 ASCII 字符串形式呈现的，当然对于编程来说这实在是太低效了，因此在编程中使用网络序的二进制值来表示这些地址，比如 IPv4 使用 uint32_t 来表示地址，而 IPv6 使用 uint8_t[8] 来表示地址。这里将介绍两组函数用来进行 ASCII 字符串地址与二进制地址的互相转换： inet_aton, inet_ntoa 以及 inet_addr 这三个定义于 \u003carpa/inet.h\u003e 的 POSIX 函数，用来将一个 IPv4 点分十进制字符串 (e.g. 192.168.1.1) 转换成一个 32-bit 网络序二进制值 inet_pton 与 inet_ntop 是比较新的两个定义在 \u003carpa/inet.h\u003e 的 POSIX 函数，可以用于 IPv4 或 IPv6 地址的字符串与二进制转换 这两组函数在第一篇中都有简单的介绍，这里给出它们的函数原型 #include \u003carpa/inet.h\u003e int inet_aton(const char *cp, struct in_addr *inp); char *inet_ntoa(struct in_addr in); in_addr_t inet_addr(const char *cp); const char *inet_ntop(int af, const void *restrict src, char *restrict dst, socklen_t size); int inet_pton(int af, const char *restrict src, void *restrict dst); 首先来看旧式函数，inet_aton 将 ASCII 点分十进制字符串形式地址转换为 32-bit 网络序二进制地址，也就是结果存储在参数 inp 中，而返回值为 1 表示成功。但是在处理 cp 是空指针时，不会返回错误而是什么都不存储。 inet_addr 与上一个函数类似，但是不同的是它不再接收 inp 参数，改为返回地址，这样它可以处理 IPv4 的地址，但遗憾的是它表示错误的方式是返回 INADDR_NONE 这个常量，其值与 IPv4 受限广播地址 255.255.255.255 相同，因此该函数无法有效处理这个地址。另外有些手册标注该函数在错误时返回 \\(-1\\) 而非 INADDR_NONE，想一下无符号返回值返回 \\(-1\\) 时应该是怎样的 (UB!)，因此这个函数已被废弃。应该尽可能避免使用该函数。 inet_ntoa 从名字上看它与 inet_aton 作用相反，返回的是 ASCII 点分十进制字符串地址格式。需要注意的是，这个字符串在函数内部使用 static 内存进行保存，因此该函数是 不可重入 (not reentrant) 的。 第一组函数结束，看看第二组，这两个新函数可以为 IPv4 和 IPv6 地址工作，其中 p 意味着表达 (presentation)，而 n 意味着数值 (numeric)。因此从名字可知，pton 是将 ASCII 字符串形式地址转换为网络序二进制形式，而 ntop 正好与其相反。 两个函数都接受 af 作为参数来标识协议族，接受其值为 AF_INET (IPv4) 或 AF_INET6 (IPv6)，如果协议族是不支持的将在 errno 中写入错误 EAFNOSUPPORT (协议族不受支持)。 因此在 pton 中，src 指代字符串地址，而 dst 就是接收二进制地址的数据，成功转换时返回 1，非有效地址则返回 0；ntop 中 src 与 dst 与其含义相反，而 size 则是调用者提供的 buffer 的大小，防止溢出。如果大小不足以容纳字符串地址时，将返回空指针并设置 errno 为 ENOSPC。大小在 \u003cnetinet/in.h\u003e 中定义了如下常量 #define INET_ADDRSTRLEN 16 // IPv4 点分十进制字符串长度 #define INET6_ADDRSTRLEN 46 // IPv6 十六进制字符串长度 ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:4:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#posix-标准函数"},{"categories":["API"],"content":" 编写协议无关的地址转换函数在使用 POSIX 标准函数时最大的问题是需要传递一个二进制地址的指针，而这个地址通常是包含在套接字地址结构中的，这样我们不得不事先创建相关协议的变量，这将我们拉到协议相关性的代码中。 // IPv4 struct sockaddr_in addr4; inet_ntop(AF_INET, \u0026addr4.sin_addr, str, INET_ADDRSTRLEN); // IPv6 struct sockaddr_in6 addr6; inet_ntop(AF_INET6, \u0026addr6.sin6_addr, str, INET6_ADDRSTRLEN); 为了解决协议相关性问题，我们可以实现自己的地址转换函数，来分离协议与结构的关系。可以使用静态缓冲区来保存函数结果，但这样将造成我们的函数不可重入且线程不安全。另外我们可以支持地址字符串后增加端口，同时将端口与地址写入结构。 // lib/sock_ntop.c #include \u003csys/socket.h\u003e #include \u003csys/un.h\u003e #include \u003cnet/if_dl.h\u003e #include \u003cnetinet/in.h\u003e #include \u003carpa/inet.h\u003e #include \u003cstrings.h\u003e #include \u003cstdio.h\u003e #include \u003cstring.h\u003e char* sock_ntop(const struct sockaddr* sa, socklen_t salen) { char portstr[8]; static char str[128]; // Unix Domain 的最大值 bzero(str, sizeof(str)); bzero(portstr, sizeof(portstr)); switch (sa-\u003esa_family) { case AF_INET: { struct sockaddr_in *sin = (struct sockaddr_in *) sa; if (inet_ntop(AF_INET, \u0026sin-\u003esin_addr, str, sizeof(str)) == NULL) { return NULL; } if (ntohs(sin-\u003esin_port) != 0) { snprintf(portstr, sizeof(portstr), \":%d\", ntohs(sin-\u003esin_port)); strcat(str, portstr); } return str; } #ifdef AF_INET6 case AF_INET6: { struct sockaddr_in6 *sin6 = (struct sockaddr_in6 *) sa; str[0] = '['; if (inet_ntop(AF_INET6, \u0026sin6-\u003esin6_addr, str + 1, sizeof(str) - 1) == NULL) { return NULL; } if (ntohs(sin6-\u003esin6_port) != 0) { snprintf(portstr, sizeof(portstr), \"]:%d\", ntohs(sin6-\u003esin6_port)); strcat(str, portstr); return str; } return str + 1; } #endif // AF_INET6 #ifdef AF_UNIX case AF_UNIX: { struct sockaddr_un *unp = (struct sockaddr_un *) sa; if (unp-\u003esun_path[0] == 0) { strcpy(str, \"(no pathname bound)\"); } else { snprintf(str, sizeof(str), \"%s\", unp-\u003esun_path); } return str; } #endif // AF_UNIX #ifdef AF_LINK case AF_LINK: { struct sockaddr_dl *sdl = (struct sockaddr_dl *) sa; if (sdl-\u003esdl_nlen \u003e 0) { snprintf(str, sizeof(str), \"%*s (index %d)\", sdl-\u003esdl_nlen, \u0026sdl-\u003esdl_data[0], sdl-\u003esdl_index); } else { snprintf(str, sizeof(str), \"AF_LINK, index=%d\", sdl-\u003esdl_index); } return str; } #endif // AF_LINK default: { snprintf(str, sizeof(str), \"sock_ntop: unknown AF_xxxx: %d, len %d\", sa-\u003esa_family, salen); return str; } } return NULL; } 另外 unp 还实现了不同的协议无关性函数 sock_bind_wild 将临时端口与通配地址绑定到套接字 sock_cmp_addr 与 sock_cmp_port 可以对比两个套接字地址结构的地址和端口 sock_set_addr 与 sock_set_port 实现对地址结构的地址与端口的设置 sock_get_port 与 sock_ntop_host 实现将地址结构中的端口和主机部分转换为字符串形式 sock_set_wild 则是将套接字地址结构的地址部分置为通配地址 ","date":"02-17","objectID":"/2022/unixnetworkprogramming_003/:4:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"Unix 套接字 API","uri":"/2022/unixnetworkprogramming_003/#编写协议无关的地址转换函数"},{"categories":["DST"],"content":"GinShio | DST mod 配置 -- 三合一与恐怖抽奖机","date":"02-15","objectID":"/2022/3in1_and_slotmachine/","series":null,"tags":["MOD"],"title":"三合一与恐怖抽奖机配置","uri":"/2022/3in1_and_slotmachine/"},{"categories":["DST"],"content":"本次介绍 mod 配置 Tropical Experience | The Volcano Biome (热带冒险 | 火山生态群)，版本 v2.70，这也就是常说的 3 合 1 三合一是将单机版 DLC 巨人国、船难和哈姆雷特合并在一张地图上的 mod，大型但兼容性较差，请谨慎添加 mod。 Kind of World (世界类型)：世界生成需要三合一还是单一世界 custom 表示三合一地图，这也是默认选项 shipwrecked 表示仅海难 hamlet 表示仅哈姆雷特 Sea World 表示世界都是海，仅出生点一点点陆地 for hamlet world (哈姆雷特世界设置) Hamlet Caves (哈姆雷特洞穴): 是否启用 Hamlet 世界的 Hamlet 洞穴，需要开启世界洞穴设置 Together Caves (巨人国洞穴): 是否启用 Hamlet 世界的巨人国洞穴，需要开启世界洞穴设置 Hamlet Clouds: 是否采用 Hamlet 边界云质地 Continent Size (大陆大小) Compact 袖珍 Default 默认 Bigger 巨大 Filling the Biomes: 生物密度 Compact Pig Ruins: 是否袖珍遗迹 for shipwrecked world (船难世界设置) How many islands (多少岛屿): 设置岛屿数量，Hamlet 与巨人国一共 8 岛，剩下的为船难岛屿数量，默认 30 (22 + 8) Shipwrecked Plus: 是否生成基于单机版船难 plus mod 的额外岛屿 Frost Island: 是否生成三合一额外的冰雪岛屿 Moon Biome: 是否在船难生成月亮生态 Hamlet Caves: 是否启用船难世界的 Hamlet 洞穴，需要开启世界洞穴设置 Together Caves: 是否启用 Hamlet 世界的巨人国洞穴，需要开启世界洞穴设置 for custom world (其他设置)，主要设置生态位置，其中包括 Disable (关闭)、 Main Land (主大陆)、Continent (大陆)、Islands (岛屿)、Arquipelago (群岛) Player Portal: 玩家的出生点，Together (巨人国)、Shipwrecked (船难) 还是 Hamlet (哈姆雷特) Reign of Giants Biomes (巨人国生态) Lunar Biomes (月岛生态) Shipwrecked Biomes (船难生态) Shipwrecked_plus: 是否生成 Shipwrecked Plus 的黄金国文明 Hamlet Biomes (Hamlet 生态) Swinesbury (猪伯利) The Royal Palace (皇宫) Pinacle (峰顶) Ant Hill (蚁穴): Hamlet 蚁穴入口和皇后区 Ancient pig ruins (远古猪人遗迹和灾变日历) Peat Forest Island (暴食森林岛屿) Gorge City (暴食镇) Frost Land (霜岛) Hamlet Caves: 是否启用 Hamlet 洞穴 (3in1 新加入的地形生态) for all worlds (世界范围事件) Volcano (火山): 生成影响巨人国与船难世界的火山 (完整版火山需要洞穴支持，而袖珍版无需洞穴) Forge Arena (熔炉竞技场): 是否启用火山洞穴里熔炉竞技场 Underwater (水下世界): 是否生成水下世界 (3in1 里新加入的船难洞穴) OCEAN SETTINGS (海洋设置) Waves (海浪) Aquatic Creatures (水生生物) Kraken (海妖克拉肯) Octopus King (章鱼王) Mangrove Biome (红树林生态) Lilypad Biome (睡莲生态) Ship Graveyard Biome (船墓生态) Coral Biome (珊瑚礁生态) GAMEPLAY SETTINGS (游戏设置) Aporkalypse (毁灭季): 每 60 天持续 20 天的大灾变 Sealnado (豹卷风) WEATHER SETTINGS (天气设置) Flood (洪水): 春天产生水坑，Disabled (关闭)、Tropical Zone (船难世界)、 All world (全世界) Wind (大风): Disabled (关闭)、Tropical-Hamlet (船难-哈姆雷特世界)、All world (全世界) Hail (冰雹) Volcanic Eruption (火山喷发): Disabled (关闭)、Tropical Zone (船难世界)、 All world (全世界) Winter Fog (大雾): Disabled (关闭)、Hamlet Zone (哈姆雷特世界)、All world (全世界) Hay Fever (花粉症): Disabled (关闭)、Hamlet Zone (哈姆雷特世界)、All world (全世界) SHARD-DEDICATED (专用服务器配置) Enable all prefabs (启用预制件): 在专用服务器上需要开启 Tropical shards (世界切片): 一个世界被称为一个 shard，通过洞穴进行切换，设置将不同 DLC 放置在不同 shard，默认将所有 DLC 放置在同一 shard Lobby exit OTHER MOD (其他 MOD 兼容) Cherry Forest (樱桃森林): Mainland (主大陆)、Island (岛屿)、Grove (小岛)、 Archipelago (群岛)、Moon Morphis (月岛) LUAJIT (LuaJIT) 其余选项保持默认即可。 -- 前提说明：mod 配置里的 ---- 关闭 (disable) 用 0 表示，开启 (enable) 用 1 表示 ---- 是 (YES) 用 true 表示，否 (NO) 用 false 表示 -- Kind of World ----- 5: hamlet ----- 10: shipwrecked ----- 15: custom ----- 20: Sea World kindofworld=15, --- for hamlet world hamletcaves_hamletworld=1, ---- Hamlet Caves togethercaves_hamletworld=0, ---- Together Caves hamletclouds=true, ---- Hamlet Clouds continentsize=2, ---- Continent: Compact (1), Default (2), Bigger (3) fillingthebiomes=4, ---- Filling the Biomes: 0% (0), 25% (1), 50% (2), 75% (3), 100% (4) compactruins=false, ---- Compact Pig Ruins --- for shipwrecked world howmanyislands=22, ---- How many islands Shipwreckedworld_plus=true, ---- Shipwrecked Plus frost_islandworld=10, ---- Frost Island, YES (10) / NO (5) Moonshipwrecked=0, ---- Moon Biome hamletcaves_shipwreckedworld=1, ---- Hamlet Caves togethercaves_shipwreckedworld=1, ---- Together Caves --- for custom world ----- Biome Value: ----- 5: Disable ----- 10: Continent ----- 15: Islands ----- 20: Main Land ----- 25: Arquipelago startlocation=5, ---- Player Portal, Together(5) / Shipwrecked (10) / Hamlet (15) Together=20, ---- Reign of Giants Biomes Moon=10, ---- Lunar Biomes Shipwrecked=25, ---- Shipwrecked Biomes Shipwrecked_plus=true, ---- Shipwrecked Plus Hamlet=10, ---- Hamlet Biomes ----- Hamlet Island Value: ----- 5: Disable ----- 10: Main Land ----- 15: Continent ----- 20: Islands pigcity1=15, ---- Swinesbury pigcity2=15, ---- The Royal Palace pinacle=1, ---- Pinacle a","date":"02-15","objectID":"/2022/3in1_and_slotmachine/:0:0","series":null,"tags":["MOD"],"title":"三合一与恐怖抽奖机配置","uri":"/2022/3in1_and_slotmachine/#"},{"categories":["ProgrammingLanguage"],"content":"GinShio | Lua 学习笔记","date":"02-09","objectID":"/2022/dst_lua_language_study/","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/"},{"categories":["ProgrammingLanguage"],"content":"Lua 是一个动态弱类型脚本语言，核心由 C 语言实现，执行效率高，可直接做 C / C++ 扩展。另外 Lua 另一个主流实现 Lua JIT 主要研究针对 Lua 的即时编译系统。 而 Lua 由于其高性能、小巧、简单、与 C 结合性好等特点，大量运用于游戏领域，而饥荒的实现以及扩展也基本使用 Lua 完成。 本章主要描述 Lua 中的词法、语法和语义，语言结构将使用通常的扩展 BNF 表示，比如 {a} 表示 0 或 多个 a， [a] 表示一个可选的 a。而关键字用黑体表示 (e.g. kword)，其他终结符使用反引号表示 (e.g. `=`) 而 Lua 学习主要以 Lua 5.1 的 官方文档 为对象。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:0:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#"},{"categories":["ProgrammingLanguage"],"content":" 词法介绍在 Lua 中标识符可以是任意字母、数字、下划线所组成的字符串 (不能以数字开头)，而标识符可以用作变量的名称和表字段名。但是在标识符命名时不能使用以下名称，因为它们是关键字。 关键字 and break do else elseif end false for function if in local nil not or repeat return then true until while Lua 是一个大小写敏感的语言，因此 And 与 AND 是完全不同的两个标识符。一般约定，由一个下划线开头并跟随大写字母的标识符 (e.g. _VERSION) 是 Lua 内部所使用的全局变量，应避免使用。 符号 + - * / % ^ \\# == ~= \u003c= \u003e= \u003c \u003e = ( ) { } [ ] ; : , . .. … 与 C 语言类似，文字字符串使用单引号或双引号分割，并可以包含转义字符。另外还可以使用 \\ 跟三位数字的方式来表示一个字符 (ASCII)，在不足三位数字时默认在前方补 0 扩展到三位。在 Lua 中有长字符串，即多行字符串，Lua 中称为长括号字符串。长括号字符串以 [[ 开始，并且这两个 [ 之间可以增加任意多的 =，结束时以 ]] 结尾，结尾必须匹配相同多的 =。根据 = 的多少，将其称为 n 级 (e.g. [==[ 称为 2 级，而 [[ 称为 0 级)。 -- 这些字符串所表达的是同一个文字字符串 a = 'alo\\n123\"' a = \"alo\\n123\\\"\" a = '\\97lo\\010123\"' a = [[alo 123\"]] a = [===[alo 123\"]===] 最重要的注释，在 Lua 中使用 -- 开头的字符串表示，这是一个行注释开始的标志，而在这之后直到行尾的所有字符串都不会被解释器解释。如果 -- 紧跟着长括号，那么这个注释将会是一个长注释。 -- 这是一个行注释，到行末为止 a = 15 * 6 --[==[ 这是一个长注释，长括号之内的部分都是注释 b = a * 5 这行将不会运行，因为这是长注释的范围之内 ]==] 数字常数约定可以编写可选的小数部分和可选的十进制指数部分，而十六进制常量以 0x 开头。 b = 3 -- 3 b = 3.0 -- 3.0 b = 3.14159 -- 3.14159 b = 314.159e-2 -- 3.14159 b = 0.314159e1 -- 3.14159 b = 0xff -- 255 b = 0x56 -- 86 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:1:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#词法介绍"},{"categories":["ProgrammingLanguage"],"content":" 值与类型Lua 是动态类型语言，这意味着值是没有类型的，所有值都是第一类值，所有值都可以被存储在变量、传递到参数、或作为函数返回的结果。 在 Lua 中有八个基本类型: nil, boolean, number, string, function, userdata, thread 和 table。Nil 是不同于任何值的 nil 类型字面量，通常表示无用的量。boolean 则是表示真假的量，nil 和 false 都可以使表示条件为假，而其他值则可以表示条件为真。number 表示全体实数，这是一个双精度浮点数。string 表示的是字符的数组，而字符以 8 bit ASCII 表示；与 C 语言不同的是，字符串可以包含任意 8 bit 字符，甚至是 \\0。 Lua 可以调用或操作 Lua 或 C 的函数，userdata 可以在 C 数据结构中直接存储 Lua 值。这种类型对应于一块原始内存，在 Lua 中除了赋值和身份测试外没有其他预定义操作。然而在使用 metatables 时程序员可以定义 userdata 值的操作。Userdata 值不可以被创建或修改，只能通过 C API 使用，这将保证宿主程序可以拥有数据的完整性。 thread 类型代表的是执行线程的标识符，这个是以协程实现的，因此不要与操作系统的线程混淆。因此即使操作系统不支持线程，Lua 也可以正常使用 thread 类型。 table 实现关联数组，这个数组不仅可以使用 number 作为下标，可以使用任何值作为下标 (除了 nil)。table 是异构 (heterogeneous) 的，索引可以包含任意类型的任意值 (除了 nil)。table 是 Lua 中唯一的数据结构机制：可以表示普通数组、符号表、集合、记录、图、树等。表示记录时，Lua 使用字段名作为索引。Lua 支持以 a.name 作为 a[\"name\"] 的语法糖使用。创建表有几种方便的方法。与索引一样，表字段的值也可以是任意类型 (nil 除外)，因为函数是一等值，所以表字段可以包含函数。 table、function、thread 和所有的 userdata 值都是 objects：变量不能实质性的包含这些值，只是引用它们。赋值、参数传递、函数返回总是操作这些值的引用，因此不会有任何复制操作。库函数类型返回一个描述给定值类型的字符串。 Lua 在运行时提供了字符串和数值之间的自动转换，任何应用于字符串的算术运算都会尝试将字符串转换为数字，相反在需要字符串的地方使用数字则会尝试相反的操作，如果需要完全控制字符串的转换需要使用 string.format。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:2:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#值与类型"},{"categories":["ProgrammingLanguage"],"content":" 变量变量存储值，在 lua 中变量分为全局变量、局部变量和表字段。定义变量时其名称 (标识符) 是唯一的。 var ::= Name 局部变量生存周期在词法空间内，这个变量可以在函数作用域内被任意的访问。如果在定义之前访问变量，其值为 nil。如果是表结构，方括号内表示其索引。这表示访问变量 prefixexp 的字段 exp，并将其值赋值给 var。语法 t[\"Name\"] 与 t.Name 是等价的 var ::= prefixexp `[` exp `]` var ::= prefixexp `.` Name 所有的全局变量被存储在起始 Lua 表中，这是一个环境变量或相似的表。每一个函数都有对这个表的引用，当函数被创建时，将继承这个环境变量表。如果你想获取这些环境变量，可以使用 getfenv 函数调用，要替换时可以使用 setfenv 调用。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:3:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#变量"},{"categories":["ProgrammingLanguage"],"content":" 声明Lua 支持一组类似与 Pascal 或 C 的几乎常规化的声明语句，其中包括赋值、控制、函数调用和变量声明。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:4:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#声明"},{"categories":["ProgrammingLanguage"],"content":" chunkLua 中执行单元被称为 chunk，chunk 是简单的语句执行序列的声明。每一个语句后可以选择性跟随分号，但是连续的 ;; 是不合法的 (因此没有空语句)。 chunk ::= {stat [`;`]} 另外 Lua 可以处理作为 chunk 的可变参数的匿名函数。chunk 可以存储在文件或主程序的字符串中，执行 chunk 时会先进行预编译将其转化为虚拟机字节码，然后再使用虚拟机执行编译的代码。chunk 也可以用 luac 编译为二进制码。在编译时，源代码与编译代码是可以互换的，Lua 自动检测文件类型并采用相应的措施。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:4:1","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#chunk"},{"categories":["ProgrammingLanguage"],"content":" block句法上 block 与 chunk 类似，是显式声明的序列，类似于 C 语言中的 {} 划分新的作用域。block 可以被显示地分割从而生成单句声明。 block ::= chunk stat ::= do block end 显式 block 可以有效地控制变量声明的作用域，也可以在其他 block 中添加 return 或 block 语句。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:4:2","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#block"},{"categories":["ProgrammingLanguage"],"content":" assignmentLua 允许多赋值语句，赋值定义语法可以在左边定义一个变量列表，表达式列表将定义在右边。 stat ::= varlist `=` explist varlist ::= var {`,` var} explist ::= exp {`,` exp} 比如在 Lua 中我要定义 a = 5, b = 3 可以写为以下方式。如果 varlist 与 explist 的长度不一样，多余的 varlist 会被赋值为 nil，而多余的 exp 会被丢弃。 a, b = 5, 3, 1 c, d, e = a - b, b - a -- a = 5, b = 3, c = 2, d = -2, e = nil 如果表达式列表以函数调用结束，则该调用返回的所有值将在被调整前进入列表。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:4:3","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#assignment"},{"categories":["ProgrammingLanguage"],"content":" Control Structures (控制结构)控制语句主要以 if 、 while 和 repeat 关键字为主的结构语句。 stat ::= while exp do block end stat ::= repeat block until exp stat ::= if exp then block { elseif exp then block } [ else block ] end 控制结构中的 condition (条件语句) 可以返回任意值， false 与 nil 代表条件语句为假的情况，其他表示真情况 (0 与空字符串同样也是真)。 repeat-until 循环结构类似 C 语言中的 do-while 语句，不过直到 exp 才算作循环块结束，因此条件可以引用循环块内的局部变量。 return 语句可以从 chunk 或 function 中返回一些值 (可以超过一个值) stat ::= return [explist] 而 break 语句将会终止 while 、 repeat 和 for 循环的执行，跳过剩下的语句。而 break 只会终止当前循环。 stat ::= break return 和 break 只能作为 block 的最后一个语句，如果需要在 block 内部使用 return 或 break，需要显式的在内部块中使用。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:4:4","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#control-structures--控制结构"},{"categories":["ProgrammingLanguage"],"content":" forfor 有两种形式：数字型和通用型。数字型 for 通过变量的算术运算来控制循环。 stat ::= for Name `=` exp `,` exp [`,` exp] do block end for v = e1, e2, e3 do block end -- 上面的 for 与下面的代码是等价的 do local var, limit, step = e1, e2, e3 if not (var and limit and step) then error() end while (step \u003e 0 and var \u003c= limit) or (step \u003c= 0 and var \u003e= limit) do local v = var block var = var + step end end 通用型 for 语句工作原理类似函数，被称为 iterators (迭代器)，每趟迭代迭代器函数都会被调用并产生新值，当得到的新值为 nil 时将结束迭代。 stat ::= for namelist in explist do block end namelist ::= Name {`,` Name} for var_1, ..., var_n in explist do block end -- 上面的 for 与下面的代码是等价的 do local f, s, var = explist while true do local var_1, ..., var_n = f(s, var) var = var_1 if var == nil then break end block end end ","date":"02-09","objectID":"/2022/dst_lua_language_study/:4:5","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#for"},{"categories":["ProgrammingLanguage"],"content":" 局部声明局部变量可以在 block 中的任何位置被声明，并且可以被初始化赋值。 stat ::= local namelist [`=` explist] 初始化赋值与多重赋值具有相同的语义，否则，所有没有被赋值的变量都使用 nil 初始化。 chunk 也是一个 block，因此变量可以在任何显式的 block 外被声明，其生命周期被扩展到 chunk 结束。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:4:6","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#局部声明"},{"categories":["ProgrammingLanguage"],"content":" 表达式基础的表达式可以表示为： exp ::= prefixexp exp ::= nil | false | true exp ::= Number exp ::= String exp ::= function exp ::= tableconstructor exp ::= `…` exp ::= exp binop exp exp ::= unop exp prefixexp ::= var | functioncall | `(` exp `)` 所有的函数调用和可变参数表达式都可以产生多个结果，如果表达式用作语句则丢弃所有返回值。如果表达式仅使用最后一个元素 (或唯一一个元素) 那么不会做任何调整，其他条件下将会丢弃除第一个值外的所有值。 f() -- 丢弃所有返回值 g(f(), x) -- f() 调整至 1 个返回值 g(x, f()) -- f() 返回所有返回值 a, b, c = f(), x -- f() 调整至 1 个返回值 a, b, c = x, f() -- f() 调整至 2 个返回值 a, b, c = f() -- f() 调整至 3 个返回值 return f() -- 返回所有 f() 的返回值 return ... -- 返回所有接受的参数 return x, y, f() -- 返回 x 、 y 以及所有 f() 的返回值 {f()} -- 将 f() 所有返回值添加到列表中 {...} -- 将所有参数添加到列表中 {f(), nil} -- f() 调整至 1 个返回值 任何表达式在括号中都产生一个值，因此 (f(x, y, z)) 始终是单个值 (第一个返回值)，如果 f 没有返回值则是 nil。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#表达式"},{"categories":["ProgrammingLanguage"],"content":" 算术运算符Lua 支持多种算术运算符 + (加)、 - (减)、 * (乘)、 / (除)、 % (取模) 以及 ^ (幂)。所有的字符串在运算中被转换为数字。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:1","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#算术运算符"},{"categories":["ProgrammingLanguage"],"content":" 比较运算符比较运算符包含 == (相等)、 ~= (不等)、 \u003c (小于)、 \u003e (大于)、 \u003c= (小于等于) 和 \u003e= (大于等于)，这些运算符总是返回 false 或 true。 对于相等运算符，首先比较运算数的类型，不同类型的运算数将会直接返回 false，然后对值进行比较。对于 Object (tables / userdata / threads / functions) 的比较，同一个 Object 才会相等。而表结构 t[0] 与 t[\"0\"] 是不同的元素。 不等号仅可以在 Number 与 String 上使用，Lua 会尝试调用元函数 lt 与 le 。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:2","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#比较运算符"},{"categories":["ProgrammingLanguage"],"content":" 逻辑运算符Lua 中逻辑运算符以 and 、 or 和 not 为主，和之前说过的一样，false 和 nil 被逻辑运算符当作假值，其他值都为真。另外 and 和 or 都有短路特性。 10 or 20 -- 10 10 or error() -- 10 nil or \"a\" -- \"a\" nil and 10 -- nil false and error() -- false false and nil -- false false or nil -- nil 10 and 20 -- 20 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:3","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#逻辑运算符"},{"categories":["ProgrammingLanguage"],"content":" 级联 (Concatenation)Lua 中 .. 表示字符串级联，如果操作数是数字或字符串，它们将被转换为字符串并进行连接，Lua 会调用元函数 concat 。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:4","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#级联--concatenation"},{"categories":["ProgrammingLanguage"],"content":" 长度操作长度操作为 #，可以计算字符串的字节数，但是对于 table 并不是表中键值对的个数，而是最大的不为 nil 的整数下标的值，这个前提是 table 中没有空洞。Lua 下标从 1 开始，因此 1 为 nil 时 #t 为 0。 t = {} -- #t = 0 t[0] = 1 -- #t = 0 t[1] = 1 -- #t = 1 t[3] = 1 -- #t = 1, t[2] 为空洞 t[2] = 1 -- #t = 3 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:5","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#长度操作"},{"categories":["ProgrammingLanguage"],"content":" 优先级Lua 中运算符可能具有不同的优先级，通常可以通过括号来改变运算符的优先级。 等级 1 or 2 and 3 \u003e \u003c \u003c= \u003e= ~= == 4 .. 5 + - 6 * / % 7 not # - (unary) 8 ^ 这些运算符，除了串联 .. 和幂运算 ^ 是右结合外，其余运算符均为左结合。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:6","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#优先级"},{"categories":["ProgrammingLanguage"],"content":" 表构造器表构造器用于创建一个空的表或者初始化其中一些字段，语法类似 tableconstructor ::= `{` [fieldlist] `}` fieldlist ::= field {fieldsep field} [fieldsep] field ::= `[` exp `]` `=` exp | Name `=` exp | exp fieldsep ::= `,` | `;` 语句 [exp1] = exp2 可以为表中添加键值为 exp1 值为 exp2 的元素，而语句 name = exp 与 [\"name\"] = exp 等价。 t = { [f(1)] = g; \"x\", \"y\"; x = 1, f(x); [30] = 23; 45, } -- 上下等价 local t = {} t[f(1)] = g t[1] = \"x\" t[2] = \"y\" t.x = 1 t[3] = f(x) t[30] = 23 t[4] = 45 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:7","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#表构造器"},{"categories":["ProgrammingLanguage"],"content":" 函数调用函数调用语法类似 functioncall ::= prefixexp args 函数调用时首先对 prefixexp 和 args 进行求值，如果 prefixexp 结果具有函数类型则传递给定参数进行调用，否则使用 prefixexp 的 call 元方法 (将 prefixexp 作为第一个参数)。 functioncall ::= prefixexp `:` Name args 这种调用方式类似于 OOP 中的 方法 ， v:name(args) 与 v.name(v, args) 语法等价。 args ::= `(` [explist] `)` args ::= tableconstructor args ::= String 所有参数表达式在函数被调用之前进行求值，如果在返回时进行函数调用 Lua 将进行尾调用优化或尾递归优化。在尾调用优化时，被调用函数将复用函数栈，因此不用担心函数调用爆栈。但是尾调用优化会删除有关调用函数的所有调试信息。需要注意优化仅发生在 return 语句仅有单个函数调用的情况下，从而完全返回调用函数的返回值，注意以下情况都不会进行尾调用优化： return (f(x)) -- 返回值数量调整为 1 return 2 * f(x) return x, f(x) -- 返回 x 和 f(x) 的所有返回值 f(x); return -- 丢弃结果 return x or f(x) -- 返回值数量调整为 1 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:8","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#函数调用"},{"categories":["ProgrammingLanguage"],"content":" 函数定义函数定义语法如下 function ::= function funcbody funcbody ::= `(` [parlist] `)` block end stat ::= function funcname funcbody stat ::= local function Name funcbody funcname ::= Name {`.` Name} [`:` Name] function f() body end function t.a.b.c.f() body end local function lf() body end -- 上下等价 f = function () body end t.a.b.c.f = function () body end local lf; lf = function () body end 函数定义将定义一个可执行表达式，Lua 解释器会预编译所有函数代码，之后执行函数代码，函数将被实例化，函数实例是表达式的最终结果。同一个函数的不同实例可以引用不同的外部变量，也可以有不同的环境变量表。 函数的参数列表语法如下 parlist ::= namelist [`,` `…`] | `…` 函数被调用时，传参个数被调整至与参数列表相同长度，除非这个函数是一个变长参数函数 (参数列表最终是 ...)。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:5:9","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#函数定义"},{"categories":["ProgrammingLanguage"],"content":" 可见性规则Lua 是词法作用域语言 (静态作用域)，因此变量的生命周期从声明后的第一条语句开始，到包含该声明的块结束为止。 x = 10 -- 全局变量 x1 = 10 do -- 创建一个新的块 local x = x -- 局部变量 x2 = 10 x = x + 1 -- 局部变量 x2 = 11 do local x = x + 1 -- 内部变量 x3 = 12 print(x) -- 打印内部变量 x3 end -- 内部变量 x3 生命周期结束 print(x) -- 打印局部变量 x2 end -- 局部变量 x2 生命周期结束 print(x) -- 打印全局变量 x1 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:6:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#可见性规则"},{"categories":["ProgrammingLanguage"],"content":" 元表 (metatables)在 Lua 中每个值都有一个元表，其中定义了原始值的行为 (其关心的特定运算)，可以修改特定字段来更改某些行为。元表可以控制一个对象的算术运算、排序比较、剪切、长度运算、下标等等，元表还可以定义在垃圾回收时调用的函数。比如对一个非数字添加 加法 运算，可以定义元表中的 __add 字段。 如果想获取这些值可以使用 getmetatable，而 setmetatable 可以修改其中的字段。对于表和所有的 userdata 类型数据，每个值都有一个私有的 metatable，而其他类型的值共享值类型的 metatable。Lua 会为每个事件绑定一个键值，在事件触发时会根据键值调用元函数。 对于这些键值，Lua 定义有不同的名字，而且这些名称之前会有双下划线 __ ，以下时常见的元表元素名称： add: + (加) 运算符 sub: - (减) 运算符 mul: * (乘) 运算符 div: / (除) 运算符 mod: % (取模) 运算符 pow: ^ (幂) 运算符 unm: - (负) 运算符 concat: .. (级联) 运算符 len: # (长度) 运算符 eq: == (相等) 运算符 lt: \u003c (小于) 运算符 le: \u003c= (小于等于) 运算符 index: [] (表下标) 运算符 newindex: [] (表下标赋值) 运算符 call: 函数调用 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:7:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#元表--metatables"},{"categories":["ProgrammingLanguage"],"content":" 环境 (Environment)除了 metatale，thread、function 以及 userdata 类型的数据还有其他表结构，被称为 环境 (environment)，环境也是一张表且多个对象可以共享相同环境。 创建线程对象、非嵌套 Lua 函数 (load、loadfile、loadstring 创建) 时共享创建线程的环境，创建 userdata 和 C 函数时会共享 C 函数对象，创建嵌套 Lua 函数时共享创建 Lua 函数的环境。 userdata 的 environment 对 Lua 来说没有意义，创建 env 只是为了方便编程。与线程相关的 env 被称作全局环境，被用于线程和非嵌套 Lua 函数创建时的默认的环境，且可以被 C 代码直接访问。C 函数相关的 env 是默认 userdata 和 C function 被创建时的环境，也可以被 C 代码直接访问。而 Lua 函数相关的 env 用于解析函数内部对全局变量的访问，也是创建嵌套 Lua 函数时的默认环境。 修改或获取 Lua 函数、线程的环境可以使用 getfenv 和 setfenv，而其他对象如果想操作环境需要访问其 C API。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:8:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#环境--environment"},{"categories":["ProgrammingLanguage"],"content":" 垃圾回收 (Garbage Collection)Lua 实现了自动内存管理，即 Lua 会自动清理没有那些申请了但不再使用的对象，这一机制使用垃圾回收来完成。 Lua 实现了一个增量标记清理收集器，使用两个数字来控制 GC 周期: GC 暂停 和 GC 步长倍数，其使用百分比作为单位 (设置 100 表示内部值 100%)。 GC 暂停控制收集器在开始新的周期之前的等待时间，较大的值意味着收集器行为不那么激进。举个例子：小于 100 时，收集器不会等待而直接开始新的周期，而 200 表示收集器在开始新的周期时等待使用的内存翻倍。 GC 步长倍数控制收集器相对内存分配的速度，较大的值意味着收集器不仅更激进，而且每次增加步长还会逐渐增大。比如说，值 100 时收集器会很慢，并且可能导致器永远不会完成一个周期；而默认值 200 表示收集器将以内存分配速度的 2 倍运行。 如果想定制 GC，可以使用 C API lua_gc 或 lua API collectgarbage。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:9:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#垃圾回收--garbage-collection"},{"categories":["ProgrammingLanguage"],"content":" GC 元方法lua_gc 可以为 userdata 修改 GC 元方法 (这个方法被称为终结者 finalizers)，这个方法允许协调 Lua GC 与外部资源管理。 GC 并不会立即回收带有 __gc 字段的 userdata，而是将其放入一个列表中，收集后 lua 对列表中的元素执行以下等价操作 function gc_event(user_data) local h = metatable(user_data).__gc if h then h(user_data) end end 每个周期结束，终结者将以创建顺序相反的顺序被调用，而 userdata 本身将会在下一个周期被回收。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:9:1","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#gc-元方法"},{"categories":["ProgrammingLanguage"],"content":" 弱表 (weak table)弱表是其中元素都是弱引用的表，GC 会忽略弱引用，也就是说只有弱引用的对象会被回收。 弱表可以是弱键、弱值，或二者都是。弱键意味着可以回收键但不能回收值，实际上，如果键或值有一个被回收的话那么整个 pair 将从表中删除。虚表用元表中的 __mode 字段控制，__mode 包含 k 表示弱键，v 表示弱值。在使用定义好的虚表时，不应修改 __mode 字段的值，否则行为未定义。 ","date":"02-09","objectID":"/2022/dst_lua_language_study/:9:2","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#弱表--weak-table"},{"categories":["ProgrammingLanguage"],"content":" 协程 (Coroutine)Lua 支持协程，协程的执行在 Lua 中依赖线程。与多线程系统的线程不同，协程需要显式调用 yield 函数主动暂停。 创建协程使用 coroutine.create，与线程类似使用参数传递协程执行函数，并返回一个协程的句柄，但不会执行协程。创建之后，以句柄为参数的第一次调用 coroutine.resume 将开始执行协程，额外的参数将传递给协程函数。 协程有两种方式终止： 协程主函数返回 (显式或隐式都可以)，resume 将返回 true 和所有函数的返回值 产生不保护错误，resume 将返回 false 外加错误信息 协程使用 coroutine.yield 时将会暂停， coroutine.resume 立即返回 true，和所有从 coroutine.yield 中返回的值。当下一次运行相同的协程时，将从 yield 开始继续执行。 coroutine.wrap 也可以创建协程，但不同的是将会返回一个函数用以调用来启动协程，函数参数通过 wrap 返回函数来传递，与 coroutine.resume 不同的是，wrap 不会产生不保护错误，因此不会有第一个 boolean 返回值来判断函数是否失败。 举个协程的例子 function foo(a) print(\"foo\", a) return coroutine.yield(2 * a) end co = coroutine.create(function (a, b) print(\"co-body\", a, b) local r = foo(a + 1) print(\"co-body\", r) local r, s = coroutine.yield(a + b, a - b) print(\"co-body\", r, s) return b, \"end\" end) print(\"main\", coroutine.resume(co, 1, 10)) -- co-body 1 10 -- foo 2 -- main true 4 print(\"main\", coroutine.resume(co, \"r\")) -- co-body r -- main true 11 -9 print(\"main\", coroutine.resume(co, \"x\", \"y\")) -- co-body x y -- main true 10 end print(\"main\", coroutine.resume(co, \"x\", \"y\")) -- main false cannot resume dead coroutine ","date":"02-09","objectID":"/2022/dst_lua_language_study/:10:0","series":null,"tags":["Note","Lua"],"title":"Lua 语言学习","uri":"/2022/dst_lua_language_study/#协程--coroutine"},{"categories":["API"],"content":"GinShio | Unix 网络编程：卷一 (3rd) 第一部分第二章：传输层协议 (TCP，UDP，SCTP)","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/"},{"categories":["API"],"content":"在传输层中，主要学习三种协议 User Datagram Protocol (UDP, 用户数据报协议) 是一种简单 (simple)、不可靠 (unreliable) 的数据报协议 Transmission Control Protocol (TCP, 传输控制协议) 是一种复杂的 (sophisticated)、可靠的 (reliable)、字节流协议 Stream Control Transmission Protocol (SCTP, 流控制传输协议) 是一种较新的、可靠的协议，但它还提供消息边界 (message boundaries)、传输层级别的多宿 (multihoming)、最小化头端阻塞 (head-of-line blocking) ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:0:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#"},{"categories":["API"],"content":" 总图 图中最左边的应用 tcpdump 直接使用数据链路层接口 BPF (BSD packet filter, BSD 分组过滤器) 或 Datalink provider interface (DLPI, 数据链路提供者接口) 进行通信。而其他应用都是用 API 所提供的 socket 或 XTI。当然在 Linux 上提供了 SOCK_PACKET 这种 socket 来访问数据链路。 IPv4 Internet Protocol (IP, 网际协议)，版本 4。 这是自 1980 年代早期以来网际协议族中的主力协议，使用 32 bit 编码地址，为 TCP、 UDP、SCTP、ICMP 和 IGMP 提供分组传递服务。 IPv6 Internet Protocol, Version 6 1990 年代中期作为 IPv4 的替代版而设计，将 32 bit 扩展到了 128 bit 来应对互联网的爆炸性增长，为 TCP、UDP、SCTP、ICMPv6 提供分组传递服务。 TCP Transmission Control Protocol TCP 是一种面向连接的协议，提供可靠、全双工的字节流服务，是一种流式 socket，定义在 RFC 793, RFC 1323, RFC 2581, RFC 2988 和 RFC 3390。 为对端发送消息时必须收到明确的答复，如果没有收到答复 TCP 将自动重发数据并等待一段时间。在尝试多次重传失败时 (大概会在 4 到 10 分钟)，TCP 会主动放弃。主要注意的是 TCP 并不会保证对端接收数据，如果可以 TCP 会尝试重传并通知用户。然而 TCP 不是 \\(100\\%\\) 可靠的协议，它的可靠只是在传送数据或失败时通知。 TCP 包含一个在客户端与服务器之间动态评估 RTT (Round-trip time, 往返时间) 的算法，因此知道一个确认等待了多长时间。举个例子，在穿过 LAN 时可能花费几微秒，而穿过 WAN 时则可能话费数秒。此外，TCP 还会持续评估给定连接的 RTT，因为 RTT 受到网络波动的变化很大。 TCP 会将数据序列在发送时按连续的序号关联。比如一个 4096 字节的数据可能被分节为 3 节 (每节最大 1500 字节)，这三节将会有一个连续的序号作为标记，TCP 将会按顺序发送这三节数据。如果这些数据没有按顺序到达，TCP 将会重新按顺序排序并转发给接收应用。如果 TCP 接收到了重复的数据，它会自动丢弃重复数据 (可能一个数据过于拥堵而非真正丢失，导致发送端重传数据，从而出现重复数据)。 TCP 提供了流量控制，即为对端明确现在有多少字节的数据可以被接收。这又被称为 * 通知窗口* (advertised window)。在任何时刻这些窗口都代表此时缓冲区所能接收的可用大小，以防止溢出。当然这些窗口数量是可以动态变化的，在接收数据时窗口大小减少，从缓冲区读取数据时窗口大小将增大。 UDP User Datagram Protocol UDP 是一种无连接的协议，是一种数据报 socket，定义在 RFC 768 。不保证数据可以到达目的地，也不保证数据按顺序到达，或者数据只会到达一次。如果数据报到达目的地址但校验和发生错误，或者丢失在网络中，它无法投递到 socket，也不会被源端自动重传。如果想要数据可以安全到达目的地址，需要增加很多的特性：对端确认、本端超时与重传。每个 UDP 数据报都有一个长度，这个长度会随数据一起发送给对端。 UDP 往往在服务器与客户端之间搭建短连接，可以使得客户端在发送完数据后，立即向其他服务器发送其他数据。 SCTP Stream Control Transmission Protocol SCTP 是一种面向连接的服务，且提供了可靠的、全双工关联以及流量控制，定义在 RFC 2960, RFC 3309 和 RFC 3286 中。是 关联 而非连接，因为往往连接表示仅有两个 IP 在通。信 与 TCP 不同的是，SCTP 是面向消息的，可以按序将不同长度的消息分别发送给对端。在两个端点的连接上可以存在多个流，每一个流分别提供可靠的消息分发，每个流相互独立不会因消息丢失而阻塞其他流。相反 TCP 连接上，丢失消息意味着阻塞该连接。 另外 SCTP 还提供了 多宿 (multihoming) 这种功能，允许单一 SCTP 端点支持多个 IP 地址。这一特性可以增强健壮性来抵抗网络故障。一个端点可以有多个冗余的网络连接，这些网络连接可能来自不同的网络设施，当关联中发生网络故障时 SCTP 可以自动切换到可用的网络设施。 ICMP Internet Control Message Protocol (网际控制消息协议) ICMP 处理在路由器与主机之间流通的错误与控制消息。这些消息通常由 TCP/IP 网络软件自身产生和处理 (不是用户程序)，但图中的 ping 与 traceroute 也使用这个协议。 IGMP Internet Group Management Protocol (网际组管理协议) IGMP 主要用于多播。 ARP Address Resolution Protocol (地址解析协议) ARP 将一个 IPv4 地址解析为一个硬件地址 (如以太网地址)，通常用于以太网、令牌环网和 FDDI 等广播网络，而在 P2P 网络中不需要此协议。 RARP Reverse Address Resolution Protocol (逆地址解析协议) RARP 作用与 ARP 相反，将一个硬件地址解析为 IPv4 地址，不过有时也将其用于无盘结点的引导。 ICMPv6 Internet Control Message Protocol, Version 6 ICMPv6 用于 IPv6 环境，综合了 ICMPv4、IGMP 以及 ARP 的功能。 BPF BSD packet filter (BSD 分组过滤器) 这是一个通常在 BSD 内核中可以找到的功能，提供了直接对数据链路层的访问能力。 DLPI Datalink Provider Interface (数据链路提供者接口) 这是一个通常在 SVR4 内核中可以找到的功能，提供了直接对数据链路层的访问能力。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:1:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#总图"},{"categories":["API"],"content":" 端口TCP、UDP 以及 SCTP 都使用 16 bit 端口号来区分不同进程，而客户端想要与服务器联系时，需要标识服务器。 TCP、UDP 以及 SCTP 定义了一组 公共端口 (well-known port)，用于标识众所周知的服务，比如 FTP 使用 TCP 21 号端口，而 TFTP 使用 UDP 69 号。另一方面用户可能会使用一些短期存活的 临时端口 (ephemeral port)，这些端口通常由传输层协议自动分配给用户唯一的端口，用户不需要知道其具体值。 IANA (the Internet Assigned Numbers Authority, 互联网号码分配局) 维护着端口号分配状态清单。 公共端口为 \\(0\\) 到 \\(1023\\) ，这些端口由 IANA 分配和控制，可能的话 TCP、UDP 和 SCTP 将会在同一服务获得相同的端口号，比如无论 TCP 还是 UDP 80 端口都是 Web 服务器，虽然所有实现都是只使用 TCP。 注册端口 (registered port) 为 \\(1024\\) 到 \\(49151\\)，这些端口不受 IANA 控制，但 IANA 登记并提供其使用情况清单，方便整个群体。同样地，尽可能将所有协议的同端口号分得同一服务。 动态端口 (dynamic) 或私有端口 (private) 范围为 \\(49152\\) 到 \\(65535\\) (这个数是 65536 的 \\(\\frac{3}{4}\\))，即临时端口，这部分是 IANA 不管的。 上图是不同实现中的端口划分方式，有一些我们需要注意的地方。 Unix 系统有着 保留端口 (reserved port) 的概念，这些端口小于 1024。这些端口号仅可以由特权程序使用。 由于历史原因，BSD 实现 (4.3BSD 开始) 以 1024 到 5000 为临时端口，因为当年支持 3977 个连接的主机相当不容易。如今很多系统实现直接采用 IANA 定义的临时端口，或像 Solaris 一样定义更大的端口。 一些少数的客户端需要保留一个端口用于认证 (例如 rlogin 和 rsh)，这些客户端使用 rresvport 创建 TCP 套接字，并在 513 到 1023 范围内尝试绑定端口 (从 1023 逆序开始)。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:2:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#端口"},{"categories":["API"],"content":" 缓冲区大小及限制IP 数据报的大小会有一些限制。 IPv4 数据报最大大小为 65535 字节 (包含 IPv4 头部)，其长度字段占 16 bit。 IPv6 数据报最大大小为 65575 字节 (包含 40 字节的 IPv6 头部)，其长度字段为数据长度，占 16 bit。 绝大多数网络有一个硬件规定的 MTU (以太网为 1500 byte)，或人为配置的 MTU (PPP)，IPv4 要求最小 MTU 为 68 字节，这将允许 IPv4 首部 (20 byte 固定长度与 40 byte 选项部分) 拼接最小的片段 (片段偏移字段以 8 byte 为单位)。而 IPv6 要求最小链路 MTU 为 1280 byte。 两主机之间路径中的最小 MTU 称为 路径 MTU (path MTU)，1500 byte 的以太网 MTU 是最常用的 path MTU。如果相反两个方向上的 path MTU 不一致被称为不对称的。 IP 数据报从接口送出时，如果大小超过相应链路的 MTU，将执行 分片 (fragmentation)，但在到达目的地址时通常不会 重组 (reassembling)。IPv4 主机对其产生的数据报执行分片，IPv4 路由对其转发的数据报执行分片。在 IPv6 协议栈中只有主机对其产生的数据报进行分片。 IPv4 首部如果设置了 不分片 (don’t fragment, DF)，无论是发送还是转发都不会将其分片。如果路由器收到了一个大小超过链路 MTU 且设置 DF 的数据报时，将产生一个 ICMPv4 destination unreachable fragmentation needed but DF bit set (目的地不可达，需分片但设置 DF) 的出错消息。同样的 IPv6 可以认为有一个隐含的 DF，当超过链路 MTU 时被路由转发将会出现 ICMPv6 packet too big 的错误消息。 IPv4 和 IPv6 都定义了 最小重组缓冲区大小 (minimum reassembly buffer size)，它是 IPv4 (576 byte) 或 IPv6 (1500 byte) 的任何实现都必须支持的最小数据报大小。比如 IPv4 数据报我们不能保证目的地址可以接受 577 byte 字节的数据报。 TCP 有一个 MSS (maximum segment size, 最大分节大小) 用于通告对端在每个分节所能发送的最大 TCP 数据量。MSS 的目的是告诉对端其重组缓冲区大小的实际值，从而试图避免分片。MSS 经常被设置为 MTU 减去 IP 和 TCP 首部的固定长度，比如以太网中的 IPv4 MSS 为 1460，IPv6 MSS 为 1440。 SCTP 基于到对端所有地址发现的最小 path MTU 来确定一个分片大小。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:3:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#缓冲区大小及限制"},{"categories":["API"],"content":" TCP 输出 每一个 TCP socket 有一个发送缓冲区，可以使用 SO_SNDBUF 选项来更改该缓冲区的大小。当进程调用 write/3 时，内核从该进程的缓冲区中复制所有数据到 write socket 的发送缓冲区，若发送缓冲区大小不足时将阻塞进程，内核不会从 write 返回，直到所有数据都复制到 socket buffer，因此当 write 返回时仅表示可以使用 buffer 而非对端接收到数据。 本端 TCP 提取 socket buffer 并以 TCP 数据传送规则将其发送，对端 TCP 必须确认收到的数据，对端 ACK 的到来才能在缓冲区中丢弃已确认的数据。另外 TCP 还会为已发送的数据保留副本，直到被对端确认为止。 本端 TCP 以 MSS 大小或更小的 chunk 将数据传送给 IP，同时每个 chunk 上都有 TCP 首部。IP 给每个 TCP 分节按上 IP 首部构成 IP 数据报，查找路由表项确定外出接口，并传递给数据链路层。而虽然数据报可能被 IP 分片，但 MSS 的作用就是尽可能防止分片。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:3:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#tcp-输出"},{"categories":["API"],"content":" UDP 输出 上图中的 socket buffer 很明显与 TCP 的区别是虚线表示，实际上这是并不真实存在的 buffer， SO_SNDBUF 的作用是更改可写入 socket 的大小上限。如果应用写一个大于上限的数据报，内核将返回 EMSGSIZE 错误。由于 UDP 并不可靠，因此无需额外的 buffer 来备份数据，而是简单的按上 8 byte UDP 首部后传递给 IP 层，在数据被发送后将丢弃数据拷贝。 write 调用成功返回表示 所写的数据报或其所有分片已被添加至数据链路层的输出队列 ，如果队列没有足够的空间存放该数据报或某个分片时，内核通常返回一个 ENOBUFS 的错误。但是某些实现中将会直接丢弃而不返回错误。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:3:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#udp-输出"},{"categories":["API"],"content":" SCTP 输出 SCTP 与 TCP 类似，因此也需要发送缓冲区，SO_SNDBUF 选项可以更改缓冲区的大小。当调用 write 时，内核从该c進行的缓冲区复制所有数据到 socket buffer。同样的当 socket buffer 容量不足时将会阻塞进程，直至所有数据被拷贝到 socket buffer。SCTP 必须等待 SACK，确认或累计超出超时重发后才会将数据从 socket buffer 中删除。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:3:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#sctp-输出"},{"categories":["API"],"content":" 标准 Internet 服务与命令行程序","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:4:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#标准-internet-服务与命令行程序"},{"categories":["API"],"content":" 标准 Internet 服务TCP/IP 多数实现中都会提供若干标准服务。一般来说无论 TCP 还是 UDP 它们的端口号都相同。 名称 TCP 端口 UDP 端口 RFC 说明 echo 7 7 862 返回客户端发送的数据 discard 9 9 863 丢弃客户端发送的数据 daytime 13 13 867 返回可读的时间与日期 chargen 19 19 864 发送连续的字符流，UDP 则每次发送随机大小的数据报 time 37 37 868 自 1900.01.01 00:00:00 以来的秒数 这些服务通常由 inetd 守护进程提供，且使用 telnet 客户程序就能完成简易测试。不过由于安全性问题，默认将关闭这些服务。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:4:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#标准-internet-服务"},{"categories":["API"],"content":" 常见命令行程序 应用 IP ICMP UDP TCP SCTP ping \\(\\checkmark\\) traceroute \\(\\checkmark\\) \\(\\checkmark\\) OSPF (路由协议) \\(\\checkmark\\) RIP (路由协议) \\(\\checkmark\\) BGP (路由协议) \\(\\checkmark\\) BOOTP (引导协议) \\(\\checkmark\\) DHCP (引导协议) \\(\\checkmark\\) NTP (时间协议) \\(\\checkmark\\) TFTP (低级 FTP) \\(\\checkmark\\) SNMP (网络管理) \\(\\checkmark\\) SMTP (电子邮件) \\(\\checkmark\\) Telnet (远程登陆) \\(\\checkmark\\) SSH (安全远程登陆) \\(\\checkmark\\) FTP (文件传输) \\(\\checkmark\\) HTTP (Web) \\(\\checkmark\\) NNTP (网络新闻) \\(\\checkmark\\) LPR (远程打印) \\(\\checkmark\\) DNS (域名系统) \\(\\checkmark\\) \\(\\checkmark\\) NFS (网络文件系统) \\(\\checkmark\\) \\(\\checkmark\\) Sun RPC (远程过程调用) \\(\\checkmark\\) \\(\\checkmark\\) DCE RPC (远程过程调用) \\(\\checkmark\\) \\(\\checkmark\\) IUA (ISDN over IP) \\(\\checkmark\\) M2UA/M3UA (SS7 电话信令) \\(\\checkmark\\) H.248 (媒体网关控制) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) H.323 (IP 电话) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) SIP (IP 电话) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:4:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#常见命令行程序"},{"categories":["API"],"content":" TCP 连接我们需要搞清楚 connect/3, accept/4 和 close/1 这些函数在操作 TCP 连接时都干了什么，因此我们需要学习 TCP 连接的相关内容，比如建立连接、断开连接、以及状态。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:5:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#tcp-连接"},{"categories":["API"],"content":" TCP 创建连接在 TCP 连接创建时往往经历以下几个阶段： 服务端接受发起的连接，这通常需要 socket/3, bind/3, listen/2 等 API，将其称为 被动打开 (passive open) 客户端通过 connect/3 进行 主动打开 (active open)，首先发送 SYN (synchronize) 字段来告知在这个连接中发送数据的初始序号。通常 SYN 不带数据，仅包含 IP 头、TCP 头以及可选的 TCP选项。 服务器必须为客户端发送的 SYN 回复 ACK (acknowledge)，并发送 SYN 来初始化本端的数据初始序号。ACK 和 SYN 将会合并在一个 TCP 数据中发送。 客户端必须以 ACK 回应服务器的 SYN 在 TCP 通信中，每个 ACK 都是本端所期待的下一个序列号，SYN 占据一个字节的序列号空间，每个 SYN 中的 ACK 确认号都是 SYN 的序列号加 1。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:5:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#tcp-创建连接"},{"categories":["API"],"content":" TCP 选项每一个 SYN 可以包含一些 TCP 选项，通常常用选项有以下几种： MSS 选项。发送 SYN 的 TCP 端将会告知 最大分节大小 (maximum segment size, MSS)，这是本次连接中每个分节所能接受的最大大小，而对端使用该 MSS 值作为分节依据。 窗口扩展选项。TCP 任何一端可以告知对端本次连接最大窗口为 65535 (首部相应字段占 16 bit)，不过当今互联网的高速网络 (大于 45 Mbps) 或长延迟路径 (卫星链路) 可能要求更大的窗口来提高吞吐量。该选项可以指定首部中通知窗口扩大的位数 (左移, 0 到 14 bit)，由此所能提供的最大窗口即 \\(65535 \\times 2^{14}\\) 。只有在两端都支持的系统上该选项才有用。 时间戳选项。对于高速连接这是必要选项，可以防止由失而复得的分组可能造成的数据损坏。类似窗口扩展，这也需要双端支持。 大多数实现都支持这些设置，后两项有时称为 RFC 1323 选项，高宽带或长延迟网络被称为 长胖管道 (long fat pipe)，因此后两项也称为 长胖管道选项。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:5:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#tcp-选项"},{"categories":["API"],"content":" TCP 连接终止相比创建连接的三次握手，终止连接往往需要四步： TCP 一端首先发起 close/1 ，我们将这一端称为 主动关闭 (active close)。主动端 TCP 将会发送 FIN 字段，这意味着所有数据发送完成。 接收 FIN 的 TCP 端被称为 被动关闭 (passive close)，接收到 FIN 后将会进行确认。 一段时间之后，被动端将会调用 close/1 来发送 FIN。 最终主动端将确认 FIN 并断开这个连接。 FIN 与 SYN 类似同样占据一个字节，而 ACK 是 FIN 序列号加 1。在第二步与第三步之间，依然可能有数据流动，但这个数据仅有被动关闭端到主动关闭端，这个状态被称为 半断开状态 (half-close)。 这在里 FIN 的发生是因为调用了 close/1 ，不过需要注意当程序主动终止时 (exit/1 或 main return) 还是被动 (接收到终止程序的信号)，所有打开的文件描述符都会被关闭，此时的 TCP sockfd 也不例外，因此这时也会发送 FIN。 最后提一点，主动关闭端是无论 server 还是 client 都可以的，并非只有 client 主动关闭。不过通常情况下都是 client 发起，在 HTTP/1.0 中是个例外。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:5:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#tcp-连接终止"},{"categories":["API"],"content":" TCP 状态转换TCP 涉及连接建立和终止的状态可以使用状态转换图来表示。TCP 连接中一共有 11 种不同的状态，并且这些状态之间有详细的转换规则，且转换基于当前状态与接收到的消息。 上图中标记了两个没有介绍过的转换：同时打开 (simultaneous open) 和 同时关闭 (simultaneous close)，它们是两端同时发送 SYN / FIN 的情形下产生的，虽然及其罕见但并不是不能发生。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:5:4","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#tcp-状态转换"},{"categories":["API"],"content":" TCP 的分节在完整的 TCP 连接中，会有建立连接、传输数据、终止连接三个阶段。 在连接建立时，客户端与服务端宣告了不同的 MSS (这不构成任何影响)，连接建立后客户端将请求的数据按 1460 字节进行分组，而服务端回复请求时则采用 536 字节分组。 还需要注意一点，数据传输过程中服务器对请求的 ACK 携带在响应之上一并发送。这种做法称之为 捎带 (piggybacking)，通常在服务器产生应答时间少于 200 ms 时发生，如果更长时间的耗时则会先发送 ACK 再进行响应。 值得注意的是，如果只是以单分节的请求响应为目的，TCP 连接将会产生 8 个分节的开销，而采用 UDP 仅仅有请求与响应的开销。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:5:5","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#tcp-的分节"},{"categories":["API"],"content":" TIME_WAIT 状态TCP 连接中，毫无疑问 TIME_WAIT 是最难理解的。这个状态将在主动关闭端经历，该端将在这个状态持续 最长分节生命期 (maximum segment lifetime, MSL) 的两倍，或者称为 2MSL。 每一个 TCP 实现中都会定义 MSL，RFC 1122 中建议 MSL 为 2 分钟，BSD 实现中一般为 30s，Linux 实现默认为 60s。MSL 是任何 IP 数据报在 Internet 上所能存活的最大时长。当然数据还会包含一个 8 bit 的 跳限 (hop limit)，即 IPv4 中哦过的 TTL 字段与 IPv6 中的跳限字段，拥有最大跳限的数据依然不能超过 MSL 时间限制。网络中报文丢失通常是路由器出现问题，比如漰溃或两个路由链路断开，路由可能需要几秒甚至几分钟来找到其他稳定的通路。在这期间，路由可能会发生循环 (由 A 发送给 B，然后 B 再发送回 A)，因此在迷途期间，发送端检测到超时将会重发该数据，重传的数据可能从其他路径到达目的地址。而可能在 MSL 之内迷失的数据也被送达目的地，这个分组被称为 迷失重复 (lost duplicate) 或 漫游重复 (wandering duplicate)。 TIME_WAIT 状态有两个存在的理由： 可靠地实现 TCP 全双工终止连接。 最终 ACK 丢失的情况下，主动关闭端必须重传 ACK，而它必须正确处理终止序列丢失的情况。 允许旧的重复分节在网络中过期 TCP 需要防止在 TIME_WAIT 期间，如果相同的 IP、port 发送来的新的数据，防止误解属于正在断开的连接的迷失重复。因此 TCP 将给处于 TIME_WAIT 状态的连接发起新的连接，而 2 MSL 可以保证任何方向上的分节都以被丢弃。保证在发起新连接时，老旧连接的数据以全部消逝。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:5:6","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#time-wait-状态"},{"categories":["API"],"content":" SCTP 关联SCTP 是与 TCP 类似的面向连接协议，关联可以被建立和终止，但与 TCP 相比有些不同。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:6:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#sctp-关联"},{"categories":["API"],"content":" SCTP 创建关联SCTP 创建关联需要四次握手。 服务端必须准备好接受接入关联，通常与 TCP 类似使用 socket/3, bind/3 和 listen/2 进行被动连接。 客户端通过 connect/3 或发送隐式打开关联消息来进行主动连接。这使得 SCTP 发送一个 INIT 消息来告知服务端，客户端有哪些 IP 地址、初始化序列号、标识该关联中的所有分组的初始标签、客户端请求的外出流数量以及客户端支持的外来流数量。 服务端会使用 INIT-ACK 确认客户端 INIT 消息，并包含同样的信息以及状态 cookie。状态 cookie 包含了服务器需要确认关联是否有效的所有状态，cookie 进行数字签名以确保其有效性。 客户端以 COOKIE-ECHO 对服务器状态 cookie 进行响应，在同一分组中还可能直接包含用户数据。 服务端确认 cookie 正确并回复 COOKIE-ACK 建立关联，当然同样可能携带用户数据。 SCTP 最少需要 4 个分组进行请求，这四次握手与 TCP 的三次握手很相似，除了 cookie 生成部分。INIT 将会携带一个验证标签 Ta 和一个初始化序列号 J。Ta 必须出现在关联中的所有分组中，对端 INIT ACK 中承载一个验证标记 Tz，而 Tz 也需要在关联有效期内出现在每个分组中。另外 STCP 服务端在 INIT-ACK 中提供了 cookie，这个 cookie 包含了设置该关联所需的所有状态，而 SCTP 协议栈无需保存所关联客户的有关信息。 由于 SCTP 支持多宿，在四次握手结束之后两端会各自选择一个主目的地址作为默认地址。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:6:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#sctp-创建关联"},{"categories":["API"],"content":" SCTP 终止关联SCTP 不提供 TCP 那样的半连接状态，当一端停止连接时另一端必须停止发送所有新数据。请求关闭关联的接受端在数据队列中的所有数据发送完之后完成关闭。 由于使用验证标签的缘故，因此 SCTP 也不需要 TIME_WAIT 状态，数据首部会包含 INIT 和 INIT-ACK 中所交换的标记，由此区分是否是旧的连接。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:6:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#sctp-终止关联"},{"categories":["API"],"content":" SCTP 的分节与 TCP 类似，SCTP 也需要经历建立关联、传输数据、终止关联三个阶段。 客户端发送 COOKIE-ECHO 时可以捎带 DATA，而服务器在 COOKIE-ACK 也可以捎带数据，一般而言当网络应用采用一到多接口样式时将会捎带一个或多个 DATA 块。 SCTP 分组信息的单位称为块 (chunk)，这是一种自述型分组，包含了块类型、若干块标记和块长度。这样做是为了方便进行多个块的绑缚，只需要简单的将多个块合并到一个 SCTP 中即可。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:6:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#sctp-的分节"},{"categories":["API"],"content":" SCTP 的选项SCTP 使用参数和块方便增设的可选特性，新的特性通过添加这两个条目之一增添，并允许通常的 SCTP 处理规则未知的参数和块。参数类型字段和块类型字段的高 2 bit 指明 SCTP 接收端该如何处理位置的参数或块。 ","date":"01-18","objectID":"/2022/unixnetworkprogramming_002/:6:4","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"传输层总述","uri":"/2022/unixnetworkprogramming_002/#sctp-的选项"},{"categories":["API"],"content":"GinShio | Unix 网络编程：卷一 (3rd) 第一部分第一章：介绍","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/"},{"categories":["API"],"content":" 信息 关于 UNP 的所有代码可以在 https://github.com/unpbook/unpv13e 上找到 ","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/:0:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/#"},{"categories":["API"],"content":" 从一个简单的时间获取客户端开始接下来，将从一个使用 TCP 连接的获取时间的客户端开始。 // 以下代码与 UNP intro/daytimetcpcli.c 等价 #include \u003cstdarg.h\u003e #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cstring.h\u003e // inet_pton/3, htons/1 #include \u003carpa/inet.h\u003e // struct sockaddr_in #include \u003cnetinet/in.h\u003e // errno #include \u003cerrno.h\u003e // socket/3, connect/3 #include \u003csys/socket.h\u003e #include \u003csys/types.h\u003e // read/3 #include \u003cunistd.h\u003e #define MAXLINE 4096 void err_sys(const char* fmt, ...) { va_list ap; va_start(ap, fmt); char buffer[MAXLINE + 1] = {0}; vsnprintf(buffer, MAXLINE, fmt, ap); int n = strlen(buffer); snprintf(buffer + n, MAXLINE - n, \":%s\", strerror(errno)); strcat(buffer, \"\\n\"); fflush(stdout); fputs(buffer, stderr); fflush(stderr); va_end(ap); exit(1); } void err_quit(const char* fmt, ...) { va_list ap; va_start(ap, fmt); char buffer[MAXLINE + 1] = {0}; vsnprintf(buffer, MAXLINE, fmt, ap); strcat(buffer, \"\\n\"); fflush(stdout); fputs(buffer, stderr); fflush(stderr); va_end(ap); exit(1); } int main(int argc, char** argv) { if (argc != 2) { err_quit(\"usage: a.out \u003cIPaddress\u003e\"); } int sockfd; if ((sockfd = socket(AF_INET, SOCK_STREAM, 0)) \u003c 0) { err_sys(\"socket error\"); } struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_port = htons(13), }; if (inet_pton(AF_INET, argv[1], \u0026servaddr.sin_addr) \u003c= 0) { err_quit(\"inet_pton error for %s\", argv[1]); } if (connect(sockfd, (struct sockaddr*) \u0026servaddr, sizeof(servaddr)) \u003c 0) { err_sys(\"connect error\"); } int n; char recvline[MAXLINE + 1]; while ((n = read(sockfd, recvline, MAXLINE)) \u003e 0) { recvline[n] = 0; if (fputs(recvline, stdout) == EOF) { err_sys(\"read error\"); } } if (n \u003c 0) { err_sys(\"read error\"); } return 0; } 当然需要自行编译一下 daytimetcpsrv 并启动它，然后就可以顺利启动 client 就可以看到获取的时间了。 ","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/:1:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/#从一个简单的时间获取客户端开始"},{"categories":["API"],"content":" socket我们最先遇到的 API 是 socket/3，这会帮助我们启动一个网络服务，我们可以自己指定 domain, type ，还有一个特殊的 protocol 参数用于指定 socket 一起使用的特定协议，但这个参数通常情况下为 0。下表展示了常用的 domain 与 type 值，摘自 Linux Kernel 5.3.18。 domain 释义 Manual AF_UNIX, AF_LOCAL Unix Domain Socket unix(7) AF_INET IPv4 ip(7) AF_INET6 IPv6 ipv6(7) AF_IPX IPX - Novell Protocol AF_NETLINK Kernel user interface device netlink(7) AF_X25 ITU-T X.25 / ISO-8208 protocol x25(7) AF_AX25 Amateur radio AX.25 protocol AF_ATMPVC Access to raw ATM PVCs AF_APPLETALK AppleTalk ddp(7) AF_PACKET Low level packet interface packet(7) AF_ALG Interface to kernel crypto API type 释义 SOCK_STREAM 提供基于连接的、顺序、可靠、双工的字节流 SOCK_DGRAM 提供无连接的、不可靠的数据报 SOCK_SEQPACKET 提供基于连接的、顺序、可靠、双工的数据报 SOCK_RAW 提供原生网络协议 SOCK_RDM 提供不保证顺序的可靠数据报 SOCK_PACKET 见 Manual packet(7) SOCK_NONBLOCK 为新打开的 fd 设置 O_NONBLOCK 标志 SOCK_CLOEXEC 为新打开的 fd 设置 O_ELOEXEC 标志 我们往往使用 AF_INET 与 SOCK_STREAM 来启动一个 TCP 连接。而该 API 会返回一个整数作为返回结果，成功时将返回新 socket 的文件描述符 (fd)，而失败时返回 \\(-1\\) 并设置 errno，而 errno 可以让我们得知具体发生了什么错误。 // 构建 IPv4 socket tcp_socket = socket(AF_INET, SOCK_STREAM, 0); udp_socket = socket(AF_INET, SOCK_DGRAM, 0); raw_socket = socket(AF_INET, SOCK_RAW, protocol); // 构建 IPv6 socket tcp6_socket = socket(AF_INET6, SOCK_STREAM, 0); raw6_socket = socket(AF_INET6, SOCK_RAW, protocol); udp6_socket = socket(AF_INET6, SOCK_DGRAM, protocol); ","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/:1:1","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/#socket"},{"categories":["API"],"content":" 指定服务器 IP 地址与端口使用 sockaddr_in 数据结构 (头文件 netinet/in.h 中) 保存服务器的 IPv4 信息，结构体如下 // IPv4 struct sockaddr_in { sa_family_t sin_family; /* address family: AF_INET */ in_port_t sin_port; /* port in network byte order */ struct in_addr sin_addr; /* internet address */ }; /* Internet address. */ typedef uint32_t in_addr_t; struct in_addr { in_addr_t s_addr; /* address in network byte order */ }; // Ipv6 struct sockaddr_in6 { sa_family_t sin6_family; /* AF_INET6 */ in_port_t sin6_port; /* port number */ uint32_t sin6_flowinfo; /* IPv6 flow information */ struct in6_addr sin6_addr; /* IPv6 address */ uint32_t sin6_scope_id; /* Scope ID (new in 2.4) */ }; struct in6_addr { unsigned char s6_addr[16]; /* IPv6 address */ }; 另外系统使用大端序或小端序，而网络编程中往往需要使用网络序，因此提供了一系列将系统序 (host byte order) 转换为网络序 (network byte order) 的函数 (头文件 arpa/inet.h)，不过在有些系统中这些函数被包含在 netinet/in.h 中。使用这些函数就可以自由的定义端口值。 uint32_t htonl(uint32_t hostlong); // 将 unsigned int 类型 host to network uint16_t htons(uint16_t hostshort); // 将 unsigned short 类型 host to network uint32_t ntohl(uint32_t netlong); // 将 unsigned int 类型 network to host uint16_t ntohs(uint16_t netshort); // 将 unsigned short 类型 network to host 最后一个与IP信息有关的函数就是 inet_pton/3 (presentation to numeric，头文件 arpa/inet.h 中)，这是一个伴随着 IPv6 诞生的 POSIX 函数，可以将 IPv4 的点分十进制字符串形式或 IPv6 的字符串行形式地址转换为 in_addr 或 in6_addr 的字节形式。这个函数会返回一个整数用来确定函数是否成功，\\(1\\) 代表了成功，而 \\(0\\) 代表没有包含有效的地址，\\(-1\\) 是最为严重的，表示不是有效的协议族，这还会将 errno 设置为 EAFNOSUPPORT。与这个函数相关的还有一个 inet_ntop/4，它与前一个函数作用相反，最后一个参数 size 则指示了 buffer 可以接收的字节大小。下面一段程序是 Manual 中给出的示例，于此贴出。 #include \u003carpa/inet.h\u003e #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cstring.h\u003e int main(int argc, char *argv[]) { unsigned char buf[sizeof(struct in6_addr)]; int domain, s; char str[INET6_ADDRSTRLEN]; if (argc != 3) { fprintf(stderr, \"Usage: %s {i4|i6|\u003cnum\u003e} string\\n\", argv[0]); exit(EXIT_FAILURE); } domain = (strcmp(argv[1], \"i4\") == 0) ? AF_INET : (strcmp(argv[1], \"i6\") == 0) ? AF_INET6 : atoi(argv[1]); s = inet_pton(domain, argv[2], buf); if (s \u003c= 0) { if (s == 0) { fprintf(stderr, \"Not in presentation format\"); } else { perror(\"inet_pton\"); } exit(EXIT_FAILURE); } if (inet_ntop(domain, buf, str, INET6_ADDRSTRLEN) == NULL) { perror(\"inet_ntop\"); exit(EXIT_FAILURE); } printf(\"%s\\n\", str); exit(EXIT_SUCCESS); } // ./a.out i4 127.0.0.1 // 127.0.0.1 // ./a.out i6 0:0:0:0:0:0:0:0 // :: // ./a.out i6 1:0:0:0:0:0:0:8 // 1::8 // ./a.out i6 0:0:0:0:0:FFFF:204.152.189.116 // ::ffff:204.152.189.116 以前常用的函数则是支持 IPv4 的 inet_addr/1 系列函数 函数 使用 inet_aton/2 点分十进制地址转换为 in_addr 结构，成功时返回 \\(1\\) inet_addr/1 点分十进制地址转换为 in_addr 结构，无效时返回 \\(-1\\) inet_network/1 与上一个函数类似，但返回的是主机序而非网络序 inet_ntoa/1 将网络序的 in_addr 转换为点分十进制字符串 inet_lnaof/1 返回 in_addr 地址中的主机地址部分 (主机序) inet_netof/1 返回 in_addr 地址中的网络号部分 (主机序) ","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/:1:2","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/#指定服务器-ip-地址与端口"},{"categories":["API"],"content":" 与服务器建立连接并读取数据建立连接使用 connect/3 完成，先看一下函数原型 // #include \u003csys/socket.h\u003e // #include \u003csys/types.h\u003e int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); 该函数接收 sockfd 用于监听主机端口，并使用 addr 接收服务器的地址、端口等信息，最后一个参数则是 addr 的数据类型大小。 当然 socket 类型会影响该函数的行为：SOCK_DGRAM 类型的 socket，addr 是默认发送和接收数据报的地址；而 SOCK_STREAM 和 SOCK_SEQPACKET 类型的 socket，将会试图与指定地址建立连接。通常来说，基于连接的 socket 只允许调用成功一次该函数，而无连接的 socket 可以多次调用该函数来更改关联的地址，并且自 Linux Kernel 2.2 以后，可以通过连接到将 sa_family 设置为 AF_UNSPEC 来解除地址关联。 读取一个文件的数据往往采用 POSIX 函数 read/3 ，接收 fd、buffer、接收的字节长度 size，最终会返回实际读取的字节长度，但是返回 -1 时代表发生错误并会设置 errno。 往往在读取服务器数据时，采用循环的方式读取，直到读到的数据大小为 0 才认为此次读取完成。虽然这个程序中每次读取都会直接读完所有数据 (因为数据只有 26 byte，而一次可以接收 4096 byte)。 在这个示例中，数据传输完成由服务器关闭连接表示，这与 HTTP/1.0 是类似的；而 SMTP 采用两个字节序的 ASCII 回车符后跟 ASCII 换行符表示数据结束；Sun RPC 和 DNS 则是用一个包含数据大小的字段来确定何时结束数据。这些结果的背后是 TCP 不提供数据标记 ，你需要自己选择如何记录数据结束。 ","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/:1:3","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/#与服务器建立连接并读取数据"},{"categories":["API"],"content":" 简单的时间获取服务端这里的服务端是对应上一节中的客户端，为客户端提供获取时间的服务。 // 以下代码与 UNP intro/daytimetcpsrv.c 等价 #include \u003cstdbool.h\u003e #include \u003cstdio.h\u003e #include \u003ctime.h\u003e #include \u003carpa/inet.h\u003e #include \u003cnetinet/in.h\u003e #include \u003cunistd.h\u003e #include \u003csys/socket.h\u003e #include \u003csys/types.h\u003e #define LISTENQ 1024 #define MAXLINE 4096 int main(int argc, const char* argv[]) { int listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) { err_sys(\"listen error\"); } struct sockaddr_in servaddr = { .sin_family = AF_INET, .sin_port = htons(13), .sin_addr.s_addr = htonl(INADDR_ANY), }; if (bind(listenfd, (struct sockaddr*) \u0026servaddr, sizeof(servaddr)) == -1) { err_sys(\"bind error\"); } if (listen(listenfd, LISTENQ) == -1) { err_sys(\"listen error\"); } while (true) { int connfd = accept(listenfd, (struct sockaddr*) NULL, NULL); if (connfd == -1) { err_ret(\"connect error\"); continue; } time_t ticks = time(NULL); char buffer[MAXLINE + 1] = {0}; snprintf(buffer, sizeof(buffer), \"%.24s\\r\\n\", ctime(\u0026ticks)); if (write(connfd, buffer, strlen(buffer)) == -1) { err_ret(\"write error\"); } if (close(connfd) == -1) { err_ret(\"close error\"); } } } 可以看到与客户端不同的是，服务器在申请 socket 时没什么变化，但接下来服务器填写了自己的 IP 与 Port 信息，Port 用于指示接下来监听的端口，而 IP 则是指定为了 INADDR_ANY，这是系统中预定的 IP 地址的值。 定义 值 释义 INADDR_ANY 0.0.0.0 接受所有 IP 发来的请求 INADDR_LOOPBACK 127.0.0.1 本地回环地址 INADDR_BROADCAST 255.255.255.255 广播地址 有趣的是，如果使用 INADDR_ANY 的话，客户端可以使用路由器、网线提供的 DHCP 地址访问服务器，而改为 INADDR_LOOPBACK 时仅可通过 127.0.0.1 进行访问。当然这些数值都需要通过 htonl/1 转换为网络序。 可能初看时不是很理解 bind 的含义，而且这个函数和 connect 的参数几乎一致。不过不同的是，connect 用于客户端向服务器发起申请连接，而 DGRAM 类型则是将 fd 与 IP 信息绑定。这里 bind 也是将 IP 信息与 fd 进行绑定，根据 Manual 的解释，sockfd 申请之后存在于地址族的名称空间之中，但没有绑定地址，因此需要此函数对 fd 与 addr 进行绑定。 最后一个 listen/2 就是监听给出的 addr 及 port，这里的 sockfd 主要是 SOCK_STREAM 与 SOCK_SEQPACKET 类型的 socket。(SOCK_DGRAM 又一次被排除在外了) listen 的第二个参数，根据 Manual 的 NOTE 章节，目前版本的 Linux 代表了成功 accept 的 TCP 连接队列的长度。 在与客户端通信时，往往使用 accept/3 接收来自客户端的连接所创建的 connfd，经过这一步后，TCP 完成连接从而状态改变为 ESTABLISHED。当数据被准备好之后，服务器就可以通过 write/3 将数据写入 connfd 发送给客户端，这与平时向文件中写内容是类似的。最后我们只需要像关闭文件一样关闭 connfd 就可以了。 但是这个程序存在一些问题， while (true) 可以让服务器一直循环等待客户端请求到来，但是一次只能接收一个请求并处理，这对大量请求情况下显然是不合适的。但在示例中，我们仅使用了标准库函数 time 和 ctim，运行速度很快。但现实中我们可能需要几十秒甚至一分钟处理一个请求，这时这样的服务器是不可接受的。 示例中的服务器被称为 迭代服务器 (iterative server)，对于每个请求都迭代执行一次；同时处理多个请求的服务器被称为 并发服务器 (concurrent server)。而从 shell 终端启动一个服务器可能会一直运行，因此我们常常使用 Unix 守护进程 (daemon)，可以在后台运行，而不跟任何终端关联地运行。 ","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/:2:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/#简单的时间获取服务端"},{"categories":["API"],"content":" Unix 标准Unix 标准通常被称作 POSIX，即 Portable Operating System Interface (可移植操作系统接口)。当然由于当时有多个机构进行标准化工作，因此 Unix 标准又有很多名字 ISO/IEC 9945, IEEE Std 1003.1 和单一 Unix 规范 (Signle UNIX Specification)，当然经过发展，POSIX 也经历过版本更新，UNP 所介绍到了 POSIX.1-2001，如今最新标准为 POSIX.1-2017。 POSIX 第一版为 IEEE Std. 1003.1-1988，这一版规范了 Unix 内核的 C 语言接口，这些接口覆盖了进程原语 (fork，exec，signals 等)、进程环境 (user ID，程序组等)、文件与目录 (所有的 I/O 函数)、终端 I/O、系统数据库 (密码文件与组文件) 以及 tar、cpio 等归档格式。不过当时 POSIX 被称为 IEEE-IX ，POSIX 这个名称是 Richard Stallman 建议使用的。 IEEE Std. 1003.1-1990 为第二版，这一版正式称为 ISO 标准 ISO/IEC 9945:1990 。这一版的改动很小，添加了新的副标题 “Part 1: System Application Program Interface (API) [C language]” IEEE Std. 1003.2-1992 扩展到了两卷本，第二卷 “Part 2: Shell and Utilities”。这一卷定义了 shell (基于 System V Bourne Shell) 与大约 100 中命令行工具 (awk， vi，yacc等)，第二卷往往也被称为 POSIX.2 IEEE Std. 1003.1b-1993 升级自 1990 版，添加了 P1003.4 工作组开发的实时扩展部分，并新增了文件同步、异步 I/O、信号量、内存管理 (mmap 与共享内存)、进程调度、始终与定时器以及消息队列。 IEEE Std. 1003.1c-1995 又被称为 ISO/IEC 9945-1:1996 ，包含了之前的 API、事实扩展、pthreads (1003.1c-1995) 以及对 1003.1b 的技术性修订。增添了三章关于线程的内容以及关于线程同步 (mutexes 与 condition_variables)、线程调度、线程同步以及信号句柄。将 ISO/IEC 9945 分为三部分 Part 1: System API (C language) — 被称为 POSIX.1 Part 2: Shell and utilities — 被称为 POSIX.2 Part 3: System administration (正在开发) IEEE Std. 1003.1g: Protocol-independent interfaces (PII) (协议无关接口) 定义了两个 API，被称为 Detailed Network Interfaces (DNI，详尽网络接口)： DNI/Socket，基于 4.4BSD socket API DNI/XTI，基于 X/Open XPG4 规范 X/Open 公司与开放软件基金会 (OSF) 合并成了 The Open Group，这是一个由厂商、工业界最终用户、政府以及学术机构共同参加的标准化组织。 1989 年 X/Open 发布了 X/Open 可移植指南第三期 (XPG3) 1992 年发布了 XPG4，而第二版于 1994 年发布。这个版本也被称作 Spec 1170 (规范 1170)，1170 是 926 个系统 API、70 个头文件 以及 174 个命令的总和。最终被称为 X/Open Signle UNIX Specification，或 Unix 95 1997 年三月发布了 Signle Unix Specification 第二版，又称为 Unix 98 。这一版的魔数上升到了 1434，而工作站魔数直接到了 3030。这是因为该规范包含了 Common Desktop Environment (CDE，公共桌面环境)，这导致必须依赖 X Window System 以及 Motif 用户接口。 再之后，单一 UNIX 规范与 POSIX.1 逐渐统一起来，UNP 主要介绍 POSIX.1-2001 (即 IEEE Std 1003.1-2001)。 ","date":"01-17","objectID":"/2022/unixnetworkprogramming_001/:3:0","series":["UNP Note"],"tags":["Note","Unix","Network","Posix"],"title":"UNP 简介","uri":"/2022/unixnetworkprogramming_001/#unix-标准"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第七章笔记","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/"},{"categories":["Algorithm⁄DataStructure"],"content":" Sorting something that you will never search is a complete waste; searching something you never sorted is merely inefficient. — Brian Christian 我们假设对数组进行排序，数组的所有位置都有元素，且长度为 N。对于排序，假设元素存在 \\(\u003c\\) 和 \\(\u003e\\) 用以将输入按一致的次序放置，比较运算是除赋值运算外仅有的能对输入数据进行的操作。这种条件下的排序称之为 比较排序 (comparison-based sorting)。另外对于已经排序完成的数组，如果可以保持原本的数据次序我们称之为 稳定排序 (stable sorting)。 当然这与 STL 的算法有一点点出入，sort 接收的是迭代器来表示待排序的范围，以及一个可选的比较器。而且 sort 的底层算法也更加复杂，这里只是简单地说明各个基础排序。 template \u003cclass Iterator\u003e void sort(Iterator begin, Iterator end); template \u003cclass Iterator, class Comparator\u003e void sort(Iterator begin, Iterator end, Comparator cmp); 为了方便理解，将使用 Wikipedia 上关于排序的动图来帮助理解这种排序。先放个大招 表格 排序算法简要比较 摘自 Wikipedia Table 1: 排序算法简要比较 名称 英文名称 稳定性 \\(Time_{avg}\\) \\(Time_{bad}\\) \\(Mem\\) 冒泡排序 bubble sort \\(\\checkmark\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(1)\\) 选择排序 selection sort \\(\\times\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(1)\\) 插入排序 insertion sort \\(\\checkmark\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(1)\\) 希尔排序 shell sort \\(\\times\\) \\(\\mathcal{O}(N^{\\frac{3}{2}})\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(1)\\) 堆排序 heap sort \\(\\times\\) \\(\\mathcal{O}(N\\log_{}{N})\\) \\(\\mathcal{O}(N\\log_{}{N})\\) \\(\\mathcal{O}(1)\\) 归并排序 merge sort \\(\\checkmark\\) \\(\\mathcal{O}(N\\log_{}{N})\\) \\(\\mathcal{O}(N\\log_{}{N})\\) \\(\\mathcal{O}(N)\\) 原地归并排序 in-place merge sort \\(\\checkmark\\) \\(\\mathcal{O}(N\\log_{}^{2}{N})\\) \\(\\mathcal{O}(N\\log_{}^{2}{N})\\) \\(\\mathcal{O}(1)\\) 快速排序 quick sort \\(\\times\\) \\(\\mathcal{O}(N\\log_{}{N})\\) \\(\\mathcal{O}(N^{2})\\) \\(\\mathcal{O}(\\log_{}{N})\\) 桶排序 bucket sort \\(\\checkmark\\) \\(\\mathcal{O}(N + k)\\) \\(\\mathcal{O}(N^{2} + k)\\) \\(\\mathcal{O}(2^{k})\\) 计数排序 counting sort \\(\\checkmark\\) \\(\\mathcal{O}(N + r)\\) \\(\\mathcal{O}(N + r)\\) \\(\\mathcal{O}(N + r)\\) 基数排序 (LSD) lsd radix sort \\(\\checkmark\\) \\(\\mathcal{O}(N \\frac{k}{d})\\) \\(\\mathcal{O}(N \\dfrac{k}{d})\\) \\(\\mathcal{O}(N + 2^{d})\\) 基数排序 (MSD) msd radix sort \\(\\checkmark\\) \\(\\mathcal{O}(N \\frac{k}{d})\\) \\(\\mathcal{O}(N \\dfrac{k}{d})\\) \\(\\mathcal{O}(N + 2^{d})\\) 其中 k 为键的大小，d 为数位大小，r 为排序的数字的范围大小。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:0:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#"},{"categories":["Algorithm⁄DataStructure"],"content":" 冒泡排序冒泡排序 (bubble sort) 是一种简单的排序，它重复走访数据，对比两个相邻的元素，如果顺序错误就将它们交换位置，直到所有数据都在正确的位置上。实际上，bubble sort 是一种朴素的排序方式，其时间复杂度为 \\(\\mathcal{O}(N^{2})\\) ，需要交换元素 \\(\\mathcal{O}(N^{2})\\) 次。当然，这是一种稳定排序！ 冒泡排序的具体做法如下： 比较相邻的元素，如果位置不正确就交换 对每一对相邻元素做同样的工作，直到结尾。当这一步完成后，最后一个元素将会正确的回到末尾位置 针对所有相邻元素重复以上步骤 (除了刚刚摆放正确的元素)，直到没有可比较的元素为止 template \u003cclass Array\u003e void bubble_sort(Array\u0026 arr) { int len = arr.size(); for (int i = 0; i \u003c len; ++i) { for (int j = 0; j \u003c len - i - 1; ++j) { if (arr[j + 1] \u003c arr[j]) { swap(arr[j], arr[j + 1]); } } } } ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:1:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#冒泡排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 插入排序插入排序 (insertion sort) 是一种直观的排序，由 \\(N-1\\) 趟 (pass) 排序组成。对于 \\(p=1\\) 到 \\(N-1\\) 趟，插入顺序保证从位置 0 到位置 p 上的元素为已排序状态。当然这基于一个事实：位置 0 到位置 \\(p-1\\) 上的元素都已排序过了。 插入排序的具体做法如下： 从第一个元素开始，该元素可以认为是已被排序的 取出下一个元素，在已排序的元素中从后向前扫描 如果已排序的元素与这个取出的元素位置不正确，将取出的元素向前移动，直到位置正确或没有已排序元素可以比较 将取出的元素插入这里，并重复步骤 2 ~ 4 直到所有元素都被排序 初始状态 34 8 64 51 32 21 当前取出元素 After \\(p=1\\) 8 34 64 51 32 21 8 After \\(p=2\\) 8 34 64 51 32 21 64 After \\(p=3\\) 8 34 51 64 32 21 51 After \\(p=4\\) 8 32 34 51 64 21 32 After \\(p=5\\) 8 21 32 34 51 64 21 由于每个嵌套循环都花费 N 次迭代，因此插入排序时间复杂度为 \\(\\mathcal{O}(N^{2})\\) ，对于 p 的每一个值最多执行 \\(p + 1\\) 次对已排序元素的检测，因此最多 \\(\\sum_{i=2}^{N}{i} = 2 + 3 + 4 + \\cdots + N = \\Theta(N^{2})\\) 。但是另一方面，如果输入的数据已经被排序了，那运行时间为 \\(\\mathcal{O}(N)\\) ，而几乎有序的情况下，insertion sort 将会很快运行完毕。 template \u003cclass Array\u003e void insertion_sort(Array\u0026 arr) { int j, len = arr.size(); for (int p = 1; p \u003c len; ++p) { auto tmp = arr[p]; for (j = p; j \u003e 0 \u0026\u0026 tmp \u003c arr[j - 1]; --j) { a[j] = a[j - 1]; } a[j] = tmp; } } ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:2:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#插入排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 希尔排序希尔排序 (shell sort) 也称 递减增量排序 (diminishing increment sort) 算法，是插入排序的一种更高效的改进版本，由发明者 Donald Shell 于 1959 年公布。Shell sort 基于 insertion sort 的以下两点性质而提出改进方法的： insertion sort 在对几乎已经排好序的数据操作时效率高，即可以达到线性排序的效率 insertion sort 一般来说是低效的，因为插入排序每次只能将数据移动一位 Shell sort 使用序列 \\(h_{1}, h_{2}, \\cdots, h_{n}\\) 这样一个增量序列，其中 \\(h_{1}=1\\) 。对于使用增量 \\(h_{k}\\) 的排序，我们可以看做是对序列 \\(a[i + j * h_{k}] (j = 0, 1, \\cdots, n)\\) 进行的 insertion sort，在这一趟排序后，对于每个 i 则有 \\(a[i] \\leq a[i + h_{k}]\\) ，即所有间隔为 \\(h_{k}\\) 的元素都被排序了，此时成为是 \\(h_{k}\\) 排序的 (\\(h_{k}\\) sorted)。之后向前选取增量，直到增量为 1 的趟排序完，算法结束。 \\(a_{1}\\) \\(a_{2}\\) \\(a_{3}\\) \\(a_{4}\\) \\(a_{5}\\) \\(a_{6}\\) \\(a_{7}\\) \\(a_{8}\\) \\(a_{9}\\) \\(a_{10}\\) \\(a_{11}\\) \\(a_{12}\\) input 62 83 18 53 7 17 95 86 47 69 25 28 after step-5 17 28 18 47 7 25 83 86 53 69 62 95 after step-3 17 7 18 47 28 25 69 62 53 83 86 95 after step-1 7 17 18 25 28 47 53 62 69 83 86 95 Shell sort 虽然实现简单，但运行时间的分析却很难。Shell sort 的运行时间依赖于所选择的增量序列。 增量序列 时间复杂度 \\(\\frac{N}{2^{i}}\\) \\(\\Theta(N^{2})\\) \\(2^{i} - 1\\) \\(\\Theta(N^{3/2})\\) \\(2^{i}3^{j}\\) \\(\\Theta(N\\log_{}^{2}{N})\\) 已知最好的步长序列是 Sedgewick 提出的 \\(1, 5, 19, 41, 109, \\cdots\\) ，这是一个下标从 0 开始的序列，偶数下标对应的步长增量由 \\(9 \\times 4^{i} - 9 \\times 2^{i} + 1\\) 提供，奇数下标对应的步长增量由 \\(2^{i+2} \\times (2^{i+2} - 3) + 1\\) 提供。在小数组中使用好的步长序列的 Shell sort 性能十分优秀。另外在大数组中，步长序列 \\((fib(i+2))^{2}\\) 表现优异。 static constexpr std::size_t SHELL_SORT_GAPS[]{8929, 3905, 2161, 929, 505, 209, 109, 41, 19, 5, 1}; template \u003cclass Array\u003e void shell_sort(Array\u0026 arr) { auto len{arr.size()}; for (const auto\u0026 gap : SHELL_SORT_GAPS) { for (std::size_t i = gap; i \u003c len; ++i) { for (std::size_t j{i}; j \u003e= gap and arr[j - gap] \u003e arr[j]; j -= gap) { std::swap(arr[j], arr[j - gap]); } } } } ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:3:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#希尔排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 堆排序堆排序 (heap sort) 是基于上一篇中提到的 binary heap 的时间复杂度 \\(\\mathcal{O}(N\\log_{}{N})\\) 的排序算法。如果我们要以升序排序数组，则将数组转换为一个 max heap，重复将 heap top 元素移除即可获取从大到小的序列。 堆排序分为两个阶段，建立 max heap 与 N 次的移除操作。第一阶段建立 heap 需要最多 \\(2N\\) 次比较，而第二阶段移除元素，每次移除元素最多用到 \\(2\\lfloor\\log_{}{i}\\rfloor\\) 次比较。因此 heap sort 的最坏时间复杂度为 \\(\\mathcal{O}(N\\log_{}{N})\\) ，而 heap sort 性能十分稳定，其平均时间复杂度也是 \\(\\mathcal{O}(N\\log_{}{N})\\) 。 我们可以利用 heap-order property 实现额外空间复杂度 \\(\\mathcal{O}(1)\\) 的原地算法：在每次将 heap top 移除 heap 时将其与堆尾互换并将堆的尺寸缩小 1，然后利用 percolate down 恢复 heap-order。 template \u003cclass Array\u003e void max_heapify(Array\u0026 arr, int start, int end) { int dad = start; int son = 1 + (dad \u003c\u003c 1); while (son \u003c= end) { if (son + 1 \u003c= end \u0026\u0026 arr[son] \u003c arr[son + 1]) { ++son; } if (arr[dad] \u003e arr[son]) { return; } swap(arr[dad], arr[son]); dad = son; son = 1 + (dad \u003c\u003c 1); } } template \u003cclass Array\u003e void heap_sort(Array\u0026 arr) { int len = arr.size(); for (int i = (len \u003e\u003e 1) - 1; i \u003e= 0; i--) { max_heapify(arr, i, len - 1); } for (int i = len - 1; i \u003e 0; i--) { swap(arr[0], arr[i]); max_heapify(arr, 0, i - 1); } } ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:4:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#堆排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 基于堆排序的选择算法你可能不知道什么是选择算法，但是你应该听过这样一个问题，如何在一个序列中找出第 k 大的元素，当然第 k 小的元素也可以用这种办法。我们可以明确建堆的时间复杂度 \\(\\mathcal{O}(N)\\) ，删除的时间复杂度 \\(\\mathcal{O}(k\\log_{}{N})\\) ，总时间复杂度 \\(\\mathcal{O}(N\\log_{}{N})\\) 。 template \u003cclass Array\u003e void max_heapify(Array\u0026 a, int i, int heap_size) { int l = i * 2 + 1, r = i * 2 + 2, largest = i; if (l \u003c heap_size \u0026\u0026 a[l] \u003e a[largest]) { largest = l; } if (r \u003c heap_size \u0026\u0026 a[r] \u003e a[largest]) { largest = r; } if (largest != i) { swap(a[i], a[largest]); max_heapify(a, largest, heap_size); } } template \u003cclass Array\u003e void build_max_heap(Array\u0026 a, int heap_size) { for (int i = heap_size / 2; i \u003e= 0; --i) { max_heapify(a, i, heap_size); } } template \u003cclass Array\u003e int selection_algorithm(Array\u0026 nums, int k) { int heap_size = nums.size(); const int loop_exit_cond = heap_size - k + 1; build_max_heap(nums, heap_size); for (int i = nums.size() - 1; i \u003e= loop_exit_cond; --i) { swap(nums[0], nums[i]); --heap_size; max_heapify(nums, 0, heap_size); } return nums[0]; } ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:4:1","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#基于堆排序的选择算法"},{"categories":["Algorithm⁄DataStructure"],"content":" 归并排序归并排序 (mergesort) 是 1945 年由 von Neumann 首次提出，以 \\(\\mathcal{O}(N\\log_{}{N})\\) 最坏情形运行时间运行，而所使用的比较次数几乎是最优解。 mergesort 最基本的操作是合并两个已排序的表，而这个时间是线性的。递归地对前半部分数据和后半部分数据进行各自归并排序，将排序后的两部分合并得到最终的排序序列。 mergesort 是典型的 divide-and-conquer (分而治之) 算法，将问题 divide (分) 为一些小问题递归求解，并 conquering (治) 的阶段将分的阶段的解修补在一起。 递归操作是自顶向下的，具体操作方法： 申请空间，用以存放合并后的已排序序列 设定指针，指向最初的两个已排序序列的起始位置 比较指针所指向的元素，选择相对较小的放入合并空间，并移动指针到下一位置 重复步骤 3 直到某一指针到达序列尾部 将剩下的所有元素复制到合并空间 template \u003cclass Array\u003e void merge_impl(Array\u0026 array, Array\u0026 reg, int start, int end) { if (start \u003e= end) { return; } int len = end - start, mid = (len + start) \u003e\u003e 1; int start1 = start, end1 = mid; int start2 = mid + 1, end2 = end; merge_impl(array, reg, start1, end1); merge_impl(array, reg, start2, end2); int k = start; while (start1 \u003c= end1 \u0026\u0026 start2 \u003c= end2) { reg[k++] = array[start1] \u003c array[start2] ? array[start1++] : array[start2++]; } while (start1 \u003c= end1) { reg[k++] = array[start1++]; } while (start2 \u003c= end2) { reg[k++] = array[start2++]; } for (k = start; k \u003c= end; ++k) { array[k] = reg[k]; } } template \u003cclass Array\u003e void merge_sort(Array\u0026 array) { Array reg{array.size()}; // reg.size == array.size merge_impl(array, reg, 0, array.size() - 1); } ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:5:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#归并排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 归并排序的分析分析递归程序有一个经典技巧： 给运行时间写出一个递推关系 。假设 N 是 2 的幂，从而总可以将它分裂成相等的两部分。对于 \\(N = 1\\) 归并排序所用时间为常数，记作 \\[T(1) = 1.\\] 对 N 个数的归并排序用时等于完成两个大小为 \\(N/2\\) 的递归排序所用时间加上合并时间，记作 \\[T(N) = 2T(N/2) + N.\\] 第一种方法：对两边同时除以 N，于是 \\[\\frac{T(N)}{N} = \\frac{T(N/2)}{N/2} + 1, \\quad \\frac{T(N/2)}{N/2} = \\frac{T(N/4)}{N/4} + 1, \\quad \\cdots \\quad, \\quad\\frac{T(2)}{2} = \\frac{T(1)}{1} + 1.\\] 将所有这些方程相加，实际上这些中间项都会被消去，我们称之为 叠缩 (telescoping) 求和，最终结果为 \\[\\frac{T(N)}{N} = \\frac{T(1)}{1} + \\log_{}{N}.\\] 由此得到最终答案 \\[T(N) = N\\log_{}{N} + N = \\mathcal{O}(N\\log_{}{N}).\\] 第二种方法：在等式右边不断代入递推关系，得到 \\[T(N) = 2T(N/2) + N, \\quad T(N) = 2(2(T(N/4)) + N/2) = 4T(N/4) + N, \\quad T(N) = 8T(N/8) + N, \\quad \\cdots \\quad, \\quad T(N) = 2^{k}T(N/2^{k}) + kN.\\] 代入 \\(k=\\log_{}{N}\\) 得到 \\[T(N) = NT(1) + N\\log_{}{N} = N\\log_{}{N} + N = \\mathcal{O}(N\\log_{}{N}).\\] 归并排序是比较次数最少的排序算法，其运行时间一般依赖于元素的比较与复制所消耗的时间。复杂的复制操作会降低排序速度，在递归交替层面我们可以从用谨慎地交换两个数组担任角色的方法，避免其来回的复制。另外复制操作的消耗依赖于编程语言，Java 中类都是采用引用传递，因此比较耗时很多但相对的移动元素快很多；C/C++ 中大对象的复制是十分缓慢的，而比较是快速的，可以考虑采用移动语义或者指针来优化复制带来的时间消耗。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:5:1","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#归并排序的分析"},{"categories":["Algorithm⁄DataStructure"],"content":" 在链表上进行归并排序归并排序的递归实现是自顶向下的，而链表上常用自下向上的迭代思路。 首先解释代码中所用到的额外函数： 函数 释义 splice(dest, src, iterator) 将 src 中的元素 iterator 转移给 dest merge(dest, src) 将有序的列表 dest 与 src 合并并转移到 dest 中 swap(a, b) 交换两个链表 a 与 b 需要特别注意的是， splice 是将结点转移，也就是说它直接将结点链接进 dest 而不是复制结点的值到 dest，因此 src 将不再存在该结点。而 merge 是将 src 转移进 dest 中，操作完成后 dest 是有序的 dest 与 src 的合并，而 src 中将不再有任何结点。代码的实现步骤如下： 取出头结点转移给 transfer 将 transfer 与 pool[cur] 中的元素合并，并将指针移动到下一个 pool 重复步骤 2 直到 pool[cur] 为空，将 transfer 的所有元素转移到 pool[cur] 中 如果 cur 与 end 相等，则将 end 加一 如果 list 不为空，则重复步骤 1 到 4 合并 pool 中的所有元素，直到 pool[end - 1] ，这就是排序之后的 list void sort(list\u0026 l) { list transfer; list pool[48]; int end = 0; for (int cur = 0; l.size() != 0; cur = 0) { splice(transfer, l, l.begin()); while (cur \u003c end \u0026\u0026 !empty(pool[cur])) { merge(transfer, pool[cur++]); } swap(pool[cur], transfer); if (cur == end) { ++end; } } for (int cur = 1; cur \u003c end; ++cur) { merge(pool[cur], pool[cur - 1]); } swap(pool[end - 1], l); } 有意思的一点，pool[i] 只会接纳 \\(2^{i} (i \\in \\mathbb{N})\\) 个已排序好的元素，因此 pool[end - 1] 中的是已排序的前半部分，而 [0, end - 1) 中则分散着后半部分链表。这种方法是一个稳定排序，时间复杂度 \\(\\mathcal{O}(N\\log_{}{N})\\) ，并且这不会进行复制操作而是对结点进行操作。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:5:2","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#在链表上进行归并排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 快速排序快速排序 (quicksort) 是在实践中已知排序算法中最快的，它的平均运行时间 \\(\\mathcal{O}(N\\log_{}{N})\\) ，最坏情形为 \\(\\mathcal{O}(N^{2})\\) ，当然只要稍加努力就可以避免这种情形。通过将 heapsort 与 quicksort 结合将得到对几乎所有输入的最快运行时间。 quicksort 同样利用了 divide-and-conquer 的思想，基本算法如下： 如果 arr 中元素个数是 0 或 1 则返回 取 arr 中任一元素 e 为 枢纽元 (pivot) 将 arr 的其他元素 \\(arr-\\{e\\}\\) 划分为两个不相交的集合 \\(arr_{1} = \\{x \\in arr-\\{e\\} | x \\leq e\\}\\) 和 \\(arr_{2} = \\{x \\in arr-\\{e\\} | x \\geq e\\}\\) 返回 \\(\\{quicksort(arr_{1}), e, quicksort(arr_{2})\\}\\) 显然算法是成立的，可是为什么 quicksort 比 mergesort 快。quicksort 递归的解决两个子问题并需要线性的划分集合，但这两个子集的大小是不保证相等的，在划分是选择适当的位置将非常的高效，以至于弥补大小不等的递归带来的消耗甚至超过 mergesort。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:6:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#快速排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 选择枢纽元虽然我们可以任意的原则 pivot，但显然一些选择是更优秀的。 错误的 pivot 选择方法 通常将第一个或最后一个作为 pivot 是简单的，如果输入随机那还是可以接受的，但如果已经预排序或逆序，那么这将造成劣质的分割，所有元素都被划分到一个集合当中。当然这种情况可能发生在所有递归中。这将造成时间花费上升到 \\(\\mathcal{O}(N^{2})\\) ，而实际上算法什么都没有做。 一种安全的做法 随机选取 pivot 是一种非常安全的策略，但是随机数生成器却是一种相对昂贵的。 三数中值的分割法 一个 N 个数的中值是第 \\(\\lfloor N/2 \\rfloor\\) 个最大的数，pivot 的最好选择就是数组的中值。但是计算整个数组的中值无疑会降低算法的速度，往往采用选取三个元素并用它们的中值作为 pivot，当然可以随机选取三个值，但一般做法事选取左端、右端以及中间三个元素的中值作为 pivot。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:6:1","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#选择枢纽元"},{"categories":["Algorithm⁄DataStructure"],"content":" 分割策略选取 pivot 后，最重要的就是如何处理等于 pivot 的元素，即我们希望将等于 pivot 的元素均分到 pivot 的两边，保证两边尽可能的平衡。 假设所有元素都不相同，那么我们用双指针 i 和 j 来指向两端的元素，前半部分是小于 pivot 的元素，后半部分则是大于 pivot 的元素。当 i 遇到大于 pivot 的元素时停下来，等待 j 寻找小于 povit 的元素，交换这两个位置的元素，然后继续，直到 i 和 j 交错，那么交错的位置就是 pivot 的位置。通过这种方法我们可以很轻松的划分 \\(arr_{1}\\) 和 \\(arr_{2}\\) 。 假设所有元素都是相同的，当遇到等于 pivot 的元素时，显然 只让 指针 i 停止 (即 \\(arr_{1} = \\{x \\in arr-\\{e\\} | x \u003c e\\}\\)) 或 只让 指针 j 停止 (即 \\(arr_{2} = \\{x \\in arr-\\{e\\} | x \u003e e\\}\\))，都会导致指针偏向其中一边。而都不停止，也是同样的结果，其时间复杂度会是 \\(\\mathcal{O}(N^{2})\\) 。现在仅剩下的方法就是让 i 和 j 在遇到等于 pivot 的元素时，停下来并交换元素，这样会造成大量无意义的交换，但是可以将 \\(arr_{1}\\) 与 \\(arr_{2}\\) 分割为大小几乎相等的集合，归并排序告诉我们其运行时间为 \\(\\mathcal{O}(N\\log_{}{N})\\) 。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:6:2","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#分割策略"},{"categories":["Algorithm⁄DataStructure"],"content":" 小数组当然在数组大小很小的时候 (通常认为是 \\(N \\leq 20\\))，quicksort 并不如 insertion sort。因此通常的解决方法是在小数组上不递归地使用 quicksort，而是改为 insertion sort 这种对小数组有效的排序。这种策略实际上可以节省大约 \\(15\\%\\) (相对自始至终使用 quicksort) 的运行时间。 一种好的 截止范围 (CUTOFF) 是 \\(N = 10\\) ，当然 CUTOFF 在 5 至 20 之间都有可能产生类似的结果。这种做法也避免了一些有害的退化情形。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:6:3","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#小数组"},{"categories":["Algorithm⁄DataStructure"],"content":" 快速排序的分析在分析之前，给出该算法的相关代码。需要注意的是，在排序范围小于 10 时，会调用 shell sort 为范围进行排序。 template \u003cclass Array\u003e void shell_sort(Array\u0026 arr, std::size_t l, std::size_t r) { for (const auto\u0026 gap : SHELL_SORT_GAPS) { std::size_t base{l + gap}; for (std::size_t i = base; i \u003c= r; ++i) { for (std::size_t j{i}; j \u003e= base and arr[j - gap] \u003e arr[j]; j -= gap) { std::swap(arr[j], arr[j - gap]); } } } } template \u003cclass Array, class Element = typename Array::value_type\u003e const Element\u0026 get_pivot(Array\u0026 arr, std::size_t l, std::size_t r) { std::size_t mid{(l + r) \u003e\u003e 1}; if (arr[mid] \u003c arr[l]) { std::swap(arr[l], arr[mid]); } if (arr[r] \u003c arr[l]) { std::swap(arr[l], arr[r]); } if (arr[r] \u003c arr[mid]) { std::swap(arr[r], arr[mid]); } std::swap(arr[mid], arr[r - 1]); return arr[r - 1]; } template \u003cclass Array\u003e void quicksort(Array\u0026 arr, std::size_t l, std::size_t r) { if (r \u003c l + 10) { return shell_sort(arr, l, r); } auto pivot{get_pivot(arr, l, r)}; std::size_t i{l}, j{r - 1}; while (true) { while (arr[++i] \u003c pivot) { continue; } while (pivot \u003c arr[--j]) { continue; } if (i \u003c j) { std::swap(arr[i], arr[j]); } else { break; } } std::swap(arr[i], arr[r - 1]); quicksort(arr, l, i - 1); quicksort(arr, i + 1, r); } template \u003cclass Array\u003e void quicksort(Array\u0026 arr) { quicksort(arr, 0ll, arr.size() - 1ll); } 对快排进行如同归并排序那样的分析。可以肯定的是 \\(T(0) = T(1) = 1\\) 和 \\(T(N) = T(i) + T(N - i - 1) + cN\\) ，我们需要考虑三种情况： 最坏情形的分析 这种情况下，我们可以认为 pivot 始终是最小元素，那么可以认为 \\(T(N) = T(N - 1) + cN\\) ，通过递推关系我们可以得到 \\[T(N) = T(1) + c\\sum_{i=2}^{N}{i} = \\mathcal{O}(N^{2}).\\] 最佳情形的分析 这种情况下，我们可以认为 pivot 始终位于中间，为了简化假设两个集合的大小恰好为原大小的一半。可以发现 \\(T(N) = 2T(N/2) + cN\\) ，与归并排序一样，最终我们可以得到 \\[T(N) = cN\\log_{}{N} + N = \\mathcal{O}(N\\log_{}{N}).\\] 平均情形的分析 当然这是最难的部分，我们假设对于 \\(arr_{1}\\) 每个大小都是等可能的，因此每个大小均有 \\(1 / N\\) 的概率。由此可知 \\(T(i)\\) 的平均值为 \\((1/N) \\sum_{j=0}^{N-1}{T(j)}\\) ，代入 \\(T(N) = T(i) + T(N - i - 1) + cN\\) 得到 \\[T(N) = \\frac{2}{N}[\\sum_{j=0}^{N-1}{T(j)}]+cN.\\] 两边同时乘以 \\(N\\) 可以消去分母上的 \\(N\\) 得到 \\[NT(N) = 2[\\sum_{j=0}^{N-1}{T(j)}]+cN.\\] 如果为 \\(T(N-1)\\) 也这样做则可以得到 \\[(N-1)T(N-1) = 2[\\sum_{j=0}^{N-2}{T(j)}]+c(N-1)^{2}.\\] 将上面两个式子相减可以消去其中的求和符号 \\[NT(N) - (N-1)T(N-1) = 2T(N-1) + 2cN - c.\\] 去除 \\(-c\\) 并改写为 \\(T(N)\\) 与 \\(T(N-1)\\) 的关系式 \\[\\frac{T(N)}{N+1}=\\frac{T(N-1)}{N}+\\frac{2c}{N+1}.\\] 如此进行 telescoping 求和即可得到 \\[\\frac{T(N)}{N+1} = \\mathcal{O}(\\log_{}{N}).\\] 即 \\(T(N) = \\mathcal{O}(N\\log_{}{N})\\) 。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:6:4","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#快速排序的分析"},{"categories":["Algorithm⁄DataStructure"],"content":" 基于快速排序的选择问题选择问题 (selection problem) 可以使用 quicksort 来解决，之前介绍了使用 heapsort 的 \\(\\mathcal{O}(N + k\\log_{}{N})\\) 选择算法，而查询中值的话这个算法达到了 \\(\\mathcal{O}(N\\log_{}{N})\\) ，这个所用时间可以给数组排序。因此我们期望获得一个更好的时间界。 实际上这个新的方法，查找集合 \\(arr\\) 中第 \\(k\\) 个最小的算法几乎与 quicksort 基本相同，我们将这个算法称为 快速选择 (quickselect)，令 \\(|arr_{i}|\\) 为 \\(arr_{i}\\) 中的元素个数，步骤如下： 如果 \\(|arr| = 1\\) ，那么 \\(k = 1\\) 并将 \\(arr\\) 中的元素作为答案返回。如果正在使用小数组的截止方法且 \\(|arr| \\leq CUTOFF\\) ，则将 \\(arr\\) 排序并返回第 \\(k\\) 个最小元 选取一个 pivot \\(e \\in arr\\) 将集合 \\(arr-{e}\\) 分割成 \\(arr_{1}\\) 与 \\(arr_{2}\\) ，就像 quicksort 中那样 如果 \\(k \\leq |arr_{1}|\\) ，那么第 \\(k\\) 个最小元必然在 \\(arr_{1}\\) 中，这种情况下直接返回 \\(quickselect(arr_{1}, k)\\) 。如果 \\(k = 1 + |arr_{1}|\\) 那么 pivot 就是第 \\(k\\) 个最小元。否则我们进行一次递归并返回 \\(quickselect(arr_{2}, k - 1 - |arr_{1}|)\\) 与 quicksort 相比 quickselect 只进行了一次递归调用而不是两次，最坏时间复杂度与 quicksort 相同，但平均时间复杂度是 \\(\\mathcal{O}(N)\\) 。 // get_pivot 与 shell_sort 两个函数与 quicksort 中相同 // quickselect 与 quicksort 极为相似 template \u003cclass Array\u003e void quickselect(const std::size_t\u0026 k, Array\u0026 arr, std::size_t l, std::size_t r) { if (l == r \u0026\u0026 k == 1) { return; } if (r \u003c l + 10) { return shell_sort(arr, l, r); } auto pivot{get_pivot(arr, l, r)}; std::size_t i{l}, j{r - 1}; while (true) { while (arr[++i] \u003c pivot) { continue; } while (pivot \u003c arr[--j]) { continue; } if (i \u003c j) { std::swap(arr[i], arr[j]); } else { break; } } std::swap(arr[i], arr[r - 1]); auto len_1{i - l}; if (len_1 != k - 1) { k \u003c= len_1 ? quickselect(k, arr, l, i - 1) : quickselect(k - len_1 - 1, arr, i + 1, r); } } template \u003cclass Array\u003e void quickselect(Array\u0026 arr, int k) { quickselect(k, arr, 0ll, arr.size() - 1ll); } ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:6:5","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#基于快速排序的选择问题"},{"categories":["Algorithm⁄DataStructure"],"content":" 间接排序对于排序来说，其中会有大量的比较与交换，一个难于复制的大对象所带来的时间成本是无法忽略的。解决方法也很简单，利用一个指向元素的指针所组成的数组，排序这些指针，从而确定元素的位置，而不是实际上的复制操作。这种被称为 中间置换 (in-situ permutation) 算法，之前介绍的对链表的排序就是这种算法。 但是对于数组这种顺序存储的结构，我们需要生成一个指针数组，并对指针数组进行 in-situ permutation。这样即使最终排序好指针数组，也需要写回原始数组，一种简单的方法是开辟等长的数组，将其按照指针数组的顺序复制一份，再复制回原始数组。其代价是 \\(\\mathcal{O}(N)\\) 的额外空间复杂度与 \\(\\mathcal{O}(2N)\\) 的复制次数。 不过在优化之前，需要简单的理解一个问题。当我们需要交换 a[2] 与 a[4] 时，需要有一个临时变量存储 a[2] ，以防 a[2] 不能被正确交换。如果需要交换三个元素，那么我们可以使用一个临时变量与 4 次赋值操作完成这个流程。将其应用在 in-situ permutation 上，初始位置的 a[i] 存储到 tmp 中，让后将 p[i] 所指向的元素赋值到 a[i] 中，以此重复直到循环结束，将 tmp 赋值到正确的位置。这样一个长度为 \\(L\\) 的循环只需要 \\(L+1\\) 次赋值，当然长度为 1 时不需要赋值。 那么对于给定 N 个元素的数组，令 \\(C_{L}\\) 是长度为 \\(L\\) 的循环的次数，元素的赋值次数 \\(M\\) 如下 \\[M = N - C_{1} + C_{2} + C_{3} + \\cdots + C_{N}.\\] 最好的情况是全是长度为 1 的循环，即每个元素都在正确的位置上，这样就不需要赋值。最坏情况是有 \\(N/2\\) 个长度为 2 的循环，此时需要 \\(3N / 2\\) 次赋值。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:7:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#间接排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 非比较排序在任何只使用比较的排序算法的最坏时间复杂度为 \\(\\Omega(N\\log_{}{N})\\) ，但是我们可以在某些情况下以 \\(\\mathcal{O}(N)\\) 时间进行排序。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:8:0","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#非比较排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 桶排序一个简单的例子是 桶排序 (bucket sort)，其将元素分到有限个桶中，每个桶再进行分别排序。当要被排序的数组内的数值是均匀分配的时候，桶排序使用线性时间 \\(\\Theta(n)\\) 。 桶排序的步骤如下： 设置一个定量的数组当作空桶子 访问序列并将元素一个一个放到对应的桶子去 对每个不是空的桶子进行排序 从不是空的桶子里把元素再放回原来的序列中 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:8:1","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#桶排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 计数排序计数排序 (counting sort) 将设置一个额外的数组，其中第 i 个元素是待排序数组中值等于 i 的元素的个数，然后根据额外数组来排序。算法的步骤如下： 找出待排序的数组中最大和最小的元素 统计数组中每个值为 i 的元素出现的次数，存入额外数组的第 i 项 反向填充目标数组 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:8:2","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#计数排序"},{"categories":["Algorithm⁄DataStructure"],"content":" 基数排序基数排序 (radix sort) 是将待排序元素按一定位进行分割，然后按每个数位进行比较。实现方式可以采用两种不同的策略，即 LSD (Least Significant Digital) 与 MSD (Most Significant Digital)，LSD 采用从最右位开始排序，MSD 采用从最左位开始排序。 radix sort 不止可以对整数进行排序，还可以用于字符串或特定格式的浮点数，为了方便以整数为例，算法的步骤如下： 将所有待比较数值统一为同样的数位长度，数位较短的数前面补零 从最低位开始，将元素放入对应数位的桶中进行排序 按照顺序从桶中取出元素，并重复步骤 2 直到元素最高位被排序 最终序列为已排序序列 基数排序的时间复杂度是 \\(\\mathcal{O}(kN)\\) ，其中 \\(N\\) 是排序元素个数， \\(k\\) 是数字位数。当然 \\(k \u003e \\log_{}{N}\\) 时 radix sort 并不比比较排序更优秀。 ","date":"08-27","objectID":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/:8:3","series":["数据结构与算法分析"],"tags":["Note","Sort"],"title":"排序算法","uri":"/2021/data_strucures_and_algorithm_analysis_007_sorting_algorithm/#基数排序"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第六章笔记","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/"},{"categories":["Algorithm⁄DataStructure"],"content":" 模型优先队列 (priority queue) 的 ADT 与 queue 类似，它们都提供了基本的 enqueue 与 dequeue 操作。但是 priority queue 可以在 dequeue 时将数据按照一定顺序弹出队列，而不是 FIFO。我们这里主要讨论每次出队最小的元素 (即 delete_min)，如果你希望进行其他一些有规范的操作，方法与这类似。 显然 priority queue 有一个朴素解，那就是在每次 delete_min 时遍历整个存储单元，找到最小的元素并删除，其时间复杂度 \\(\\mathcal{O}(N)\\) ，当然插入元素的时间复杂度会好很多，只需要 \\(\\mathcal{O}(1)\\) 。当然你也可以将其反过来，在插入时就找到最小的元素。 显然这是无法接受的，即使是利用前几篇中介绍过的 AVL 树都可以将其时间复杂度压缩到 \\(\\mathcal{O}(\\log_{}{N})\\) 。不过这有点太过分了，平衡 BST 的很多操作可能是用不上的，而且为了优先队列再实现一个平衡树实在是太难为人了。 我们将要介绍的工具叫做 二叉堆 (binary heap)，用以实现有限队列。但是需要注意的是， 堆 (heap) 这里指的是一种数据结构，而非操作系统中用以分配动态内存的地方。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:1:0","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#模型"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉堆","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:0","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#二叉堆"},{"categories":["Algorithm⁄DataStructure"],"content":" 结构性质binary heap 是一棵被完全填满的二叉树，或者说是一棵 complete binary tree。由于 complete binary tree 的排列十分有规律，因此我们可以将其转化为数组，不再需要链来链接它。 对于数组任一位置 \\(i\\) 上的元素，其左右儿子分别在在位置 \\(2i + 1\\) 和 \\(2i +2\\) 上，而它的父亲则在位置 \\(\\lfloor(i - 1)/2\\rfloor\\) 上。当然如果根从 \\(1\\) 开始，那么位置 \\(i\\) 上元素的左右儿子的位置分别为 \\(2i\\) 和 \\(2i + 1\\) ，而父亲的位置是 \\(\\lfloor i/2 \\rfloor\\) 。以下未说明的情况，我们将 1 作为 root 的下标。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:1","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#结构性质"},{"categories":["Algorithm⁄DataStructure"],"content":" 堆序性质保证 heap 可以快速执行的是 堆序性质 (heap-order property)。我们希望找到的是最小的元素，因此最小的元素在根上，而它的任一子树也是 heap，那么可以得出任一结点小于其所有后裔，这种结构被称为 小顶堆 (min heap)；而相反的，任一结点大于其所有后裔，则被称为 大顶堆 (max heap)。 heap-order 保证我们可以在 \\(\\mathcal{O}(1)\\) 的时间复杂度内找到想要的元素。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:2","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#堆序性质"},{"categories":["Algorithm⁄DataStructure"],"content":" 堆的操作如果我们希望对 heap 做一些操作，可能会破坏 heap-order property，所以我们应该保证无论如何操作，都可以恢复其性质。 binary heap 的插入操作要在堆中插入一个元素，我们首先需要在尾部建立一个空穴，用以存放元素。为了不破坏 heap-order，我们比较插入元素与其父结点元素：如果元素可以放入空穴则插入完成，否则将父结点放入空穴，空穴转变为了父结点，自底向上递归直到元素插入。 这个过程被称为 上滤 (percolate up)。percolate up 的最坏时间复杂度是 \\(\\mathcal{O}(\\log_{}{N})\\) ，这是需要 percolate up 到 root。但是平均来看， percolate up 的结束要早得多，平均需要 2.607 次比较，因此元素平均上移 1.607 层，平均时间复杂度 \\(\\mathcal{O}(1)\\) 。 这里给出向堆中插入的元素的代码。有一个小技巧，交换元素需要三条赋值语句，如果 percolate up n，则需要 \\(3n\\) 条赋值语句，采用直接赋值覆盖的方法，则只需要 \\(n + 1\\) 次赋值。 template \u003cclass Comparable\u003e void insert(heap\u0026 h, Comparable x) { int hole = ++h.cur_size; while (hole \u003e 1 \u0026\u0026 x \u003c h[hole / 2]) { h[hole] = h[hole / 2]; hole /= 2; } h[hole] = x; } binary heap 的移除操作找出目标元素显然简单的多，因为 root 就是目标，但是如何将其从 heap 中移除。我们依然采取建立空穴的方法，只不过这次空穴建立在了 root。 我们进行与 percolate up 类似的操作，只不过这次从上向下进行，这被称为 下滤 (percolate down)。我们从 root 出发，将孩子中的较小元素移动到空穴，并继续向下找去，直到空穴成为 leaf。到达 leaf 后，我们将最后一个结点值赋值给空穴，并删除最后一个结点，这样就能让 heap 的长度减一。 在实现时我们需要注意一个细节，当 heap 中元素的数量为偶数时，有的结点可能只有一个孩子。有一个小技巧，可以将一个大于任何 heap 元素的标识放在末尾，这样我们可以假设所有结点都有两个孩子，当然请小心处理。percolate down 的最坏与平均复杂度都是 \\(\\mathcal{O}(\\log_{}{N})\\) 的。 void erase(heap\u0026 h) { auto tmp = h[h.cur_size--]; int hole = 1; while (hole * 2 \u003c= h.cur_size) { int child = hole * 2; if (child != h.cur_size \u0026\u0026 h[child + 1] \u003c h[child]) { ++child; } if (h[child] \u003c tmp) { h[hole] = h[child]; } else { break; } } h[hole] = tmp; } binary heap 的其他操作可以明确，min heap 中对查找最大元素并没有帮助，最大的元素在 leaf 上，但有半数的元素都是 leaf。在 heap 中我们不得不进行线性查找才能获取到特定元素。 当然我们还可以在 heap 上进行其他操作。 decrease_key(p, delta) ：将位置 p 的元素减小 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate up。 increase_key(p, delta) ：将位置 p 的元素增加 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate down。 remove(p) ：将位置 p 的元素移除。 build_heap ：通过原始集合构建一个堆，这个过程也被称为 堆化 (heapify)。这个过程的平均运行时间是 \\(\\mathcal{O}(N)\\) 的，最坏时间复杂度是 \\(\\mathcal{O}(N\\log_{}{N})\\) 的。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#堆的操作"},{"categories":["Algorithm⁄DataStructure"],"content":" 堆的操作如果我们希望对 heap 做一些操作，可能会破坏 heap-order property，所以我们应该保证无论如何操作，都可以恢复其性质。 binary heap 的插入操作要在堆中插入一个元素，我们首先需要在尾部建立一个空穴，用以存放元素。为了不破坏 heap-order，我们比较插入元素与其父结点元素：如果元素可以放入空穴则插入完成，否则将父结点放入空穴，空穴转变为了父结点，自底向上递归直到元素插入。 这个过程被称为 上滤 (percolate up)。percolate up 的最坏时间复杂度是 \\(\\mathcal{O}(\\log_{}{N})\\) ，这是需要 percolate up 到 root。但是平均来看， percolate up 的结束要早得多，平均需要 2.607 次比较，因此元素平均上移 1.607 层，平均时间复杂度 \\(\\mathcal{O}(1)\\) 。 这里给出向堆中插入的元素的代码。有一个小技巧，交换元素需要三条赋值语句，如果 percolate up n，则需要 \\(3n\\) 条赋值语句，采用直接赋值覆盖的方法，则只需要 \\(n + 1\\) 次赋值。 template void insert(heap\u0026 h, Comparable x) { int hole = ++h.cur_size; while (hole \u003e 1 \u0026\u0026 x \u003c h[hole / 2]) { h[hole] = h[hole / 2]; hole /= 2; } h[hole] = x; } binary heap 的移除操作找出目标元素显然简单的多，因为 root 就是目标，但是如何将其从 heap 中移除。我们依然采取建立空穴的方法，只不过这次空穴建立在了 root。 我们进行与 percolate up 类似的操作，只不过这次从上向下进行，这被称为 下滤 (percolate down)。我们从 root 出发，将孩子中的较小元素移动到空穴，并继续向下找去，直到空穴成为 leaf。到达 leaf 后，我们将最后一个结点值赋值给空穴，并删除最后一个结点，这样就能让 heap 的长度减一。 在实现时我们需要注意一个细节，当 heap 中元素的数量为偶数时，有的结点可能只有一个孩子。有一个小技巧，可以将一个大于任何 heap 元素的标识放在末尾，这样我们可以假设所有结点都有两个孩子，当然请小心处理。percolate down 的最坏与平均复杂度都是 \\(\\mathcal{O}(\\log_{}{N})\\) 的。 void erase(heap\u0026 h) { auto tmp = h[h.cur_size--]; int hole = 1; while (hole * 2 \u003c= h.cur_size) { int child = hole * 2; if (child != h.cur_size \u0026\u0026 h[child + 1] \u003c h[child]) { ++child; } if (h[child] \u003c tmp) { h[hole] = h[child]; } else { break; } } h[hole] = tmp; } binary heap 的其他操作可以明确，min heap 中对查找最大元素并没有帮助，最大的元素在 leaf 上，但有半数的元素都是 leaf。在 heap 中我们不得不进行线性查找才能获取到特定元素。 当然我们还可以在 heap 上进行其他操作。 decrease_key(p, delta) ：将位置 p 的元素减小 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate up。 increase_key(p, delta) ：将位置 p 的元素增加 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate down。 remove(p) ：将位置 p 的元素移除。 build_heap ：通过原始集合构建一个堆，这个过程也被称为 堆化 (heapify)。这个过程的平均运行时间是 \\(\\mathcal{O}(N)\\) 的，最坏时间复杂度是 \\(\\mathcal{O}(N\\log_{}{N})\\) 的。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#binary-heap-的插入操作"},{"categories":["Algorithm⁄DataStructure"],"content":" 堆的操作如果我们希望对 heap 做一些操作，可能会破坏 heap-order property，所以我们应该保证无论如何操作，都可以恢复其性质。 binary heap 的插入操作要在堆中插入一个元素，我们首先需要在尾部建立一个空穴，用以存放元素。为了不破坏 heap-order，我们比较插入元素与其父结点元素：如果元素可以放入空穴则插入完成，否则将父结点放入空穴，空穴转变为了父结点，自底向上递归直到元素插入。 这个过程被称为 上滤 (percolate up)。percolate up 的最坏时间复杂度是 \\(\\mathcal{O}(\\log_{}{N})\\) ，这是需要 percolate up 到 root。但是平均来看， percolate up 的结束要早得多，平均需要 2.607 次比较，因此元素平均上移 1.607 层，平均时间复杂度 \\(\\mathcal{O}(1)\\) 。 这里给出向堆中插入的元素的代码。有一个小技巧，交换元素需要三条赋值语句，如果 percolate up n，则需要 \\(3n\\) 条赋值语句，采用直接赋值覆盖的方法，则只需要 \\(n + 1\\) 次赋值。 template void insert(heap\u0026 h, Comparable x) { int hole = ++h.cur_size; while (hole \u003e 1 \u0026\u0026 x \u003c h[hole / 2]) { h[hole] = h[hole / 2]; hole /= 2; } h[hole] = x; } binary heap 的移除操作找出目标元素显然简单的多，因为 root 就是目标，但是如何将其从 heap 中移除。我们依然采取建立空穴的方法，只不过这次空穴建立在了 root。 我们进行与 percolate up 类似的操作，只不过这次从上向下进行，这被称为 下滤 (percolate down)。我们从 root 出发，将孩子中的较小元素移动到空穴，并继续向下找去，直到空穴成为 leaf。到达 leaf 后，我们将最后一个结点值赋值给空穴，并删除最后一个结点，这样就能让 heap 的长度减一。 在实现时我们需要注意一个细节，当 heap 中元素的数量为偶数时，有的结点可能只有一个孩子。有一个小技巧，可以将一个大于任何 heap 元素的标识放在末尾，这样我们可以假设所有结点都有两个孩子，当然请小心处理。percolate down 的最坏与平均复杂度都是 \\(\\mathcal{O}(\\log_{}{N})\\) 的。 void erase(heap\u0026 h) { auto tmp = h[h.cur_size--]; int hole = 1; while (hole * 2 \u003c= h.cur_size) { int child = hole * 2; if (child != h.cur_size \u0026\u0026 h[child + 1] \u003c h[child]) { ++child; } if (h[child] \u003c tmp) { h[hole] = h[child]; } else { break; } } h[hole] = tmp; } binary heap 的其他操作可以明确，min heap 中对查找最大元素并没有帮助，最大的元素在 leaf 上，但有半数的元素都是 leaf。在 heap 中我们不得不进行线性查找才能获取到特定元素。 当然我们还可以在 heap 上进行其他操作。 decrease_key(p, delta) ：将位置 p 的元素减小 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate up。 increase_key(p, delta) ：将位置 p 的元素增加 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate down。 remove(p) ：将位置 p 的元素移除。 build_heap ：通过原始集合构建一个堆，这个过程也被称为 堆化 (heapify)。这个过程的平均运行时间是 \\(\\mathcal{O}(N)\\) 的，最坏时间复杂度是 \\(\\mathcal{O}(N\\log_{}{N})\\) 的。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#binary-heap-的移除操作"},{"categories":["Algorithm⁄DataStructure"],"content":" 堆的操作如果我们希望对 heap 做一些操作，可能会破坏 heap-order property，所以我们应该保证无论如何操作，都可以恢复其性质。 binary heap 的插入操作要在堆中插入一个元素，我们首先需要在尾部建立一个空穴，用以存放元素。为了不破坏 heap-order，我们比较插入元素与其父结点元素：如果元素可以放入空穴则插入完成，否则将父结点放入空穴，空穴转变为了父结点，自底向上递归直到元素插入。 这个过程被称为 上滤 (percolate up)。percolate up 的最坏时间复杂度是 \\(\\mathcal{O}(\\log_{}{N})\\) ，这是需要 percolate up 到 root。但是平均来看， percolate up 的结束要早得多，平均需要 2.607 次比较，因此元素平均上移 1.607 层，平均时间复杂度 \\(\\mathcal{O}(1)\\) 。 这里给出向堆中插入的元素的代码。有一个小技巧，交换元素需要三条赋值语句，如果 percolate up n，则需要 \\(3n\\) 条赋值语句，采用直接赋值覆盖的方法，则只需要 \\(n + 1\\) 次赋值。 template void insert(heap\u0026 h, Comparable x) { int hole = ++h.cur_size; while (hole \u003e 1 \u0026\u0026 x \u003c h[hole / 2]) { h[hole] = h[hole / 2]; hole /= 2; } h[hole] = x; } binary heap 的移除操作找出目标元素显然简单的多，因为 root 就是目标，但是如何将其从 heap 中移除。我们依然采取建立空穴的方法，只不过这次空穴建立在了 root。 我们进行与 percolate up 类似的操作，只不过这次从上向下进行，这被称为 下滤 (percolate down)。我们从 root 出发，将孩子中的较小元素移动到空穴，并继续向下找去，直到空穴成为 leaf。到达 leaf 后，我们将最后一个结点值赋值给空穴，并删除最后一个结点，这样就能让 heap 的长度减一。 在实现时我们需要注意一个细节，当 heap 中元素的数量为偶数时，有的结点可能只有一个孩子。有一个小技巧，可以将一个大于任何 heap 元素的标识放在末尾，这样我们可以假设所有结点都有两个孩子，当然请小心处理。percolate down 的最坏与平均复杂度都是 \\(\\mathcal{O}(\\log_{}{N})\\) 的。 void erase(heap\u0026 h) { auto tmp = h[h.cur_size--]; int hole = 1; while (hole * 2 \u003c= h.cur_size) { int child = hole * 2; if (child != h.cur_size \u0026\u0026 h[child + 1] \u003c h[child]) { ++child; } if (h[child] \u003c tmp) { h[hole] = h[child]; } else { break; } } h[hole] = tmp; } binary heap 的其他操作可以明确，min heap 中对查找最大元素并没有帮助，最大的元素在 leaf 上，但有半数的元素都是 leaf。在 heap 中我们不得不进行线性查找才能获取到特定元素。 当然我们还可以在 heap 上进行其他操作。 decrease_key(p, delta) ：将位置 p 的元素减小 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate up。 increase_key(p, delta) ：将位置 p 的元素增加 \\(\\delta\\) 。这有可能破坏 heap-order，因此需要对其进行 percolate down。 remove(p) ：将位置 p 的元素移除。 build_heap ：通过原始集合构建一个堆，这个过程也被称为 堆化 (heapify)。这个过程的平均运行时间是 \\(\\mathcal{O}(N)\\) 的，最坏时间复杂度是 \\(\\mathcal{O}(N\\log_{}{N})\\) 的。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#binary-heap-的其他操作"},{"categories":["Algorithm⁄DataStructure"],"content":" d 堆binary heap 的实现简单，因此大部分时候 priority queue 优先使用其作为实现。d 堆 (d-ary heap) 是 binary heap 的简单推广，其每个结点总有 d 个孩子。所以简单的说， binary heap 就是一种 2-堆。 当然我们可以继续使用一个数组表示 d-ary heap，但是找出 node 和 parent 的乘法和除法都有个因子 d，因此更好的做法是使用 \\(d = 2^{x}\\) ，这样可以使用位运算加速除法过程。 一个显而易见的结论，当 d 增大时，其深度也将减少，因此 insert 时间复杂度是 \\(\\mathcal{O}(\\log_{d}{N})\\) ，但 erase 操作就会费时很多，erase 的时间复杂度是 \\(\\mathcal{O}(d\\log_{d}{N})\\) 。当然在 insert 远多于 erase 的算法中，d-ary heap 可以有效降低时间复杂度。不过实践证明，4-ary heap 可以胜过 binary heap。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:2:4","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#d-堆"},{"categories":["Algorithm⁄DataStructure"],"content":" 左式堆设计一种像 binary heap 又能以 \\(\\mathcal{O}(N)\\) 的时间复杂度处理 merge，并且只使用一个数组的堆结构是困难的。因此大部分需要有效合并的数据结构都是链式的，但这可能导致其他操作变慢。 左式堆 (leftist heap) 像 binary heap 那样既有结构性质又有 heap-order property，不过所有的堆其 heap-order property 都是一样的，所以我们只需要关注它的结构性质。 leftist heap 也是二叉树，但区别是：leftist heap 并不是理想平衡的，而是趋于非常不平衡的。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:3:0","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#左式堆"},{"categories":["Algorithm⁄DataStructure"],"content":" 左式堆的性质将任一结点 X 的 零路径长 (null path length) \\(npl(X)\\) 或 \\(s(X)\\) 定义为从 X 到一个布局有两个孩子的结点的最短路径长。因此具有 \\(degree = 0 \\lor 1\\) 的结点 \\(npl = 1\\) ，而 \\(npl(null) = 0\\) ，任意结点的 NPL 比其所有孩子的 NPL 的最小值加 1。对于 heap 中的每一个结点 X，左孩子的 NPL 至少与右孩子的 NPL 相等，这样树的结构更偏向于向左子树添加深度，因此称之为 leftist heap。 在右路径上有 r 个结点的左式树必然至少有 \\(2^{r} - 1\\) 个结点，而 N 个结点的左式树有一条右路径最多含有 \\(\\lfloor\\log_{}{(N+1)}\\rfloor\\) 个结点。在左式堆上的操作，将所有工作都放在右路径上进行，以保证树不会过深。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:3:1","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#左式堆的性质"},{"categories":["Algorithm⁄DataStructure"],"content":" 左式堆的操作leftist heap 中的基本操作是合并，而插入、移除是合并的特殊情形。插入元素可以看作是一个大堆和一个只有根结点的堆进行合并；移除元素时我们将会得到两个堆，将这两个堆进行合并即可得到新的 heap。 在合并时，递归地将 具有大的根值的堆 与 具有小的根值的堆 的 右子树 合并，如果右子树的 NPL 大于左子树的 NPL，则将两棵子树交互，以满足 leftist heap 的性质要求。 执行合并的时间与右路径的长度成正比，而递归调用时，每一个被访问的结点执行常数工作量，因此合并 leftist heap 的时间界为 \\(\\mathcal{O}(\\log_{}{N})\\) 。以下代码展示了合并操作的递归实现，如果你希望使用 loop 进行实现可能会有些困难，但可以肯定的是无论如何实现其结果等价。 BinaryTreeNode* merge_impl(BinaryTreeNode* h1, BinaryTreeNode* h2) { BinaryTreeNode* merge(BinaryTreeNode* h1, BinaryTreeNode* h2); // 声明 merge 函数 if (h1-\u003eleft == nullptr) { h1-\u003eleft = h2; return h1; } h1-\u003eright = merge(h1-\u003eright, h2); if (get_npl(h1-\u003eleft) \u003c get_npl(h1-\u003eright)) { swap(h1-\u003eleft, h1-\u003eright); } set_npl(h1, 1 + get_npl(h1-\u003eright)); return h1; } BinaryTreeNode* merge(BinaryTreeNode* h1, BinaryTreeNode* h2) { if (h1 == nullptr) { return h2; } if (h2 == nullptr) { return h1; } return h1-\u003edata \u003c h2-\u003edata ? merge_impl(h1, h2) : merge_impl(h2, h1); } ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:3:2","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#左式堆的操作"},{"categories":["Algorithm⁄DataStructure"],"content":" 斜堆斜堆 (skew heap) 是 leftist heap 的自调节形式，skew heap 与 leftist heap 的关系类似于 AVL tree 与 splay tree 之间的关系。skew heap 不对树的结构进行限制，右路径可以任意长，因此所有操作的最坏运行时间为 \\(\\mathcal{O}(N)\\) 。但是正如 splay tree，它的 amortized 运行时间为 \\(\\mathcal{O}(\\log_{}{N})\\) 。skew heap 的基本操作也是合并，且操作与 leftist heap 是类似的，唯一的不同是 skew heap 不再存储 NPL，交换孩子是无条件的。 BinaryTreeNode* merge_impl(BinaryTreeNode* h1, BinaryTreeNode* h2) { BinaryTreeNode* merge(BinaryTreeNode* h1, BinaryTreeNode* h2); // 声明 merge 函数 h1-\u003eright = merge(h1-\u003eright, h2); swap(h1-\u003eleft, h1-\u003eright); return h1; } BinaryTreeNode* merge(BinaryTreeNode* h1, BinaryTreeNode* h2) { if (h1 == nullptr) { return h2; } if (h2 == nullptr) { return h1; } return h1-\u003edata \u003c h2-\u003edata ? merge_impl(h1, h2) : merge_impl(h2, h1); } ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:3:3","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#斜堆"},{"categories":["Algorithm⁄DataStructure"],"content":" 二项队列二项队列不同于前面介绍的所有优先队列的实现，其是堆序的树的集合，称为森林。森林中的每一棵 二项树 (binomial tree) 都是有约束的堆序树，每一个高度上至多存在一棵二项树，高度为 0 的二项树是一颗单结点树；高度为 k 的二项树 \\(B_k\\) 通过将一棵二项树 \\(B_{k-1}\\) 附接到另一棵二项树 \\(B_{k-1}\\) 的根上构成。 高度为 k 的二项树恰好有 \\(2^k\\) 个结点，而深度 d 处的结点树是二项系数 \\[\\left(\\begin{aligned} k \\\\ d \\end{aligned}\\right).\\] 如果将堆序施加于二项树上，并允许任意高度上最多一棵二项树，那么能够用二项树的集合唯一地表示任意大小的优先队列。例如，大小为 13 的优先队列可以用森林 \\(B_3\\) 、\\(B_2\\) 、\\(B_0\\) 表示，将这种二项队列写作 \\(1101\\)。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:4:0","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#二项队列"},{"categories":["Algorithm⁄DataStructure"],"content":" 二项队列操作最小元可以通过搜索所有树的根找出，最多有 \\(\\log_{}N\\) 棵不同的树，因此最小元可以以 \\(\\mathcal{O}(\\log N)\\) 时间找出。若记住当最小元在其他操作期间变化时更新它，那么可以保留最小元的信息并以 \\(\\mathcal{O}(1)\\) 时间执行操作。 合并两个二项队列在概念上很容易，对于两个同高度的二项树可以合并为更高的树，让值大的根成为值小的根的子树。有时合并后，可能出现三棵高度相同的树，在两个队列中各取一棵继续合并即可。直到没有高度相同的树为止，合并结束。 插入可以看作合并的特殊情况，创建一棵单结点树，然后执行合并即可。删除操作由找到具有最小根的二项树来完成，将该树先从森林中移除，删除掉根后，拆解为新的二项队列，最后合并这两个队列即可。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:4:1","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#二项队列操作"},{"categories":["Algorithm⁄DataStructure"],"content":" 二项队列的实现为了保证快速合并，可以按高度大小递减的顺序保存这些二项树的根。而二项树的结点，可以像树一样存储，一个儿子指针域，一个兄弟指针域，和一个元素域。 struct BinomialNode { Comparable element; BinomialNode* child; BinomialNode* sibling; BinomialNode(const Comparable\u0026 e, BinomialNode* c, BinomialNode* s) : element(e), child(c), sibling(s) {} }; class BinomialQueue { private: int size; vector\u003cBinomialNode*\u003e forest; public: BinomialQueue() = default; }; 合并两个二项队列的实现，首先需要确定如何合并两棵同高度的树。 BinomialNode* merge_tree(BinomialNode* t1, BinomialNode* t2) { if (t2-\u003eelement \u003c t1-\u003eelement) { return merge(t2, t1); } t2-\u003esibling = t1-\u003echild; t1-\u003echild = t2; return t1; } 对于实现合并操作，在任意时刻，仅处理高度为 i 的那些树，并且始终从高度最低的树开始像最高的树合并。 // 将二项队列 B 合并到 A，并清空 B void merge(BinomialQueue\u0026 a, BinomialQueue\u0026 b) { a.size += b.size; if (a.size \u003e a.capacity()) { int old_forest_size = a.forest.size(); int new_forest_size = max(old_forest_size, b.forest.size()) + 1; a.forest.resize(new_forest_size); for (int i = old_forest_size; i \u003c new_forest_size; ++i) { a.forest[i] = nullptr; } } BinomialNode* carry = nullptr; for (int i = 0, j = 1; j \u003c= a.size; ++i, j *= 2) { BinomialNode* t1 = a.forest[i]; BinomialNode* t2 = i \u003c b.forest.size() ? b.forest[i] : nullptr; int which_case = t1 == nullptr ? 0 : 1; which_case += t2 == nullptr ? 0 : 2; which_case += carry == nullptr ? 0 : 4; switch (which_case) { case 0: { // no tree [[fallthrough]]; } case 1: { // only t1 break; } case 2: { // only t2 a.forest[i] = t2; b.forest[i] = nullptr; break; } case 3: { // t1 and t2 exist carry = merge_tree(t1, t2); a.forest[i] = b.forest[i] = nullptr; break; } case 4: { // Only carry a.forest[i] = carry; carry = nullptr; break; } case 5: { // t1 and carry exist carry = merge_tree(t1, carry); a.forest[i] = nullptr; break; } case 6: { // t2 and carry exist carry = merge_tree(t2, carry); b.forest[i] = nullptr; break; } case 7: { // all exist a.forest[i] = carry; carry = merge_tree(t1, t2); b.forest[i] = nullptr; break; } } } b.forest.clean(); b.size = 0; } ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/:4:2","series":["数据结构与算法分析"],"tags":["Note","Heap"],"title":"堆结构","uri":"/2021/data_strucures_and_algorithm_analysis_006_heap_structure/#二项队列的实现"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第五章笔记","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/"},{"categories":["Algorithm⁄DataStructure"],"content":" She made a hash of the proper names, to be sure. — Grant Allen ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:0:0","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#"},{"categories":["Algorithm⁄DataStructure"],"content":" 散列函数如果可以将存储的数据，其中某一项用于查找，则这个项被称为 键 (key)，而通过一定规则将键映射到表中的一个合适的单元，这个规则被称为 散列函数 (hash function)。我们希望 hash 足够简单且保证两个不同的 key 映射到不同的单元，但是单元是有限的，因此我们需要寻找一个 hash function 尽量均匀的产生 hash value。当映射不是单射而是多射时，即发生了 冲突 (collision)，有两个不同的 key 经过 hash function 得到了相同的 hash value，我们应该处理这个 collision。 顺便一提，我们一般使用 hash value 对表长进行取模，从而确定数据在表中的位置，表长为素数是可以很好的让 hash value 取模后均匀分布在表中。 我们假设一个简单的字符串 hash function，即将字符串中所有的字符的 ASCII 相加所得到的。如果表很大，就不能很好的平均分配数据。比如 \\(TableSize = 10'007\\) ，并设所有的键长度为 8，而这些键的最大 hash value 不超过 1016 (\\(127 * 8\\))，这显然不是平均分配的。 如果假设 Key 至少有 3 个字符，并且设置一个只考虑前 3 个字符的 hash function： \\(c_{0} + 27 * c_{1} + 27 * 27 * c_{2}\\) 。我们假设键的前三个字符是随机的，表的大小依然是 10007，那么我们就会得到一个均匀分布的 hash value。但是英文实际上并不是随机的，虽然前三个字符有 \\(26^{3} = 16576\\) 种可能的组合，但是事实证明 3 个字母不同的组合数实际上只有 2851 。 再列出最后一个简单的 hash function： \\[\\sum_{i=0}^{length-1}{Key[length-i-1] * 37^{i}}.\\] 这将使用 key 中的所有字符，计算一个关于 \\(37\\) 的多项式。可以发现利用这个 functino 需要修正其值可能为负，因为 hash value 计算过程中可能会存在溢出。 这是一个不错的算法，但并不完美，如果键过长的话可能导致计算时间的增加，你可以使用奇数位字符或者其他方法来优化函数。选定了 hash function，那么接下来的主要问题就是如何解决 collision。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:1:0","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#散列函数"},{"categories":["Algorithm⁄DataStructure"],"content":" 分离链接法分离链接法 (separate chaining) 也被称为拉链法，这种做法是将散列到同一个值的所有元素保存到一个链表中，或者一个 BST 甚至另一个散列表。 我们假定散列表的 装填因子 (load factor) \\(\\lambda\\) 为散列表中的元素个数与散列表大小的比值。执行一次查找操作所需要的工作是：计算散列函数值 (\\(\\mathcal{O}(1)\\)) 并遍历冲突所组成的表。在一次不成功的查找中，要访问的结点数平均为 \\(\\lambda + e^{-\\lambda} \\approx \\lambda\\) ；成功的查找则需要遍历大约 \\(1 + (\\lambda / 2)\\) 个链。 被搜索的表包含了一个存储匹配的结点再加上 0 或更多其他的结点，因此在 N 个元素的表以及 M 个链表中，其他结点的期望个数为 \\((N - 1) / M = \\lambda - 1 / M\\) ，当 M 很大时可以认为这就是 \\(\\lambda\\) 。因此可以发现，散列表的大小并不重要，重要的是元素的 load factor。使用 separate chaining 时，我们往往认为 \\(\\lambda \\approx 1\\) 最好，当 \\(\\lambda \u003e 1\\) 时我们就扩展表并对其中的元素进行 再散列 (rehash)。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:2:0","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#分离链接法"},{"categories":["Algorithm⁄DataStructure"],"content":" 不使用链表的散列表由于 separate chaining 需要使用链表，给新单元分配地址可能需要额外的时间，并且还要求第二种数据结构的实现，因此另一种解决方法是将 collision 发生时尝试选择另一个单元，直到找到空白单元为止。 更正式的表达方式是： 单元 \\(h_{0}(x), h_{1}(x), \\cdots\\) 一次进行尝试，其中 \\(h_{i}(x) = (hash(x) + f(i)) mod Length_{table}\\) 且 \\(f(0) = 0\\) 。函数 \\(f\\) 是冲突解决函数。因为所有的数据都要放在表内，因此所需要的表就远大于 separate chaining 的表。我们将这种方案称之为 探测散列表 (probing hash tables)，而 probing hash 的 \\(\\lambda\\) 一般来说应该低于 \\(0.5\\)。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:3:0","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#不使用链表的散列表"},{"categories":["Algorithm⁄DataStructure"],"content":" 线性探测线性探测时 \\(f\\) 是关于 \\(i\\) 的线性函数，一般情况下 \\(f(i) = i\\) ，即当冲突发生时从下一个单元开始依次测试是否为空单元，直到找到相应空单元为止。 我们以序列 {89, 18, 49, 58, 69} 为例，散列函数为 \\(f(i) = i \\mod 10\\)：前两次插入都是开放单元，直接插入即可；插入 49 时将与 89 发生冲突，将其放入下一开放地址 0 中；插入 58 时将先后与 18、89、49 发生冲突，最终经过三次比较落入开放地址 1 中；等等。 由此可见当表足够大时，总能找到一个开放单元，但需要花费相当多的时间。更糟的是，即使表较为空，这些占据的单元也会形成一些区块，其结果被称为 一次聚集 (primary clustering)。于是，需要尝试多次才能解决冲突。线性探测的预期探测次数对于插入和不成功的查找来说，大约需要 \\(\\frac{1}{2}(1 + 1 / (1 - \\lambda)^{2})\\) ，对于成功的查找来说则是 \\(\\frac{1}{2}(1 + 1 / (1 - \\lambda))\\) 。 在 Weiss 的书中为我们介绍了如何使用积分计算插入时间平均值的方法来估计平均值，并将线性探测的性能与其他更随机的方法的期望性能做了比较，如下图所示。 可以看到如果 \\(\\lambda = 0.75\\) 时，使用线性探测进行插入预计需要 8.5 次探测；如果 \\(\\lambda = 0.9\\) 时，则需要 50 此探测；而 \\(\\lambda = 0.5\\) 时，平均插入操作只需要 2.5 次探测，成功的查找平均需要 1.5 次探测。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:3:1","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#线性探测"},{"categories":["Algorithm⁄DataStructure"],"content":" 平方探测平方探测是消除线性探测中 primary clustering 的解决方法，探测函数 \\(f\\) 是关于 \\(i\\) 的二次函数，一般情况下 \\(f(i) = i^{2}\\) 。 我们以相同的序列为例：当 49 与 89 冲突时，其下一个位置是一个开放单元，49 就被放置在这里；58 先后与 18、89 发生冲突，然后探测位置 \\((8 + 2^{2})\\,mod\\,10\\) ，由于该位置是开放的单元，因此 58 就放在这里。 对于平方探测来说，散列表被近乎填满是一个糟糕的情况：当表被填满超过一半时，若表的大小不是素数，甚至可以在表未被填满一半前，就不能保证找到空单元了。因为最多只有一般的表可以用作被选单元。但是如果表的大小是素数，那么在表至少有一半是空的时候，总能够插入一个新的单元。 如果哪怕表有比一半多一个的位置被填满，那么插入都有可能失败 (虽然这种可能性很小)。另外如果表的大小是素数 (形如 \\(4k + 3\\))，且使用的探测函数为 \\(f(i) = \\pm i^{2}\\) ，那么整个表均可以被探测到。如果不是素数大小的表，那么备选单元的个数会极大的减少。 虽然平方探测排除了 primary clustering，但是散列到同一位置的元素将探测相同的备选单元，这被称为 二次聚集 (secondary clustering)。幸运的是，预期探测次数对于插入和不成功的查找来说，大约需要 \\(1 / (1 - \\lambda)\\) ，对于成功的查找来说则是 \\(-\\frac{1}{\\lambda}\\ln(1 - \\lambda)\\) 。 顺便一提，如果在使用 probing 方案的 hash table 中需要删除一个元素，那么不能直接删除这个元素，而是需要惰性删除，否则可能造成查询操作失败。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:3:2","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#平方探测"},{"categories":["Algorithm⁄DataStructure"],"content":" 双散列双散列 (double hashing) 是准备另一个 hash function，而探测函数 \\(f\\) 是关于 \\(i\\) 与第二个 hash function 的函数，一般情况下 \\(f(i) = i * hash_{2}(x)\\) 。对于这种方法，如果第二个 hash function 选择的并不好，则会有很大的问题，需要保证运算结果不为 0 且所有单元都可以被检测到。诸如 \\(hash_{2}(x) = R - (x\\,mod\\,R)\\) (R 是小于 Len 的素数) 这样的函数可以取得不错的效果。 双散列预期的探测次数与随机冲突解决方法的情形相同，在理论上这十分有吸引力，但备选单元可能提前用完，而平方探测的实现更简单且效率更高。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:3:3","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#双散列"},{"categories":["Algorithm⁄DataStructure"],"content":" 再散列对于 \\(\\lambda\\) 超过一定比例的散列表来说，我们需要对其进行扩容，与顺序实现的现行表类似，一般扩容倍数选择在 1.5 或 2 倍，从采用一个相关的新 hash function。之后需要扫描原始散列表，并计算所有未删除的元素的新散列值，插入到新表中。整个操作过程被称为 再散列 (rehashing)。 rehashing 的操作无疑是昂贵的，它需要 \\(\\mathcal{O}(N)\\) 的时间复杂度。不过幸运的是这并不常发生，而且 rehashing 时表中必然已存在 \\(N / 2\\) 个元素，均摊到每个元素的插入上，基本的开销是一个常数。 当然再散列也可以有不同的策略，最常见的就是在表到达一半时就 rehashing，当然也可以当插入失败时才进行 rehashing。还有一种 途中 (middle-of-the-road) 的策略：当表到达某个 load factor 时进行 rehashing。可以肯定的是，随着 load factor 的增加，表的性能在下降，因此选择一个好的截止点可能是最好的策略。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:4:0","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#再散列"},{"categories":["Algorithm⁄DataStructure"],"content":" 可扩散列在上一篇文章中，讲解了针对将数据存储于磁盘当中的 B 树，现在我们需要思考如何将散列表存储于磁盘当中。我们依然需要考虑的是检索数据时所需的磁盘读取次数。 假设有这样的前提：任意时刻都有 N 个记录要存储，N 随时间变化而变化；最多可以把 M 个记录放入一个磁盘区块。我们假设 \\(M = 4\\) 。 如果使用 separate chaining 或 probing hashing，即使是理想分布的散列表，在一次查询操作中，都可能因冲突而引起对多个区块的访问。当表变得过满时，必须 rehashing 用 \\(\\mathcal{O}(N)\\) 的巨大代价来访问磁盘。 一种聪明的方法是 可扩散列 (extendible hashing)，它允许用两次磁盘访问执行一次查找，插入操作也需要很少的磁盘访问。这种算法与 B 树的思想相当。 ","date":"08-25","objectID":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/:5:0","series":["数据结构与算法分析"],"tags":["Note","HashTable"],"title":"散列表","uri":"/2021/data_strucures_and_algorithm_analysis_005_hash_table/#可扩散列"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第四章后半部分、高级数据结构笔记","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/"},{"categories":["Algorithm⁄DataStructure"],"content":"如果给定一个序列，你将如何在这个序列中查找一个给定元素 target，当找到时返回该元素的迭代器，否则返回末尾迭代器。首先排除时间复杂度 \\(\\mathcal{O}(N)\\) 的朴素算法，这不是本文的重点。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:0:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#"},{"categories":["Algorithm⁄DataStructure"],"content":" 二分查找二分法 (Dichotomy) 是一种思想，将一个整体事物分割成两部分，这两部分必须是互补事件，即所有事物必须属于双方中的一方且互斥。如此我们就可以在 \\(\\mathcal{O}(1)\\) 的时间内将问题大小减半。 二分查找 (binary search)，又称折半查找，这是一种可以在 \\(\\mathcal{O}(\\log_{}{N})\\) 时间复杂度下完成查找的算法。二分查找要求序列必须是有序的，才能正确执行：将序列划分为两部分，如果中间值大于 target，意味着这之后的值都大于 target，需要继续向前找；如果中间值小于 target，意味着这之前的所有值都小于 target，需要继续向后找。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:1:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#二分查找"},{"categories":["Algorithm⁄DataStructure"],"content":" AVL 树上一篇介绍树时分析了 BST 中为什么很容易发生不平衡现象。在极端情况下，只有一个 leaf 的树，在查找元素时其时间复杂度退化为 \\(\\mathcal{O}(N)\\) 。 为了防止 BST 退化为链表，必须保证其可以维持树的平衡，一次需要有一个 平衡条件 (balanced condition)。如果每个结点都要求其左右子树具有相同的高度，显然是不可能的，因为这样实在是太难了。在 1962 年，由苏联计算机科学家 G.M.Adelson-Velsky 和 Evgenii Landis 在其论文 An algorithm for the organization of information 中公开了数据结构 AVL (Adelson-Velsky and Landis) 树，这是计算机科学中 最早被发现的 自平衡二叉树。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:2:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#avl-树"},{"categories":["Algorithm⁄DataStructure"],"content":" AVL 的平衡因子AVL 树将子树的高度限制在差为 1，即一个结点，如果其左子树与由子树的高度差 \\(|D_{h}| \\leq 1\\) ，则认为这棵树是平衡的。因此带有平衡因子 \\(-1\\) 、 \\(0\\) 或 \\(1\\) 的结点被认为是平衡的，而 \\(-2\\) 或 \\(2\\) 的平衡因子被认为是需要调整的。平衡因子可以直接存储于结点之中，也可以利用存储在结点中的子树高度计算得出。 简单地计算，一棵 AVL 树的高度最多为 \\(1.44 \\log_{}{(N + 2)} - 1.328\\) ，实际上的高度只比 \\(\\log_{}{N}\\) 稍微多一些。一棵高度为 \\(h\\) 的 AVL 树，其最少结点数 \\(S(h) = S(h - 1) + S(h - 2) + 1\\) ，\\(S(0)=1, S(1) = 2\\) 。而 AVL 的所有操作均可以在 \\(\\mathcal{O}(\\log_{}{N})\\) 复杂度下完成。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:2:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#avl-的平衡因子"},{"categories":["Algorithm⁄DataStructure"],"content":" AVL 的插入操作在进行插入操作时，和普通的 BST 类似，但是不一样的是需要更新路径上所有结点的平衡信息，并插入完成后有可能破坏 AVL 的特性。如果特性被破坏后，需要恢复平衡才能算插入结束。实际上，总可以通过简单的操作进行修正，这种操作被称为 旋转 (rotation)。 将必须重新平衡的结点叫作 \\(\\alpha\\) ，由于任意结点最多有两个孩子，因此高度不平衡时， \\(\\alpha\\) 点的两棵子树的高度差 2。这种不平衡可能出现在下面 4 中情况中： 对 \\(\\alpha\\) 的左孩子的左子树进行插入 对 \\(\\alpha\\) 的左孩子的右子树进行插入 对 \\(\\alpha\\) 的右孩子的左子树进行插入 对 \\(\\alpha\\) 的右孩子的右子树进行插入 情况 1 和 4 关于结点 \\(\\alpha\\) 镜像对称，情况 2 和 3 关于结点 \\(\\alpha\\) 镜像对称。因此从逻辑上来讲，我们只需要考虑两种情况，而编程时需要考虑上面介绍到的所有 4 种情况。 单旋转情况 1 是插入发生在「外边」的情形，我们称之为 一字形 (zig-zig)，可以用 单旋转 (single rotation) 解决。假设结点 \\(n\\) 不满足 AVL 平衡性质，因为其左子树比右子树深 2 层，可以对其进行单旋转修正。修正的过程是：将左子树的根 \\(l\\) 向上移动一层，而将 \\(n\\) 向下移动一层， \\(n\\) 作为 \\(l\\) 的孩子出现在树中。下图展示了插入后出现不平衡的结点 (红色) 、如何旋转、多余子树如何处理以及子树的层数 (蓝字)。 对应的情况 4 也是 zig-zig，只需要旋转的方向与操作相镜像即可处理。 双旋转对于情况 2 、 3 来说，插入在「树内」从而导致 AVL 树无效，这种情况被称为 之字形 (zig-zag)，而子树太深通过 single rotation 无法让树平衡，解决这种内部的情形需要 * 双旋转* (double rotation) 解决。 对应的情况 3 也是 zig-zag，只需要旋转的方向与操作相镜像即可处理。 对 AVL 树插入的总结可以发现，无论单旋转与双旋转，它都由两个最基本的操作组成：将结点进行左旋 (left rotation) 或右旋 (right rotation)，并将多余的一棵子树挂载到下降结点上。Wikipedia 用以下这幅图概括了 4 种情况。 // 左旋 void rotate_left(Node* node) { Node* child = node-\u003eright; node-\u003eright = child-\u003eleft; if (child-\u003eleft != nullptr) { child-\u003eleft-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eleft = node; node-\u003eparent = child; return child; } // 右旋 void rotate_right(Node* node) { Node* child = node-\u003eleft; node-\u003eleft = child-\u003eright; if (child-\u003eright != nullptr) { child-\u003eright-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eright = node; node-\u003eparent = child; return child; } 在进行编程时，可以首先定义左右旋这两种基本操作，在根据情况判断如何组合。对于编程细节，远比理论多得多，编写正确的 loop 算法相对于 recursion 并不是一件容易的事，因此更多的会使用 recursion 进行实现。 还有一个重要问题是如何高效的对高度信息进行存储，可以采用平衡因子作为存储而不是一个 int 类型的高度，或者更近一步，利用 2 bit 存储平衡因子 (毕竟只有 3 个状态)。如果你希望将其隐藏到指针中，也是个不错的选择。存储平衡因子将得到些许速度优势，但丧失了简明性，如果你使用隐藏于指针的方法，更加剧的这一问题，不过好消息是你能为此剩下不少内存空间。最后，以 Wikipedia 上一副构建 AVL 树的动图作为本小节的结束吧。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:2:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#avl-的插入操作"},{"categories":["Algorithm⁄DataStructure"],"content":" AVL 的插入操作在进行插入操作时，和普通的 BST 类似，但是不一样的是需要更新路径上所有结点的平衡信息，并插入完成后有可能破坏 AVL 的特性。如果特性被破坏后，需要恢复平衡才能算插入结束。实际上，总可以通过简单的操作进行修正，这种操作被称为 旋转 (rotation)。 将必须重新平衡的结点叫作 \\(\\alpha\\) ，由于任意结点最多有两个孩子，因此高度不平衡时， \\(\\alpha\\) 点的两棵子树的高度差 2。这种不平衡可能出现在下面 4 中情况中： 对 \\(\\alpha\\) 的左孩子的左子树进行插入 对 \\(\\alpha\\) 的左孩子的右子树进行插入 对 \\(\\alpha\\) 的右孩子的左子树进行插入 对 \\(\\alpha\\) 的右孩子的右子树进行插入 情况 1 和 4 关于结点 \\(\\alpha\\) 镜像对称，情况 2 和 3 关于结点 \\(\\alpha\\) 镜像对称。因此从逻辑上来讲，我们只需要考虑两种情况，而编程时需要考虑上面介绍到的所有 4 种情况。 单旋转情况 1 是插入发生在「外边」的情形，我们称之为 一字形 (zig-zig)，可以用 单旋转 (single rotation) 解决。假设结点 \\(n\\) 不满足 AVL 平衡性质，因为其左子树比右子树深 2 层，可以对其进行单旋转修正。修正的过程是：将左子树的根 \\(l\\) 向上移动一层，而将 \\(n\\) 向下移动一层， \\(n\\) 作为 \\(l\\) 的孩子出现在树中。下图展示了插入后出现不平衡的结点 (红色) 、如何旋转、多余子树如何处理以及子树的层数 (蓝字)。 对应的情况 4 也是 zig-zig，只需要旋转的方向与操作相镜像即可处理。 双旋转对于情况 2 、 3 来说，插入在「树内」从而导致 AVL 树无效，这种情况被称为 之字形 (zig-zag)，而子树太深通过 single rotation 无法让树平衡，解决这种内部的情形需要 * 双旋转* (double rotation) 解决。 对应的情况 3 也是 zig-zag，只需要旋转的方向与操作相镜像即可处理。 对 AVL 树插入的总结可以发现，无论单旋转与双旋转，它都由两个最基本的操作组成：将结点进行左旋 (left rotation) 或右旋 (right rotation)，并将多余的一棵子树挂载到下降结点上。Wikipedia 用以下这幅图概括了 4 种情况。 // 左旋 void rotate_left(Node* node) { Node* child = node-\u003eright; node-\u003eright = child-\u003eleft; if (child-\u003eleft != nullptr) { child-\u003eleft-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eleft = node; node-\u003eparent = child; return child; } // 右旋 void rotate_right(Node* node) { Node* child = node-\u003eleft; node-\u003eleft = child-\u003eright; if (child-\u003eright != nullptr) { child-\u003eright-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eright = node; node-\u003eparent = child; return child; } 在进行编程时，可以首先定义左右旋这两种基本操作，在根据情况判断如何组合。对于编程细节，远比理论多得多，编写正确的 loop 算法相对于 recursion 并不是一件容易的事，因此更多的会使用 recursion 进行实现。 还有一个重要问题是如何高效的对高度信息进行存储，可以采用平衡因子作为存储而不是一个 int 类型的高度，或者更近一步，利用 2 bit 存储平衡因子 (毕竟只有 3 个状态)。如果你希望将其隐藏到指针中，也是个不错的选择。存储平衡因子将得到些许速度优势，但丧失了简明性，如果你使用隐藏于指针的方法，更加剧的这一问题，不过好消息是你能为此剩下不少内存空间。最后，以 Wikipedia 上一副构建 AVL 树的动图作为本小节的结束吧。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:2:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#单旋转"},{"categories":["Algorithm⁄DataStructure"],"content":" AVL 的插入操作在进行插入操作时，和普通的 BST 类似，但是不一样的是需要更新路径上所有结点的平衡信息，并插入完成后有可能破坏 AVL 的特性。如果特性被破坏后，需要恢复平衡才能算插入结束。实际上，总可以通过简单的操作进行修正，这种操作被称为 旋转 (rotation)。 将必须重新平衡的结点叫作 \\(\\alpha\\) ，由于任意结点最多有两个孩子，因此高度不平衡时， \\(\\alpha\\) 点的两棵子树的高度差 2。这种不平衡可能出现在下面 4 中情况中： 对 \\(\\alpha\\) 的左孩子的左子树进行插入 对 \\(\\alpha\\) 的左孩子的右子树进行插入 对 \\(\\alpha\\) 的右孩子的左子树进行插入 对 \\(\\alpha\\) 的右孩子的右子树进行插入 情况 1 和 4 关于结点 \\(\\alpha\\) 镜像对称，情况 2 和 3 关于结点 \\(\\alpha\\) 镜像对称。因此从逻辑上来讲，我们只需要考虑两种情况，而编程时需要考虑上面介绍到的所有 4 种情况。 单旋转情况 1 是插入发生在「外边」的情形，我们称之为 一字形 (zig-zig)，可以用 单旋转 (single rotation) 解决。假设结点 \\(n\\) 不满足 AVL 平衡性质，因为其左子树比右子树深 2 层，可以对其进行单旋转修正。修正的过程是：将左子树的根 \\(l\\) 向上移动一层，而将 \\(n\\) 向下移动一层， \\(n\\) 作为 \\(l\\) 的孩子出现在树中。下图展示了插入后出现不平衡的结点 (红色) 、如何旋转、多余子树如何处理以及子树的层数 (蓝字)。 对应的情况 4 也是 zig-zig，只需要旋转的方向与操作相镜像即可处理。 双旋转对于情况 2 、 3 来说，插入在「树内」从而导致 AVL 树无效，这种情况被称为 之字形 (zig-zag)，而子树太深通过 single rotation 无法让树平衡，解决这种内部的情形需要 * 双旋转* (double rotation) 解决。 对应的情况 3 也是 zig-zag，只需要旋转的方向与操作相镜像即可处理。 对 AVL 树插入的总结可以发现，无论单旋转与双旋转，它都由两个最基本的操作组成：将结点进行左旋 (left rotation) 或右旋 (right rotation)，并将多余的一棵子树挂载到下降结点上。Wikipedia 用以下这幅图概括了 4 种情况。 // 左旋 void rotate_left(Node* node) { Node* child = node-\u003eright; node-\u003eright = child-\u003eleft; if (child-\u003eleft != nullptr) { child-\u003eleft-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eleft = node; node-\u003eparent = child; return child; } // 右旋 void rotate_right(Node* node) { Node* child = node-\u003eleft; node-\u003eleft = child-\u003eright; if (child-\u003eright != nullptr) { child-\u003eright-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eright = node; node-\u003eparent = child; return child; } 在进行编程时，可以首先定义左右旋这两种基本操作，在根据情况判断如何组合。对于编程细节，远比理论多得多，编写正确的 loop 算法相对于 recursion 并不是一件容易的事，因此更多的会使用 recursion 进行实现。 还有一个重要问题是如何高效的对高度信息进行存储，可以采用平衡因子作为存储而不是一个 int 类型的高度，或者更近一步，利用 2 bit 存储平衡因子 (毕竟只有 3 个状态)。如果你希望将其隐藏到指针中，也是个不错的选择。存储平衡因子将得到些许速度优势，但丧失了简明性，如果你使用隐藏于指针的方法，更加剧的这一问题，不过好消息是你能为此剩下不少内存空间。最后，以 Wikipedia 上一副构建 AVL 树的动图作为本小节的结束吧。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:2:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#双旋转"},{"categories":["Algorithm⁄DataStructure"],"content":" AVL 的插入操作在进行插入操作时，和普通的 BST 类似，但是不一样的是需要更新路径上所有结点的平衡信息，并插入完成后有可能破坏 AVL 的特性。如果特性被破坏后，需要恢复平衡才能算插入结束。实际上，总可以通过简单的操作进行修正，这种操作被称为 旋转 (rotation)。 将必须重新平衡的结点叫作 \\(\\alpha\\) ，由于任意结点最多有两个孩子，因此高度不平衡时， \\(\\alpha\\) 点的两棵子树的高度差 2。这种不平衡可能出现在下面 4 中情况中： 对 \\(\\alpha\\) 的左孩子的左子树进行插入 对 \\(\\alpha\\) 的左孩子的右子树进行插入 对 \\(\\alpha\\) 的右孩子的左子树进行插入 对 \\(\\alpha\\) 的右孩子的右子树进行插入 情况 1 和 4 关于结点 \\(\\alpha\\) 镜像对称，情况 2 和 3 关于结点 \\(\\alpha\\) 镜像对称。因此从逻辑上来讲，我们只需要考虑两种情况，而编程时需要考虑上面介绍到的所有 4 种情况。 单旋转情况 1 是插入发生在「外边」的情形，我们称之为 一字形 (zig-zig)，可以用 单旋转 (single rotation) 解决。假设结点 \\(n\\) 不满足 AVL 平衡性质，因为其左子树比右子树深 2 层，可以对其进行单旋转修正。修正的过程是：将左子树的根 \\(l\\) 向上移动一层，而将 \\(n\\) 向下移动一层， \\(n\\) 作为 \\(l\\) 的孩子出现在树中。下图展示了插入后出现不平衡的结点 (红色) 、如何旋转、多余子树如何处理以及子树的层数 (蓝字)。 对应的情况 4 也是 zig-zig，只需要旋转的方向与操作相镜像即可处理。 双旋转对于情况 2 、 3 来说，插入在「树内」从而导致 AVL 树无效，这种情况被称为 之字形 (zig-zag)，而子树太深通过 single rotation 无法让树平衡，解决这种内部的情形需要 * 双旋转* (double rotation) 解决。 对应的情况 3 也是 zig-zag，只需要旋转的方向与操作相镜像即可处理。 对 AVL 树插入的总结可以发现，无论单旋转与双旋转，它都由两个最基本的操作组成：将结点进行左旋 (left rotation) 或右旋 (right rotation)，并将多余的一棵子树挂载到下降结点上。Wikipedia 用以下这幅图概括了 4 种情况。 // 左旋 void rotate_left(Node* node) { Node* child = node-\u003eright; node-\u003eright = child-\u003eleft; if (child-\u003eleft != nullptr) { child-\u003eleft-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eleft = node; node-\u003eparent = child; return child; } // 右旋 void rotate_right(Node* node) { Node* child = node-\u003eleft; node-\u003eleft = child-\u003eright; if (child-\u003eright != nullptr) { child-\u003eright-\u003eparent = node; } child-\u003eparent = node-\u003eparent; child-\u003eright = node; node-\u003eparent = child; return child; } 在进行编程时，可以首先定义左右旋这两种基本操作，在根据情况判断如何组合。对于编程细节，远比理论多得多，编写正确的 loop 算法相对于 recursion 并不是一件容易的事，因此更多的会使用 recursion 进行实现。 还有一个重要问题是如何高效的对高度信息进行存储，可以采用平衡因子作为存储而不是一个 int 类型的高度，或者更近一步，利用 2 bit 存储平衡因子 (毕竟只有 3 个状态)。如果你希望将其隐藏到指针中，也是个不错的选择。存储平衡因子将得到些许速度优势，但丧失了简明性，如果你使用隐藏于指针的方法，更加剧的这一问题，不过好消息是你能为此剩下不少内存空间。最后，以 Wikipedia 上一副构建 AVL 树的动图作为本小节的结束吧。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:2:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#对-avl-树插入的总结"},{"categories":["Algorithm⁄DataStructure"],"content":" AVL 的移除操作AVL 树的移除与 BST 相当，同样地，移除操作可能会破坏 AVL 特性，因此我们在移除元素后，同样需要对树进行平衡才能算操作完成。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#avl-的移除操作"},{"categories":["Algorithm⁄DataStructure"],"content":" 伸展树或许你没有听过这种数据结构，但是它确实存在且就在这里。 伸展树 (splay tree) 是一种相对简单的数据结构，它保证从空树开始任意连续 M 次对树的操作最多花费 \\(\\mathcal{O}(M \\log_{}{N})\\) 的时间。这种保证并不排除单次花费 \\(\\mathcal{O}(N)\\) 时间的可能，并且也不保证每次操作的最坏情况都是 \\(\\mathcal{O}(\\log_{}{N})\\) 的，不过好在实际上是一样的。当 M 次操作的序列总的最坏情况花费 \\(\\mathcal{O}(Mf(N))\\) 时间，我们就说它的 摊还 (amortized) 运行时间为 \\(\\mathcal{O}(f(N))\\) 的。因此 splay tree 的每次操作的摊还时间复杂度为 \\(\\mathcal{O}(\\log_{}{N})\\) 。 当然 splay tree 有着一个事实基础：对于 BST 来说，每次操作最坏情形花费 \\(\\mathcal{O}(N)\\) 并非不好，只要它相对不常发生就行。为了保证 amortized，splay tree 进行一个结点访问就会发生移动操作，它要经过一系列旋转操作被推到 root 上。但是当这个结点过深时，重新将这棵树构造为平衡树可能会花费比旋转更少的时间。 实际上，从 splay tree 中也可以看出局部性思想，当一个结点被访问时，它与其附近的结点可能会被频繁的访问，而远端的结点可能很难被访问到。当抖动发生时，splay tree 将会面临非常巨大的开销，因为它总是需要调整较深层的结点。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:3:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#伸展树"},{"categories":["Algorithm⁄DataStructure"],"content":" 简单的旋转在实现 splay tree 时，访问一个结点并将其推向 root 时，最简单的方法就是 单旋转 ，即该结点与其父结点实施旋转操作。很明显这可以将一个很深的叶结点推向 root，但其父结点也被推向了与其差不多的深度。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:3:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#简单的旋转"},{"categories":["Algorithm⁄DataStructure"],"content":" 伸展伸展 (splaying) 的方法类似于旋转，不过在从底向上的旋转上，有些其他操作。 令 X 是访问路径上的非根结点，在这个路径上实现旋转。如果 X 的 parent 是树根只需要旋转 X 与其 parent 即可；否则需要考虑如下情况： 之字形 (zig-zag)：只需要像 AVL 一样执行双旋转 一字形 (zig-zig)：将 X 作为根，父结点和祖父结点分别是其孩子的结点，多余子结点向上过继 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:3:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#伸展"},{"categories":["Algorithm⁄DataStructure"],"content":" B树我们一直认为始终将数据存储于 RAM 中，而不是磁盘中，但如果数据太大以至于不得不放在磁盘时，大 \\(\\mathcal{O}\\) 模型将不再适用。因为 \\(\\mathcal{O}\\) 认为所有操作是相等的，但是涉及磁盘 IO 时，它的代价实在太高了。绝大多数情况下控制运行时间的是磁盘访问次数，因此我们更愿意一次从磁盘中取出大量数据并进行大量计算。 假设你现在将一棵 1000 万个结点的 BST 存储于磁盘之上，显然一棵不平衡的 BST 可能让你访问磁盘 1000 万次；如果你的存储结构是 AVL 树，那会好很多，绝大多数情况下你只需要 \\(\\log_{}{N}\\) 而非 \\(1.44 \\log_{}{N}\\) 次磁盘访问，这大概是 25 次磁盘访问。如果可以将这 1000 万结点的树，压缩到一个非常小的常数，哪怕是一个非常复杂的数据结构，对于 CPU 来说也是不成问题的。显然二叉树并不是一个好的选择，存储层数最低的完全二叉树的高度是 \\(\\log_{}{N}\\) 。如果我们增加树的 degree，那么树的分支会极大增加，而深度却会急剧减少。这样的 M 路分支树被称为 M叉查找树 (M-ary search tree)，可以明确的是 compelete M-ary tree 的高度是 \\(\\log_{M}{N}\\) 。 为了防止 M-ary tree 退化，我们会为其加入更加严格的平衡条件，以保证其它的平衡。 1970 年 Rudolf Bayer 与 Edward M. McCreight 在波音研究实验室 (Boeing Research Labs) 发现了自平衡的 B树 (B-tree)。不过不像其他结构那样，两位作者都没有给出这里 B 的含义，你可以认为是 Balanced、Bayer 甚至是 Boeing，不过 Knuth 在 1980 年发表的论文 CS144C classroom lecture about disk storage and B-trees 中推测其中的含义可能是后两种。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:4:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#b树"},{"categories":["Algorithm⁄DataStructure"],"content":" B树的相关术语与定义实际上对于 B 树的相关定义与术语并不统一，阶 (order) 被 Knuth 定义为最大数量的子节点 (即最大数量的键加一)。Bayer 认为叶子层是最下面一层的键，而 Knuth 认为叶子层是最下面一层键之下的一层。而在实现上，叶子可能保留了完整的数据记录，也可能只保留了指向完整数据记录的指针。 根据 Knuth 的定义，一个 m 阶 B 树具有以下属性： 每个结点最多有 m 个子结点 除根外的所有内部结点最少有 \\(\\lceil\\frac{m}{2}\\rceil\\) 个子结点 如果根结点不是叶结点，那么它至少有两个子结点 有 k 个子结点的非叶子结点拥有 \\(k - 1\\) 个键 所有的叶子结点都在同一层 我们定义一个结点最少拥有的子结点数 \\(L = \\lceil\\frac{m}{2}\\rceil\\) ，而最多拥有的子结点数 \\(U = m\\) ；而包含的元素的个数最少 \\(\\lfloor\\frac{m}{2}\\rfloor\\) 个，最多 \\(m - 1\\) 个。 根结点 拥有子结点数量的上限与内部结点相同，但没有下限。当树的元素数量小于 \\(L - 1\\) 时，根结点是唯一结点且没有任何子结点 叶子结点 没有子结点或指向子结点的指针，当然能存储的最大元素数依然是 \\(m-1\\) 内部结点 除叶结点与根结点外的所有结点，它们通常被表示为一组有序的元素和指向子结点的指针。每一个内部结点含有的子结点范围为 \\([L, U]\\) ，含有元素的数量在 \\([L - 1, U - 1]\\) ，而 \\(U = 2L\\) 或 \\(U = 2L - 1\\) ，因此所有内部结点至少是半满的 一个深度为 \\(n+1\\) 的B树可以容纳的元素数量大约是深度为 n 的B树的 U 倍，但是搜索、插入和删除操作的开销也会增加，当然开销的增加速度是极为缓慢的。B树在每一个节点中都存储值，所有的节点有着相同的结构。然而，因为叶子节点没有子节点，所以可以通过使用专门的结构来提高B树的性能。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:4:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#b树的相关术语与定义"},{"categories":["Algorithm⁄DataStructure"],"content":" B树的插入操作对于所有平衡树来说，其都是建立在 BST 的基础之上，因此插入一个元素时都需要从根结点开始，找到新元素应该被添加的位置。当找到要插入的结点时，将会有以下情况： 如果结点拥有元素数量小于最大值，那么有空间容纳新元素，将元素插入到这一结点，并保持结点中的元素有序 否则这个结点已满，将它平均分裂成两个结点： 从该结点的原有元素和新元素中选择出中位数 小于这一中位数的元素放入左边结点，大于的元素放入右边结点 中位数元素将会被插入到父结点中，插入过程以同样的方法递归向上进行，直到元素被插入到树中。如果最终插入根结点但其已满，选出中位数作为新的根，将根结点分裂为两个结点作为新根的子结点 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:4:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#b树的插入操作"},{"categories":["Algorithm⁄DataStructure"],"content":" B树的移除操作由于 B 树也是一种 BST，因此移除非叶结点时，也先将其交换到叶结点之后再进行移除操作。因此重点是将移除叶结点，然后调整树的约束条件使其满足。 移除B树叶子结点中的元素，发生下溢出不满足B树约束时，进行再平衡 由于内部结点的元素是左右两个子树的中间值，因此需要合并这两个子树，但可以肯定的是，左子树的所有元素依然小于右子树中的所有元素。 选择一个新的中间值 (左子树的最大值或右子树的最小值)，将其替换掉被移除的元素，需要注意的是贡献新中间值的结点为叶子结点 判断贡献中间值的叶结点是否满足约束，如果不满足则从该叶子开始再平衡 可以发现，所有需要再平衡的结点都是从叶结点开始的，并向根结点进行，直到树重新平衡 如果缺少元素结点的右兄弟存在且拥有多余的元素，那么进行左旋 将中间值复制到左子结点的最后 将中间值替换为右子结点的最小元素，并从右子结点中移除该元素 树已再次平衡，结束操作 否则，如果缺少元素结点的左兄弟存在且拥有多余的元素，那么进行右旋 将中间值复制到右子结点的开始 将中间值替换为左子结点的最小元素，并从左子结点中移除该元素 树已再次平衡，结束操作 否则，将其与一个直接兄弟结点以及中间值进行合并 将中间值、左子结点、右子结点都合并到一个结点上 (假设为左子结点)，并将中间值和右子结点从父结点中移除 判断当前父结点的情况 如果父结点是根且父结点中没有其他元素，则将合并之后的结点作为新的根 如果父结点不是根，且父结点满足约束，则树已再次平衡，结束操作 如果父结点不满足内部结点的要求，对父结点进行再平衡操作。如果有多余的子树，则将这棵子树旋转给不平衡结点即可 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:4:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#b树的移除操作"},{"categories":["Algorithm⁄DataStructure"],"content":" B+树B+树 (B Plus Tree) 是 B Tree 的一个变种。有人将 B-Tree 读作 B减树 是不正确的， B-Tree 中的 - 是一个连字符。既然是变种，那就有差异： 内部结点不再存储数据，只对其键进行存储，用以查找叶结点，数据全部由叶子结点存储 叶结点之间采用指针相连，你可以顺序从叶结点的头部遍历到尾部，而不需要其他额外的操作 如何选取键作为父结点的元素可以快速查找子结点，最简单的方法就是选取左子结点的最大值或右子结点的最小值，一般根据实现进行选择。 对 B+ 树操作时，与 B 树几乎一致，但区别是 B+ 树需要从叶结点开始递归向上，且 B+ 树需要修改父结点的分割值，而 B 树不用。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:4:4","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#b-plus-树"},{"categories":["Algorithm⁄DataStructure"],"content":" 红黑树红黑树 (red-black tree) 是一种自平衡二叉树，于 1972 年由 Rudolf Bayer 发明，发明时被称为 对称二叉 B 树，现代名称红黑树来自 Knuth 的博士生 Robert Sedgewick 于 1978 年发表的论文。红黑树的结构复杂，但操作有着良好的最坏情况运行时间：它可以在 \\(\\mathcal{O}(\\log_{}{N})\\) 时间内完成查找、插入和删除操作。 红黑树是具有下列着色性质的 BST： 每个结点要么是黑色要么是红色 根是黑色的 如果一个结点是红色的，那么它的子结点必须是黑色的 从一个结点到一个 NULL 指针的每一条路径都必须包含相同数目的黑色结点 根据着色规则，red-black tree 高度最多是 \\(2\\log_{}(N+1)\\) ，因此查找保证是一种对数的操作。当然还有一条约定，空结点 nullptr 我们假设其为黑色，这样我们可以在不违反约定的情况下，方便操作。 通常困难在于将一个插入一个新结点后，如果将结点涂为黑色将违反性质 4，因为这会让路径上的黑色结点数量加一，但其他路径上黑色结点数量不变。 因此在插入结点时，默认结点为红色，父结点为黑色时，直接插入。在以下的情况中不再讨论这种情况。父结点是红色则会违反规则 3，在这种情况下必须修改树以满足所有性质。 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:5:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#红黑树"},{"categories":["Algorithm⁄DataStructure"],"content":" 红黑树的自底向上插入如果新插入的结点 X 是红色的，它有父结点 P，兄弟结点 S，叔父结点 U，以及祖父结点 G。那么需要考虑几种情况： 如果 P、U 都是红色的，意味着 G 是黑色的，可以将 P、U 重绘为黑色将 G 重绘为红色，这样既不会违反规则 3 也不会违反规则 4。但是 G 之上的情况我们不知道，G 也可能是根，因此需要对 G 进行递归地向上进行重绘操作 如果 P 与 U 只有一个是红色的，意味着 G 是黑色的，而插入的 X 虽然不违反规则 4 但是违反了规则 3，迫使 X 或 P 变为黑色，而这样又会违反规则 4。为了让树再次符合要求，我们对其需要进行旋转操作并重绘结点颜色，其实这里的旋转操作与 AVL 树中是一致的，只是将结点的平衡因子转换为了颜色信息。 a. 当 X、P、G 形成 zig-zig 时，我们采用 single rotation b. 当 X、P、G 形成 zig-zag 时，我们采用 double rotation // 该函数仅处理父结点是红色的情况，黑色情况则直接插入即可 // 使用头结点方便处理，头结点的 parent 指向 root，root 的 parent 指向 head void insert_help(Node* node, Node* head) { // 当结点不是树的根或父结点是红色时，进行循环 while (node != head-\u003eparent \u0026\u0026 node-\u003eparent-\u003etag == RED) { Node* uncle = get_uncle(node); Node* grandparent = get_grandparent(node); Node* parent = node-\u003eparent; // 如果叔父结点不为空且为红色，符合情况 1 if (uncle != nullptr \u0026\u0026 uncle-\u003etag == RED) { parent-\u003etag = uncle-\u003etag = BLACK; grandparent-\u003etag = RED; node = grandparent; continue; } // 判断 zig-zig 或 zig-zag 类型，进行相应的旋转 if (parent == grandparent-\u003eleft) { if (node == parent-\u003eright) { // l-r 的 zig-zag node = parent; parent = rotate_left(parent); } rotate_right(grandparent); // l-l 的 zig-zig } else { if (node == node-\u003eparent-\u003eleft) { // r-l 的 zig-zag node = parent; parent = rotate_right(parent); } ratate_left(grandparent); // r-r 的 zig-zig } grandparent-\u003etag = RED; parent-\u003etag = BLACK; } head-\u003eparent-\u003etag = BLACK; } ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:5:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#红黑树的自底向上插入"},{"categories":["Algorithm⁄DataStructure"],"content":" 红黑树的自顶向下插入自底向上的操作需要父指针或栈保存路径，而自顶向下时实际是对红黑树应用自顶向下保证 S 不会时红色的过程。 在向下的过程中，如果结点 N 有两个红色的孩子时，将孩子重绘为黑色，结点 N 重绘为红色。结点 N 与其父结点 P 都为红色时，将违反红黑树的着色性质，此时对其进行 zig-zig 或 zig-zag 旋转即可。至于叔父结点 U 在自顶向下的过程中排除了红色的可能。 // 自顶向下插入，value 是待插入的值 void insert(Node* node, Node* head, T\u0026 value, Node** pos = nullptr) { bool inserted = false; if (node == nullptr) { // 插入结点，默认为红色 node = new Node(value); *pos = node; inserted = true; } if (node-\u003eleft != nullptr \u0026\u0026 node-\u003eleft-\u003etag == RED \u0026\u0026 node-\u003eright != nullptr \u0026\u0026 node-\u003eright-\u003etag == RED) { node-\u003eleft-\u003etag = node-\u003eright-\u003etag = BLACK; node-\u003etag = RED; } head-\u003eparent-\u003etag = BLACK; Node* gp = get_grandparent(node); Node* parent = node-\u003eparent; if (node-\u003etag == RED \u0026\u0026 parent-\u003etag == RED) { // 判断 zig-zig 或 zig-zag 类型，进行相应的旋转 if (parent == gp-\u003eleft) { if (node == parent-\u003eright) { // l-r 的 zig-zag parent = rotate_left(parent); } rotate_right(gp); // l-l 的 zig-zig } else { if (node == parent-\u003eleft) { // r-l 的 zig-zag parent = rotate_right(parent); } ratate_left(gp); // r-r 的 zig-zig } gp-\u003etag = RED; parent-\u003etag = BLACK; } if (inserted) { return; } if (node-\u003eval \u003c value) { insert(node-\u003eleft, head, \u0026node-\u003eleft, value); } else { insert(node-\u003eright, head, \u0026node-\u003eright, value); } } ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:5:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#红黑树的自顶向下插入"},{"categories":["Algorithm⁄DataStructure"],"content":" 红黑树的自顶向下删除删除结点时，所有情况都可以归结于删除一个叶结点，因为删除带有两个孩子的结点，都可以与其左子树最大结点或右子树最小结点的值进行交换，这只改变了值没有改变颜色，并不影响红黑树的性质。之后删除交换后的叶结点即可。对于红色叶结点，我们可以将其直接删除，这不影响红黑树的结构，如果有孩子我们只需要用其孩子代替它即可。如此我们需要保证在自顶向下过程中保证叶结点是红色的。 假设当前结点是 N，其兄弟结点 S、父结点 P、叔父结点 U 和祖父结点 G。开始时需要将树根重涂为红色，沿树向下遍历，当到达一个结点时，确保 P 是红色、N 和 S 是黑色。在此过程中会遇到一些情况： N 有两个黑色的孩子，此时 a. S 也有两个黑色的孩子，那么重涂反转 N、S、P 的颜色，树结构不变 b. S 有红色的孩子，根据红色的孩子进行 signal rotate 或 double rotate。如果两个孩子都是红色，任选一个进行旋转即可 N 有红色的孩子，此时向下递归 a. 新的 N 是红色，继续递归 b. 新的 N 是黑色，对 S 和 P 进行旋转，S 成为 P 的父结点，重绘 P 与 S 的颜色，即可得到红色的父结点 P。对于 P 来说，回到情况 1 ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:5:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#红黑树的自顶向下删除"},{"categories":["Algorithm⁄DataStructure"],"content":" AA 树二叉B树 (Binary B-tree) 是一种简单但颇有竞争力的实现，被称为 BB 树，可以理解为带有附加条件的红黑树：一个结点最多有一个红色的孩子。 当然还有一些法则： 只有右孩子可以是红色的，这样总可以使用内部结点的右子树的最小结点代替该结点 递归地编写过程 信息存储在整数中，而不是 bit 与每个结点一起存储。这个信息主要是结点的层次信息 a. 若是 1，则该结点是叶结点 b. 是父结点的层次，则该结点是红色的 c. 比父结点的层次少 1，该结点是黑色的 简单地，我们就可以得到一颗 AA 树。并且左孩子必然比其父结点低一个层次，右孩子可能比父结点低 0 或 1 个层次，但不会更多。 struct AANode { Comparable element; AANode* left; AANode* right; int level; AANode() : left(nullptr), right(nullptr), level(1) {} AANode(const Comparable\u0026 e) : element(e), left(nullptr), right(nullptr), level(1) {} }; ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:6:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#aa-树"},{"categories":["Algorithm⁄DataStructure"],"content":" AA树的插入操作水平链接 (horizontal link) 是一个结点与同层次上的孩子所建立的链接，这种水平链接都是右链接，并且不含有两个连续的水平链接。 不过这有一些情况需要注意，插入新结点时，可能导致左水平链 (插入 2) 或连续两个右水平链 (插入 45)。通过右旋可以消除掉左水平链，而左旋则可以消除多余的右水平链，这两个过程分别称为 skew 和 split。 由于新建了一个左水平结点或连续右水平结点，会引起结点 N 的原始父结点 P 的一些问题，这些问题可以通过上滤 skew/split 的方法解决。 void skew(AANode* node) { if (node-\u003eleft-\u003elevel == node-\u003elevel) { rotate_right(node); } } void split(AANode* node) { if (node-\u003eright-\u003eright-\u003elevel == node-\u003elevel) { rotate_left(node); ++node-\u003elevel; } } 有了上述基础，AA 树的插入操作可以说是相当简单，和非平衡 BST 的插入实现基本一致。 void insert(const Comparable\u0026 val, AANode*\u0026 root) { root == nullptr ? static_cast\u003cvoid\u003e(root = new AANode(val)) : insert(val, val \u003c root-\u003eelement ? root-\u003eleft : root-\u003eright); skew(root); split(root); } ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:6:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#aa树的插入操作"},{"categories":["Algorithm⁄DataStructure"],"content":" AA树的删除操作删除操作对比插入操作就会复杂许多。但是值得注意的是，我们为了编程更加容易增添了一些法则，这些法则为我们去除了一些特殊情况。可以肯定，若结点不是叶结点，那么它一定有右结点，那么在删除操作时，总可以使用右子树上最小的孩子替代这个结点，保证它是在第一层。在递归过程中，非叶结点的层次可能会被破坏，实际上只有递归路径上的子结点可能受到影响，我们依然需要处理这些情况。 实际上，只需要三次 skew 与两次 split 就可以完全重新安排这些水平的边。 void remove(const Comparable\u0026 val, AANode* root) { if (root == nullptr) { return; } static AANode* last_node = root; // 寻找替换结点 static AANode* deleted_node = nullptr; // 待删除结点 if (val \u003c root-\u003eelement) { remove(val, root-\u003eleft); } else { deleted_node = root; remove(val, root-\u003eright); } if (root == last_node) { // 如果 val 结点是叶结点，直接删除 if (deleted_node == nullptr || val != deleted_node-\u003eelement) { return; } deleted_node-\u003eelement = root-\u003eelement; deleted_node = nullptr; root = root-\u003eright; delete last_node; } else { // 不是树的底部，需要对其进行平衡 if (root-\u003eleft-\u003elevel \u003c root-\u003elevel - 1 || root-\u003eright-\u003elevel \u003c root-\u003elevel - 1) { if (root-\u003eright-\u003elevel \u003e --root-\u003elevel) { root-\u003eright-\u003elevel = root-\u003elevel; } skew(root); skew(root-\u003eright); skew(root-\u003eright-\u003eright); split(root); split(root-\u003eright); } } } ","date":"08-23","objectID":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/:6:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"查找结构","uri":"/2021/data_strucures_and_algorithm_analysis_004_searching_structure/#aa树的删除操作"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第三章笔记","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/"},{"categories":["Algorithm⁄DataStructure"],"content":" Not all roots are buried down in the ground, some are at the top of a tree. — Jinvirle ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:0:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#"},{"categories":["Algorithm⁄DataStructure"],"content":" 树 (tree)Tree 是一些结点的集合，这个集合可以是空集；若不是空集，则 Tree 是由称为 根 的结点 r 以及零或多个非空的子树 \\(T_{1}, T_{2}, \\cdots, T_{N}\\) 组成，这些子树的根都与 r 有一条有向边 (edge) 连接。这些子树的根被称为根 r 的孩子 (child)，而 r 是这些 child 的父亲 (parent)。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:1:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#树--tree"},{"categories":["Algorithm⁄DataStructure"],"content":" 树的属性根据给出的树的递归定义，可以发现一个树是由 \\(N\\) 个 node 和 \\(N - 1\\) 条 edge 的集合。而除 root 外的所有 node 都有一个由其 parent 指向它的 edge。在树中有一些特殊的属性是需要注意的，这里先给出相关概念与示例，如果不是很理解，可以通过结合示例来理解这些概念。 结点的度 (degree) 一个节点含有的子树的个数称为该节点的度 树的度 (degree of tree) 一棵树中最大的 node degree 称为树的度 叶结点 (leaf) 或称终端结点，如果结点满足 \\(degree = 0\\) 则该结点为叶结点 分支结点 (branch node) 或称内部结点 (internal node)、非终端结点，度不为 0 的结点 层次 (level) 从 root 开始，root 所在的层为第 1 层，root 的 child 为第二层，以此类推 关系 树就像一本族谱，从 root 开始结点直接有一定的亲缘关系 兄弟 (sibling): 具有相同父节点的节点互为兄弟节点 叔父 (uncle): 父结点的兄弟结点为该结点的叔父结点 堂兄弟: 父结点在同一层的结点互为堂兄弟 路径 (path) 结点 \\(n_{1}, n_{2}, \\cdots, n_{k}\\) 的一个序列，使得对于 \\(1 \\leq i \u003c k\\) 满足 \\(n_{i}\\) 是 \\(n_{i + 1}\\) 的 parent，则这个序列被称为从 \\(n_{1}\\) 到结点 \\(n_{k}\\) 的 path。其路径长度 (length) 为路径上的 edge 的数量，即 \\(k - 1\\) 。特别地，每个结点到自己的 path lenth 为 0 深度 (depth) 对于结点 \\(n_{i}\\) ，从 root 到 \\(n_{i}\\) 的唯一路径的长度 (\\(Depth_{root} = 0\\)) 高度 (height) 对于结点 \\(n_{i}\\) ，从 \\(n_{i}\\) 到 leaf 的最长路径长度 (\\(Height_{leaf} = 0\\)) 树的高度 或称树的深度，其总是等于根的高度，或最深的结点的深度，可以认为一棵空树的高度为 \\(-1\\) 祖先 (ancestor) 对于结点 \\(n_{i}\\) 与 \\(n_{j}\\) 存在一条 \\(n_{i}\\) 到 \\(n_{j}\\) 的路径，那么称 \\(n_{i}\\) 是 \\(n_{j}\\) 的祖先 (ancestor)，而 \\(n_{j}\\) 是 \\(n_{i}\\) 的 后裔 (descendant) 距离 (distance) 对于结点 \\(n_{i}\\) 与 \\(n_{j}\\) ，从最近的公共祖先结点 \\(n_{k}\\) 分别到它们的路径长度之和被称为距离 (distance)。特别地，如果 \\(n_{i} = n_{k}\\) ，则 \\(n_{i}\\) 与 \\(n_{j}\\) 的距离为 \\(n_{i}\\) 到 \\(n_{j}\\) 的路径的长度 信息 严蔚敏老师的数据结构中，或者往常的实现中，根的高度为 1，而叶的深度也为 1，树的高度一般指其最大的层次，因此认为空树的高度为 0。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:1:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#树的属性"},{"categories":["Algorithm⁄DataStructure"],"content":" 树的实现实现树的一种方法是在每一个结点上，除数据外还需要一些链域来指向该结点的每个子结点，然而由于每个结点的子结点数量是不确定的，我们不能直接建立到各个子结点的直接链接。如果申请一定大小的空间以存放子结点，则可能会造成空间的浪费，或不足。因此我们链表的形式存储子结点，而父结点中只存储第一个子结点的指针，如果该链域为空则意味着该结点是叶结点 (\\(degree = 0\\))。每个结点中存在一个指向其下一个兄弟的指针，为遍历父结点的所有孩子提供了方法，当该结点 \\(next\\_sibling = nullptr\\) 时意味着这是父结点的最后一个子结点。 struct TreeBaseNode { TreeBaseNode* first_child; TreeBaseNode* next_sibling; }; template \u003cclass Element\u003e struct TreeNode { Element data; }; 如果我们用这个结构实现上述图示的树，可以画一下其表示。 可以发现，除非该结点是 leaf，否则我们很难判断该结点的 degree。且在计算深度与距离时，要十分小心在兄弟间步进，因为兄弟间步进并不会增加其与 parent 的距离。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:1:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#树的实现"},{"categories":["Algorithm⁄DataStructure"],"content":" 树的遍历与应用观察你系统中的文件系统，回到文件系统的顶层 / (root)，并浏览一些目录你会发现， 嗯，你让我发现什么， 整个目录结构与 tree 是类似的，我们也常常将其称为目录树。 / ├── etc │ ├── X11 │ │ ├── Xreset.d │ │ │ └── README │ │ ├── Xresources │ │ │ ├── x11-common │ │ │ └── xpdf │ │ └── Xsession.d │ │ ├── 90x11-common_ssh-agent │ │ ├── 95dbus_update-activation-env │ │ └── 99x11-common_start │ ├── emacs │ │ └── site-start.d │ │ ├── 50asymptote.el │ │ ├── 50autoconf.el │ │ ├── 50dictionaries-common.el │ │ ├── 50erlang-mode.el │ │ ├── 50latex-cjk-common.el │ │ ├── 50latex-cjk-thai.el │ │ ├── 50latexmk.el │ │ └── 50texlive-lang-english.el │ └── fish │ ├── completions │ ├── conf.d │ ├── functions │ └── config.fish ├── home │ └── ginshio ├── mnt ├── run ├── tmp ├── usr │ ├── bin │ │ ├── X11 -\u003e . │ │ ├── 7z │ │ ├── 7za │ │ ├── 7zr │ │ ├── cc -\u003e /etc/alternatives/cc │ │ ├── ccache │ │ ├── clang -\u003e ../lib/llvm-14/bin/clang │ │ ├── clang++ -\u003e ../lib/llvm-14/bin/clang++ │ │ ├── emacs -\u003e /etc/alternatives/emacs │ │ ├── g++ -\u003e g++-11 │ │ ├── gcc -\u003e gcc-11 │ │ ├── zstd │ │ ├── zstdcat -\u003e zstd │ │ └── zstdmt -\u003e zstd │ ├── include │ │ ├── SDL2 │ │ │ ├── SDL.h │ │ │ ├── SDL_assert.h │ │ │ ├── SDL_atomic.h │ │ │ └── SDL_audio.h │ │ └── X11 │ │ ├── ICE │ │ ├── SM │ │ ├── Xaw │ │ ├── Xcursor │ │ ├── Xmu │ │ ├── Xtrans │ │ ├── bitmaps │ │ └── dri │ └── lib │ ├── X11 │ ├── emacs │ │ └── 28.1 │ └── x86_64-linux-gnu │ ├── libQt5Gui.so.5 -\u003e libQt5Gui.so.5.15.3 │ ├── libQt5Gui.so.5.15 -\u003e libQt5Gui.so.5.15.3 │ ├── libQt5Gui.so.5.15.3 │ ├── libQt5Help.so.5 -\u003e libQt5Help.so.5.15.3 │ ├── libQt5Help.so.5.15 -\u003e libQt5Help.so.5.15.3 │ ├── libQt5Help.so.5.15.3 │ ├── liblzma.a │ ├── liblzma.so -\u003e /lib/x86_64-linux-gnu/liblzma.so.5.2.5 │ ├── liblzma.so.5 -\u003e liblzma.so.5.2.5 │ └── liblzma.so.5.2.5 └── var ├── backups ├── cache ├── crash ├── lib ├── local ├── log ├── mail └── tmp 这颗目录树稍微有些复杂了，不过问题不大。一般文件系统中采用路径名来访问一个文件，而我们可以像遍历树一样遍历这个文件系统，将每个文件打印出来，并按照层级来缩进文件名称。 深度有限遍历 (DFS)给出一个代码实现： void filesystem::list_all(file\u0026 f, int depth = 0) const { print_name(f, depth); // 打印文件的名称 if (is_directory(f)) { for (file p : get_file_list(f)) { // 遍历目录中的每个文件 list_all(p, depth + 1); } } } 最终的输出结果可能是： / |--- mnt/ |--- home/ |--- GinShio/ |--- usr |--- LICENSE |--- lib/ |--- libQt5Core.so |--- X11/ |--- display-manager |--- etc/ |--- displaymanagers/ |--- console |--- lightdm |--- sddm |--- xdm |--- libstdc++.so.6 |--- mozilla/ |--- kmozillahelper |--- bin/ |--- latexmk |--- pdftk |--- zsh ..... ..... 在遍历中，每访问一个结点时，对结点的处理工作总是比其子结点的处理先进行，这种先处理根再处理子结点的策略被称为 前序遍历 (preorder traversal)。而另一种常用的遍历方法是 后序遍历 (postorder traversal)，即在结点的所有子结点处理完成后再对其进行处理。无论这两种遍历的哪一个，在遍历这个树时总是可以在 \\(\\mathcal{O}(N)\\) 的时间复杂度里完成。对于目录的 postorder traversal 留给读者思考并实现。 现在考虑这两种算法有什么共通的特点。有没有发现它们都是在一棵子树上处理完所有结点之后再转移到另一棵子树上，这种一直向着 child 递归，直到全部递归结束时再向 sibling 递归的算法，就被称之为 深度优先搜索 (Depth-first Search, DFS)。由于 DFS 使用递归算法，因此 DFS 总能被改写为 loop，非 tail recursion 的递归有可能需要 stack 的帮助才能改为 loop。 广度优先遍历 (BFS)请回看 树的实现 一节的图，图中的树如果以一层一层遍历，当一层的所有结点都被遍历完时，再进入更深一层，从这层的第一个结点开始处理。这种遍历方式被称为 广度优先遍历 (Breadth-first Search, BFS) 或者是层序遍历。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:1:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#树的遍历与应用"},{"categories":["Algorithm⁄DataStructure"],"content":" 树的遍历与应用观察你系统中的文件系统，回到文件系统的顶层 / (root)，并浏览一些目录你会发现， 嗯，你让我发现什么， 整个目录结构与 tree 是类似的，我们也常常将其称为目录树。 / ├── etc │ ├── X11 │ │ ├── Xreset.d │ │ │ └── README │ │ ├── Xresources │ │ │ ├── x11-common │ │ │ └── xpdf │ │ └── Xsession.d │ │ ├── 90x11-common_ssh-agent │ │ ├── 95dbus_update-activation-env │ │ └── 99x11-common_start │ ├── emacs │ │ └── site-start.d │ │ ├── 50asymptote.el │ │ ├── 50autoconf.el │ │ ├── 50dictionaries-common.el │ │ ├── 50erlang-mode.el │ │ ├── 50latex-cjk-common.el │ │ ├── 50latex-cjk-thai.el │ │ ├── 50latexmk.el │ │ └── 50texlive-lang-english.el │ └── fish │ ├── completions │ ├── conf.d │ ├── functions │ └── config.fish ├── home │ └── ginshio ├── mnt ├── run ├── tmp ├── usr │ ├── bin │ │ ├── X11 -\u003e . │ │ ├── 7z │ │ ├── 7za │ │ ├── 7zr │ │ ├── cc -\u003e /etc/alternatives/cc │ │ ├── ccache │ │ ├── clang -\u003e ../lib/llvm-14/bin/clang │ │ ├── clang++ -\u003e ../lib/llvm-14/bin/clang++ │ │ ├── emacs -\u003e /etc/alternatives/emacs │ │ ├── g++ -\u003e g++-11 │ │ ├── gcc -\u003e gcc-11 │ │ ├── zstd │ │ ├── zstdcat -\u003e zstd │ │ └── zstdmt -\u003e zstd │ ├── include │ │ ├── SDL2 │ │ │ ├── SDL.h │ │ │ ├── SDL_assert.h │ │ │ ├── SDL_atomic.h │ │ │ └── SDL_audio.h │ │ └── X11 │ │ ├── ICE │ │ ├── SM │ │ ├── Xaw │ │ ├── Xcursor │ │ ├── Xmu │ │ ├── Xtrans │ │ ├── bitmaps │ │ └── dri │ └── lib │ ├── X11 │ ├── emacs │ │ └── 28.1 │ └── x86_64-linux-gnu │ ├── libQt5Gui.so.5 -\u003e libQt5Gui.so.5.15.3 │ ├── libQt5Gui.so.5.15 -\u003e libQt5Gui.so.5.15.3 │ ├── libQt5Gui.so.5.15.3 │ ├── libQt5Help.so.5 -\u003e libQt5Help.so.5.15.3 │ ├── libQt5Help.so.5.15 -\u003e libQt5Help.so.5.15.3 │ ├── libQt5Help.so.5.15.3 │ ├── liblzma.a │ ├── liblzma.so -\u003e /lib/x86_64-linux-gnu/liblzma.so.5.2.5 │ ├── liblzma.so.5 -\u003e liblzma.so.5.2.5 │ └── liblzma.so.5.2.5 └── var ├── backups ├── cache ├── crash ├── lib ├── local ├── log ├── mail └── tmp 这颗目录树稍微有些复杂了，不过问题不大。一般文件系统中采用路径名来访问一个文件，而我们可以像遍历树一样遍历这个文件系统，将每个文件打印出来，并按照层级来缩进文件名称。 深度有限遍历 (DFS)给出一个代码实现： void filesystem::list_all(file\u0026 f, int depth = 0) const { print_name(f, depth); // 打印文件的名称 if (is_directory(f)) { for (file p : get_file_list(f)) { // 遍历目录中的每个文件 list_all(p, depth + 1); } } } 最终的输出结果可能是： / |--- mnt/ |--- home/ |--- GinShio/ |--- usr |--- LICENSE |--- lib/ |--- libQt5Core.so |--- X11/ |--- display-manager |--- etc/ |--- displaymanagers/ |--- console |--- lightdm |--- sddm |--- xdm |--- libstdc++.so.6 |--- mozilla/ |--- kmozillahelper |--- bin/ |--- latexmk |--- pdftk |--- zsh ..... ..... 在遍历中，每访问一个结点时，对结点的处理工作总是比其子结点的处理先进行，这种先处理根再处理子结点的策略被称为 前序遍历 (preorder traversal)。而另一种常用的遍历方法是 后序遍历 (postorder traversal)，即在结点的所有子结点处理完成后再对其进行处理。无论这两种遍历的哪一个，在遍历这个树时总是可以在 \\(\\mathcal{O}(N)\\) 的时间复杂度里完成。对于目录的 postorder traversal 留给读者思考并实现。 现在考虑这两种算法有什么共通的特点。有没有发现它们都是在一棵子树上处理完所有结点之后再转移到另一棵子树上，这种一直向着 child 递归，直到全部递归结束时再向 sibling 递归的算法，就被称之为 深度优先搜索 (Depth-first Search, DFS)。由于 DFS 使用递归算法，因此 DFS 总能被改写为 loop，非 tail recursion 的递归有可能需要 stack 的帮助才能改为 loop。 广度优先遍历 (BFS)请回看 树的实现 一节的图，图中的树如果以一层一层遍历，当一层的所有结点都被遍历完时，再进入更深一层，从这层的第一个结点开始处理。这种遍历方式被称为 广度优先遍历 (Breadth-first Search, BFS) 或者是层序遍历。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:1:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#深度有限遍历--dfs"},{"categories":["Algorithm⁄DataStructure"],"content":" 树的遍历与应用观察你系统中的文件系统，回到文件系统的顶层 / (root)，并浏览一些目录你会发现， 嗯，你让我发现什么， 整个目录结构与 tree 是类似的，我们也常常将其称为目录树。 / ├── etc │ ├── X11 │ │ ├── Xreset.d │ │ │ └── README │ │ ├── Xresources │ │ │ ├── x11-common │ │ │ └── xpdf │ │ └── Xsession.d │ │ ├── 90x11-common_ssh-agent │ │ ├── 95dbus_update-activation-env │ │ └── 99x11-common_start │ ├── emacs │ │ └── site-start.d │ │ ├── 50asymptote.el │ │ ├── 50autoconf.el │ │ ├── 50dictionaries-common.el │ │ ├── 50erlang-mode.el │ │ ├── 50latex-cjk-common.el │ │ ├── 50latex-cjk-thai.el │ │ ├── 50latexmk.el │ │ └── 50texlive-lang-english.el │ └── fish │ ├── completions │ ├── conf.d │ ├── functions │ └── config.fish ├── home │ └── ginshio ├── mnt ├── run ├── tmp ├── usr │ ├── bin │ │ ├── X11 -\u003e . │ │ ├── 7z │ │ ├── 7za │ │ ├── 7zr │ │ ├── cc -\u003e /etc/alternatives/cc │ │ ├── ccache │ │ ├── clang -\u003e ../lib/llvm-14/bin/clang │ │ ├── clang++ -\u003e ../lib/llvm-14/bin/clang++ │ │ ├── emacs -\u003e /etc/alternatives/emacs │ │ ├── g++ -\u003e g++-11 │ │ ├── gcc -\u003e gcc-11 │ │ ├── zstd │ │ ├── zstdcat -\u003e zstd │ │ └── zstdmt -\u003e zstd │ ├── include │ │ ├── SDL2 │ │ │ ├── SDL.h │ │ │ ├── SDL_assert.h │ │ │ ├── SDL_atomic.h │ │ │ └── SDL_audio.h │ │ └── X11 │ │ ├── ICE │ │ ├── SM │ │ ├── Xaw │ │ ├── Xcursor │ │ ├── Xmu │ │ ├── Xtrans │ │ ├── bitmaps │ │ └── dri │ └── lib │ ├── X11 │ ├── emacs │ │ └── 28.1 │ └── x86_64-linux-gnu │ ├── libQt5Gui.so.5 -\u003e libQt5Gui.so.5.15.3 │ ├── libQt5Gui.so.5.15 -\u003e libQt5Gui.so.5.15.3 │ ├── libQt5Gui.so.5.15.3 │ ├── libQt5Help.so.5 -\u003e libQt5Help.so.5.15.3 │ ├── libQt5Help.so.5.15 -\u003e libQt5Help.so.5.15.3 │ ├── libQt5Help.so.5.15.3 │ ├── liblzma.a │ ├── liblzma.so -\u003e /lib/x86_64-linux-gnu/liblzma.so.5.2.5 │ ├── liblzma.so.5 -\u003e liblzma.so.5.2.5 │ └── liblzma.so.5.2.5 └── var ├── backups ├── cache ├── crash ├── lib ├── local ├── log ├── mail └── tmp 这颗目录树稍微有些复杂了，不过问题不大。一般文件系统中采用路径名来访问一个文件，而我们可以像遍历树一样遍历这个文件系统，将每个文件打印出来，并按照层级来缩进文件名称。 深度有限遍历 (DFS)给出一个代码实现： void filesystem::list_all(file\u0026 f, int depth = 0) const { print_name(f, depth); // 打印文件的名称 if (is_directory(f)) { for (file p : get_file_list(f)) { // 遍历目录中的每个文件 list_all(p, depth + 1); } } } 最终的输出结果可能是： / |--- mnt/ |--- home/ |--- GinShio/ |--- usr |--- LICENSE |--- lib/ |--- libQt5Core.so |--- X11/ |--- display-manager |--- etc/ |--- displaymanagers/ |--- console |--- lightdm |--- sddm |--- xdm |--- libstdc++.so.6 |--- mozilla/ |--- kmozillahelper |--- bin/ |--- latexmk |--- pdftk |--- zsh ..... ..... 在遍历中，每访问一个结点时，对结点的处理工作总是比其子结点的处理先进行，这种先处理根再处理子结点的策略被称为 前序遍历 (preorder traversal)。而另一种常用的遍历方法是 后序遍历 (postorder traversal)，即在结点的所有子结点处理完成后再对其进行处理。无论这两种遍历的哪一个，在遍历这个树时总是可以在 \\(\\mathcal{O}(N)\\) 的时间复杂度里完成。对于目录的 postorder traversal 留给读者思考并实现。 现在考虑这两种算法有什么共通的特点。有没有发现它们都是在一棵子树上处理完所有结点之后再转移到另一棵子树上，这种一直向着 child 递归，直到全部递归结束时再向 sibling 递归的算法，就被称之为 深度优先搜索 (Depth-first Search, DFS)。由于 DFS 使用递归算法，因此 DFS 总能被改写为 loop，非 tail recursion 的递归有可能需要 stack 的帮助才能改为 loop。 广度优先遍历 (BFS)请回看 树的实现 一节的图，图中的树如果以一层一层遍历，当一层的所有结点都被遍历完时，再进入更深一层，从这层的第一个结点开始处理。这种遍历方式被称为 广度优先遍历 (Breadth-first Search, BFS) 或者是层序遍历。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:1:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#广度优先遍历--bfs"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树对树加以限制，如果树的度为 2，那么就称这颗树为 二叉树 (binary tree)。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的性质在一棵二叉树上，有一些重要的性质： 第 i 层 (\\(i \\in \\mathbb{N}^{*}\\)) 上最多有 \\(2^{i - 1}\\) 个结点 层次为 \\(k (k \\in \\mathbb{N}^{*})\\) 的树最多有 \\(2^{k} -1\\) 个结点 如果叶结点的数量为 \\(n_{0}\\) ， \\(degree = 2\\) 的结点的数量为 \\(n_{2}\\) ，则 \\(n_{0} = n_{2} + 1\\) 如果将二叉树的每一层填满，那么这颗二叉树被称之为 满二叉树 (full binary tree)；如果这颗二叉树除最后一层外都是满的，且最后一层要么是满的，要么是右边缺少连续的若干结点，那么称这颗二叉树为 完全二叉树 (complete binary tree)。 由于 full binary tree 与 complete binary tree 是特殊的二叉树，因此它们也有一些确定性的性质。我们假设总结点数为 \\(k\\) ，树的高度 (即树的层数) 为 \\(h\\) ，其中某一层为第 \\(i\\) 层，则有以下性质： 性质 满二叉树 完全二叉树 总结点数 \\(k\\) \\(2^{h} - 1\\) \\(2^{h-1} \\leq k \\leq 2^{h} - 1\\) 树的高度 \\(h\\) \\(\\log_{2}{k} + 1\\) \\(\\log_{2}{(k + 1)}\\) ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树的性质"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的实现为实现二叉树，我们可以为其采用双向链表的结构，但不再是指向结点的 prev 和 next，而是指向该结点的 left child 和 right child。 struct BinaryTreeBaseNode { BinaryTreeBaseNode* left,* right; }; template \u003cclass Element\u003e struct BinaryTreeNode : BinaryTreeBaseNode { Element data; }; 在这里给出求解二叉树 root 中，node 的高度和深度 // 求解结点 node 的高度 int node_height(BinaryTreeBaseNode* node) { if (node == nullptr) { return -1; } return max(binary_tree_height(node-\u003eleft), binary_tree_height(node-\u003eright)) + 1; } // 求解结点 node 在树 root 中的深度 int node_depth(BinaryTreeBaseNode* root, const BinaryTreeBaseNode* node) { if (root == nullptr) { return -1; } if (root == node) { return 0; } int left_depth = node_depth(root-\u003eleft, node); int right_depth = node_depth(root-\u003eright, node); return left_depth == -1 ? (right_depth += right_depth != -1) : (left_depth + 1); } 如果想要从结点向上求解某些数据时，并不容易做到，因为 child 没有指向 parent 的指针，需要遍历树找到 node 的 parent 才能操作。 // 求解结点 node 的 parent，如果不存在返回 nullptr BinaryTreeBaseNode* get_parent(BinaryTreeBaseNode* root, const BinaryTreeBaseNode* node) { if (root == nullptr || root == node) { return nullptr; } if (root-\u003eleft == node || root-\u003eright == node) { return root; } auto left = get_parent(root-\u003eleft, node); return left == nullptr ? get_parent(root-\u003eright, node) : left; } 为了方便实现我们自然而然的会在链域中添加指向 parent 的指针。这样在求解 sibling、 uncle 时十分方便，并且求解结点的深度时不再需要将其等价为 root 到 node 的路径长。需要注意的是，root 是没有 parent 的。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树的实现"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的遍历还记得之前提到的 postorder traversal 与 preorder traversal 吗，它们对二叉树同样适用。不过先别急，既然现在 child 的数量确定了，能不能将对结点的处理放在两个结点的处理之间完成呢？当然没问题！这种处理方式就是 中序遍历 (inorder traversal)，当然这也是 DFS 的一种。 如果将当前结点标记为 N，左子结点标记为 L，右子结点标记为 R，那么前序遍历就可以表示为 NLR，中序遍历可以表示为 LNR，后序遍历可以表示为 LRN。 二叉树的前序遍历 recursion void preorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } process(root); preorder(root-\u003eleft); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { process(root); s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); root = root-\u003erightl; } } 二叉树的中序遍历 recursion void inorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); process(root); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); process(root); root = root-\u003erightl; } } 二叉树的后序遍历 recursion void postorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); preorder(root-\u003eright); process(root); } loop 在后序遍历中，在左子结点处理完成后，只有结点没有右子结点或右子结点处理完之后，才能对结点进行处理。因此需要判别当前结点的 右子结点为空 或 刚刚处理过的结点 是该结点的右子结点。判断右子结点为空十分简单，但是问题是如何记录刚刚访问过的结点？ 利用一个变量指向正在处理的结点，当指向下一个待处理的结点时，其值就是该结点的上一个处理的结点，即处理前驱。 void postorder(BinaryTreeBaseNode* root) { stack s; BinaryTreeBaseNode* prev = nullptr; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); if (root-\u003eright == nullptr || root-\u003eright == prev) { prev = root; process(root); root = nullptr; } else { s.push(root); root = root-\u003eright; } } } 一个异构的前序遍历 如果你在对一个单词串进行翻转时，有一个简单可行的方法：先将单词串整体翻转，之后再逐词翻转。这样你就得到了一个对单词串的翻转！ \\[a\\ good\\ example\\quad\\Longrightarrow\\quad elpmaxe\\ doog\\ a\\quad\\Longrightarrow\\quad example\\ good\\ a\\] 这种异构的翻转也可以用在二叉树的 DFS 遍历上，前序遍历时遍历的结点顺序为 NLR (Node-\u003eLeft-\u003eRight)，而后续遍历的结点顺序为 LRN ，对后续遍历的顺序进行翻转就变为了 NRL 。如果以 NRL 的顺序进行遍历，最后将结果翻转也可以得到一个后序遍历的序列，这本质上是一种前序遍历的异构。 二叉树的层序遍历DFS 天生与 stack 结合在一起，而 BFS 与 queue 结合在一起。因此对于以上三种 DFS 遍历，使用 recursion 是一种简单、高效的理解与编码，而层序遍历则更适合于 loop。 void levelorder(BinaryTreeBaseNode* root) { queue q; q.push(root); while (!q.empty()) { for (int i = 0, cur_level_size = q.size(); i \u003c cur_level_size; ++i) { root = q.front(); q.pop(); process(root); if (root-\u003eleft != nullptr) { q.push(root-\u003eleft); } if (root-\u003eright != nullptr) { q.push(root-\u003eright); } } } } Morris 遍历在以上介绍的三种 DFS 遍历中，无论是 recursion 还是 loop 实现，都需要 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(N)\\) 的空间复杂度。而 1979 年由 J.H.Morris 在他的 论文 中提出了一种遍历方式，可以利用 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(1)\\) 的空间复杂度完成遍历。其核心思想是利用二叉树中的空闲指针，以实现空间复杂度的降低。 以 postorder 为例说明其算法的具体思路： 如果当前结点的左子树为空，则遍历右子树 如果当前结点的左子树不为空，在当前结点的左子树中找到当前结点在中序遍历中的前驱结点 如果前驱的右子结点为空，则将前驱结点的右子结点设置为当前结点，当前结点更新为其左子结点 如果前驱的右子结点为当前结点，则将其重新置空。倒序处理从当前结点的左子结点到该前驱结点路径上的所有结点。完成后将当前结点更新为当前结点的右子结点 重复步骤 1、2 直到遍历结束 void __reverse_process(BinaryTreeBaseNode* node) { if (node == nullptr) { return; } __reverse_process(node-\u003eright); process(node); } void postorderTraversal(BinaryTreeBaseNode* root) { BinaryTreeBaseNode* cur = root,* prev = nullptr; while (cur != nullptr) { prev = cur-\u003eleft; if (prev != nullptr) { while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != cur) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = cur; cur = cur-\u003eleft; continue; } prev-\u003eright = nullptr; __reverse_process(cur-\u003eleft); } cur = cur-\u003eright; } __reverse_precess(root); } 迭代器既然可以遍历一棵树，那么依然希望可以在这棵树上暂停下来，对结点进行一些操作，再继续进行迭代。当我们选择的遍历方法不一样时，其迭代时的前驱与后继就不相同。 如果现在给定一个迭代器，应该如何找到迭代器的前驱与后继迭代器。这里给出求解中序遍历前驱的算法步骤与代码，求解中序遍历后继的算法与前驱的算法类似，因此只给出代码。 求解前驱 如果结点的左子树存在，则前驱是结点左子树上最大的结点 如果结点的左子树不存在，则需要寻找结点的 parent 若结点是 parent 的右子树上的结点，则 parent 是其前驱 若结点是 parent 的左子树上的结点，继续向上寻找，直到 parent 为 nullptr 或是其 parent 的右子树上的结点 // 寻找结点 node 的前驱 BinaryTreeBaseNode* get_previous(BinaryTreeBaseNode* node) { if (node-\u003eleft != nullptr) { node = node-\u003eleft; while (node-\u003eright != nullptr) { node = node-\u003eright; } } else { auto paren","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树的遍历"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的遍历还记得之前提到的 postorder traversal 与 preorder traversal 吗，它们对二叉树同样适用。不过先别急，既然现在 child 的数量确定了，能不能将对结点的处理放在两个结点的处理之间完成呢？当然没问题！这种处理方式就是 中序遍历 (inorder traversal)，当然这也是 DFS 的一种。 如果将当前结点标记为 N，左子结点标记为 L，右子结点标记为 R，那么前序遍历就可以表示为 NLR，中序遍历可以表示为 LNR，后序遍历可以表示为 LRN。 二叉树的前序遍历 recursion void preorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } process(root); preorder(root-\u003eleft); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { process(root); s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); root = root-\u003erightl; } } 二叉树的中序遍历 recursion void inorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); process(root); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); process(root); root = root-\u003erightl; } } 二叉树的后序遍历 recursion void postorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); preorder(root-\u003eright); process(root); } loop 在后序遍历中，在左子结点处理完成后，只有结点没有右子结点或右子结点处理完之后，才能对结点进行处理。因此需要判别当前结点的 右子结点为空 或 刚刚处理过的结点 是该结点的右子结点。判断右子结点为空十分简单，但是问题是如何记录刚刚访问过的结点？ 利用一个变量指向正在处理的结点，当指向下一个待处理的结点时，其值就是该结点的上一个处理的结点，即处理前驱。 void postorder(BinaryTreeBaseNode* root) { stack s; BinaryTreeBaseNode* prev = nullptr; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); if (root-\u003eright == nullptr || root-\u003eright == prev) { prev = root; process(root); root = nullptr; } else { s.push(root); root = root-\u003eright; } } } 一个异构的前序遍历 如果你在对一个单词串进行翻转时，有一个简单可行的方法：先将单词串整体翻转，之后再逐词翻转。这样你就得到了一个对单词串的翻转！ \\[a\\ good\\ example\\quad\\Longrightarrow\\quad elpmaxe\\ doog\\ a\\quad\\Longrightarrow\\quad example\\ good\\ a\\] 这种异构的翻转也可以用在二叉树的 DFS 遍历上，前序遍历时遍历的结点顺序为 NLR (Node-\u003eLeft-\u003eRight)，而后续遍历的结点顺序为 LRN ，对后续遍历的顺序进行翻转就变为了 NRL 。如果以 NRL 的顺序进行遍历，最后将结果翻转也可以得到一个后序遍历的序列，这本质上是一种前序遍历的异构。 二叉树的层序遍历DFS 天生与 stack 结合在一起，而 BFS 与 queue 结合在一起。因此对于以上三种 DFS 遍历，使用 recursion 是一种简单、高效的理解与编码，而层序遍历则更适合于 loop。 void levelorder(BinaryTreeBaseNode* root) { queue q; q.push(root); while (!q.empty()) { for (int i = 0, cur_level_size = q.size(); i \u003c cur_level_size; ++i) { root = q.front(); q.pop(); process(root); if (root-\u003eleft != nullptr) { q.push(root-\u003eleft); } if (root-\u003eright != nullptr) { q.push(root-\u003eright); } } } } Morris 遍历在以上介绍的三种 DFS 遍历中，无论是 recursion 还是 loop 实现，都需要 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(N)\\) 的空间复杂度。而 1979 年由 J.H.Morris 在他的 论文 中提出了一种遍历方式，可以利用 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(1)\\) 的空间复杂度完成遍历。其核心思想是利用二叉树中的空闲指针，以实现空间复杂度的降低。 以 postorder 为例说明其算法的具体思路： 如果当前结点的左子树为空，则遍历右子树 如果当前结点的左子树不为空，在当前结点的左子树中找到当前结点在中序遍历中的前驱结点 如果前驱的右子结点为空，则将前驱结点的右子结点设置为当前结点，当前结点更新为其左子结点 如果前驱的右子结点为当前结点，则将其重新置空。倒序处理从当前结点的左子结点到该前驱结点路径上的所有结点。完成后将当前结点更新为当前结点的右子结点 重复步骤 1、2 直到遍历结束 void __reverse_process(BinaryTreeBaseNode* node) { if (node == nullptr) { return; } __reverse_process(node-\u003eright); process(node); } void postorderTraversal(BinaryTreeBaseNode* root) { BinaryTreeBaseNode* cur = root,* prev = nullptr; while (cur != nullptr) { prev = cur-\u003eleft; if (prev != nullptr) { while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != cur) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = cur; cur = cur-\u003eleft; continue; } prev-\u003eright = nullptr; __reverse_process(cur-\u003eleft); } cur = cur-\u003eright; } __reverse_precess(root); } 迭代器既然可以遍历一棵树，那么依然希望可以在这棵树上暂停下来，对结点进行一些操作，再继续进行迭代。当我们选择的遍历方法不一样时，其迭代时的前驱与后继就不相同。 如果现在给定一个迭代器，应该如何找到迭代器的前驱与后继迭代器。这里给出求解中序遍历前驱的算法步骤与代码，求解中序遍历后继的算法与前驱的算法类似，因此只给出代码。 求解前驱 如果结点的左子树存在，则前驱是结点左子树上最大的结点 如果结点的左子树不存在，则需要寻找结点的 parent 若结点是 parent 的右子树上的结点，则 parent 是其前驱 若结点是 parent 的左子树上的结点，继续向上寻找，直到 parent 为 nullptr 或是其 parent 的右子树上的结点 // 寻找结点 node 的前驱 BinaryTreeBaseNode* get_previous(BinaryTreeBaseNode* node) { if (node-\u003eleft != nullptr) { node = node-\u003eleft; while (node-\u003eright != nullptr) { node = node-\u003eright; } } else { auto paren","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树的前序遍历"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的遍历还记得之前提到的 postorder traversal 与 preorder traversal 吗，它们对二叉树同样适用。不过先别急，既然现在 child 的数量确定了，能不能将对结点的处理放在两个结点的处理之间完成呢？当然没问题！这种处理方式就是 中序遍历 (inorder traversal)，当然这也是 DFS 的一种。 如果将当前结点标记为 N，左子结点标记为 L，右子结点标记为 R，那么前序遍历就可以表示为 NLR，中序遍历可以表示为 LNR，后序遍历可以表示为 LRN。 二叉树的前序遍历 recursion void preorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } process(root); preorder(root-\u003eleft); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { process(root); s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); root = root-\u003erightl; } } 二叉树的中序遍历 recursion void inorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); process(root); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); process(root); root = root-\u003erightl; } } 二叉树的后序遍历 recursion void postorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); preorder(root-\u003eright); process(root); } loop 在后序遍历中，在左子结点处理完成后，只有结点没有右子结点或右子结点处理完之后，才能对结点进行处理。因此需要判别当前结点的 右子结点为空 或 刚刚处理过的结点 是该结点的右子结点。判断右子结点为空十分简单，但是问题是如何记录刚刚访问过的结点？ 利用一个变量指向正在处理的结点，当指向下一个待处理的结点时，其值就是该结点的上一个处理的结点，即处理前驱。 void postorder(BinaryTreeBaseNode* root) { stack s; BinaryTreeBaseNode* prev = nullptr; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); if (root-\u003eright == nullptr || root-\u003eright == prev) { prev = root; process(root); root = nullptr; } else { s.push(root); root = root-\u003eright; } } } 一个异构的前序遍历 如果你在对一个单词串进行翻转时，有一个简单可行的方法：先将单词串整体翻转，之后再逐词翻转。这样你就得到了一个对单词串的翻转！ \\[a\\ good\\ example\\quad\\Longrightarrow\\quad elpmaxe\\ doog\\ a\\quad\\Longrightarrow\\quad example\\ good\\ a\\] 这种异构的翻转也可以用在二叉树的 DFS 遍历上，前序遍历时遍历的结点顺序为 NLR (Node-\u003eLeft-\u003eRight)，而后续遍历的结点顺序为 LRN ，对后续遍历的顺序进行翻转就变为了 NRL 。如果以 NRL 的顺序进行遍历，最后将结果翻转也可以得到一个后序遍历的序列，这本质上是一种前序遍历的异构。 二叉树的层序遍历DFS 天生与 stack 结合在一起，而 BFS 与 queue 结合在一起。因此对于以上三种 DFS 遍历，使用 recursion 是一种简单、高效的理解与编码，而层序遍历则更适合于 loop。 void levelorder(BinaryTreeBaseNode* root) { queue q; q.push(root); while (!q.empty()) { for (int i = 0, cur_level_size = q.size(); i \u003c cur_level_size; ++i) { root = q.front(); q.pop(); process(root); if (root-\u003eleft != nullptr) { q.push(root-\u003eleft); } if (root-\u003eright != nullptr) { q.push(root-\u003eright); } } } } Morris 遍历在以上介绍的三种 DFS 遍历中，无论是 recursion 还是 loop 实现，都需要 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(N)\\) 的空间复杂度。而 1979 年由 J.H.Morris 在他的 论文 中提出了一种遍历方式，可以利用 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(1)\\) 的空间复杂度完成遍历。其核心思想是利用二叉树中的空闲指针，以实现空间复杂度的降低。 以 postorder 为例说明其算法的具体思路： 如果当前结点的左子树为空，则遍历右子树 如果当前结点的左子树不为空，在当前结点的左子树中找到当前结点在中序遍历中的前驱结点 如果前驱的右子结点为空，则将前驱结点的右子结点设置为当前结点，当前结点更新为其左子结点 如果前驱的右子结点为当前结点，则将其重新置空。倒序处理从当前结点的左子结点到该前驱结点路径上的所有结点。完成后将当前结点更新为当前结点的右子结点 重复步骤 1、2 直到遍历结束 void __reverse_process(BinaryTreeBaseNode* node) { if (node == nullptr) { return; } __reverse_process(node-\u003eright); process(node); } void postorderTraversal(BinaryTreeBaseNode* root) { BinaryTreeBaseNode* cur = root,* prev = nullptr; while (cur != nullptr) { prev = cur-\u003eleft; if (prev != nullptr) { while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != cur) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = cur; cur = cur-\u003eleft; continue; } prev-\u003eright = nullptr; __reverse_process(cur-\u003eleft); } cur = cur-\u003eright; } __reverse_precess(root); } 迭代器既然可以遍历一棵树，那么依然希望可以在这棵树上暂停下来，对结点进行一些操作，再继续进行迭代。当我们选择的遍历方法不一样时，其迭代时的前驱与后继就不相同。 如果现在给定一个迭代器，应该如何找到迭代器的前驱与后继迭代器。这里给出求解中序遍历前驱的算法步骤与代码，求解中序遍历后继的算法与前驱的算法类似，因此只给出代码。 求解前驱 如果结点的左子树存在，则前驱是结点左子树上最大的结点 如果结点的左子树不存在，则需要寻找结点的 parent 若结点是 parent 的右子树上的结点，则 parent 是其前驱 若结点是 parent 的左子树上的结点，继续向上寻找，直到 parent 为 nullptr 或是其 parent 的右子树上的结点 // 寻找结点 node 的前驱 BinaryTreeBaseNode* get_previous(BinaryTreeBaseNode* node) { if (node-\u003eleft != nullptr) { node = node-\u003eleft; while (node-\u003eright != nullptr) { node = node-\u003eright; } } else { auto paren","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树的中序遍历"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的遍历还记得之前提到的 postorder traversal 与 preorder traversal 吗，它们对二叉树同样适用。不过先别急，既然现在 child 的数量确定了，能不能将对结点的处理放在两个结点的处理之间完成呢？当然没问题！这种处理方式就是 中序遍历 (inorder traversal)，当然这也是 DFS 的一种。 如果将当前结点标记为 N，左子结点标记为 L，右子结点标记为 R，那么前序遍历就可以表示为 NLR，中序遍历可以表示为 LNR，后序遍历可以表示为 LRN。 二叉树的前序遍历 recursion void preorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } process(root); preorder(root-\u003eleft); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { process(root); s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); root = root-\u003erightl; } } 二叉树的中序遍历 recursion void inorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); process(root); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); process(root); root = root-\u003erightl; } } 二叉树的后序遍历 recursion void postorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); preorder(root-\u003eright); process(root); } loop 在后序遍历中，在左子结点处理完成后，只有结点没有右子结点或右子结点处理完之后，才能对结点进行处理。因此需要判别当前结点的 右子结点为空 或 刚刚处理过的结点 是该结点的右子结点。判断右子结点为空十分简单，但是问题是如何记录刚刚访问过的结点？ 利用一个变量指向正在处理的结点，当指向下一个待处理的结点时，其值就是该结点的上一个处理的结点，即处理前驱。 void postorder(BinaryTreeBaseNode* root) { stack s; BinaryTreeBaseNode* prev = nullptr; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); if (root-\u003eright == nullptr || root-\u003eright == prev) { prev = root; process(root); root = nullptr; } else { s.push(root); root = root-\u003eright; } } } 一个异构的前序遍历 如果你在对一个单词串进行翻转时，有一个简单可行的方法：先将单词串整体翻转，之后再逐词翻转。这样你就得到了一个对单词串的翻转！ \\[a\\ good\\ example\\quad\\Longrightarrow\\quad elpmaxe\\ doog\\ a\\quad\\Longrightarrow\\quad example\\ good\\ a\\] 这种异构的翻转也可以用在二叉树的 DFS 遍历上，前序遍历时遍历的结点顺序为 NLR (Node-\u003eLeft-\u003eRight)，而后续遍历的结点顺序为 LRN ，对后续遍历的顺序进行翻转就变为了 NRL 。如果以 NRL 的顺序进行遍历，最后将结果翻转也可以得到一个后序遍历的序列，这本质上是一种前序遍历的异构。 二叉树的层序遍历DFS 天生与 stack 结合在一起，而 BFS 与 queue 结合在一起。因此对于以上三种 DFS 遍历，使用 recursion 是一种简单、高效的理解与编码，而层序遍历则更适合于 loop。 void levelorder(BinaryTreeBaseNode* root) { queue q; q.push(root); while (!q.empty()) { for (int i = 0, cur_level_size = q.size(); i \u003c cur_level_size; ++i) { root = q.front(); q.pop(); process(root); if (root-\u003eleft != nullptr) { q.push(root-\u003eleft); } if (root-\u003eright != nullptr) { q.push(root-\u003eright); } } } } Morris 遍历在以上介绍的三种 DFS 遍历中，无论是 recursion 还是 loop 实现，都需要 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(N)\\) 的空间复杂度。而 1979 年由 J.H.Morris 在他的 论文 中提出了一种遍历方式，可以利用 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(1)\\) 的空间复杂度完成遍历。其核心思想是利用二叉树中的空闲指针，以实现空间复杂度的降低。 以 postorder 为例说明其算法的具体思路： 如果当前结点的左子树为空，则遍历右子树 如果当前结点的左子树不为空，在当前结点的左子树中找到当前结点在中序遍历中的前驱结点 如果前驱的右子结点为空，则将前驱结点的右子结点设置为当前结点，当前结点更新为其左子结点 如果前驱的右子结点为当前结点，则将其重新置空。倒序处理从当前结点的左子结点到该前驱结点路径上的所有结点。完成后将当前结点更新为当前结点的右子结点 重复步骤 1、2 直到遍历结束 void __reverse_process(BinaryTreeBaseNode* node) { if (node == nullptr) { return; } __reverse_process(node-\u003eright); process(node); } void postorderTraversal(BinaryTreeBaseNode* root) { BinaryTreeBaseNode* cur = root,* prev = nullptr; while (cur != nullptr) { prev = cur-\u003eleft; if (prev != nullptr) { while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != cur) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = cur; cur = cur-\u003eleft; continue; } prev-\u003eright = nullptr; __reverse_process(cur-\u003eleft); } cur = cur-\u003eright; } __reverse_precess(root); } 迭代器既然可以遍历一棵树，那么依然希望可以在这棵树上暂停下来，对结点进行一些操作，再继续进行迭代。当我们选择的遍历方法不一样时，其迭代时的前驱与后继就不相同。 如果现在给定一个迭代器，应该如何找到迭代器的前驱与后继迭代器。这里给出求解中序遍历前驱的算法步骤与代码，求解中序遍历后继的算法与前驱的算法类似，因此只给出代码。 求解前驱 如果结点的左子树存在，则前驱是结点左子树上最大的结点 如果结点的左子树不存在，则需要寻找结点的 parent 若结点是 parent 的右子树上的结点，则 parent 是其前驱 若结点是 parent 的左子树上的结点，继续向上寻找，直到 parent 为 nullptr 或是其 parent 的右子树上的结点 // 寻找结点 node 的前驱 BinaryTreeBaseNode* get_previous(BinaryTreeBaseNode* node) { if (node-\u003eleft != nullptr) { node = node-\u003eleft; while (node-\u003eright != nullptr) { node = node-\u003eright; } } else { auto paren","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树的后序遍历"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的遍历还记得之前提到的 postorder traversal 与 preorder traversal 吗，它们对二叉树同样适用。不过先别急，既然现在 child 的数量确定了，能不能将对结点的处理放在两个结点的处理之间完成呢？当然没问题！这种处理方式就是 中序遍历 (inorder traversal)，当然这也是 DFS 的一种。 如果将当前结点标记为 N，左子结点标记为 L，右子结点标记为 R，那么前序遍历就可以表示为 NLR，中序遍历可以表示为 LNR，后序遍历可以表示为 LRN。 二叉树的前序遍历 recursion void preorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } process(root); preorder(root-\u003eleft); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { process(root); s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); root = root-\u003erightl; } } 二叉树的中序遍历 recursion void inorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); process(root); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); process(root); root = root-\u003erightl; } } 二叉树的后序遍历 recursion void postorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); preorder(root-\u003eright); process(root); } loop 在后序遍历中，在左子结点处理完成后，只有结点没有右子结点或右子结点处理完之后，才能对结点进行处理。因此需要判别当前结点的 右子结点为空 或 刚刚处理过的结点 是该结点的右子结点。判断右子结点为空十分简单，但是问题是如何记录刚刚访问过的结点？ 利用一个变量指向正在处理的结点，当指向下一个待处理的结点时，其值就是该结点的上一个处理的结点，即处理前驱。 void postorder(BinaryTreeBaseNode* root) { stack s; BinaryTreeBaseNode* prev = nullptr; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); if (root-\u003eright == nullptr || root-\u003eright == prev) { prev = root; process(root); root = nullptr; } else { s.push(root); root = root-\u003eright; } } } 一个异构的前序遍历 如果你在对一个单词串进行翻转时，有一个简单可行的方法：先将单词串整体翻转，之后再逐词翻转。这样你就得到了一个对单词串的翻转！ \\[a\\ good\\ example\\quad\\Longrightarrow\\quad elpmaxe\\ doog\\ a\\quad\\Longrightarrow\\quad example\\ good\\ a\\] 这种异构的翻转也可以用在二叉树的 DFS 遍历上，前序遍历时遍历的结点顺序为 NLR (Node-\u003eLeft-\u003eRight)，而后续遍历的结点顺序为 LRN ，对后续遍历的顺序进行翻转就变为了 NRL 。如果以 NRL 的顺序进行遍历，最后将结果翻转也可以得到一个后序遍历的序列，这本质上是一种前序遍历的异构。 二叉树的层序遍历DFS 天生与 stack 结合在一起，而 BFS 与 queue 结合在一起。因此对于以上三种 DFS 遍历，使用 recursion 是一种简单、高效的理解与编码，而层序遍历则更适合于 loop。 void levelorder(BinaryTreeBaseNode* root) { queue q; q.push(root); while (!q.empty()) { for (int i = 0, cur_level_size = q.size(); i \u003c cur_level_size; ++i) { root = q.front(); q.pop(); process(root); if (root-\u003eleft != nullptr) { q.push(root-\u003eleft); } if (root-\u003eright != nullptr) { q.push(root-\u003eright); } } } } Morris 遍历在以上介绍的三种 DFS 遍历中，无论是 recursion 还是 loop 实现，都需要 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(N)\\) 的空间复杂度。而 1979 年由 J.H.Morris 在他的 论文 中提出了一种遍历方式，可以利用 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(1)\\) 的空间复杂度完成遍历。其核心思想是利用二叉树中的空闲指针，以实现空间复杂度的降低。 以 postorder 为例说明其算法的具体思路： 如果当前结点的左子树为空，则遍历右子树 如果当前结点的左子树不为空，在当前结点的左子树中找到当前结点在中序遍历中的前驱结点 如果前驱的右子结点为空，则将前驱结点的右子结点设置为当前结点，当前结点更新为其左子结点 如果前驱的右子结点为当前结点，则将其重新置空。倒序处理从当前结点的左子结点到该前驱结点路径上的所有结点。完成后将当前结点更新为当前结点的右子结点 重复步骤 1、2 直到遍历结束 void __reverse_process(BinaryTreeBaseNode* node) { if (node == nullptr) { return; } __reverse_process(node-\u003eright); process(node); } void postorderTraversal(BinaryTreeBaseNode* root) { BinaryTreeBaseNode* cur = root,* prev = nullptr; while (cur != nullptr) { prev = cur-\u003eleft; if (prev != nullptr) { while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != cur) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = cur; cur = cur-\u003eleft; continue; } prev-\u003eright = nullptr; __reverse_process(cur-\u003eleft); } cur = cur-\u003eright; } __reverse_precess(root); } 迭代器既然可以遍历一棵树，那么依然希望可以在这棵树上暂停下来，对结点进行一些操作，再继续进行迭代。当我们选择的遍历方法不一样时，其迭代时的前驱与后继就不相同。 如果现在给定一个迭代器，应该如何找到迭代器的前驱与后继迭代器。这里给出求解中序遍历前驱的算法步骤与代码，求解中序遍历后继的算法与前驱的算法类似，因此只给出代码。 求解前驱 如果结点的左子树存在，则前驱是结点左子树上最大的结点 如果结点的左子树不存在，则需要寻找结点的 parent 若结点是 parent 的右子树上的结点，则 parent 是其前驱 若结点是 parent 的左子树上的结点，继续向上寻找，直到 parent 为 nullptr 或是其 parent 的右子树上的结点 // 寻找结点 node 的前驱 BinaryTreeBaseNode* get_previous(BinaryTreeBaseNode* node) { if (node-\u003eleft != nullptr) { node = node-\u003eleft; while (node-\u003eright != nullptr) { node = node-\u003eright; } } else { auto paren","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉树的层序遍历"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的遍历还记得之前提到的 postorder traversal 与 preorder traversal 吗，它们对二叉树同样适用。不过先别急，既然现在 child 的数量确定了，能不能将对结点的处理放在两个结点的处理之间完成呢？当然没问题！这种处理方式就是 中序遍历 (inorder traversal)，当然这也是 DFS 的一种。 如果将当前结点标记为 N，左子结点标记为 L，右子结点标记为 R，那么前序遍历就可以表示为 NLR，中序遍历可以表示为 LNR，后序遍历可以表示为 LRN。 二叉树的前序遍历 recursion void preorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } process(root); preorder(root-\u003eleft); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { process(root); s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); root = root-\u003erightl; } } 二叉树的中序遍历 recursion void inorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); process(root); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); process(root); root = root-\u003erightl; } } 二叉树的后序遍历 recursion void postorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); preorder(root-\u003eright); process(root); } loop 在后序遍历中，在左子结点处理完成后，只有结点没有右子结点或右子结点处理完之后，才能对结点进行处理。因此需要判别当前结点的 右子结点为空 或 刚刚处理过的结点 是该结点的右子结点。判断右子结点为空十分简单，但是问题是如何记录刚刚访问过的结点？ 利用一个变量指向正在处理的结点，当指向下一个待处理的结点时，其值就是该结点的上一个处理的结点，即处理前驱。 void postorder(BinaryTreeBaseNode* root) { stack s; BinaryTreeBaseNode* prev = nullptr; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); if (root-\u003eright == nullptr || root-\u003eright == prev) { prev = root; process(root); root = nullptr; } else { s.push(root); root = root-\u003eright; } } } 一个异构的前序遍历 如果你在对一个单词串进行翻转时，有一个简单可行的方法：先将单词串整体翻转，之后再逐词翻转。这样你就得到了一个对单词串的翻转！ \\[a\\ good\\ example\\quad\\Longrightarrow\\quad elpmaxe\\ doog\\ a\\quad\\Longrightarrow\\quad example\\ good\\ a\\] 这种异构的翻转也可以用在二叉树的 DFS 遍历上，前序遍历时遍历的结点顺序为 NLR (Node-\u003eLeft-\u003eRight)，而后续遍历的结点顺序为 LRN ，对后续遍历的顺序进行翻转就变为了 NRL 。如果以 NRL 的顺序进行遍历，最后将结果翻转也可以得到一个后序遍历的序列，这本质上是一种前序遍历的异构。 二叉树的层序遍历DFS 天生与 stack 结合在一起，而 BFS 与 queue 结合在一起。因此对于以上三种 DFS 遍历，使用 recursion 是一种简单、高效的理解与编码，而层序遍历则更适合于 loop。 void levelorder(BinaryTreeBaseNode* root) { queue q; q.push(root); while (!q.empty()) { for (int i = 0, cur_level_size = q.size(); i \u003c cur_level_size; ++i) { root = q.front(); q.pop(); process(root); if (root-\u003eleft != nullptr) { q.push(root-\u003eleft); } if (root-\u003eright != nullptr) { q.push(root-\u003eright); } } } } Morris 遍历在以上介绍的三种 DFS 遍历中，无论是 recursion 还是 loop 实现，都需要 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(N)\\) 的空间复杂度。而 1979 年由 J.H.Morris 在他的 论文 中提出了一种遍历方式，可以利用 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(1)\\) 的空间复杂度完成遍历。其核心思想是利用二叉树中的空闲指针，以实现空间复杂度的降低。 以 postorder 为例说明其算法的具体思路： 如果当前结点的左子树为空，则遍历右子树 如果当前结点的左子树不为空，在当前结点的左子树中找到当前结点在中序遍历中的前驱结点 如果前驱的右子结点为空，则将前驱结点的右子结点设置为当前结点，当前结点更新为其左子结点 如果前驱的右子结点为当前结点，则将其重新置空。倒序处理从当前结点的左子结点到该前驱结点路径上的所有结点。完成后将当前结点更新为当前结点的右子结点 重复步骤 1、2 直到遍历结束 void __reverse_process(BinaryTreeBaseNode* node) { if (node == nullptr) { return; } __reverse_process(node-\u003eright); process(node); } void postorderTraversal(BinaryTreeBaseNode* root) { BinaryTreeBaseNode* cur = root,* prev = nullptr; while (cur != nullptr) { prev = cur-\u003eleft; if (prev != nullptr) { while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != cur) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = cur; cur = cur-\u003eleft; continue; } prev-\u003eright = nullptr; __reverse_process(cur-\u003eleft); } cur = cur-\u003eright; } __reverse_precess(root); } 迭代器既然可以遍历一棵树，那么依然希望可以在这棵树上暂停下来，对结点进行一些操作，再继续进行迭代。当我们选择的遍历方法不一样时，其迭代时的前驱与后继就不相同。 如果现在给定一个迭代器，应该如何找到迭代器的前驱与后继迭代器。这里给出求解中序遍历前驱的算法步骤与代码，求解中序遍历后继的算法与前驱的算法类似，因此只给出代码。 求解前驱 如果结点的左子树存在，则前驱是结点左子树上最大的结点 如果结点的左子树不存在，则需要寻找结点的 parent 若结点是 parent 的右子树上的结点，则 parent 是其前驱 若结点是 parent 的左子树上的结点，继续向上寻找，直到 parent 为 nullptr 或是其 parent 的右子树上的结点 // 寻找结点 node 的前驱 BinaryTreeBaseNode* get_previous(BinaryTreeBaseNode* node) { if (node-\u003eleft != nullptr) { node = node-\u003eleft; while (node-\u003eright != nullptr) { node = node-\u003eright; } } else { auto paren","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#morris-遍历"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉树的遍历还记得之前提到的 postorder traversal 与 preorder traversal 吗，它们对二叉树同样适用。不过先别急，既然现在 child 的数量确定了，能不能将对结点的处理放在两个结点的处理之间完成呢？当然没问题！这种处理方式就是 中序遍历 (inorder traversal)，当然这也是 DFS 的一种。 如果将当前结点标记为 N，左子结点标记为 L，右子结点标记为 R，那么前序遍历就可以表示为 NLR，中序遍历可以表示为 LNR，后序遍历可以表示为 LRN。 二叉树的前序遍历 recursion void preorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } process(root); preorder(root-\u003eleft); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { process(root); s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); root = root-\u003erightl; } } 二叉树的中序遍历 recursion void inorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); process(root); preorder(root-\u003eright); } loop void preorder(BinaryTreeBaseNode* root) { stack s; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); process(root); root = root-\u003erightl; } } 二叉树的后序遍历 recursion void postorder(BinaryTreeBaseNode* root) { if (root == nullptr) { return; } preorder(root-\u003eleft); preorder(root-\u003eright); process(root); } loop 在后序遍历中，在左子结点处理完成后，只有结点没有右子结点或右子结点处理完之后，才能对结点进行处理。因此需要判别当前结点的 右子结点为空 或 刚刚处理过的结点 是该结点的右子结点。判断右子结点为空十分简单，但是问题是如何记录刚刚访问过的结点？ 利用一个变量指向正在处理的结点，当指向下一个待处理的结点时，其值就是该结点的上一个处理的结点，即处理前驱。 void postorder(BinaryTreeBaseNode* root) { stack s; BinaryTreeBaseNode* prev = nullptr; while (!s.empty() || root != nullptr) { while (root != nullptr) { s.push(root); root = root-\u003eleft; } root = s.top(); s.pop(); if (root-\u003eright == nullptr || root-\u003eright == prev) { prev = root; process(root); root = nullptr; } else { s.push(root); root = root-\u003eright; } } } 一个异构的前序遍历 如果你在对一个单词串进行翻转时，有一个简单可行的方法：先将单词串整体翻转，之后再逐词翻转。这样你就得到了一个对单词串的翻转！ \\[a\\ good\\ example\\quad\\Longrightarrow\\quad elpmaxe\\ doog\\ a\\quad\\Longrightarrow\\quad example\\ good\\ a\\] 这种异构的翻转也可以用在二叉树的 DFS 遍历上，前序遍历时遍历的结点顺序为 NLR (Node-\u003eLeft-\u003eRight)，而后续遍历的结点顺序为 LRN ，对后续遍历的顺序进行翻转就变为了 NRL 。如果以 NRL 的顺序进行遍历，最后将结果翻转也可以得到一个后序遍历的序列，这本质上是一种前序遍历的异构。 二叉树的层序遍历DFS 天生与 stack 结合在一起，而 BFS 与 queue 结合在一起。因此对于以上三种 DFS 遍历，使用 recursion 是一种简单、高效的理解与编码，而层序遍历则更适合于 loop。 void levelorder(BinaryTreeBaseNode* root) { queue q; q.push(root); while (!q.empty()) { for (int i = 0, cur_level_size = q.size(); i \u003c cur_level_size; ++i) { root = q.front(); q.pop(); process(root); if (root-\u003eleft != nullptr) { q.push(root-\u003eleft); } if (root-\u003eright != nullptr) { q.push(root-\u003eright); } } } } Morris 遍历在以上介绍的三种 DFS 遍历中，无论是 recursion 还是 loop 实现，都需要 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(N)\\) 的空间复杂度。而 1979 年由 J.H.Morris 在他的 论文 中提出了一种遍历方式，可以利用 \\(\\mathcal{O}(N)\\) 的时间复杂度与 \\(\\mathcal{O}(1)\\) 的空间复杂度完成遍历。其核心思想是利用二叉树中的空闲指针，以实现空间复杂度的降低。 以 postorder 为例说明其算法的具体思路： 如果当前结点的左子树为空，则遍历右子树 如果当前结点的左子树不为空，在当前结点的左子树中找到当前结点在中序遍历中的前驱结点 如果前驱的右子结点为空，则将前驱结点的右子结点设置为当前结点，当前结点更新为其左子结点 如果前驱的右子结点为当前结点，则将其重新置空。倒序处理从当前结点的左子结点到该前驱结点路径上的所有结点。完成后将当前结点更新为当前结点的右子结点 重复步骤 1、2 直到遍历结束 void __reverse_process(BinaryTreeBaseNode* node) { if (node == nullptr) { return; } __reverse_process(node-\u003eright); process(node); } void postorderTraversal(BinaryTreeBaseNode* root) { BinaryTreeBaseNode* cur = root,* prev = nullptr; while (cur != nullptr) { prev = cur-\u003eleft; if (prev != nullptr) { while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != cur) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = cur; cur = cur-\u003eleft; continue; } prev-\u003eright = nullptr; __reverse_process(cur-\u003eleft); } cur = cur-\u003eright; } __reverse_precess(root); } 迭代器既然可以遍历一棵树，那么依然希望可以在这棵树上暂停下来，对结点进行一些操作，再继续进行迭代。当我们选择的遍历方法不一样时，其迭代时的前驱与后继就不相同。 如果现在给定一个迭代器，应该如何找到迭代器的前驱与后继迭代器。这里给出求解中序遍历前驱的算法步骤与代码，求解中序遍历后继的算法与前驱的算法类似，因此只给出代码。 求解前驱 如果结点的左子树存在，则前驱是结点左子树上最大的结点 如果结点的左子树不存在，则需要寻找结点的 parent 若结点是 parent 的右子树上的结点，则 parent 是其前驱 若结点是 parent 的左子树上的结点，继续向上寻找，直到 parent 为 nullptr 或是其 parent 的右子树上的结点 // 寻找结点 node 的前驱 BinaryTreeBaseNode* get_previous(BinaryTreeBaseNode* node) { if (node-\u003eleft != nullptr) { node = node-\u003eleft; while (node-\u003eright != nullptr) { node = node-\u003eright; } } else { auto paren","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#迭代器"},{"categories":["Algorithm⁄DataStructure"],"content":" 示例：表达式树下图展示了一棵 表达式树 (expression tree)，leaf node 是操作数 (operand)，而 internal node 为运算符 (operator)。由于所有操作都是二元的，因此这颗树为二叉树。每个 operator 的 operand 分别是其两个子树的运算结果。 这个树对应的表达式为 \\(a+b*c + (d*e+f)*g\\) ，如果我们对这颗树进行 postorder traversal 将得到序列 \\(a b c * + d e * f + g * +\\) ，这是一个后缀表达式；如果对其进行 preorder traversal，则会得到前缀表达式 \\(+ + a * b c * + * d e f g\\) ；最后试一下 inorder traversal，其结果应该是中缀表达式，不过其序列并没有带括号。 从 postorder traversal 的结果，可以很轻松的构建其这棵树。留给读者进行实现，这里将不再说明。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:2:4","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#示例-表达式树"},{"categories":["Algorithm⁄DataStructure"],"content":" 二叉查找树 (BST)假设树上每个结点都存储了一项数据，如果这些数据是杂乱无章的插入树中，那查找这些数据时并不容易，需要 \\(\\mathcal{O}(N)\\) 的时间复杂度来遍历每个结点搜索数据。 如果想要时间复杂度降到 \\(\\mathcal{O}(\\log_{}{N})\\) ，则需要在常数时间内，将问题的大小缩减。如果为一个结点加上限制，比如子树上的值总比当前结点的值大，而另一边总比当前结点的值小，如此便在常数时间内可以将问题的大小减半，可以判断接下来搜索左子树还是右子树。这种加以限制的二叉树被称为 二叉查找树 (Binary Search Tree, BST)。假定 BST 中左结点总是严格小于当前结点的值，而右结点总是不小于当前结点的值。 二叉树的遍历四种方法很简单，如果将其用于 BST 上有什么效果呢： 前序遍历： \\(6, 2, 1, 4, 3, 8, 7, 9\\) 中序遍历： \\(1, 2, 3, 4, 6, 7, 8, 9\\) 后序遍历： \\(1, 3, 4, 2, 7, 9, 8, 6\\) 层序遍历： \\(6, 2, 8, 1, 4, 7, 9, 3\\) ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:3:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#二叉查找树--bst"},{"categories":["Algorithm⁄DataStructure"],"content":" BST 中进行查找对 BST 的查找操作中，以下三种操作是最为简单的。 判断元素是否存在，存在时将返回 true ，反之返回 false template \u003cclass Element\u003e bool contains(BinaryTreeNode\u003cElement\u003e* root, const Element\u0026 target) { if (root == nullptr) { return false; } if (root-\u003edata == target) { return true; } return contains(root-\u003edata \u003c target ? root-\u003eright : root-\u003eleft, target); } 查找最小值并返回其结点 template \u003cclass Element\u003e BinaryTreeNode\u003cElement\u003e* find_min(BinaryTreeNode\u003cElement\u003e* root) { if (root == nullptr) { return nullptr; } return root-\u003eleft == nullptr ? root : find_min(root-\u003eleft); } 查找最大值并返回其结点 template \u003cclass Element\u003e BinaryTreeNode\u003cElement\u003e* find_max(BinaryTreeNode\u003cElement\u003e* root) { if (root != nullptr) { while (root-\u003eright != nullptr) { root = root-\u003eright; } } return root; } 刚刚我们定义了 BST 中 N、L、R 的关系，简单的数学表达即 \\(L \u003c N \\land N \\leq R\\) 。如果这颗二叉树里有相同的元素，如何找出这些元素的范围。实际上这个问题可以转换为求解 BST 上，给定元素的上下界，下界 (\\(bound_{lower}\\)) 是首个 不小于 给定元素的结点，上界 (\\(bound_{upper}\\)) 为首个 严格大于 给定元素的结点，相同元素的范围即 \\([bound_{lower}, bound_{upper})\\) 。 // 获取下界 template \u003cclass Element\u003e BinaryTreeNode* get_lower_bound(BinaryTreeNode* root, const Element\u0026 target) { auto result = root; while (root != nullptr) { if (!(root-\u003edata \u003c target)) { result = root; root = root-\u003eleft; } else { root = root-\u003eright; } } return result; } // 获取上界 template \u003cclass Element\u003e BinaryTreeNode* get_upper_bound(BinaryTreeNode* root, const Element\u0026 target) { auto result = root; while (root != nullptr) { if (target \u003c root-\u003edata) { result = root; root = root-\u003eleft; } else { root = root-\u003eright; } } return result; } ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:3:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#bst-中进行查找"},{"categories":["Algorithm⁄DataStructure"],"content":" BST 中进行插入与移除操作插入一个元素在 BST 上的操作十分简单，与 contains 函数一样，以 BST 的定义顺着 BST 向下寻找，直到结点的子结点为 nullptr 为止，将这个插入的结点挂载到这个查找到的子结点上。 如果是移除操作呢？我们一直忽略了如何在二叉树中移除一个元素，因为正常的一棵二叉树中，如果你想移除一个结点，你需要处理移除结点之后 parent 与 child 之间的关系。这并不好处理，你不确定这些 child 是否可以挂载到 parent 上，继续以 parent 的子结点出现。幸运的是，你可以直接将其值与一个 leaf 交换，并直接删除 leaf 就好，这样你就没有 parent 的担忧了。 这种交换的方式可以用于 BST 吗？当然是完全可以。现在只剩下一个问题了，如何保证在移除结点后，这棵树依然是 BST，稍微转换一下问题的问法：和哪个 leaf 交换不会影响 BST 的结构。 当然是和其前驱或者后继交换后再删除不会影响 BST 的整体结构，如果前驱或后继并不是 leaf，那么递归地交换结点的值，直到结点是 leaf 为止。如果这个结点本身就是 leaf，那不用找了，决定就是你了！ 可选择前驱还是后继呢，如果结点有右子树，则代表着其后继在右子树中；如果结点有左子树，则表达其前驱在左子树中。如果没有对应的子树，代表其前驱或者后继需要回到父结点寻找，为了不必要的复杂度，一般选择在其子树中寻找前驱 / 后继结点。如果你找到了一个结点的前驱 / 后继，但它不是叶结点，那需要继续寻找这个结点的前驱 / 后继，直到待删除的结点成为叶结点为止。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:3:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#bst-中进行插入与移除操作"},{"categories":["Algorithm⁄DataStructure"],"content":" BST 的平均情况分析一棵树的所有结点的深度和称为 内部路径长 (internal path length)，我们尝试计算 BST 平均路径长。令 \\(D(N)\\) 是具有 N 个结点的某棵树 T 的内部路径长，则有 \\(D(1) = 0\\) 。一棵 N 结点树是由一棵 \\(i (0 \\leq i \u003c N)\\) 结点左子树和一棵 \\(N - i - 1\\) 结点右子树及深度为 0 的根组成的，则可以得到递推关系 \\[D(N) = D(i) + D(N - i - 1) + N - 1.\\] 如果所有子树的大小都是等可能出现的，那么 \\(D(i)\\) 与 \\(D(N - i - 1)\\) 的平均值都是 \\((1/N)\\sum_{j=0}^{N-1}{D(j)}\\) ，于是 \\[D(N) = \\frac{2}{N}[\\sum_{j=0}^{N-1}{D(j)}] + N - 1.\\] 得到平均值 \\(D(N) = \\mathcal{O}(N \\log_{}{N})\\) ，因此结点的预期深度 \\(\\mathcal{O}(\\log_{}{N})\\) ，但这不意味着所有操作的平均运行时间是 \\(\\mathcal{O}(\\log_{}{N})\\) 。 Weiss 在书中为我们展示了一个随机生成的 500 个结点的 BST，其期望平均深度为 9.98。 如果交替插入和删除 \\(\\Theta(N^{2})\\) 次，那么树的平均期望深度将是 \\(\\Theta(\\sqrt{N})\\) 。而下图展示了在 25 万次插入移除随机值之后树的样子，结点的平均深度为 \\(12.51\\) 。其中有可能的一个原因是，在移除结点时 remove 总是倾向于移除结点的前驱，而保留了结点的后继。我们可以尝试随机移除结点前驱或后继的方法来缓解这种不平衡。还有一个原因是一个给定序列，由根 (给定序列的第一个元素) 的值决定这棵树的偏向，如果根元素过大则会导致左子树的结点更多，因为序列中大部位数都小于根，反之则导致右子树结点增多。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:3:3","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#bst-的平均情况分析"},{"categories":["Algorithm⁄DataStructure"],"content":" 线索二叉树 (TBT)如果一棵二叉树，所有原本为空的右孩子改为指向该结点的中序遍历的后继，所有原本为空的左孩子改为指向该结点的中序遍历的前驱，那么修改后的二叉树被称为 线索二叉树 (Threaded binary tree, TBT)。指向前驱、后继的指针被称为线索，对二叉树以某种遍历顺序进行扫描并为每个结点添加线索的过程称为二叉树的 线索化 ，进行线索化的目的是为了加快查找二叉树中某节点的前驱和后继的速度。 TBT 能线性地遍历二叉树，从而比递归的中序遍历更快。使用 TBT 也能够方便的找到一个结点的父结点，这比显式地使用父结点指针或者栈效率更高。这在栈空间有限，或者无法使用存储父节点的栈时很有作用。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:4:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#线索二叉树--tbt"},{"categories":["Algorithm⁄DataStructure"],"content":" TBT 的存储结构如果一棵二叉树线索化之后，需要分辨哪些是线索哪些是边，因此我们不得不修改数据结构，使得有 field 来指示该结点的左或右孩子是否是线索。 警告 该节剩下内容将作为扩展部分，可以进行选择性阅览。 我们改写为代码，由于 tag 实际上至于要 1 bit 就能指示线索，这时候 C / C++ 的优势就体现出来了，我们可以通过 位域 限制 tag 的大小，并将两个 tag 合并在 1 Byte 上来减少结构体空洞带来的内存浪费。 // 假设运行在 UNIX-like 64 bit OS 之上 template \u003cclass Element\u003e struct ThreadedBinaryTreeNode { Element data; unsigned char ltag : 1; unsigned char rtag : 1; ThreadedBinaryTreeNode* left,* right; }; 在之前的内容中，所有代码尽量都避免 C / C++ 的一些深度的语言特性，来避免读者因为编程语言的特性而带来的困扰。但是下面这个例子，将展示 C / C++ 因为其底层、灵活而展示出的强大。 在 LP64 模型下指针大小为 64 bit，从堆上分配来的内存的地址，起始地址能够被其最宽的成员大小整除。那么含有指针的 TreadedBinaryTreeNode 在分配时其地址可以被 8 byte 整除，这是什么概念的，就是其地址的 低 3 bit 一定为 0。我们可以充分利用这 3 bit，在不改变二叉树结点结构的情况下，辨别该结点是否是线索。如此整个结构体大小缩小了 8 byte。其实这个技巧在很多 C 代码中都有使用，甚至你可以考虑将结构体空洞废物利用起来，或者 C 的宏编程，这些奇技淫巧威力强大但降低了代码的可读性。 使用最低 3 bit 存储状态，那么我们在使用时就不能直接使用指针了，那么下列函数可能会对你使用这 3 bit 有帮助。 enum NodeStatus { LINK = 0, THREAD = 1, }; constexpr uintptr_t HIDE_BIT{3}; inline BinaryTreeBaseNode* get_node(BinaryTreeBaseNode* node) { return reinterpret_cast\u003cBinaryTreeBaseNode*\u003e(reinterpret_cast\u003cuintptr_t\u003e(node) \u0026 ~HIDE_BIT); } inline BinaryTreeBaseNode* get_left(BinaryTreeBaseNode* node) { return get_node(get_node(node)-\u003eleft); } inline BinaryTreeBaseNode* get_right(BinaryTreeBaseNode* node) { return get_node(get_node(node)-\u003eright); } inline uintptr_t get_status(BinaryTreeBaseNode* node) { return reinterpret_cast\u003cuintptr_t\u003e(node) \u0026 HIDE_BIT; } inline void set_left_link(BinaryTreeBaseNode* node, BinaryTreeBaseNode* left) { node-\u003eleft = left; } inline void set_left_thread(BinaryTreeBaseNode* node, BinaryTreeBaseNode* left) { node-\u003eleft = reinterpret_cast\u003cBinaryTreeBaseNode*\u003e(reinterpret_cast\u003cuintptr_t\u003e(left) | NodeStatus::THREAD); } inline void set_right_link(BinaryTreeBaseNode* node, BinaryTreeBaseNode* right) { node-\u003eright = right; } inline void set_right_thread(BinaryTreeBaseNode* node, BinaryTreeBaseNode* right) { node-\u003eright = reinterpret_cast\u003cBinaryTreeBaseNode*\u003e(reinterpret_cast\u003cuintptr_t\u003e(right) | NodeStatus::THREAD); } inline bool is_thread(BinaryTreeBaseNode* node) { return get_status(node) == NodeStatus::THREAD; } ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:4:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#tbt-的存储结构"},{"categories":["Algorithm⁄DataStructure"],"content":" 线索化线索化时需要记录结点的前驱，如果你仔细观察 Morris Traversal，你可能会发现， Morris Traversal 是在构建一部分线索二叉树。 template \u003cclass Element\u003e void inorder_threading(ThreadedBinaryTreeNode\u003cElement\u003e* root) { auto curr = root, prev = root; while (curr != nullptr) { if (curr-\u003eleft == nullptr) { curr-\u003eleft = prev; curr-\u003eltag = NodeStatus::THREAD; prev = curr; goto RIGHTING; } prev = curr-\u003eleft; while (prev-\u003eright != nullptr \u0026\u0026 prev-\u003eright != curr) { prev = prev-\u003eright; } if (prev-\u003eright == nullptr) { prev-\u003eright = curr; prev-\u003ertag = NodeStatus::THREAD; curr = curr-\u003eleft; continue; } prev = prev-\u003eright; RIGHTING: curr = curr-\u003eright; } // 由于 root 的后继的左线索指向不正确，需要对其进行修正 if (root-\u003eright != nullptr) { curr = root-\u003eright; while (curr-\u003eltag == NodeStatus::LINK) { curr = curr-\u003eleft; } curr-\u003eleft = root; curr-\u003eltag = NodeStatus::THREAD; } // 由于中序遍历第一个结点的左线索指向不正确，将其置空。其最后一个结点同样也是置空的 curr = root; while (curr-\u003eltag == NodeStatus::LINK) { curr = curr-\u003eleft; } curr-\u003eleft = nullptr; curr-\u003eltag = NodeStatus::LINK; } ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:4:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#线索化"},{"categories":["Algorithm⁄DataStructure"],"content":" 树和森林其实最后一点内容并没有多少，主要探讨树、森林、二叉树的关系，以及在严蔚敏老师的数据结构中提到的其他有关树的一些实现方式。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:5:0","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#树和森林"},{"categories":["Algorithm⁄DataStructure"],"content":" 树的其他实现方式我们可以用不同的实现方法来表示 树的属性 一节中的树结构。 父结点表示法 如果我们将所有结点放入一个顺序存储中，以下标直接存取结点，并在结点中表示其父结点的下标 孩子表示法 我们对父结点表示法稍加修改，结点中不再存放其父结点的下标，而是改为所有子结点的下标 兄弟表示法 即上文提到的树的表示方法。回过头我们再观察其结构，很容易发现这其实就是一棵二叉树，其左子结点代表其下所有子结点，而右结点代表其兄弟结点 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:5:1","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#树的其他实现方式"},{"categories":["Algorithm⁄DataStructure"],"content":" 森林由 \\(m (m \\in \\mathbb{N})\\) 棵互不相交的树的集合，称之为森林，即这些树没有公共 ancestor。我们可以将不同的树的根看作是 sibling，那么我们可以很轻松的将森林转换为一棵二叉树。 ","date":"08-19","objectID":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/:5:2","series":["数据结构与算法分析"],"tags":["Note","BinaryTree"],"title":"树结构","uri":"/2021/data_strucures_and_algorithm_analysis_003_tree_structure/#森林"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第三章笔记","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/"},{"categories":["Algorithm⁄DataStructure"],"content":" Should array indices start at 0 or 1? My compromise of 0.5 was rejected without, I thought, proper consideration. — Stan Kelly-Bootle ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:0:0","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#"},{"categories":["Algorithm⁄DataStructure"],"content":" 表 (List)我们将形如 \\(a_0, a_1, a_2, \\cdots, a_{N-1}\\) 组成的有限序列称为 list，这个 list 的大小是 \\(N (N \\in \\mathbb{N})\\) ，我们将大小为 0 的表称之为 空表 (empty list)。 除空表外的任何表，我们从 0 开始标记元素，最后一个元素的下标为 \\(N - 1\\) ，那么第 \\(i (i \\in \\mathbb{N}^{*})\\) 个元素是 \\(a_{i-1}\\) ，称 \\(a_{i}\\) 是 \\(a_{i + 1}\\) 的 前驱 ， \\(a_{i}\\) 是 \\(a_{i - 1}\\) 的 后继 。 警告 严蔚敏老师的数据结构中，第 \\(i (i \\in \\mathbb{N}^{*})\\) 个元素是 \\(a_{i}\\) 。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:0","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#表--list"},{"categories":["Algorithm⁄DataStructure"],"content":" List ADT template \u003cclass T, class Iter\u003e concept sequence_container = requires(T a, const T\u0026 b, typename T::const_iterator pos, Iter first, Iter last, typename T::iterator self_first, typename T::iterator self_last, size_type count, const typename T::value_type\u0026 value) { requires container\u003cT\u003e; requires input_iterator\u003cIter\u003e; // iterator { a.rbegin() } -\u003e typename T::reverse_iterator; { a.rend() } -\u003e typename T::reverse_iterator; { b.rbegin() } -\u003e typename T::const_reverse_iterator; { b.rend() } -\u003e typename T::const_reverse_iterator; { a.crbegin() } -\u003e typename T::const_reverse_iterator; { a.crend() } -\u003e typename T::const_reverse_iterator; // access { a.front() } -\u003e typename T::reference; { b.front() } -\u003e typename T::const_reference; { a.back() } -\u003e typename T::reference; { b.back() } -\u003e typename T::const_reference; // capacity a.resize(count, value); // modifier { a.insert(pos, value) } -\u003e typename T::iterator; { a.insert(pos, count, value) } -\u003e typename T::iterator; { a.insert(pos, first, last) } -\u003e typename T::iterator; { a.erase(pos) } -\u003e typename T::iterator; { a.erase(self_first, self_last) } -\u003e typename T::iterator; a.push_front(value); a.pop_front(); a.push_back(value); a.pop_back(); }; 函数名称 操作说明 rbegin() 获取指向逆向起始位置的 iterator – reverse_iterator rend() 获取指向逆向末尾位置的 iterator – reverse_iterator crbegin() 获取指向起始位置的 const_iterator – const_reverse_iterator crend() 获取指向末尾位置的 const_iterator – const_reverse_iterator front() 获取 container 第一个元素的引用 back() 获取 container 最后一个元素的引用 resize(count, value) 将 container 中元素数量限制到 count 个，若旧 size 不足 count 则使用 value 补齐 insert(pos, value) 在 pos 前插入一个 value，返回指向被插入 value 的迭代器 insert(pos, count, value) 在 pos 前插入 count 个 value，返回指向首个被插入元素的迭代器 insert(pos, first, last) 在 pos 前插入来自范围 \\([first, last)\\) 的元素，返回指向首个被插入元素的迭代器 erase(pos) 移除位于 pos 的元素，返回指向 pos 的后随迭代器 erase(self_first, self_last) 移除范围 \\([self\\_first, self\\_last)\\) 的元素，返回最后移除元素的后随迭代器 push_front(value) 将给定元素 value 添加到 container 开始 pop_front() 将第一个元素从 container 中删除 push_back(value) 将给定元素 value 添加到 container 末尾 pop_back() 将最后一个元素从 container 中删除 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:1","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#list-adt"},{"categories":["Algorithm⁄DataStructure"],"content":" 线性表的实现 顺序实现对表的所有操作都可以使用数组实现。数组是静态分配的，无法扩容，常常使用动态分配一段数组，当容量不够时进行生长。可以生长意味着不需要对表的大小的最大值进行估计。 在生长过程中，需要线性表分配一个全新的数组，并将之前的所有元素复制进新的数组中，复制完毕后将原数组释放。因此如果你的线性表频繁要求生长，那么会导致严重的性能开销，因为每次都需要 \\(\\Theta(N)\\) 来复制每个元素。如果生长系数过大，比如说 100 倍，但是无法使用那么多时，将造成存储空间的大量浪费。因此生长一般选取 2 倍 或 1.5 倍 比例，保证不会过于频繁生长，并使存储空间由不会浪费太多。 下图就是我们根据数组对线性表的实现： 现在思考一个问题，在使用 ADT *_back 与 *_front 时，它们两个有没有差别。 *_back 操作时直接将元素在尾端加入或移除，时间复杂度 \\(\\Theta(1)\\) *_front 操作时，由于 push 操作导致前端没有位置可以存储元素，而 pop 操作将导致前端产生一个空缺，因此它们都需要将之后的元素集体后移或前移，时间复杂度 \\(\\Theta(N)\\) 我们尝试给出一个存储结构，如下。这里并没有采用传统的使用整型变量记录当前长度和分配的容量，而是采用三个指针。其中 start 是该 container 的基址， finish 是后随最后一个元素的指针， end 则是后随数组空间的指针。因此在计算当前长度时只需要 \\(finish - start\\) 即可，当 \\(finish = start\\) 意味着当前线性表为空，当 \\(finish = end\\) 时意味着当前线性边需要生长。 template \u003cclass Element\u003e struct SequenceList { Element* start; Element* finish; Element* end; }; 这里存储结构中并没有给出迭代器，这是因为这是一个数组结构，我们可以将指针当作迭代器使用，这个迭代器是符合 contiguous_iterator 的。因此在实现该结构时，我们可以为其提供随即访问的接口 – operator[] 和 at ，它们接收一个 size_type 类型参数 n 用以 \\(\\Theta(1)\\) 时间复杂度访问 \\(start + n\\) 的元素。 信息 在使用顺序实现时，应该注意其支持快速的随机访问能力，在尾部具有高效操作，但中间或头部操作很低效。 单链表实现为了避免插入和删除的线性开销，我们允许线性表可以不连续存储，以避免修改时的整体移动。这种方式被称之为 链表 (linked list)，linked list 由一系列在内存中不必连续的结点组成，每个结点均含有元素域和到指向后继结点的链域。该链的最后一个结点置空 (nullptr 或 NULL) 以避免不必要的麻烦。 由于这样的 linked list 是单向的，因此我们也称其为单链表。由于结点是单向 Traverse 的，我们无法向前 Traverse，因此单链表 iterator 是一个 forward_iterator 。但这也造成了一点点麻烦，我们失去了随机访问元素的能力，只能以 \\(\\mathcal{O}(N)\\) 的复杂度进行结点的访问，除非你已经拥有了该结点的迭代器。当你拥有一个结点的迭代器时，可以以 \\(\\mathcal{O}(1)\\) 的时间复杂度对其进行操作，删除或插入一个结点。 如何获取到单链表的长度呢？如果增加一个额外的长度域，对于这些结点来说是不必要的，我们只需要一个记录长度的域就好；而在结点中增加域不止造成了内存的浪费，如果用此记录长度，在对结点操作时，我们将丢失正确的长度信息，除非以 \\(\\mathcal{O}(N)\\) 的代价修改所有结点上的长度域。我们引入一个特殊的头结点，每个线性表实例只需要一个 head 即可。为了快速在尾部进行插入，我们也需要一个指向尾部的域，方便插入操作，移除操作只能由缓慢的 Traverse 找到前驱结点 最后说明一下 end 迭代器指向 nullptr 的原因，由于我们在遍历时，认为区间是 \\([first, last)\\) ，因此如果是有 finish field 作为 end 迭代器，那么我们将丢失最后一个结点。 单链表的存储结构 这里的实现使用了 BaseNode ，并在实现 Head 和 Node 时分别继承 BaseNode。由于 BaseNode 只实现关于链表链域的操作，虽然 Head 和 Node 有着不同的操作，但共享其 base class 所提供的链域操作。 struct ForwardListBaseNode { // 单链表基础结点，用于存储并处理链域 ForwardListBaseNode* next; }; struct ForwardListHead : ForwardListBaseNode { // 单链表的头结点，用于存储长度与尾结点 size_t size; ForwardListBaseNode* finish; }; template \u003cclass Element\u003e struct ForwardListNode : ForwardListBaseNode { // 单链表的结点，用于存储真正的数据 Element value; }; 单链表 BaseNode 的实现 刚刚说了 BaseNode 主要实现对链域的操作，对一个结点，主要有插入、移除结点两种操作。受限于 forward_iterator ，为了运行效率，我们对 ADT 的插入删除进行一些修改。 函数 修改前 修改后 insert(pos, value) 在 pos 前插入一个 value 在 pos 之后插入一个 value insert(pos, first, last) 在 pos 之前插入范围 \\([first, last)\\) 的元素 在 pos 之后插入该区间元素 erase(pos) 移除位于 pos 的元素 移除 pos 之后一个元素 erase(self_first, self_last) 移除范围 \\([self\\_first, self\\_last)\\) 的元素 移除范围 \\((self\\_first, self\\_last)\\) 的元素 pop_back() 移除最后一个元素 删除该方法，不再提供 可以看到修改后，函数主要将该位置 pos 之后的元素进行删除，因此我们可以实现以下四个函数，用以对 insert 与 erase 的支持。但是 erase 与 insert 中都没有实现对边界条件的判定，这应该由具体实现 ForwardList 时完成。 // 将 node 插入到 pos 之后 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* node) { node-\u003enext = pos-\u003enext; pos-\u003enext = node; } // 由实现范围 [first, last) 上迭代器到单链表的构造，接收单链表 [first, last) 并插入 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* first, ForwardListBaseNode* last) { last-\u003enext = pos-\u003enext; pos-\u003enext = first; } // 移除 pos 之后一个的元素，并将其返回 ForwardListBaseNode* erase(ForwardListBaseNode* pos) { ForwardListBaseNode* erase = pos-\u003enext; pos-\u003enext = erase-\u003enext; return erase; } // 移除 [first + 1, last) 的所有元素，并将其 first + 1 返回 ForwardListBaseNode* erase(ForwardListBaseNode* first, ForwardListBaseNode* last) { ForwardListBaseNode* erase = first-\u003enext; first-\u003enext = last; return erase; } 对于以上的代码进行分析，我们可以得知，一旦位置、端结点确定，从 linked list 中添加或移除任意多的连续结点，其时间复杂度是 \\(\\mathcal{O}(1)\\) 的。至于构造和析构 \\([first, last)\\) 上的元素，不再 BaseNode 的讨论范围内，它们不是针对链域的操作。 需要注意的是，我们在实现 erase 的过程中并没有删除 erase 结点指向的 next，也就是说虽然它已经不在链表中，但是通过访问其 next field 依然可以访问曾经的后继。这一操作主要是为了释放结点，erase 移除 \\((first, last)\\) 后将返回 first 结点的后继，即第一个被移除的结点，我们可以依次对这些结点进行释放，直到准备释放的结点变为 last 为止。当然我们也可以将其设置为 nullptr，只不过判断条件变为了 \\(node != nullptr\\) ，不过不修改也能完成这样的操作且开销更小。 双链表单链表如果要删除当前结点，则必须遍历寻找该结点的前驱，才能将其删除。这种方法时间复杂度变成了线性，有什么方法可以让我们","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#线性表的实现"},{"categories":["Algorithm⁄DataStructure"],"content":" 线性表的实现 顺序实现对表的所有操作都可以使用数组实现。数组是静态分配的，无法扩容，常常使用动态分配一段数组，当容量不够时进行生长。可以生长意味着不需要对表的大小的最大值进行估计。 在生长过程中，需要线性表分配一个全新的数组，并将之前的所有元素复制进新的数组中，复制完毕后将原数组释放。因此如果你的线性表频繁要求生长，那么会导致严重的性能开销，因为每次都需要 \\(\\Theta(N)\\) 来复制每个元素。如果生长系数过大，比如说 100 倍，但是无法使用那么多时，将造成存储空间的大量浪费。因此生长一般选取 2 倍 或 1.5 倍 比例，保证不会过于频繁生长，并使存储空间由不会浪费太多。 下图就是我们根据数组对线性表的实现： 现在思考一个问题，在使用 ADT *_back 与 *_front 时，它们两个有没有差别。 *_back 操作时直接将元素在尾端加入或移除，时间复杂度 \\(\\Theta(1)\\) *_front 操作时，由于 push 操作导致前端没有位置可以存储元素，而 pop 操作将导致前端产生一个空缺，因此它们都需要将之后的元素集体后移或前移，时间复杂度 \\(\\Theta(N)\\) 我们尝试给出一个存储结构，如下。这里并没有采用传统的使用整型变量记录当前长度和分配的容量，而是采用三个指针。其中 start 是该 container 的基址， finish 是后随最后一个元素的指针， end 则是后随数组空间的指针。因此在计算当前长度时只需要 \\(finish - start\\) 即可，当 \\(finish = start\\) 意味着当前线性表为空，当 \\(finish = end\\) 时意味着当前线性边需要生长。 template struct SequenceList { Element* start; Element* finish; Element* end; }; 这里存储结构中并没有给出迭代器，这是因为这是一个数组结构，我们可以将指针当作迭代器使用，这个迭代器是符合 contiguous_iterator 的。因此在实现该结构时，我们可以为其提供随即访问的接口 – operator[] 和 at ，它们接收一个 size_type 类型参数 n 用以 \\(\\Theta(1)\\) 时间复杂度访问 \\(start + n\\) 的元素。 信息 在使用顺序实现时，应该注意其支持快速的随机访问能力，在尾部具有高效操作，但中间或头部操作很低效。 单链表实现为了避免插入和删除的线性开销，我们允许线性表可以不连续存储，以避免修改时的整体移动。这种方式被称之为 链表 (linked list)，linked list 由一系列在内存中不必连续的结点组成，每个结点均含有元素域和到指向后继结点的链域。该链的最后一个结点置空 (nullptr 或 NULL) 以避免不必要的麻烦。 由于这样的 linked list 是单向的，因此我们也称其为单链表。由于结点是单向 Traverse 的，我们无法向前 Traverse，因此单链表 iterator 是一个 forward_iterator 。但这也造成了一点点麻烦，我们失去了随机访问元素的能力，只能以 \\(\\mathcal{O}(N)\\) 的复杂度进行结点的访问，除非你已经拥有了该结点的迭代器。当你拥有一个结点的迭代器时，可以以 \\(\\mathcal{O}(1)\\) 的时间复杂度对其进行操作，删除或插入一个结点。 如何获取到单链表的长度呢？如果增加一个额外的长度域，对于这些结点来说是不必要的，我们只需要一个记录长度的域就好；而在结点中增加域不止造成了内存的浪费，如果用此记录长度，在对结点操作时，我们将丢失正确的长度信息，除非以 \\(\\mathcal{O}(N)\\) 的代价修改所有结点上的长度域。我们引入一个特殊的头结点，每个线性表实例只需要一个 head 即可。为了快速在尾部进行插入，我们也需要一个指向尾部的域，方便插入操作，移除操作只能由缓慢的 Traverse 找到前驱结点 最后说明一下 end 迭代器指向 nullptr 的原因，由于我们在遍历时，认为区间是 \\([first, last)\\) ，因此如果是有 finish field 作为 end 迭代器，那么我们将丢失最后一个结点。 单链表的存储结构 这里的实现使用了 BaseNode ，并在实现 Head 和 Node 时分别继承 BaseNode。由于 BaseNode 只实现关于链表链域的操作，虽然 Head 和 Node 有着不同的操作，但共享其 base class 所提供的链域操作。 struct ForwardListBaseNode { // 单链表基础结点，用于存储并处理链域 ForwardListBaseNode* next; }; struct ForwardListHead : ForwardListBaseNode { // 单链表的头结点，用于存储长度与尾结点 size_t size; ForwardListBaseNode* finish; }; template struct ForwardListNode : ForwardListBaseNode { // 单链表的结点，用于存储真正的数据 Element value; }; 单链表 BaseNode 的实现 刚刚说了 BaseNode 主要实现对链域的操作，对一个结点，主要有插入、移除结点两种操作。受限于 forward_iterator ，为了运行效率，我们对 ADT 的插入删除进行一些修改。 函数 修改前 修改后 insert(pos, value) 在 pos 前插入一个 value 在 pos 之后插入一个 value insert(pos, first, last) 在 pos 之前插入范围 \\([first, last)\\) 的元素 在 pos 之后插入该区间元素 erase(pos) 移除位于 pos 的元素 移除 pos 之后一个元素 erase(self_first, self_last) 移除范围 \\([self\\_first, self\\_last)\\) 的元素 移除范围 \\((self\\_first, self\\_last)\\) 的元素 pop_back() 移除最后一个元素 删除该方法，不再提供 可以看到修改后，函数主要将该位置 pos 之后的元素进行删除，因此我们可以实现以下四个函数，用以对 insert 与 erase 的支持。但是 erase 与 insert 中都没有实现对边界条件的判定，这应该由具体实现 ForwardList 时完成。 // 将 node 插入到 pos 之后 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* node) { node-\u003enext = pos-\u003enext; pos-\u003enext = node; } // 由实现范围 [first, last) 上迭代器到单链表的构造，接收单链表 [first, last) 并插入 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* first, ForwardListBaseNode* last) { last-\u003enext = pos-\u003enext; pos-\u003enext = first; } // 移除 pos 之后一个的元素，并将其返回 ForwardListBaseNode* erase(ForwardListBaseNode* pos) { ForwardListBaseNode* erase = pos-\u003enext; pos-\u003enext = erase-\u003enext; return erase; } // 移除 [first + 1, last) 的所有元素，并将其 first + 1 返回 ForwardListBaseNode* erase(ForwardListBaseNode* first, ForwardListBaseNode* last) { ForwardListBaseNode* erase = first-\u003enext; first-\u003enext = last; return erase; } 对于以上的代码进行分析，我们可以得知，一旦位置、端结点确定，从 linked list 中添加或移除任意多的连续结点，其时间复杂度是 \\(\\mathcal{O}(1)\\) 的。至于构造和析构 \\([first, last)\\) 上的元素，不再 BaseNode 的讨论范围内，它们不是针对链域的操作。 需要注意的是，我们在实现 erase 的过程中并没有删除 erase 结点指向的 next，也就是说虽然它已经不在链表中，但是通过访问其 next field 依然可以访问曾经的后继。这一操作主要是为了释放结点，erase 移除 \\((first, last)\\) 后将返回 first 结点的后继，即第一个被移除的结点，我们可以依次对这些结点进行释放，直到准备释放的结点变为 last 为止。当然我们也可以将其设置为 nullptr，只不过判断条件变为了 \\(node != nullptr\\) ，不过不修改也能完成这样的操作且开销更小。 双链表单链表如果要删除当前结点，则必须遍历寻找该结点的前驱，才能将其删除。这种方法时间复杂度变成了线性，有什么方法可以让我们","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#顺序实现"},{"categories":["Algorithm⁄DataStructure"],"content":" 线性表的实现 顺序实现对表的所有操作都可以使用数组实现。数组是静态分配的，无法扩容，常常使用动态分配一段数组，当容量不够时进行生长。可以生长意味着不需要对表的大小的最大值进行估计。 在生长过程中，需要线性表分配一个全新的数组，并将之前的所有元素复制进新的数组中，复制完毕后将原数组释放。因此如果你的线性表频繁要求生长，那么会导致严重的性能开销，因为每次都需要 \\(\\Theta(N)\\) 来复制每个元素。如果生长系数过大，比如说 100 倍，但是无法使用那么多时，将造成存储空间的大量浪费。因此生长一般选取 2 倍 或 1.5 倍 比例，保证不会过于频繁生长，并使存储空间由不会浪费太多。 下图就是我们根据数组对线性表的实现： 现在思考一个问题，在使用 ADT *_back 与 *_front 时，它们两个有没有差别。 *_back 操作时直接将元素在尾端加入或移除，时间复杂度 \\(\\Theta(1)\\) *_front 操作时，由于 push 操作导致前端没有位置可以存储元素，而 pop 操作将导致前端产生一个空缺，因此它们都需要将之后的元素集体后移或前移，时间复杂度 \\(\\Theta(N)\\) 我们尝试给出一个存储结构，如下。这里并没有采用传统的使用整型变量记录当前长度和分配的容量，而是采用三个指针。其中 start 是该 container 的基址， finish 是后随最后一个元素的指针， end 则是后随数组空间的指针。因此在计算当前长度时只需要 \\(finish - start\\) 即可，当 \\(finish = start\\) 意味着当前线性表为空，当 \\(finish = end\\) 时意味着当前线性边需要生长。 template struct SequenceList { Element* start; Element* finish; Element* end; }; 这里存储结构中并没有给出迭代器，这是因为这是一个数组结构，我们可以将指针当作迭代器使用，这个迭代器是符合 contiguous_iterator 的。因此在实现该结构时，我们可以为其提供随即访问的接口 – operator[] 和 at ，它们接收一个 size_type 类型参数 n 用以 \\(\\Theta(1)\\) 时间复杂度访问 \\(start + n\\) 的元素。 信息 在使用顺序实现时，应该注意其支持快速的随机访问能力，在尾部具有高效操作，但中间或头部操作很低效。 单链表实现为了避免插入和删除的线性开销，我们允许线性表可以不连续存储，以避免修改时的整体移动。这种方式被称之为 链表 (linked list)，linked list 由一系列在内存中不必连续的结点组成，每个结点均含有元素域和到指向后继结点的链域。该链的最后一个结点置空 (nullptr 或 NULL) 以避免不必要的麻烦。 由于这样的 linked list 是单向的，因此我们也称其为单链表。由于结点是单向 Traverse 的，我们无法向前 Traverse，因此单链表 iterator 是一个 forward_iterator 。但这也造成了一点点麻烦，我们失去了随机访问元素的能力，只能以 \\(\\mathcal{O}(N)\\) 的复杂度进行结点的访问，除非你已经拥有了该结点的迭代器。当你拥有一个结点的迭代器时，可以以 \\(\\mathcal{O}(1)\\) 的时间复杂度对其进行操作，删除或插入一个结点。 如何获取到单链表的长度呢？如果增加一个额外的长度域，对于这些结点来说是不必要的，我们只需要一个记录长度的域就好；而在结点中增加域不止造成了内存的浪费，如果用此记录长度，在对结点操作时，我们将丢失正确的长度信息，除非以 \\(\\mathcal{O}(N)\\) 的代价修改所有结点上的长度域。我们引入一个特殊的头结点，每个线性表实例只需要一个 head 即可。为了快速在尾部进行插入，我们也需要一个指向尾部的域，方便插入操作，移除操作只能由缓慢的 Traverse 找到前驱结点 最后说明一下 end 迭代器指向 nullptr 的原因，由于我们在遍历时，认为区间是 \\([first, last)\\) ，因此如果是有 finish field 作为 end 迭代器，那么我们将丢失最后一个结点。 单链表的存储结构 这里的实现使用了 BaseNode ，并在实现 Head 和 Node 时分别继承 BaseNode。由于 BaseNode 只实现关于链表链域的操作，虽然 Head 和 Node 有着不同的操作，但共享其 base class 所提供的链域操作。 struct ForwardListBaseNode { // 单链表基础结点，用于存储并处理链域 ForwardListBaseNode* next; }; struct ForwardListHead : ForwardListBaseNode { // 单链表的头结点，用于存储长度与尾结点 size_t size; ForwardListBaseNode* finish; }; template struct ForwardListNode : ForwardListBaseNode { // 单链表的结点，用于存储真正的数据 Element value; }; 单链表 BaseNode 的实现 刚刚说了 BaseNode 主要实现对链域的操作，对一个结点，主要有插入、移除结点两种操作。受限于 forward_iterator ，为了运行效率，我们对 ADT 的插入删除进行一些修改。 函数 修改前 修改后 insert(pos, value) 在 pos 前插入一个 value 在 pos 之后插入一个 value insert(pos, first, last) 在 pos 之前插入范围 \\([first, last)\\) 的元素 在 pos 之后插入该区间元素 erase(pos) 移除位于 pos 的元素 移除 pos 之后一个元素 erase(self_first, self_last) 移除范围 \\([self\\_first, self\\_last)\\) 的元素 移除范围 \\((self\\_first, self\\_last)\\) 的元素 pop_back() 移除最后一个元素 删除该方法，不再提供 可以看到修改后，函数主要将该位置 pos 之后的元素进行删除，因此我们可以实现以下四个函数，用以对 insert 与 erase 的支持。但是 erase 与 insert 中都没有实现对边界条件的判定，这应该由具体实现 ForwardList 时完成。 // 将 node 插入到 pos 之后 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* node) { node-\u003enext = pos-\u003enext; pos-\u003enext = node; } // 由实现范围 [first, last) 上迭代器到单链表的构造，接收单链表 [first, last) 并插入 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* first, ForwardListBaseNode* last) { last-\u003enext = pos-\u003enext; pos-\u003enext = first; } // 移除 pos 之后一个的元素，并将其返回 ForwardListBaseNode* erase(ForwardListBaseNode* pos) { ForwardListBaseNode* erase = pos-\u003enext; pos-\u003enext = erase-\u003enext; return erase; } // 移除 [first + 1, last) 的所有元素，并将其 first + 1 返回 ForwardListBaseNode* erase(ForwardListBaseNode* first, ForwardListBaseNode* last) { ForwardListBaseNode* erase = first-\u003enext; first-\u003enext = last; return erase; } 对于以上的代码进行分析，我们可以得知，一旦位置、端结点确定，从 linked list 中添加或移除任意多的连续结点，其时间复杂度是 \\(\\mathcal{O}(1)\\) 的。至于构造和析构 \\([first, last)\\) 上的元素，不再 BaseNode 的讨论范围内，它们不是针对链域的操作。 需要注意的是，我们在实现 erase 的过程中并没有删除 erase 结点指向的 next，也就是说虽然它已经不在链表中，但是通过访问其 next field 依然可以访问曾经的后继。这一操作主要是为了释放结点，erase 移除 \\((first, last)\\) 后将返回 first 结点的后继，即第一个被移除的结点，我们可以依次对这些结点进行释放，直到准备释放的结点变为 last 为止。当然我们也可以将其设置为 nullptr，只不过判断条件变为了 \\(node != nullptr\\) ，不过不修改也能完成这样的操作且开销更小。 双链表单链表如果要删除当前结点，则必须遍历寻找该结点的前驱，才能将其删除。这种方法时间复杂度变成了线性，有什么方法可以让我们","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#单链表实现"},{"categories":["Algorithm⁄DataStructure"],"content":" 线性表的实现 顺序实现对表的所有操作都可以使用数组实现。数组是静态分配的，无法扩容，常常使用动态分配一段数组，当容量不够时进行生长。可以生长意味着不需要对表的大小的最大值进行估计。 在生长过程中，需要线性表分配一个全新的数组，并将之前的所有元素复制进新的数组中，复制完毕后将原数组释放。因此如果你的线性表频繁要求生长，那么会导致严重的性能开销，因为每次都需要 \\(\\Theta(N)\\) 来复制每个元素。如果生长系数过大，比如说 100 倍，但是无法使用那么多时，将造成存储空间的大量浪费。因此生长一般选取 2 倍 或 1.5 倍 比例，保证不会过于频繁生长，并使存储空间由不会浪费太多。 下图就是我们根据数组对线性表的实现： 现在思考一个问题，在使用 ADT *_back 与 *_front 时，它们两个有没有差别。 *_back 操作时直接将元素在尾端加入或移除，时间复杂度 \\(\\Theta(1)\\) *_front 操作时，由于 push 操作导致前端没有位置可以存储元素，而 pop 操作将导致前端产生一个空缺，因此它们都需要将之后的元素集体后移或前移，时间复杂度 \\(\\Theta(N)\\) 我们尝试给出一个存储结构，如下。这里并没有采用传统的使用整型变量记录当前长度和分配的容量，而是采用三个指针。其中 start 是该 container 的基址， finish 是后随最后一个元素的指针， end 则是后随数组空间的指针。因此在计算当前长度时只需要 \\(finish - start\\) 即可，当 \\(finish = start\\) 意味着当前线性表为空，当 \\(finish = end\\) 时意味着当前线性边需要生长。 template struct SequenceList { Element* start; Element* finish; Element* end; }; 这里存储结构中并没有给出迭代器，这是因为这是一个数组结构，我们可以将指针当作迭代器使用，这个迭代器是符合 contiguous_iterator 的。因此在实现该结构时，我们可以为其提供随即访问的接口 – operator[] 和 at ，它们接收一个 size_type 类型参数 n 用以 \\(\\Theta(1)\\) 时间复杂度访问 \\(start + n\\) 的元素。 信息 在使用顺序实现时，应该注意其支持快速的随机访问能力，在尾部具有高效操作，但中间或头部操作很低效。 单链表实现为了避免插入和删除的线性开销，我们允许线性表可以不连续存储，以避免修改时的整体移动。这种方式被称之为 链表 (linked list)，linked list 由一系列在内存中不必连续的结点组成，每个结点均含有元素域和到指向后继结点的链域。该链的最后一个结点置空 (nullptr 或 NULL) 以避免不必要的麻烦。 由于这样的 linked list 是单向的，因此我们也称其为单链表。由于结点是单向 Traverse 的，我们无法向前 Traverse，因此单链表 iterator 是一个 forward_iterator 。但这也造成了一点点麻烦，我们失去了随机访问元素的能力，只能以 \\(\\mathcal{O}(N)\\) 的复杂度进行结点的访问，除非你已经拥有了该结点的迭代器。当你拥有一个结点的迭代器时，可以以 \\(\\mathcal{O}(1)\\) 的时间复杂度对其进行操作，删除或插入一个结点。 如何获取到单链表的长度呢？如果增加一个额外的长度域，对于这些结点来说是不必要的，我们只需要一个记录长度的域就好；而在结点中增加域不止造成了内存的浪费，如果用此记录长度，在对结点操作时，我们将丢失正确的长度信息，除非以 \\(\\mathcal{O}(N)\\) 的代价修改所有结点上的长度域。我们引入一个特殊的头结点，每个线性表实例只需要一个 head 即可。为了快速在尾部进行插入，我们也需要一个指向尾部的域，方便插入操作，移除操作只能由缓慢的 Traverse 找到前驱结点 最后说明一下 end 迭代器指向 nullptr 的原因，由于我们在遍历时，认为区间是 \\([first, last)\\) ，因此如果是有 finish field 作为 end 迭代器，那么我们将丢失最后一个结点。 单链表的存储结构 这里的实现使用了 BaseNode ，并在实现 Head 和 Node 时分别继承 BaseNode。由于 BaseNode 只实现关于链表链域的操作，虽然 Head 和 Node 有着不同的操作，但共享其 base class 所提供的链域操作。 struct ForwardListBaseNode { // 单链表基础结点，用于存储并处理链域 ForwardListBaseNode* next; }; struct ForwardListHead : ForwardListBaseNode { // 单链表的头结点，用于存储长度与尾结点 size_t size; ForwardListBaseNode* finish; }; template struct ForwardListNode : ForwardListBaseNode { // 单链表的结点，用于存储真正的数据 Element value; }; 单链表 BaseNode 的实现 刚刚说了 BaseNode 主要实现对链域的操作，对一个结点，主要有插入、移除结点两种操作。受限于 forward_iterator ，为了运行效率，我们对 ADT 的插入删除进行一些修改。 函数 修改前 修改后 insert(pos, value) 在 pos 前插入一个 value 在 pos 之后插入一个 value insert(pos, first, last) 在 pos 之前插入范围 \\([first, last)\\) 的元素 在 pos 之后插入该区间元素 erase(pos) 移除位于 pos 的元素 移除 pos 之后一个元素 erase(self_first, self_last) 移除范围 \\([self\\_first, self\\_last)\\) 的元素 移除范围 \\((self\\_first, self\\_last)\\) 的元素 pop_back() 移除最后一个元素 删除该方法，不再提供 可以看到修改后，函数主要将该位置 pos 之后的元素进行删除，因此我们可以实现以下四个函数，用以对 insert 与 erase 的支持。但是 erase 与 insert 中都没有实现对边界条件的判定，这应该由具体实现 ForwardList 时完成。 // 将 node 插入到 pos 之后 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* node) { node-\u003enext = pos-\u003enext; pos-\u003enext = node; } // 由实现范围 [first, last) 上迭代器到单链表的构造，接收单链表 [first, last) 并插入 void insert(ForwardListBaseNode* pos, ForwardListBaseNode* first, ForwardListBaseNode* last) { last-\u003enext = pos-\u003enext; pos-\u003enext = first; } // 移除 pos 之后一个的元素，并将其返回 ForwardListBaseNode* erase(ForwardListBaseNode* pos) { ForwardListBaseNode* erase = pos-\u003enext; pos-\u003enext = erase-\u003enext; return erase; } // 移除 [first + 1, last) 的所有元素，并将其 first + 1 返回 ForwardListBaseNode* erase(ForwardListBaseNode* first, ForwardListBaseNode* last) { ForwardListBaseNode* erase = first-\u003enext; first-\u003enext = last; return erase; } 对于以上的代码进行分析，我们可以得知，一旦位置、端结点确定，从 linked list 中添加或移除任意多的连续结点，其时间复杂度是 \\(\\mathcal{O}(1)\\) 的。至于构造和析构 \\([first, last)\\) 上的元素，不再 BaseNode 的讨论范围内，它们不是针对链域的操作。 需要注意的是，我们在实现 erase 的过程中并没有删除 erase 结点指向的 next，也就是说虽然它已经不在链表中，但是通过访问其 next field 依然可以访问曾经的后继。这一操作主要是为了释放结点，erase 移除 \\((first, last)\\) 后将返回 first 结点的后继，即第一个被移除的结点，我们可以依次对这些结点进行释放，直到准备释放的结点变为 last 为止。当然我们也可以将其设置为 nullptr，只不过判断条件变为了 \\(node != nullptr\\) ，不过不修改也能完成这样的操作且开销更小。 双链表单链表如果要删除当前结点，则必须遍历寻找该结点的前驱，才能将其删除。这种方法时间复杂度变成了线性，有什么方法可以让我们","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#双链表"},{"categories":["Algorithm⁄DataStructure"],"content":" 一些关于表的算法为了屏蔽一些不必要的实现细节，因此我们约定，使用 iterator 进行 traverse，且 iterator 可以通过 handle 取得底层的链表结点。而函数参数中的引用类型 T\u0026 则表示着对该形式参数的修改将会修改实际参数。 合并两个已排序链表现在假设两个链表都已按照从小到大排列，将两个链表 a 与 b 合并到 c，且合并后的链表也按照从小到大进行排列。 void __transfer(iterator\u0026 pos, iterator\u0026 c) { iterator it = pos++; insert(c.handle(), it.handle(), pos.handle()); } void merge(iterator a_begin, iterator a_end, iterator b_begin, iterator b_end, iterator\u0026 c) { while (a_begin != a_end \u0026\u0026 b_begin != b_end) { __transfer(*a_begin \u003c *b_begin ? a_begin : b_begin, c); } if (a_begin != a_end) { insert(c.handle(), a_begin.handle(), a_end.handle()); } if (b_begin != b_end) { insert(c.handle(), b_begin.handle(), b_end.handle()); } } 引入了 __transfer 函数将找到的 a、b 当前最小的元素插入 c 中，并使其迭代器向前步进一。在 a 或 b 结束之后，我们将 a 或 b 剩余的元素全部添加到 c 的后面，这些元素是最大的一批。分析该算法的时间复杂度得 \\(\\mathcal{O}(size_{a}+size_{b}-1)\\) 。 反转反转链表是一个很有意思的操作，尤其是针对没有前驱结点的单链表来说。 void reverse(ForwardListBaseNode* head) { ForwardListBaseNode* curr = head-\u003enext; head = nullptr; while (curr != nullptr) { ListNode* next = curr-\u003enext; curr-\u003enext = head; head = curr; curr = next; } } 这个方法直接使用到了 ForwardListHead ，利用 head 指向当前结点的前驱，当 traverse 完成后，head 也顺利指向最终结果。其时间复杂度 \\(\\mathcal{O}(N)\\) 。我们可以将其改为递归方式，时间复杂度不变： ForwardListBaseNode* __recursion(ForwardListBaseNode* node, ForwardListBaseNode* head) { if (!node) { return nullptr; } if (node-\u003enext == nullptr) { head-\u003enext = node; return nullptr; } ForwardListBaseNode* tmp = __recursion(node-\u003enext); node-\u003enext-\u003enext = node; node-\u003enext = nullptr; return tmp; } void reverse(ForwardListBaseNode* head) { __recursion(head-\u003enext, head); } 双链表的操作也很精彩！由于实现是循环的，因此我们只需要将每个结点的前驱后继按顺序调换位置即可。其时间复杂度同样是 \\(\\mathcal{O}(N)\\) 。 void reverse(BidirectionalListBaseNode* head) { BidirectionalListBaseNode* curr = head-\u003enext,* temp; while (curr != head) { temp = curr-\u003enext; curr-\u003enext = curr-\u003eprev; curr = curr-\u003eprev = temp; } temp = head-\u003enext; head-\u003enext = head-\u003eprev; head-\u003eprev = temp; } ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#一些关于表的算法"},{"categories":["Algorithm⁄DataStructure"],"content":" 一些关于表的算法为了屏蔽一些不必要的实现细节，因此我们约定，使用 iterator 进行 traverse，且 iterator 可以通过 handle 取得底层的链表结点。而函数参数中的引用类型 T\u0026 则表示着对该形式参数的修改将会修改实际参数。 合并两个已排序链表现在假设两个链表都已按照从小到大排列，将两个链表 a 与 b 合并到 c，且合并后的链表也按照从小到大进行排列。 void __transfer(iterator\u0026 pos, iterator\u0026 c) { iterator it = pos++; insert(c.handle(), it.handle(), pos.handle()); } void merge(iterator a_begin, iterator a_end, iterator b_begin, iterator b_end, iterator\u0026 c) { while (a_begin != a_end \u0026\u0026 b_begin != b_end) { __transfer(*a_begin \u003c *b_begin ? a_begin : b_begin, c); } if (a_begin != a_end) { insert(c.handle(), a_begin.handle(), a_end.handle()); } if (b_begin != b_end) { insert(c.handle(), b_begin.handle(), b_end.handle()); } } 引入了 __transfer 函数将找到的 a、b 当前最小的元素插入 c 中，并使其迭代器向前步进一。在 a 或 b 结束之后，我们将 a 或 b 剩余的元素全部添加到 c 的后面，这些元素是最大的一批。分析该算法的时间复杂度得 \\(\\mathcal{O}(size_{a}+size_{b}-1)\\) 。 反转反转链表是一个很有意思的操作，尤其是针对没有前驱结点的单链表来说。 void reverse(ForwardListBaseNode* head) { ForwardListBaseNode* curr = head-\u003enext; head = nullptr; while (curr != nullptr) { ListNode* next = curr-\u003enext; curr-\u003enext = head; head = curr; curr = next; } } 这个方法直接使用到了 ForwardListHead ，利用 head 指向当前结点的前驱，当 traverse 完成后，head 也顺利指向最终结果。其时间复杂度 \\(\\mathcal{O}(N)\\) 。我们可以将其改为递归方式，时间复杂度不变： ForwardListBaseNode* __recursion(ForwardListBaseNode* node, ForwardListBaseNode* head) { if (!node) { return nullptr; } if (node-\u003enext == nullptr) { head-\u003enext = node; return nullptr; } ForwardListBaseNode* tmp = __recursion(node-\u003enext); node-\u003enext-\u003enext = node; node-\u003enext = nullptr; return tmp; } void reverse(ForwardListBaseNode* head) { __recursion(head-\u003enext, head); } 双链表的操作也很精彩！由于实现是循环的，因此我们只需要将每个结点的前驱后继按顺序调换位置即可。其时间复杂度同样是 \\(\\mathcal{O}(N)\\) 。 void reverse(BidirectionalListBaseNode* head) { BidirectionalListBaseNode* curr = head-\u003enext,* temp; while (curr != head) { temp = curr-\u003enext; curr-\u003enext = curr-\u003eprev; curr = curr-\u003eprev = temp; } temp = head-\u003enext; head-\u003enext = head-\u003eprev; head-\u003eprev = temp; } ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#合并两个已排序链表"},{"categories":["Algorithm⁄DataStructure"],"content":" 一些关于表的算法为了屏蔽一些不必要的实现细节，因此我们约定，使用 iterator 进行 traverse，且 iterator 可以通过 handle 取得底层的链表结点。而函数参数中的引用类型 T\u0026 则表示着对该形式参数的修改将会修改实际参数。 合并两个已排序链表现在假设两个链表都已按照从小到大排列，将两个链表 a 与 b 合并到 c，且合并后的链表也按照从小到大进行排列。 void __transfer(iterator\u0026 pos, iterator\u0026 c) { iterator it = pos++; insert(c.handle(), it.handle(), pos.handle()); } void merge(iterator a_begin, iterator a_end, iterator b_begin, iterator b_end, iterator\u0026 c) { while (a_begin != a_end \u0026\u0026 b_begin != b_end) { __transfer(*a_begin \u003c *b_begin ? a_begin : b_begin, c); } if (a_begin != a_end) { insert(c.handle(), a_begin.handle(), a_end.handle()); } if (b_begin != b_end) { insert(c.handle(), b_begin.handle(), b_end.handle()); } } 引入了 __transfer 函数将找到的 a、b 当前最小的元素插入 c 中，并使其迭代器向前步进一。在 a 或 b 结束之后，我们将 a 或 b 剩余的元素全部添加到 c 的后面，这些元素是最大的一批。分析该算法的时间复杂度得 \\(\\mathcal{O}(size_{a}+size_{b}-1)\\) 。 反转反转链表是一个很有意思的操作，尤其是针对没有前驱结点的单链表来说。 void reverse(ForwardListBaseNode* head) { ForwardListBaseNode* curr = head-\u003enext; head = nullptr; while (curr != nullptr) { ListNode* next = curr-\u003enext; curr-\u003enext = head; head = curr; curr = next; } } 这个方法直接使用到了 ForwardListHead ，利用 head 指向当前结点的前驱，当 traverse 完成后，head 也顺利指向最终结果。其时间复杂度 \\(\\mathcal{O}(N)\\) 。我们可以将其改为递归方式，时间复杂度不变： ForwardListBaseNode* __recursion(ForwardListBaseNode* node, ForwardListBaseNode* head) { if (!node) { return nullptr; } if (node-\u003enext == nullptr) { head-\u003enext = node; return nullptr; } ForwardListBaseNode* tmp = __recursion(node-\u003enext); node-\u003enext-\u003enext = node; node-\u003enext = nullptr; return tmp; } void reverse(ForwardListBaseNode* head) { __recursion(head-\u003enext, head); } 双链表的操作也很精彩！由于实现是循环的，因此我们只需要将每个结点的前驱后继按顺序调换位置即可。其时间复杂度同样是 \\(\\mathcal{O}(N)\\) 。 void reverse(BidirectionalListBaseNode* head) { BidirectionalListBaseNode* curr = head-\u003enext,* temp; while (curr != head) { temp = curr-\u003enext; curr-\u003enext = curr-\u003eprev; curr = curr-\u003eprev = temp; } temp = head-\u003enext; head-\u003enext = head-\u003eprev; head-\u003eprev = temp; } ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:1:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#反转"},{"categories":["Algorithm⁄DataStructure"],"content":" 栈 (Stack)Stack 是一种受限的线性结构，其末尾称之为 栈顶 (top)，元素进入栈称为 入栈 (push)，从栈中移除称为 出栈 (pop)。push 只能从 top 进行，元素加入结构的末尾； pop 也只能从 top 进行，移除的元素总是 top 的元素。由于其受限的特性，导致了数据只能以 先进后出 (First-In Last-Out, FILO) 的方式操作。整个栈中仅有 top 元素可见。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:2:0","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#栈--stack"},{"categories":["Algorithm⁄DataStructure"],"content":" Stack ADT template \u003cclass T\u003e concept stack = requires(T a, const T\u0026 b, const typename T::value_type\u0026 value) { requires swappable\u003cT\u003e; requires erasable\u003ctypename T::value_type\u003e; requires same\u003ctypename T::reference, typename T::value_type\u0026\u003e; requires same\u003ctypename T::const_reference, const typename T::value_type\u0026\u003e; requires unsigned\u003ctypename T::size_type\u003e; { a.empty() } -\u003e boolean; { a.size() } -\u003e typename T::size_type; { a.top() } -\u003e typename T::reference; { b.top() } -\u003e typename T::const_reference; a.push(value); a.pop(); }; 函数名称 操作说明 top() 获取栈顶元素的引用 push(value) 将元素 value 入栈 pop() 将栈顶元素出栈 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:2:1","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#stack-adt"},{"categories":["Algorithm⁄DataStructure"],"content":" stack 的实现无论实现的效率如何，线性结构一般都支持从尾部插入、移除元素，因此 stack 的实现可以直接使用已经实现的线性容器，并对这些容器的接口进行包装，以实现对操作的限制。 因此这样对 container 进行包装的方式，被称为 适配器 (adaptor)。adaptor 可以根据自己的需求，选择合适的 container 进行包装。比如使用顺序实现的线性表或双链表进行包装，这里的具体实现就不再展开，栈的思想比其实现更为重要。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:2:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#stack-的实现"},{"categories":["Algorithm⁄DataStructure"],"content":" stack 的应用也许你会想，这限制了线性表的操作，这还有什么用呢，那么接下来我们将看到几个例子。 平衡符号我们有时候需要检测符号是否符合要求，比如说只有方括号与圆括号组成的一个序列，如果这个序列的括号可以正确匹配则序列符合要求，否则不符合要求。如 [()[]] 是一个符合要求的需要，而 [(]) 不符合要求。 stack s; for (auto bracket : sequence) { if (bracket == '[' || bracket == '(') { s.push(bracket); } else { if (s.empty()) { return ERROR; } auto top = s.top(); s.pop(); if ((top == '[' \u0026\u0026 bracket == ')') || (top == '(' \u0026\u0026 bracket == ']')) { return ERROR; } } } 后缀表达式当你在计算器上输入 a + b * c + d ，有没有好奇为什么计算器可以理解正确的优先级，而不是将其理解 (a + b) * c 。或许因为它遵循优先级，才显得这是很理所应当的，而后者是不可理喻的。那么我们需要探寻的是计算器如何遵循优先级。 在上述示例中，我们先计算 b * c ，之后计算 + a 和 + d ，这个顺序你觉得像什么？是不是一个序列入栈并出栈的一个可能的序列 b -\u003e c -\u003e a -\u003e d 。那么问题来了，数据在入栈之后，什么时候出栈呢。数据 b、c 的出栈是因为相乘，而 a 是因为与前面的结果相加，出栈是因为遇到了符号。为了方便起见，将一次计算结果也放入栈中，那么在每次遇到符号时，我们将从栈中弹出两个数字，经过运算将结果压入栈中。那我们可以把这个表达式写为 \\[a \\quad b \\quad c \\quad * \\quad + \\quad d \\quad +.\\] 而这种写法就是 后缀 (postfix) 或者说逆波兰 (revwerse Polish)，我们平常使用的被称为 中缀 (infix) 表达式。另外 postfix expression 有个好处，那就是并不需要括号的支持，在序列中的顺序决定了运算顺序，而不需要再为某个子表达式添加括号来提升运算顺序。 计算逆波兰表达式 我们写出这个计算过程，其时间复杂度为 \\(\\mathcal{O}(N)\\)，最终栈中唯一的元素就是表达式的结果。 stack s; for (auto symbol : sequence) { if (is_op(symbol)) { auto b = s.top(); s.pop(); auto a = s.top(); s.pop(); // 假设存在 eval 函数，且 eval 可以执行操作 a op b，并返回相应的结果 s.push(eval(a, symbol, b)); } else { s.push(symbol); } } 中缀表达式转后缀表达式 那既然会计算 postfix 了，那如何将一个 infix expression 转换为 postfix expression。 我们需要一个用以存储运算符的栈 operation，以及一个用以存储后缀表达式的线性表 sequence。算法的基本思路是：依次读入表达式的符号，如果是操作数则入栈 sequence，否则和 operation 栈顶进行比较。如果 op 优先级高于栈顶元素则入栈，反之将 operation 中的元素依次弹出到 sequence 中，直到出现一个比 op 优先级小的运算符，弹出操作完成后将 op 压入 operation。最终表达式结束时，将栈中剩余符号全部弹出到 sequence 即可。 你会发现这个算法并没有处理括号，括号带来了复杂性，我们现在单独的说一下括号。当遇到左括号时，我们将其压入 operation，除右括号外任何运算符的优先级都低于左括号，因此只有右括号到来时，我们将栈中元素弹出，直到弹出一个左括号。我们在处理过程中并不将右括号入栈，并在左括号弹出栈后也不将其压入 sequence。这里我们给出表格来表示运算符的优先级，并根据表格实现一个优先级比较的函数，其中列符号表示 待弹出/压入 的运算符，行符号表示受比较的运算符。 符号 \\(+\\) \\(-\\) \\(\\times\\) \\(\\div\\) \\((\\) \\(+\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(-\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\\times\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\\div\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\((\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\()\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) // 比较运算符 o 和 p，如果 o 大于 p 则返回 true，否则返回 false bool compare(operation o, operation p) { switch (o) { case PLUS: case MINUS: return is_plus(p) || is_minus(p); // o 是加号或减号 case TIMES: case DIVISION: return !is_left_bracket(p); // o 是乘号或除号 case LEFT_BRACKET: return false; // o 是左括号 case RIGHT_BRACKET: return true; // o 是右括号 defalut: return ERROR; } } 在上述比较操作的基础上，我们可以轻松的实现一个中缀表达式转后缀表达式的过程。分析该算法的时间复杂度，该算法需要遍历整个 infix expression，并会额外遍历一遍 operation，因此复杂度为 \\(\\Theta(N)\\) 。 // 接收一个中缀表达式序列，返回一个后缀表达式序列 sequence_container infix2postfix(sequence_container infix) { stack s; sequence_container postfix; for (auto symbol : infix) { // 当前元素是一个操作数 if (!is_operation(symbol)) { postfix.push_back(symbol); continue; } // 当前元素是右括号且栈不为空，弹出运算符 if (!s.empty() \u0026\u0026 is_right_bracket(symbol)) { // 将运算符弹出到 postfix 序列中，直到运算符为左括号或空栈为止 while (!s.empty() \u0026\u0026 !is_left_bracket(s.top())) { postfix.push_back(s.top()); s.pop(); } // 将左括号移除 if (!s.empty() \u0026\u0026 is_left_bracket(s.top())) { s.pop(); } continue; } // 当前元素优先级小于栈顶元素，弹出运算符，直到元素优先级大于栈顶或空栈为止 if (!s.empty() \u0026\u0026 !compare(symbol, s.top())) { while (!s.empty() \u0026\u0026 compare(s.top(), symbol)) { postfix.push_back(s.top()); s.pop(); } } s.push(symbol); } while (!s.empty()) { postfix.push_back(s.top()); s.pop(); } return postfix; } 前缀表达式 既然有 infix 与 postfix，怎么会没有前缀表达式 (prefix) 呢。就如其字面意思，运算符在操作数之前。因此我们需要表示 \\(5 + 2\\) 时就可以写成 + 5 2 ，好像还不错，但感觉并没有什么用。 如果我们允许，在同一个运算符下的参数，都遵循该运算，那么我们就可以将 \\(1 + 2 + 3 + 4 + 5 + 6\\) 这一大长串写为 + 1 2 3 4 5 6 ，这样感觉还不错吧！ 实际上，有编程语言采用前缀表达式作为基础的书写格式。其实你已经见过了，在 第一篇 中实现 Fibonacci 时就使用的这种语言，实际上是 Scheme (Lisp 的一种方言)，或者说这就是最基本的 Lisp 代码。 Lisp 代码其实是相当简单的！Lisp 使用括号作为分界符 (我想你已经想起 NASA 与 Lisp 的笑话了，我先笑为敬 xD 。其使用前缀表达式，因此括号中的第一个标识符就是运算符，因此引论中的 factorial (阶乘) 写为了 (* n (factorial (- n 1))) ，即 \\(n * (n - 1)!\\) 。 很简单吧！最后感受一下 Lisp 与前缀表达式的魅力吧，用 lisp 实现表达式 \\[\\frac{5+4+(2-(3-(6+\\frac{4}{5})))}{3(6-2)(2-7)}.\\] (/ (+ 5 4 (- 2 3) 6 (/ 4 5)) ;; 这里进行了去括号操作 (* 3 (- 6 2) (- 2 7))) 运算符的结合性 大部分时刻我们都会忽略运算符的结合性问题，因为","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#stack-的应用"},{"categories":["Algorithm⁄DataStructure"],"content":" stack 的应用也许你会想，这限制了线性表的操作，这还有什么用呢，那么接下来我们将看到几个例子。 平衡符号我们有时候需要检测符号是否符合要求，比如说只有方括号与圆括号组成的一个序列，如果这个序列的括号可以正确匹配则序列符合要求，否则不符合要求。如 [()[]] 是一个符合要求的需要，而 [(]) 不符合要求。 stack s; for (auto bracket : sequence) { if (bracket == '[' || bracket == '(') { s.push(bracket); } else { if (s.empty()) { return ERROR; } auto top = s.top(); s.pop(); if ((top == '[' \u0026\u0026 bracket == ')') || (top == '(' \u0026\u0026 bracket == ']')) { return ERROR; } } } 后缀表达式当你在计算器上输入 a + b * c + d ，有没有好奇为什么计算器可以理解正确的优先级，而不是将其理解 (a + b) * c 。或许因为它遵循优先级，才显得这是很理所应当的，而后者是不可理喻的。那么我们需要探寻的是计算器如何遵循优先级。 在上述示例中，我们先计算 b * c ，之后计算 + a 和 + d ，这个顺序你觉得像什么？是不是一个序列入栈并出栈的一个可能的序列 b -\u003e c -\u003e a -\u003e d 。那么问题来了，数据在入栈之后，什么时候出栈呢。数据 b、c 的出栈是因为相乘，而 a 是因为与前面的结果相加，出栈是因为遇到了符号。为了方便起见，将一次计算结果也放入栈中，那么在每次遇到符号时，我们将从栈中弹出两个数字，经过运算将结果压入栈中。那我们可以把这个表达式写为 \\[a \\quad b \\quad c \\quad * \\quad + \\quad d \\quad +.\\] 而这种写法就是 后缀 (postfix) 或者说逆波兰 (revwerse Polish)，我们平常使用的被称为 中缀 (infix) 表达式。另外 postfix expression 有个好处，那就是并不需要括号的支持，在序列中的顺序决定了运算顺序，而不需要再为某个子表达式添加括号来提升运算顺序。 计算逆波兰表达式 我们写出这个计算过程，其时间复杂度为 \\(\\mathcal{O}(N)\\)，最终栈中唯一的元素就是表达式的结果。 stack s; for (auto symbol : sequence) { if (is_op(symbol)) { auto b = s.top(); s.pop(); auto a = s.top(); s.pop(); // 假设存在 eval 函数，且 eval 可以执行操作 a op b，并返回相应的结果 s.push(eval(a, symbol, b)); } else { s.push(symbol); } } 中缀表达式转后缀表达式 那既然会计算 postfix 了，那如何将一个 infix expression 转换为 postfix expression。 我们需要一个用以存储运算符的栈 operation，以及一个用以存储后缀表达式的线性表 sequence。算法的基本思路是：依次读入表达式的符号，如果是操作数则入栈 sequence，否则和 operation 栈顶进行比较。如果 op 优先级高于栈顶元素则入栈，反之将 operation 中的元素依次弹出到 sequence 中，直到出现一个比 op 优先级小的运算符，弹出操作完成后将 op 压入 operation。最终表达式结束时，将栈中剩余符号全部弹出到 sequence 即可。 你会发现这个算法并没有处理括号，括号带来了复杂性，我们现在单独的说一下括号。当遇到左括号时，我们将其压入 operation，除右括号外任何运算符的优先级都低于左括号，因此只有右括号到来时，我们将栈中元素弹出，直到弹出一个左括号。我们在处理过程中并不将右括号入栈，并在左括号弹出栈后也不将其压入 sequence。这里我们给出表格来表示运算符的优先级，并根据表格实现一个优先级比较的函数，其中列符号表示 待弹出/压入 的运算符，行符号表示受比较的运算符。 符号 \\(+\\) \\(-\\) \\(\\times\\) \\(\\div\\) \\((\\) \\(+\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(-\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\\times\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\\div\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\((\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\()\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) // 比较运算符 o 和 p，如果 o 大于 p 则返回 true，否则返回 false bool compare(operation o, operation p) { switch (o) { case PLUS: case MINUS: return is_plus(p) || is_minus(p); // o 是加号或减号 case TIMES: case DIVISION: return !is_left_bracket(p); // o 是乘号或除号 case LEFT_BRACKET: return false; // o 是左括号 case RIGHT_BRACKET: return true; // o 是右括号 defalut: return ERROR; } } 在上述比较操作的基础上，我们可以轻松的实现一个中缀表达式转后缀表达式的过程。分析该算法的时间复杂度，该算法需要遍历整个 infix expression，并会额外遍历一遍 operation，因此复杂度为 \\(\\Theta(N)\\) 。 // 接收一个中缀表达式序列，返回一个后缀表达式序列 sequence_container infix2postfix(sequence_container infix) { stack s; sequence_container postfix; for (auto symbol : infix) { // 当前元素是一个操作数 if (!is_operation(symbol)) { postfix.push_back(symbol); continue; } // 当前元素是右括号且栈不为空，弹出运算符 if (!s.empty() \u0026\u0026 is_right_bracket(symbol)) { // 将运算符弹出到 postfix 序列中，直到运算符为左括号或空栈为止 while (!s.empty() \u0026\u0026 !is_left_bracket(s.top())) { postfix.push_back(s.top()); s.pop(); } // 将左括号移除 if (!s.empty() \u0026\u0026 is_left_bracket(s.top())) { s.pop(); } continue; } // 当前元素优先级小于栈顶元素，弹出运算符，直到元素优先级大于栈顶或空栈为止 if (!s.empty() \u0026\u0026 !compare(symbol, s.top())) { while (!s.empty() \u0026\u0026 compare(s.top(), symbol)) { postfix.push_back(s.top()); s.pop(); } } s.push(symbol); } while (!s.empty()) { postfix.push_back(s.top()); s.pop(); } return postfix; } 前缀表达式 既然有 infix 与 postfix，怎么会没有前缀表达式 (prefix) 呢。就如其字面意思，运算符在操作数之前。因此我们需要表示 \\(5 + 2\\) 时就可以写成 + 5 2 ，好像还不错，但感觉并没有什么用。 如果我们允许，在同一个运算符下的参数，都遵循该运算，那么我们就可以将 \\(1 + 2 + 3 + 4 + 5 + 6\\) 这一大长串写为 + 1 2 3 4 5 6 ，这样感觉还不错吧！ 实际上，有编程语言采用前缀表达式作为基础的书写格式。其实你已经见过了，在 第一篇 中实现 Fibonacci 时就使用的这种语言，实际上是 Scheme (Lisp 的一种方言)，或者说这就是最基本的 Lisp 代码。 Lisp 代码其实是相当简单的！Lisp 使用括号作为分界符 (我想你已经想起 NASA 与 Lisp 的笑话了，我先笑为敬 xD 。其使用前缀表达式，因此括号中的第一个标识符就是运算符，因此引论中的 factorial (阶乘) 写为了 (* n (factorial (- n 1))) ，即 \\(n * (n - 1)!\\) 。 很简单吧！最后感受一下 Lisp 与前缀表达式的魅力吧，用 lisp 实现表达式 \\[\\frac{5+4+(2-(3-(6+\\frac{4}{5})))}{3(6-2)(2-7)}.\\] (/ (+ 5 4 (- 2 3) 6 (/ 4 5)) ;; 这里进行了去括号操作 (* 3 (- 6 2) (- 2 7))) 运算符的结合性 大部分时刻我们都会忽略运算符的结合性问题，因为","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#平衡符号"},{"categories":["Algorithm⁄DataStructure"],"content":" stack 的应用也许你会想，这限制了线性表的操作，这还有什么用呢，那么接下来我们将看到几个例子。 平衡符号我们有时候需要检测符号是否符合要求，比如说只有方括号与圆括号组成的一个序列，如果这个序列的括号可以正确匹配则序列符合要求，否则不符合要求。如 [()[]] 是一个符合要求的需要，而 [(]) 不符合要求。 stack s; for (auto bracket : sequence) { if (bracket == '[' || bracket == '(') { s.push(bracket); } else { if (s.empty()) { return ERROR; } auto top = s.top(); s.pop(); if ((top == '[' \u0026\u0026 bracket == ')') || (top == '(' \u0026\u0026 bracket == ']')) { return ERROR; } } } 后缀表达式当你在计算器上输入 a + b * c + d ，有没有好奇为什么计算器可以理解正确的优先级，而不是将其理解 (a + b) * c 。或许因为它遵循优先级，才显得这是很理所应当的，而后者是不可理喻的。那么我们需要探寻的是计算器如何遵循优先级。 在上述示例中，我们先计算 b * c ，之后计算 + a 和 + d ，这个顺序你觉得像什么？是不是一个序列入栈并出栈的一个可能的序列 b -\u003e c -\u003e a -\u003e d 。那么问题来了，数据在入栈之后，什么时候出栈呢。数据 b、c 的出栈是因为相乘，而 a 是因为与前面的结果相加，出栈是因为遇到了符号。为了方便起见，将一次计算结果也放入栈中，那么在每次遇到符号时，我们将从栈中弹出两个数字，经过运算将结果压入栈中。那我们可以把这个表达式写为 \\[a \\quad b \\quad c \\quad * \\quad + \\quad d \\quad +.\\] 而这种写法就是 后缀 (postfix) 或者说逆波兰 (revwerse Polish)，我们平常使用的被称为 中缀 (infix) 表达式。另外 postfix expression 有个好处，那就是并不需要括号的支持，在序列中的顺序决定了运算顺序，而不需要再为某个子表达式添加括号来提升运算顺序。 计算逆波兰表达式 我们写出这个计算过程，其时间复杂度为 \\(\\mathcal{O}(N)\\)，最终栈中唯一的元素就是表达式的结果。 stack s; for (auto symbol : sequence) { if (is_op(symbol)) { auto b = s.top(); s.pop(); auto a = s.top(); s.pop(); // 假设存在 eval 函数，且 eval 可以执行操作 a op b，并返回相应的结果 s.push(eval(a, symbol, b)); } else { s.push(symbol); } } 中缀表达式转后缀表达式 那既然会计算 postfix 了，那如何将一个 infix expression 转换为 postfix expression。 我们需要一个用以存储运算符的栈 operation，以及一个用以存储后缀表达式的线性表 sequence。算法的基本思路是：依次读入表达式的符号，如果是操作数则入栈 sequence，否则和 operation 栈顶进行比较。如果 op 优先级高于栈顶元素则入栈，反之将 operation 中的元素依次弹出到 sequence 中，直到出现一个比 op 优先级小的运算符，弹出操作完成后将 op 压入 operation。最终表达式结束时，将栈中剩余符号全部弹出到 sequence 即可。 你会发现这个算法并没有处理括号，括号带来了复杂性，我们现在单独的说一下括号。当遇到左括号时，我们将其压入 operation，除右括号外任何运算符的优先级都低于左括号，因此只有右括号到来时，我们将栈中元素弹出，直到弹出一个左括号。我们在处理过程中并不将右括号入栈，并在左括号弹出栈后也不将其压入 sequence。这里我们给出表格来表示运算符的优先级，并根据表格实现一个优先级比较的函数，其中列符号表示 待弹出/压入 的运算符，行符号表示受比较的运算符。 符号 \\(+\\) \\(-\\) \\(\\times\\) \\(\\div\\) \\((\\) \\(+\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(-\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\\times\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\\div\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\((\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\()\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) // 比较运算符 o 和 p，如果 o 大于 p 则返回 true，否则返回 false bool compare(operation o, operation p) { switch (o) { case PLUS: case MINUS: return is_plus(p) || is_minus(p); // o 是加号或减号 case TIMES: case DIVISION: return !is_left_bracket(p); // o 是乘号或除号 case LEFT_BRACKET: return false; // o 是左括号 case RIGHT_BRACKET: return true; // o 是右括号 defalut: return ERROR; } } 在上述比较操作的基础上，我们可以轻松的实现一个中缀表达式转后缀表达式的过程。分析该算法的时间复杂度，该算法需要遍历整个 infix expression，并会额外遍历一遍 operation，因此复杂度为 \\(\\Theta(N)\\) 。 // 接收一个中缀表达式序列，返回一个后缀表达式序列 sequence_container infix2postfix(sequence_container infix) { stack s; sequence_container postfix; for (auto symbol : infix) { // 当前元素是一个操作数 if (!is_operation(symbol)) { postfix.push_back(symbol); continue; } // 当前元素是右括号且栈不为空，弹出运算符 if (!s.empty() \u0026\u0026 is_right_bracket(symbol)) { // 将运算符弹出到 postfix 序列中，直到运算符为左括号或空栈为止 while (!s.empty() \u0026\u0026 !is_left_bracket(s.top())) { postfix.push_back(s.top()); s.pop(); } // 将左括号移除 if (!s.empty() \u0026\u0026 is_left_bracket(s.top())) { s.pop(); } continue; } // 当前元素优先级小于栈顶元素，弹出运算符，直到元素优先级大于栈顶或空栈为止 if (!s.empty() \u0026\u0026 !compare(symbol, s.top())) { while (!s.empty() \u0026\u0026 compare(s.top(), symbol)) { postfix.push_back(s.top()); s.pop(); } } s.push(symbol); } while (!s.empty()) { postfix.push_back(s.top()); s.pop(); } return postfix; } 前缀表达式 既然有 infix 与 postfix，怎么会没有前缀表达式 (prefix) 呢。就如其字面意思，运算符在操作数之前。因此我们需要表示 \\(5 + 2\\) 时就可以写成 + 5 2 ，好像还不错，但感觉并没有什么用。 如果我们允许，在同一个运算符下的参数，都遵循该运算，那么我们就可以将 \\(1 + 2 + 3 + 4 + 5 + 6\\) 这一大长串写为 + 1 2 3 4 5 6 ，这样感觉还不错吧！ 实际上，有编程语言采用前缀表达式作为基础的书写格式。其实你已经见过了，在 第一篇 中实现 Fibonacci 时就使用的这种语言，实际上是 Scheme (Lisp 的一种方言)，或者说这就是最基本的 Lisp 代码。 Lisp 代码其实是相当简单的！Lisp 使用括号作为分界符 (我想你已经想起 NASA 与 Lisp 的笑话了，我先笑为敬 xD 。其使用前缀表达式，因此括号中的第一个标识符就是运算符，因此引论中的 factorial (阶乘) 写为了 (* n (factorial (- n 1))) ，即 \\(n * (n - 1)!\\) 。 很简单吧！最后感受一下 Lisp 与前缀表达式的魅力吧，用 lisp 实现表达式 \\[\\frac{5+4+(2-(3-(6+\\frac{4}{5})))}{3(6-2)(2-7)}.\\] (/ (+ 5 4 (- 2 3) 6 (/ 4 5)) ;; 这里进行了去括号操作 (* 3 (- 6 2) (- 2 7))) 运算符的结合性 大部分时刻我们都会忽略运算符的结合性问题，因为","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#后缀表达式"},{"categories":["Algorithm⁄DataStructure"],"content":" stack 的应用也许你会想，这限制了线性表的操作，这还有什么用呢，那么接下来我们将看到几个例子。 平衡符号我们有时候需要检测符号是否符合要求，比如说只有方括号与圆括号组成的一个序列，如果这个序列的括号可以正确匹配则序列符合要求，否则不符合要求。如 [()[]] 是一个符合要求的需要，而 [(]) 不符合要求。 stack s; for (auto bracket : sequence) { if (bracket == '[' || bracket == '(') { s.push(bracket); } else { if (s.empty()) { return ERROR; } auto top = s.top(); s.pop(); if ((top == '[' \u0026\u0026 bracket == ')') || (top == '(' \u0026\u0026 bracket == ']')) { return ERROR; } } } 后缀表达式当你在计算器上输入 a + b * c + d ，有没有好奇为什么计算器可以理解正确的优先级，而不是将其理解 (a + b) * c 。或许因为它遵循优先级，才显得这是很理所应当的，而后者是不可理喻的。那么我们需要探寻的是计算器如何遵循优先级。 在上述示例中，我们先计算 b * c ，之后计算 + a 和 + d ，这个顺序你觉得像什么？是不是一个序列入栈并出栈的一个可能的序列 b -\u003e c -\u003e a -\u003e d 。那么问题来了，数据在入栈之后，什么时候出栈呢。数据 b、c 的出栈是因为相乘，而 a 是因为与前面的结果相加，出栈是因为遇到了符号。为了方便起见，将一次计算结果也放入栈中，那么在每次遇到符号时，我们将从栈中弹出两个数字，经过运算将结果压入栈中。那我们可以把这个表达式写为 \\[a \\quad b \\quad c \\quad * \\quad + \\quad d \\quad +.\\] 而这种写法就是 后缀 (postfix) 或者说逆波兰 (revwerse Polish)，我们平常使用的被称为 中缀 (infix) 表达式。另外 postfix expression 有个好处，那就是并不需要括号的支持，在序列中的顺序决定了运算顺序，而不需要再为某个子表达式添加括号来提升运算顺序。 计算逆波兰表达式 我们写出这个计算过程，其时间复杂度为 \\(\\mathcal{O}(N)\\)，最终栈中唯一的元素就是表达式的结果。 stack s; for (auto symbol : sequence) { if (is_op(symbol)) { auto b = s.top(); s.pop(); auto a = s.top(); s.pop(); // 假设存在 eval 函数，且 eval 可以执行操作 a op b，并返回相应的结果 s.push(eval(a, symbol, b)); } else { s.push(symbol); } } 中缀表达式转后缀表达式 那既然会计算 postfix 了，那如何将一个 infix expression 转换为 postfix expression。 我们需要一个用以存储运算符的栈 operation，以及一个用以存储后缀表达式的线性表 sequence。算法的基本思路是：依次读入表达式的符号，如果是操作数则入栈 sequence，否则和 operation 栈顶进行比较。如果 op 优先级高于栈顶元素则入栈，反之将 operation 中的元素依次弹出到 sequence 中，直到出现一个比 op 优先级小的运算符，弹出操作完成后将 op 压入 operation。最终表达式结束时，将栈中剩余符号全部弹出到 sequence 即可。 你会发现这个算法并没有处理括号，括号带来了复杂性，我们现在单独的说一下括号。当遇到左括号时，我们将其压入 operation，除右括号外任何运算符的优先级都低于左括号，因此只有右括号到来时，我们将栈中元素弹出，直到弹出一个左括号。我们在处理过程中并不将右括号入栈，并在左括号弹出栈后也不将其压入 sequence。这里我们给出表格来表示运算符的优先级，并根据表格实现一个优先级比较的函数，其中列符号表示 待弹出/压入 的运算符，行符号表示受比较的运算符。 符号 \\(+\\) \\(-\\) \\(\\times\\) \\(\\div\\) \\((\\) \\(+\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(-\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\\times\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\(\\div\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003c\\) \\((\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\(\u003c\\) \\()\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) \\(\u003e\\) // 比较运算符 o 和 p，如果 o 大于 p 则返回 true，否则返回 false bool compare(operation o, operation p) { switch (o) { case PLUS: case MINUS: return is_plus(p) || is_minus(p); // o 是加号或减号 case TIMES: case DIVISION: return !is_left_bracket(p); // o 是乘号或除号 case LEFT_BRACKET: return false; // o 是左括号 case RIGHT_BRACKET: return true; // o 是右括号 defalut: return ERROR; } } 在上述比较操作的基础上，我们可以轻松的实现一个中缀表达式转后缀表达式的过程。分析该算法的时间复杂度，该算法需要遍历整个 infix expression，并会额外遍历一遍 operation，因此复杂度为 \\(\\Theta(N)\\) 。 // 接收一个中缀表达式序列，返回一个后缀表达式序列 sequence_container infix2postfix(sequence_container infix) { stack s; sequence_container postfix; for (auto symbol : infix) { // 当前元素是一个操作数 if (!is_operation(symbol)) { postfix.push_back(symbol); continue; } // 当前元素是右括号且栈不为空，弹出运算符 if (!s.empty() \u0026\u0026 is_right_bracket(symbol)) { // 将运算符弹出到 postfix 序列中，直到运算符为左括号或空栈为止 while (!s.empty() \u0026\u0026 !is_left_bracket(s.top())) { postfix.push_back(s.top()); s.pop(); } // 将左括号移除 if (!s.empty() \u0026\u0026 is_left_bracket(s.top())) { s.pop(); } continue; } // 当前元素优先级小于栈顶元素，弹出运算符，直到元素优先级大于栈顶或空栈为止 if (!s.empty() \u0026\u0026 !compare(symbol, s.top())) { while (!s.empty() \u0026\u0026 compare(s.top(), symbol)) { postfix.push_back(s.top()); s.pop(); } } s.push(symbol); } while (!s.empty()) { postfix.push_back(s.top()); s.pop(); } return postfix; } 前缀表达式 既然有 infix 与 postfix，怎么会没有前缀表达式 (prefix) 呢。就如其字面意思，运算符在操作数之前。因此我们需要表示 \\(5 + 2\\) 时就可以写成 + 5 2 ，好像还不错，但感觉并没有什么用。 如果我们允许，在同一个运算符下的参数，都遵循该运算，那么我们就可以将 \\(1 + 2 + 3 + 4 + 5 + 6\\) 这一大长串写为 + 1 2 3 4 5 6 ，这样感觉还不错吧！ 实际上，有编程语言采用前缀表达式作为基础的书写格式。其实你已经见过了，在 第一篇 中实现 Fibonacci 时就使用的这种语言，实际上是 Scheme (Lisp 的一种方言)，或者说这就是最基本的 Lisp 代码。 Lisp 代码其实是相当简单的！Lisp 使用括号作为分界符 (我想你已经想起 NASA 与 Lisp 的笑话了，我先笑为敬 xD 。其使用前缀表达式，因此括号中的第一个标识符就是运算符，因此引论中的 factorial (阶乘) 写为了 (* n (factorial (- n 1))) ，即 \\(n * (n - 1)!\\) 。 很简单吧！最后感受一下 Lisp 与前缀表达式的魅力吧，用 lisp 实现表达式 \\[\\frac{5+4+(2-(3-(6+\\frac{4}{5})))}{3(6-2)(2-7)}.\\] (/ (+ 5 4 (- 2 3) 6 (/ 4 5)) ;; 这里进行了去括号操作 (* 3 (- 6 2) (- 2 7))) 运算符的结合性 大部分时刻我们都会忽略运算符的结合性问题，因为","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:2:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#函数调用"},{"categories":["Algorithm⁄DataStructure"],"content":" 队列 (Queue)Queue 也是一种受限的线性结构，其末尾被称为队尾 (rear)，而头部被称为队首 (front)。向队列中添加元素被称为 入队 (enqueue)，enqueue 只能在队尾操作；从队列中移除元素被称为 出队 (dequeue)，dequeue 只能在队首操作。因此这种数据结构也被称为 先进先出 (First-In First-Out, FIFO)。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:0","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#队列--queue"},{"categories":["Algorithm⁄DataStructure"],"content":" Queue ADT template \u003cclass T\u003e concept queue = requires(T a, const T\u0026 b, const typename T::value_type\u0026 value) { requires swappable\u003cT\u003e; requires erasable\u003ctypename T::value_type\u003e; requires same\u003ctypename T::reference, typename T::value_type\u0026\u003e; requires same\u003ctypename T::const_reference, const typename T::value_type\u0026\u003e; requires unsigned\u003ctypename T::size_type\u003e; { a.empty() } -\u003e boolean; { a.size() } -\u003e typename T::size_type; { a.front() } -\u003e typename T::reference; { b.front() } -\u003e typename T::const_reference; { a.back() } -\u003e typename T::reference; { b.back() } -\u003e typename T::const_reference; a.push(value); a.pop(); }; 函数名称 操作说明 fron() 获取队首元素的引用 top() 获取队尾元素的引用 push(value) 将元素 value 入队 pop() 将队首元素出队 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:1","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#queue-adt"},{"categories":["Algorithm⁄DataStructure"],"content":" 队列的顺序实现队列本质上是受限的线性表，因此其与 stack 一样可以直接在线性表上做 adaptor，方便快速的实现。但是对于顺序实现的线性表来说，在队首操作时间复杂度为 \\(\\mathcal{O}(N)\\) ，其代价太高。我们需要优化现有结构，让其操作时间复杂度降为 \\(\\mathcal{O}(1)\\) 。 循环队列对线性表的顺序实现进行简单的改进，使用两个指针 start 与 finish 指向队首元素与队尾元素，而数组边界使用 begin 与 end 指示。插入元素时使 \\(finish + 1\\) ，删除时使 \\(start + 1\\) 。但是当 finish 到达数组边界时，就会发生问题，无论 start 前是否剩余空位，都不能再添加元素，因为 finish 已到达边界。这种情况被称为 假溢出 。 显然这个小改进并不能满足需求，为了正常使用，我们假设这个数组是头尾相接的循环数组。因此逻辑上的循环数组不用担心假溢出问题，但也需要每次插入、移除元素时需要检查指针是否到达数组边界，如果已在边界则移动到数组的另一边。 现在思考一下真溢出问题，数组被完完全全的填满了，没有可以容纳元素的方法。这样我们不得不申请更大的一块数组，并将其中元素完整复制进去。当生长时需要 \\(\\mathcal{O}(N)\\) 的时间复杂度完成迁移，并且需要完全按照从 front 到 rear 的顺序进行。 分块的双端队列对于循环队列的缺点进行改进，我们将使用一个全新的方式实现顺序存储。具体思路是：将多个相同大小的块数组组合起来，元素可以被存放在多个不连续的块上，但其连续存储。使用两个指针 start 与 finish 分别指向队首元素与队尾元素，对于每个块有单独的指针指向其头结点。 可以看到，由多个相同大小的块组成了整个存储结构，并且元素在其中顺序存储。可以发现有些块指针并没有引用块，在我们需要的时候，我们可以为其请求一个块，这样我们的数据可以持续的向两边生长，而不需要在生长重新拷贝整个结构。 由于其是多个块数组实现的，且元素顺序、连续排列，因此其可以实现 随机访问 ，其迭代器类型为 radom_access_iterator 。至于跨块访问，应该由实现者对其处理，对使用者透明，使用时可以将其逻辑上作为一个大的块。 信息 可以高效的在两端进行插入、移除元素，但由于分块的特性，需要由实现隐藏其底层块。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#队列的顺序实现"},{"categories":["Algorithm⁄DataStructure"],"content":" 队列的顺序实现队列本质上是受限的线性表，因此其与 stack 一样可以直接在线性表上做 adaptor，方便快速的实现。但是对于顺序实现的线性表来说，在队首操作时间复杂度为 \\(\\mathcal{O}(N)\\) ，其代价太高。我们需要优化现有结构，让其操作时间复杂度降为 \\(\\mathcal{O}(1)\\) 。 循环队列对线性表的顺序实现进行简单的改进，使用两个指针 start 与 finish 指向队首元素与队尾元素，而数组边界使用 begin 与 end 指示。插入元素时使 \\(finish + 1\\) ，删除时使 \\(start + 1\\) 。但是当 finish 到达数组边界时，就会发生问题，无论 start 前是否剩余空位，都不能再添加元素，因为 finish 已到达边界。这种情况被称为 假溢出 。 显然这个小改进并不能满足需求，为了正常使用，我们假设这个数组是头尾相接的循环数组。因此逻辑上的循环数组不用担心假溢出问题，但也需要每次插入、移除元素时需要检查指针是否到达数组边界，如果已在边界则移动到数组的另一边。 现在思考一下真溢出问题，数组被完完全全的填满了，没有可以容纳元素的方法。这样我们不得不申请更大的一块数组，并将其中元素完整复制进去。当生长时需要 \\(\\mathcal{O}(N)\\) 的时间复杂度完成迁移，并且需要完全按照从 front 到 rear 的顺序进行。 分块的双端队列对于循环队列的缺点进行改进，我们将使用一个全新的方式实现顺序存储。具体思路是：将多个相同大小的块数组组合起来，元素可以被存放在多个不连续的块上，但其连续存储。使用两个指针 start 与 finish 分别指向队首元素与队尾元素，对于每个块有单独的指针指向其头结点。 可以看到，由多个相同大小的块组成了整个存储结构，并且元素在其中顺序存储。可以发现有些块指针并没有引用块，在我们需要的时候，我们可以为其请求一个块，这样我们的数据可以持续的向两边生长，而不需要在生长重新拷贝整个结构。 由于其是多个块数组实现的，且元素顺序、连续排列，因此其可以实现 随机访问 ，其迭代器类型为 radom_access_iterator 。至于跨块访问，应该由实现者对其处理，对使用者透明，使用时可以将其逻辑上作为一个大的块。 信息 可以高效的在两端进行插入、移除元素，但由于分块的特性，需要由实现隐藏其底层块。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#循环队列"},{"categories":["Algorithm⁄DataStructure"],"content":" 队列的顺序实现队列本质上是受限的线性表，因此其与 stack 一样可以直接在线性表上做 adaptor，方便快速的实现。但是对于顺序实现的线性表来说，在队首操作时间复杂度为 \\(\\mathcal{O}(N)\\) ，其代价太高。我们需要优化现有结构，让其操作时间复杂度降为 \\(\\mathcal{O}(1)\\) 。 循环队列对线性表的顺序实现进行简单的改进，使用两个指针 start 与 finish 指向队首元素与队尾元素，而数组边界使用 begin 与 end 指示。插入元素时使 \\(finish + 1\\) ，删除时使 \\(start + 1\\) 。但是当 finish 到达数组边界时，就会发生问题，无论 start 前是否剩余空位，都不能再添加元素，因为 finish 已到达边界。这种情况被称为 假溢出 。 显然这个小改进并不能满足需求，为了正常使用，我们假设这个数组是头尾相接的循环数组。因此逻辑上的循环数组不用担心假溢出问题，但也需要每次插入、移除元素时需要检查指针是否到达数组边界，如果已在边界则移动到数组的另一边。 现在思考一下真溢出问题，数组被完完全全的填满了，没有可以容纳元素的方法。这样我们不得不申请更大的一块数组，并将其中元素完整复制进去。当生长时需要 \\(\\mathcal{O}(N)\\) 的时间复杂度完成迁移，并且需要完全按照从 front 到 rear 的顺序进行。 分块的双端队列对于循环队列的缺点进行改进，我们将使用一个全新的方式实现顺序存储。具体思路是：将多个相同大小的块数组组合起来，元素可以被存放在多个不连续的块上，但其连续存储。使用两个指针 start 与 finish 分别指向队首元素与队尾元素，对于每个块有单独的指针指向其头结点。 可以看到，由多个相同大小的块组成了整个存储结构，并且元素在其中顺序存储。可以发现有些块指针并没有引用块，在我们需要的时候，我们可以为其请求一个块，这样我们的数据可以持续的向两边生长，而不需要在生长重新拷贝整个结构。 由于其是多个块数组实现的，且元素顺序、连续排列，因此其可以实现 随机访问 ，其迭代器类型为 radom_access_iterator 。至于跨块访问，应该由实现者对其处理，对使用者透明，使用时可以将其逻辑上作为一个大的块。 信息 可以高效的在两端进行插入、移除元素，但由于分块的特性，需要由实现隐藏其底层块。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:2","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#分块的双端队列"},{"categories":["Algorithm⁄DataStructure"],"content":" 分块双端队列的实现由于分块双端队列的复杂性，我们将详细说明一下其实现细节。 分块双端队列的迭代器由于迭代器肩负着隐藏底层块结构的作用，并且还要支持随机访问数据。因此迭代器的实现很重要。 template \u003cclass Element\u003e struct iterator { Element* cur; // 迭代器指向的元素 Element* first; // 当前元素所在块数组的起始指针 Element* last; // 当前元素所在块数组的末尾指针 Element** node; // 当前元素所在的块 }; 为了进行随机访问，必须确定当前元素所在的块，才能在不同块之间进行随机访问。在进行随机访问的示例中， chunk_capacity 是一个获取每个块数组可以容纳有多少元素的函数，因此确认每个块的末尾边界。而 __set_node 根据 it 当前指向的元素与将步进的块数量，来设置随机访问的目标结点正确的块信息。 template \u003cclass Element\u003e void __set_node(iterator\u003cElement\u003e\u0026 it, const difference_type\u0026 n) { it.node += n; it.first = *it.node; it.last = *it.node + chunk_capacity\u003cElement\u003e(); } template \u003cclass Element\u003e iterator\u003cElement\u003e\u0026 operator+=(iterator\u003cElement\u003e\u0026 it, const difference_type\u0026 n) { difference_type cap = chunk_capacity\u003cElement\u003e(); const difference_type offset = n + (it.cur - it.first); if (0 \u003c= offset \u0026\u0026 offset \u003c chunk_cap) { it.cur += n; } else { const difference_type tmp = offset \u003c 0 ? -((-offset - 1) / cap) - 1 : offset / cap; __set_node(it, tmp); it.cur = it.first + (offset - tmp * cap); } return it; } 视线放在 operator+= 这个函数，offset 用于判断当前结点需要向前或后步进多少个元素，加 \\(it.cur - it.first\\) 是为了将相对起点从 cur 移动到当前所在块的开始位置 first。如果 \\(0 \\leq offset \\leq capacity\\) 则意味着这次随机访问并不会变更所在块，否则需要计算变更到哪一块。仔细判断步进方向与步进大小，向前步进时将移动 \\(\\frac{-offset - 1}{cap} - 1\\) 个块，而向后步进时则需要移动 \\(\\frac{offset}{cap}\\) 个块。最终元素的位置将相对新的起始指针 \\(offset - tmp * cap\\) 个元素。 这是相对复杂的步进，而其他步进与此差不了多少，就不再举例说明。 分块双端队列的存储结构在其存储结构中，需要有一个指针指向块指针的数组的首元素，简单的说就是指向指针的指针，没有使用的块指针应该置空 (nullptr 或 NULL)。其中还需要两个 iterator 分别指向队首元素与队尾元素。 template \u003cclass Element\u003e struct Base { using iterator = iterator\u003cElement\u003e; size_t size; Element** chunks; iterator begin; iterator end; }; ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#分块双端队列的实现"},{"categories":["Algorithm⁄DataStructure"],"content":" 分块双端队列的实现由于分块双端队列的复杂性，我们将详细说明一下其实现细节。 分块双端队列的迭代器由于迭代器肩负着隐藏底层块结构的作用，并且还要支持随机访问数据。因此迭代器的实现很重要。 template struct iterator { Element* cur; // 迭代器指向的元素 Element* first; // 当前元素所在块数组的起始指针 Element* last; // 当前元素所在块数组的末尾指针 Element** node; // 当前元素所在的块 }; 为了进行随机访问，必须确定当前元素所在的块，才能在不同块之间进行随机访问。在进行随机访问的示例中， chunk_capacity 是一个获取每个块数组可以容纳有多少元素的函数，因此确认每个块的末尾边界。而 __set_node 根据 it 当前指向的元素与将步进的块数量，来设置随机访问的目标结点正确的块信息。 template void __set_node(iterator\u0026 it, const difference_type\u0026 n) { it.node += n; it.first = *it.node; it.last = *it.node + chunk_capacity(); } template iterator\u0026 operator+=(iterator\u0026 it, const difference_type\u0026 n) { difference_type cap = chunk_capacity(); const difference_type offset = n + (it.cur - it.first); if (0 \u003c= offset \u0026\u0026 offset \u003c chunk_cap) { it.cur += n; } else { const difference_type tmp = offset \u003c 0 ? -((-offset - 1) / cap) - 1 : offset / cap; __set_node(it, tmp); it.cur = it.first + (offset - tmp * cap); } return it; } 视线放在 operator+= 这个函数，offset 用于判断当前结点需要向前或后步进多少个元素，加 \\(it.cur - it.first\\) 是为了将相对起点从 cur 移动到当前所在块的开始位置 first。如果 \\(0 \\leq offset \\leq capacity\\) 则意味着这次随机访问并不会变更所在块，否则需要计算变更到哪一块。仔细判断步进方向与步进大小，向前步进时将移动 \\(\\frac{-offset - 1}{cap} - 1\\) 个块，而向后步进时则需要移动 \\(\\frac{offset}{cap}\\) 个块。最终元素的位置将相对新的起始指针 \\(offset - tmp * cap\\) 个元素。 这是相对复杂的步进，而其他步进与此差不了多少，就不再举例说明。 分块双端队列的存储结构在其存储结构中，需要有一个指针指向块指针的数组的首元素，简单的说就是指向指针的指针，没有使用的块指针应该置空 (nullptr 或 NULL)。其中还需要两个 iterator 分别指向队首元素与队尾元素。 template struct Base { using iterator = iterator; size_t size; Element** chunks; iterator begin; iterator end; }; ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#分块双端队列的迭代器"},{"categories":["Algorithm⁄DataStructure"],"content":" 分块双端队列的实现由于分块双端队列的复杂性，我们将详细说明一下其实现细节。 分块双端队列的迭代器由于迭代器肩负着隐藏底层块结构的作用，并且还要支持随机访问数据。因此迭代器的实现很重要。 template struct iterator { Element* cur; // 迭代器指向的元素 Element* first; // 当前元素所在块数组的起始指针 Element* last; // 当前元素所在块数组的末尾指针 Element** node; // 当前元素所在的块 }; 为了进行随机访问，必须确定当前元素所在的块，才能在不同块之间进行随机访问。在进行随机访问的示例中， chunk_capacity 是一个获取每个块数组可以容纳有多少元素的函数，因此确认每个块的末尾边界。而 __set_node 根据 it 当前指向的元素与将步进的块数量，来设置随机访问的目标结点正确的块信息。 template void __set_node(iterator\u0026 it, const difference_type\u0026 n) { it.node += n; it.first = *it.node; it.last = *it.node + chunk_capacity(); } template iterator\u0026 operator+=(iterator\u0026 it, const difference_type\u0026 n) { difference_type cap = chunk_capacity(); const difference_type offset = n + (it.cur - it.first); if (0 \u003c= offset \u0026\u0026 offset \u003c chunk_cap) { it.cur += n; } else { const difference_type tmp = offset \u003c 0 ? -((-offset - 1) / cap) - 1 : offset / cap; __set_node(it, tmp); it.cur = it.first + (offset - tmp * cap); } return it; } 视线放在 operator+= 这个函数，offset 用于判断当前结点需要向前或后步进多少个元素，加 \\(it.cur - it.first\\) 是为了将相对起点从 cur 移动到当前所在块的开始位置 first。如果 \\(0 \\leq offset \\leq capacity\\) 则意味着这次随机访问并不会变更所在块，否则需要计算变更到哪一块。仔细判断步进方向与步进大小，向前步进时将移动 \\(\\frac{-offset - 1}{cap} - 1\\) 个块，而向后步进时则需要移动 \\(\\frac{offset}{cap}\\) 个块。最终元素的位置将相对新的起始指针 \\(offset - tmp * cap\\) 个元素。 这是相对复杂的步进，而其他步进与此差不了多少，就不再举例说明。 分块双端队列的存储结构在其存储结构中，需要有一个指针指向块指针的数组的首元素，简单的说就是指向指针的指针，没有使用的块指针应该置空 (nullptr 或 NULL)。其中还需要两个 iterator 分别指向队首元素与队尾元素。 template struct Base { using iterator = iterator; size_t size; Element** chunks; iterator begin; iterator end; }; ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:3:3","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#分块双端队列的存储结构"},{"categories":["Algorithm⁄DataStructure"],"content":" 串 (string)串是一种特殊的线性结构，它的内部元素只存储字符，因此又称为字符串。其特殊性主要来源于我们对字符序列的依赖程度很高，而特化一个线性结构并为其增加一些针对于字符的常用算法，可以方便我们的使用，提高编码效率。 在大部分的实现中，string 都有一个标志结尾的字符 \\0 ，其 ASCII 值为 0，因此在遇到 \\0 时就认为这个字符串结束。但是有一些实现使用单独的变量来标记，因此这种字符串中即使存在 \\0 也可能并不是串的结尾。因此串的长度为真实的长度减一 (因为 \\0 将占用一个位置)。长度为 0 的字符串被称为空串，一般使用 \\(\\varnothing\\) 表示，其中只有一个 \\0 。 ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:4:0","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#串--string"},{"categories":["Algorithm⁄DataStructure"],"content":" 串的匹配在一个串中寻找指定子串是一个最常用的算法，解决方法也有多种。我们将指定的串称之为匹配串，并假设原串长度为 m，匹配串长度为 n。 朴素算法从下标为 0 开始比较原串与匹配串，若不相同，则移位到下标为 1，直到找完原串的所有子字符串。这个算法很简单，也很好理解，其时间复杂度为 \\(\\mathcal{O}(mn)\\) 。 int strstr(const string\u0026 source, const string\u0026 pattern) { int m = source.size(), n = pattern.size(); for (int i = 0; i + n \u003c= m; i++) { bool flag = true; for (int j = 0; j \u003c n; j++) { if (source[i + j] != pattern[j]) { flag = false; break; } } if (flag) { return i; } } return -1; } KMP 算法KMP 实际上是三位计算机科学家的名字缩写，Knuth、Morris 和 Pratt，有意思的是，之后你还会见到 Morris 的名字，而 Pratt 的博士生导师就是 Knuth。 Knuth 1938 年生，1977 年访问中国时姚期智的夫人储枫为其取的中文名高德纳。老爷子的成就实在太多了， 计算机程序设计艺术、\\(\\TeX\\)、 METAFONT、文学式编程、LR解析理论 等等，还获得过冯诺伊曼奖与图灵奖。而老爷子是个十分有趣的人，\\(\\TeX\\) 的版本号趋近于 \\(\\pi\\) 而 METAFONT 的版本号趋近于 \\(e\\)；为了他的著作他还开了家银行，为在他的著作中找的任何错误的人奖励 2.56 美元 (256 pennies is one hexadecimal dollar)，并对每个有价值的建议提供 0.32 美元的奖金。如今他还在十二月份安排了讲座。如果你想了解老爷子可以访问他的 个人主页。 KMP 的主要思想是：一个词在不匹配时本身就包含足够的信息来确定下一个匹配可能的开始位置。此算法利用这一特性以避免重新检查先前匹配的字符，因此 KMP 的核心算法即求解本身包含的信息。这一信息被称为前缀函数，记作 \\(\\pi(i)\\) 。对于区间 \\([0:i] (0 \\leq i \u003c n)\\) ，\\(\\pi(i)\\) 是最长的一对相等的真前缀与真后缀的长度，如果没有符合条件的真前缀/真后缀则 \\(\\pi(i) = 0\\) 。真前缀、真后缀即与串本身不相等的前缀 / 后缀子串。 假设有匹配串 aabaab ，则有前缀函数 \\(\\pi(0) = 0\\) ，串 \\(s[0:0]\\) 没有真前缀 \\(\\pi(1) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[1] ，长度为 1 \\(\\pi(2) = 0\\) ，串 \\(s[0:2]\\) 没有相等的真前缀与真后缀 \\(\\pi(3) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[3] ，长度为 1 \\(\\pi(4) = 2\\) ，一对最长相等真前缀、真后缀为 s[0:1] 和 s[3:4] ，长度为 2 \\(\\pi(5) = 0\\) ，一对最长相等真前缀、真后缀为 s[0:2] 和 s[3:5] ，长度为 3 接下来就是 KMP 如何使用前缀函数，前缀函数代表了当前如果匹配失败了，在 已匹配的串 中，有多少真后缀是与真前缀相同的，那么在接下来的匹配中我们可以直接忽略这些相同的真前缀 / 真后缀，从而接着匹配字符串，跳过这些不必要的匹配。 前缀函数的实现 观察前缀函数，我们可以观察到： 如果 \\(s[i] = s[\\pi(i - 1)]\\) ，那么 \\(\\pi(i) = \\pi(i - 1) + 1\\) 如果 \\(s[i] \\neq s[\\pi(i - 1)]\\) ，那么需要递归地向前寻找 当满足 \\(s[i] = s[j], j = \\pi(\\pi(\\pi(\\dots)) - 1)\\) 时， \\(\\pi(i) = \\pi(j) + 1\\) 当全部不满足时，则 \\(\\pi(i) = 0\\) void get_prefix_array(const string\u0026 pattern, const int len, int pi[]) { pi[0] = 0; for (int i = 1, j = 0; i \u003c len; i++) { while (j \u003e 0 \u0026\u0026 pattern[i] != pattern[j]) { j = pi[j - 1]; } j += pattern[i] == pattern[j]; pi[i] = j; } } KMP 的实现 我们需要利用先生成前缀数组，再对原串进行遍历匹配模式串，因此总的时间复杂度需要 \\(\\mathcal{O}(m + n)\\)。 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int pi[m]; get_prefix_array(pattern, n, pi); for (int i = 0, j = 0; i \u003c n; i++) { while (j \u003e 0 \u0026\u0026 source[i] != pattern[j]) { j = pi[j - 1]; } if (source[i] == pattern[j]) { j++; } if (j == m) { return i - m + 1; } } return -1; } Sunday 算法Sunday 算法是 BM 算法的变种，与 KMP 的核心思路一样，利用 pattern 已给出的信息，最大程度的跳过不匹配的字符。 Sunday 的思想较为简单，处理一个 pattern 偏移表，该表主要映射了 pattern 串中存在的每个字符到末尾的距离，如果有多个相同字符，则用更靠近末尾的映射替换之前的值。 Sunday 算法如果发现无法匹配，则观察这个坏字符的下一个位置的字符 c 来决定步进的长度： 如果 c 不存在于 pattern 中，直接将 pattern 的起始位置与 c 的下一个字符对齐 如果 c 存在于 pattern 中，则将最靠近末尾的该字符与 c 对齐 // 为实现的简便，假设 source 中只包含 ASCII 字符 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int shift[128] = {0}; for (int i = 0; i \u003c m; i++) { shift[pattern[i]] = m - i; } for (int i = 0, end = n - m + 1; i \u003c end;) { int j = 0; while (source[i + j] == pattern[j]) { ++j; if (j == m) { return i; } } i += shift[source[i + m]] == 0 ? m + 1 : shift[source[i + m]]; } return -1; } ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:4:1","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#串的匹配"},{"categories":["Algorithm⁄DataStructure"],"content":" 串的匹配在一个串中寻找指定子串是一个最常用的算法，解决方法也有多种。我们将指定的串称之为匹配串，并假设原串长度为 m，匹配串长度为 n。 朴素算法从下标为 0 开始比较原串与匹配串，若不相同，则移位到下标为 1，直到找完原串的所有子字符串。这个算法很简单，也很好理解，其时间复杂度为 \\(\\mathcal{O}(mn)\\) 。 int strstr(const string\u0026 source, const string\u0026 pattern) { int m = source.size(), n = pattern.size(); for (int i = 0; i + n \u003c= m; i++) { bool flag = true; for (int j = 0; j \u003c n; j++) { if (source[i + j] != pattern[j]) { flag = false; break; } } if (flag) { return i; } } return -1; } KMP 算法KMP 实际上是三位计算机科学家的名字缩写，Knuth、Morris 和 Pratt，有意思的是，之后你还会见到 Morris 的名字，而 Pratt 的博士生导师就是 Knuth。 Knuth 1938 年生，1977 年访问中国时姚期智的夫人储枫为其取的中文名高德纳。老爷子的成就实在太多了， 计算机程序设计艺术、\\(\\TeX\\)、 METAFONT、文学式编程、LR解析理论 等等，还获得过冯诺伊曼奖与图灵奖。而老爷子是个十分有趣的人，\\(\\TeX\\) 的版本号趋近于 \\(\\pi\\) 而 METAFONT 的版本号趋近于 \\(e\\)；为了他的著作他还开了家银行，为在他的著作中找的任何错误的人奖励 2.56 美元 (256 pennies is one hexadecimal dollar)，并对每个有价值的建议提供 0.32 美元的奖金。如今他还在十二月份安排了讲座。如果你想了解老爷子可以访问他的 个人主页。 KMP 的主要思想是：一个词在不匹配时本身就包含足够的信息来确定下一个匹配可能的开始位置。此算法利用这一特性以避免重新检查先前匹配的字符，因此 KMP 的核心算法即求解本身包含的信息。这一信息被称为前缀函数，记作 \\(\\pi(i)\\) 。对于区间 \\([0:i] (0 \\leq i \u003c n)\\) ，\\(\\pi(i)\\) 是最长的一对相等的真前缀与真后缀的长度，如果没有符合条件的真前缀/真后缀则 \\(\\pi(i) = 0\\) 。真前缀、真后缀即与串本身不相等的前缀 / 后缀子串。 假设有匹配串 aabaab ，则有前缀函数 \\(\\pi(0) = 0\\) ，串 \\(s[0:0]\\) 没有真前缀 \\(\\pi(1) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[1] ，长度为 1 \\(\\pi(2) = 0\\) ，串 \\(s[0:2]\\) 没有相等的真前缀与真后缀 \\(\\pi(3) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[3] ，长度为 1 \\(\\pi(4) = 2\\) ，一对最长相等真前缀、真后缀为 s[0:1] 和 s[3:4] ，长度为 2 \\(\\pi(5) = 0\\) ，一对最长相等真前缀、真后缀为 s[0:2] 和 s[3:5] ，长度为 3 接下来就是 KMP 如何使用前缀函数，前缀函数代表了当前如果匹配失败了，在 已匹配的串 中，有多少真后缀是与真前缀相同的，那么在接下来的匹配中我们可以直接忽略这些相同的真前缀 / 真后缀，从而接着匹配字符串，跳过这些不必要的匹配。 前缀函数的实现 观察前缀函数，我们可以观察到： 如果 \\(s[i] = s[\\pi(i - 1)]\\) ，那么 \\(\\pi(i) = \\pi(i - 1) + 1\\) 如果 \\(s[i] \\neq s[\\pi(i - 1)]\\) ，那么需要递归地向前寻找 当满足 \\(s[i] = s[j], j = \\pi(\\pi(\\pi(\\dots)) - 1)\\) 时， \\(\\pi(i) = \\pi(j) + 1\\) 当全部不满足时，则 \\(\\pi(i) = 0\\) void get_prefix_array(const string\u0026 pattern, const int len, int pi[]) { pi[0] = 0; for (int i = 1, j = 0; i \u003c len; i++) { while (j \u003e 0 \u0026\u0026 pattern[i] != pattern[j]) { j = pi[j - 1]; } j += pattern[i] == pattern[j]; pi[i] = j; } } KMP 的实现 我们需要利用先生成前缀数组，再对原串进行遍历匹配模式串，因此总的时间复杂度需要 \\(\\mathcal{O}(m + n)\\)。 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int pi[m]; get_prefix_array(pattern, n, pi); for (int i = 0, j = 0; i \u003c n; i++) { while (j \u003e 0 \u0026\u0026 source[i] != pattern[j]) { j = pi[j - 1]; } if (source[i] == pattern[j]) { j++; } if (j == m) { return i - m + 1; } } return -1; } Sunday 算法Sunday 算法是 BM 算法的变种，与 KMP 的核心思路一样，利用 pattern 已给出的信息，最大程度的跳过不匹配的字符。 Sunday 的思想较为简单，处理一个 pattern 偏移表，该表主要映射了 pattern 串中存在的每个字符到末尾的距离，如果有多个相同字符，则用更靠近末尾的映射替换之前的值。 Sunday 算法如果发现无法匹配，则观察这个坏字符的下一个位置的字符 c 来决定步进的长度： 如果 c 不存在于 pattern 中，直接将 pattern 的起始位置与 c 的下一个字符对齐 如果 c 存在于 pattern 中，则将最靠近末尾的该字符与 c 对齐 // 为实现的简便，假设 source 中只包含 ASCII 字符 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int shift[128] = {0}; for (int i = 0; i \u003c m; i++) { shift[pattern[i]] = m - i; } for (int i = 0, end = n - m + 1; i \u003c end;) { int j = 0; while (source[i + j] == pattern[j]) { ++j; if (j == m) { return i; } } i += shift[source[i + m]] == 0 ? m + 1 : shift[source[i + m]]; } return -1; } ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:4:1","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#朴素算法"},{"categories":["Algorithm⁄DataStructure"],"content":" 串的匹配在一个串中寻找指定子串是一个最常用的算法，解决方法也有多种。我们将指定的串称之为匹配串，并假设原串长度为 m，匹配串长度为 n。 朴素算法从下标为 0 开始比较原串与匹配串，若不相同，则移位到下标为 1，直到找完原串的所有子字符串。这个算法很简单，也很好理解，其时间复杂度为 \\(\\mathcal{O}(mn)\\) 。 int strstr(const string\u0026 source, const string\u0026 pattern) { int m = source.size(), n = pattern.size(); for (int i = 0; i + n \u003c= m; i++) { bool flag = true; for (int j = 0; j \u003c n; j++) { if (source[i + j] != pattern[j]) { flag = false; break; } } if (flag) { return i; } } return -1; } KMP 算法KMP 实际上是三位计算机科学家的名字缩写，Knuth、Morris 和 Pratt，有意思的是，之后你还会见到 Morris 的名字，而 Pratt 的博士生导师就是 Knuth。 Knuth 1938 年生，1977 年访问中国时姚期智的夫人储枫为其取的中文名高德纳。老爷子的成就实在太多了， 计算机程序设计艺术、\\(\\TeX\\)、 METAFONT、文学式编程、LR解析理论 等等，还获得过冯诺伊曼奖与图灵奖。而老爷子是个十分有趣的人，\\(\\TeX\\) 的版本号趋近于 \\(\\pi\\) 而 METAFONT 的版本号趋近于 \\(e\\)；为了他的著作他还开了家银行，为在他的著作中找的任何错误的人奖励 2.56 美元 (256 pennies is one hexadecimal dollar)，并对每个有价值的建议提供 0.32 美元的奖金。如今他还在十二月份安排了讲座。如果你想了解老爷子可以访问他的 个人主页。 KMP 的主要思想是：一个词在不匹配时本身就包含足够的信息来确定下一个匹配可能的开始位置。此算法利用这一特性以避免重新检查先前匹配的字符，因此 KMP 的核心算法即求解本身包含的信息。这一信息被称为前缀函数，记作 \\(\\pi(i)\\) 。对于区间 \\([0:i] (0 \\leq i \u003c n)\\) ，\\(\\pi(i)\\) 是最长的一对相等的真前缀与真后缀的长度，如果没有符合条件的真前缀/真后缀则 \\(\\pi(i) = 0\\) 。真前缀、真后缀即与串本身不相等的前缀 / 后缀子串。 假设有匹配串 aabaab ，则有前缀函数 \\(\\pi(0) = 0\\) ，串 \\(s[0:0]\\) 没有真前缀 \\(\\pi(1) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[1] ，长度为 1 \\(\\pi(2) = 0\\) ，串 \\(s[0:2]\\) 没有相等的真前缀与真后缀 \\(\\pi(3) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[3] ，长度为 1 \\(\\pi(4) = 2\\) ，一对最长相等真前缀、真后缀为 s[0:1] 和 s[3:4] ，长度为 2 \\(\\pi(5) = 0\\) ，一对最长相等真前缀、真后缀为 s[0:2] 和 s[3:5] ，长度为 3 接下来就是 KMP 如何使用前缀函数，前缀函数代表了当前如果匹配失败了，在 已匹配的串 中，有多少真后缀是与真前缀相同的，那么在接下来的匹配中我们可以直接忽略这些相同的真前缀 / 真后缀，从而接着匹配字符串，跳过这些不必要的匹配。 前缀函数的实现 观察前缀函数，我们可以观察到： 如果 \\(s[i] = s[\\pi(i - 1)]\\) ，那么 \\(\\pi(i) = \\pi(i - 1) + 1\\) 如果 \\(s[i] \\neq s[\\pi(i - 1)]\\) ，那么需要递归地向前寻找 当满足 \\(s[i] = s[j], j = \\pi(\\pi(\\pi(\\dots)) - 1)\\) 时， \\(\\pi(i) = \\pi(j) + 1\\) 当全部不满足时，则 \\(\\pi(i) = 0\\) void get_prefix_array(const string\u0026 pattern, const int len, int pi[]) { pi[0] = 0; for (int i = 1, j = 0; i \u003c len; i++) { while (j \u003e 0 \u0026\u0026 pattern[i] != pattern[j]) { j = pi[j - 1]; } j += pattern[i] == pattern[j]; pi[i] = j; } } KMP 的实现 我们需要利用先生成前缀数组，再对原串进行遍历匹配模式串，因此总的时间复杂度需要 \\(\\mathcal{O}(m + n)\\)。 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int pi[m]; get_prefix_array(pattern, n, pi); for (int i = 0, j = 0; i \u003c n; i++) { while (j \u003e 0 \u0026\u0026 source[i] != pattern[j]) { j = pi[j - 1]; } if (source[i] == pattern[j]) { j++; } if (j == m) { return i - m + 1; } } return -1; } Sunday 算法Sunday 算法是 BM 算法的变种，与 KMP 的核心思路一样，利用 pattern 已给出的信息，最大程度的跳过不匹配的字符。 Sunday 的思想较为简单，处理一个 pattern 偏移表，该表主要映射了 pattern 串中存在的每个字符到末尾的距离，如果有多个相同字符，则用更靠近末尾的映射替换之前的值。 Sunday 算法如果发现无法匹配，则观察这个坏字符的下一个位置的字符 c 来决定步进的长度： 如果 c 不存在于 pattern 中，直接将 pattern 的起始位置与 c 的下一个字符对齐 如果 c 存在于 pattern 中，则将最靠近末尾的该字符与 c 对齐 // 为实现的简便，假设 source 中只包含 ASCII 字符 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int shift[128] = {0}; for (int i = 0; i \u003c m; i++) { shift[pattern[i]] = m - i; } for (int i = 0, end = n - m + 1; i \u003c end;) { int j = 0; while (source[i + j] == pattern[j]) { ++j; if (j == m) { return i; } } i += shift[source[i + m]] == 0 ? m + 1 : shift[source[i + m]]; } return -1; } ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:4:1","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#kmp-算法"},{"categories":["Algorithm⁄DataStructure"],"content":" 串的匹配在一个串中寻找指定子串是一个最常用的算法，解决方法也有多种。我们将指定的串称之为匹配串，并假设原串长度为 m，匹配串长度为 n。 朴素算法从下标为 0 开始比较原串与匹配串，若不相同，则移位到下标为 1，直到找完原串的所有子字符串。这个算法很简单，也很好理解，其时间复杂度为 \\(\\mathcal{O}(mn)\\) 。 int strstr(const string\u0026 source, const string\u0026 pattern) { int m = source.size(), n = pattern.size(); for (int i = 0; i + n \u003c= m; i++) { bool flag = true; for (int j = 0; j \u003c n; j++) { if (source[i + j] != pattern[j]) { flag = false; break; } } if (flag) { return i; } } return -1; } KMP 算法KMP 实际上是三位计算机科学家的名字缩写，Knuth、Morris 和 Pratt，有意思的是，之后你还会见到 Morris 的名字，而 Pratt 的博士生导师就是 Knuth。 Knuth 1938 年生，1977 年访问中国时姚期智的夫人储枫为其取的中文名高德纳。老爷子的成就实在太多了， 计算机程序设计艺术、\\(\\TeX\\)、 METAFONT、文学式编程、LR解析理论 等等，还获得过冯诺伊曼奖与图灵奖。而老爷子是个十分有趣的人，\\(\\TeX\\) 的版本号趋近于 \\(\\pi\\) 而 METAFONT 的版本号趋近于 \\(e\\)；为了他的著作他还开了家银行，为在他的著作中找的任何错误的人奖励 2.56 美元 (256 pennies is one hexadecimal dollar)，并对每个有价值的建议提供 0.32 美元的奖金。如今他还在十二月份安排了讲座。如果你想了解老爷子可以访问他的 个人主页。 KMP 的主要思想是：一个词在不匹配时本身就包含足够的信息来确定下一个匹配可能的开始位置。此算法利用这一特性以避免重新检查先前匹配的字符，因此 KMP 的核心算法即求解本身包含的信息。这一信息被称为前缀函数，记作 \\(\\pi(i)\\) 。对于区间 \\([0:i] (0 \\leq i \u003c n)\\) ，\\(\\pi(i)\\) 是最长的一对相等的真前缀与真后缀的长度，如果没有符合条件的真前缀/真后缀则 \\(\\pi(i) = 0\\) 。真前缀、真后缀即与串本身不相等的前缀 / 后缀子串。 假设有匹配串 aabaab ，则有前缀函数 \\(\\pi(0) = 0\\) ，串 \\(s[0:0]\\) 没有真前缀 \\(\\pi(1) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[1] ，长度为 1 \\(\\pi(2) = 0\\) ，串 \\(s[0:2]\\) 没有相等的真前缀与真后缀 \\(\\pi(3) = 1\\) ，一对最长相等真前缀、真后缀为 s[0] 和 s[3] ，长度为 1 \\(\\pi(4) = 2\\) ，一对最长相等真前缀、真后缀为 s[0:1] 和 s[3:4] ，长度为 2 \\(\\pi(5) = 0\\) ，一对最长相等真前缀、真后缀为 s[0:2] 和 s[3:5] ，长度为 3 接下来就是 KMP 如何使用前缀函数，前缀函数代表了当前如果匹配失败了，在 已匹配的串 中，有多少真后缀是与真前缀相同的，那么在接下来的匹配中我们可以直接忽略这些相同的真前缀 / 真后缀，从而接着匹配字符串，跳过这些不必要的匹配。 前缀函数的实现 观察前缀函数，我们可以观察到： 如果 \\(s[i] = s[\\pi(i - 1)]\\) ，那么 \\(\\pi(i) = \\pi(i - 1) + 1\\) 如果 \\(s[i] \\neq s[\\pi(i - 1)]\\) ，那么需要递归地向前寻找 当满足 \\(s[i] = s[j], j = \\pi(\\pi(\\pi(\\dots)) - 1)\\) 时， \\(\\pi(i) = \\pi(j) + 1\\) 当全部不满足时，则 \\(\\pi(i) = 0\\) void get_prefix_array(const string\u0026 pattern, const int len, int pi[]) { pi[0] = 0; for (int i = 1, j = 0; i \u003c len; i++) { while (j \u003e 0 \u0026\u0026 pattern[i] != pattern[j]) { j = pi[j - 1]; } j += pattern[i] == pattern[j]; pi[i] = j; } } KMP 的实现 我们需要利用先生成前缀数组，再对原串进行遍历匹配模式串，因此总的时间复杂度需要 \\(\\mathcal{O}(m + n)\\)。 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int pi[m]; get_prefix_array(pattern, n, pi); for (int i = 0, j = 0; i \u003c n; i++) { while (j \u003e 0 \u0026\u0026 source[i] != pattern[j]) { j = pi[j - 1]; } if (source[i] == pattern[j]) { j++; } if (j == m) { return i - m + 1; } } return -1; } Sunday 算法Sunday 算法是 BM 算法的变种，与 KMP 的核心思路一样，利用 pattern 已给出的信息，最大程度的跳过不匹配的字符。 Sunday 的思想较为简单，处理一个 pattern 偏移表，该表主要映射了 pattern 串中存在的每个字符到末尾的距离，如果有多个相同字符，则用更靠近末尾的映射替换之前的值。 Sunday 算法如果发现无法匹配，则观察这个坏字符的下一个位置的字符 c 来决定步进的长度： 如果 c 不存在于 pattern 中，直接将 pattern 的起始位置与 c 的下一个字符对齐 如果 c 存在于 pattern 中，则将最靠近末尾的该字符与 c 对齐 // 为实现的简便，假设 source 中只包含 ASCII 字符 int strstr(const string\u0026 source, const string\u0026 pattern) { int n = source.size(), m = pattern.size(); int shift[128] = {0}; for (int i = 0; i \u003c m; i++) { shift[pattern[i]] = m - i; } for (int i = 0, end = n - m + 1; i \u003c end;) { int j = 0; while (source[i + j] == pattern[j]) { ++j; if (j == m) { return i; } } i += shift[source[i + m]] == 0 ? m + 1 : shift[source[i + m]]; } return -1; } ","date":"08-16","objectID":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/:4:1","series":["数据结构与算法分析"],"tags":["Note","List","Stack","Queue","String"],"title":"线性数据结构","uri":"/2021/data_strucures_and_algorithm_analysis_002_linear_data_structure/#sunday-算法"},{"categories":["Algorithm⁄DataStructure"],"content":"GinShio | 数据结构与算法分析第一、二章笔记","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/"},{"categories":["Algorithm⁄DataStructure"],"content":" I will, in fact, claim that the difference between a bad programmer and a good one is whether he considers his code or his data structures more important. Bad programmers worry about the code. Good programmers worry about data structures and their relationships. — Linus Torvalds ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:0:0","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#"},{"categories":["Algorithm⁄DataStructure"],"content":" 基本的数学知识首先我们需要复习一些在初高中可能学过的基础数学知识。 ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:1:0","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#基本的数学知识"},{"categories":["Algorithm⁄DataStructure"],"content":" 集合集合 (Set) 是基本的数学概念，指具体的某种性质的事物的总体，集合中的事物称之为 元素 (element)。 element 通常使用小写字母表示，而 set 通常使用大写字母表示。若 \\(x\\) 是集合 \\(A\\) 中的元素，记作 \\(x \\in A\\) ；反之不在集合中记作 \\(x \\notin A\\) 。当两个 set 中所包含的 element 完全一样时，就称这两个 set 相等，记作 \\(A = B\\) 。 集合中的所有元素地位相同，元素间是无序的。这些元素是唯一的，即在同一个集合中对一个元素只包含一次。而元素只有存在或不存在于这个集合两种状态。而我们常用的集合有以下几种： 集合 符号 自然数 \\(\\mathbb{N}\\) 整数 \\(\\mathbb{Z}\\) 有理数 \\(\\mathbb{Q}\\) 实数 \\(\\mathbb{R}\\) 复数 \\(\\mathbb{C}\\) 使用上角标 \\(*\\) 表示去零之后的集合，而上角标 \\(+\\) 和 \\(-\\) 分别表示只包含正数部分与负数部分的集合。如果希望包含零 (比如非负数集合) 则可以在使用上角标的同时使用下角标 \\(0\\) 。 ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:1:1","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#集合"},{"categories":["Algorithm⁄DataStructure"],"content":" 运算模运算 简单的说就是 取余数 ，在数学上被称为 同余 (congruent)，记作 \\(A \\equiv B (mod\\ N)\\) 。这意味着无论 A 还是 B 被 N 除，其余数都是相同的，即 \\(81 \\equiv 61 \\equiv 1 (mod\\ 10)\\) 。同余在 programming 中用的还是比较多的，大部分语言中使用 % 代表同余运算。这里我们着重列一些同余的性质，至于证明亲请放过我！ 传递性 当有 3 个数，其中 a 与 b 同余 N、b 与 a 同余 N，则 a、c 同余 N \\[a \\equiv b (mod\\ N), b \\equiv c (mod\\ N) \\Longrightarrow a \\equiv c (mod\\ N)\\] 保持运算 \\[a \\equiv b (mod\\ N), c \\equiv d (mod\\ N) \\Longrightarrow a \\pm c \\equiv b \\pm d (mod\\ N),\\ ac \\equiv bd (mod\\ N)\\] 可以引申该性质: \\[a \\equiv b (mod\\ N) \\Longrightarrow an \\equiv bn (mod\\ N) (\\forall n \\in \\Z),\\ a^{n} \\equiv b^{n} (mod\\ N) (\\forall n \\in \\mathbb{N}^{0})\\] 放大缩小底数 \\[k \\in \\Z, n \\in \\Z^{+} \\Longrightarrow (kN \\pm a)^{n} \\equiv (\\pm a)^{n} (mod\\ N)\\] 放大缩小模数 \\[k \\in \\mathbb{N}^{*}, a \\equiv b (mod\\ N) \\Longrightarrow \\exists! \\ ka \\equiv kb (mod\\ kN)\\] 除法原理 \\[ka \\equiv kb (mod\\ N), k \\bot N \\Longrightarrow a \\equiv b (mod\\ N)\\] 费马小定理 \\[p \\in \\mathbb{P} \\Longrightarrow a^{p-1} \\equiv 1 (mod\\ p)\\] 欧拉定理 \\[N \\in \\Z^{+}, a \\in \\Z^{+}, gcd(a, N) = 1 \\Longrightarrow a^{\\varphi(N)} \\equiv 1 (mod\\ N)\\] 下来说一下运算法则，最不成问题的应该是幂运算，其次是幂的逆运算 – 对数。最后的级数，嗯，就不会说了 ，我也不会 。 指数 对数 级数 \\(X^{A}X^{B} = X^{A+B}\\) \\(\\log_{}{AB} = \\log_{}{A} + \\log_{}{B}\\) \\(\\sum_{i=1}^{N}{i} = \\dfrac{N(N + 1)}{2} \\approx \\dfrac{N^{2}}{2}\\) \\(\\dfrac{X^{A}}{X^{B}} = X^{A-B}\\) \\(\\log_{}{\\dfrac{A}{B}} = \\log_{}{A} - \\log_{}{B}\\) \\(\\sum_{i=1}^{N}{i^{2}} = \\dfrac{N(N + 1)(2N + 1)}{6} \\approx \\dfrac{N^3}{3}\\) \\((X^{A})^{B} = X^{AB}\\) \\(\\log_{}{A^{B}} = B \\log_{}{A}\\) \\(\\sum_{i=1}^{N}{i^{k}} \\approx \\dfrac{N^{k + 1}}{\\lvert k + 1 \\rvert}\\), \\((k \\neq -1)\\) \\(X^{N} + X^{N} = 2 X^{N} \\neq X^{2N}\\) \\(\\log_{A}{B} = \\dfrac{\\log_{C}{B}}{\\log_{C}{A}}\\) \\(\\sum_{i=1}^{N}{A^{i}} = \\dfrac{A^{N+1} - 1}{A - 1}\\) \\(2^{N} + 2^{N} = 2^{N + 1}\\) \\(\\log_{A^{C}}{B^{D}} = \\dfrac{D}{C}\\log_{A}{B}\\) \\(\\sum_{i=n_{0}}^{N}{f(i)} = \\sum_{i=1}^{N}{f(i)} - \\sum_{i=1}^{n_{0}-1}{f(i)}\\) ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:1:2","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#运算"},{"categories":["Algorithm⁄DataStructure"],"content":" 算法分析","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:2:0","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#算法分析"},{"categories":["Algorithm⁄DataStructure"],"content":" 数学基础算法 (Algorithm) 是为求解一个问题所需要遵循的、被清楚地指定的简单指令集合。对于一个问题，一旦某种算法给定并且被确定是正确的，那么重要的一步是确定该算法将需要多少如时间或空间等资源量的问题。首先介绍以下四个定义： 大 \\(\\mathcal{O}\\) 符号 (big O notation)，又称 渐近符号 ，用于描述一个函数的数量级渐近上界，记作 \\(T(N) = \\mathcal{O}(f(N))\\) 。 \\[\\exists c \\in \\mathbb{N}^{*}, \\exists n_{0} \\in \\mathbb{N}^{*}, N \\geq n_{0} \\Longrightarrow T(N) \\leq c f(N)\\] 例如有 \\(T(n) = 4n^{2} - 2n + 2\\) ，写作 \\(T(n) \\in \\mathcal{O}(n^{2})\\) 或 \\(T(n) = \\mathcal{O}(n^2)\\) 符号 名称 \\(\\mathcal{O}(1)\\) 常数阶 \\(\\mathcal{O}(\\log_{}{n})\\) 对数阶 \\(\\mathcal{O}[(\\log_{}{n})^{c}]\\) 多对数阶 \\(\\mathcal{O}(n)\\) 线性阶 \\(\\mathcal{O}(n \\log_{}^{*}{n})\\) 迭代对数阶 \\(\\mathcal{O}(n \\log_{}{n})\\) 线性对数阶 \\(\\mathcal{O}(n^{2})\\) 平方阶 \\(\\mathcal{O}(n^{c}), c \\in \\mathbb{N}^{*}\\) 多项式阶 (代数阶) \\(\\mathcal{O}(c^{n})\\) 指数阶 (几何阶) \\(\\mathcal{O}(n!)\\) 阶乘阶 (组合阶) 大 \\(\\Omega\\) 符号 (big Omega notation)，与 big O notation 类似，big O 表示函数增长到一定程度时总小于一个特定函数的常数倍，big Omega 则表示总大于一个特定函数的常数倍。记作 \\(T(N) = \\Omega(f(N))\\) 。 \\[\\exists c \\in \\mathbb{N}^{*}, \\exists n_{0} \\in \\mathbb{N}^{*}, N \\geq n_{0} \\Longrightarrow N \\geq c f(N)\\] 大 \\(\\Theta\\) 符号 (big Theta notation)，是 big O 与 big Omega 的结合，由 Knuth 教授于 1970 年提出。这是在教科书上容易被误用的符号，可能会将 \\(\\Theta\\) 误用为 \\(\\mathcal{O}\\) 。 \\[\\exists!\\ T(N) = \\mathcal{O}(f(N)), \\exists!\\ T(N) = \\Omega(f(N)) \\Longrightarrow T(N) = \\Theta(f(N))\\] 小 \\(\\mathcal{o}\\) 符号 ，如果说 bit O 是无穷大渐近，那么小 o 符号则表示的是无穷小渐近，记作 \\(T(N) = \\mathcal{o}(f(N))\\) 。 \\[\\forall c \\in N^{*}, \\exists n_{0}, N \u003e n_{0} \\Longrightarrow T(N) \u003c c f(N)\\] \\(T(N) = \\mathcal{o}(f(N))\\) 或者用 big O 与 big Theta 来理解小 o 符号： \\[T(N) = \\mathcal{O}(f(N)) \\land T(N) \\neq \\Theta(f(N)) \\Longrightarrow T(N) = \\mathcal{o}(f(N))\\] 可以发现，在使用 \\(\\mathcal{O}\\) 时常常可以忽略低阶项或常数项，当然也会忽略高阶项的系数。因此 \\(\\mathcal{O}\\) 是一种精度较低的估计。 我们可以通过计算极限 \\(\\lim_{N \\to \\infty}{\\dfrac{f(N)}{g(N)}}\\) 来确定函数 \\(f(N)\\) 与 \\(g(N)\\) 的相对增长率。最终我们可以求解 (比如说洛必达法则) 出四种可能性： \\(\\lim=0\\) ，即 \\(f(N)\\) 是 \\(g(N)\\) 的高阶无穷小，这意味着 \\(f(N) = \\mathcal{o}(g(N))\\) \\(\\lim=c (c \\neq 0)\\) ，即 \\(f(N)\\) 是 \\(g(N)\\) 的同阶无穷小，这意味着 \\(f(N) = \\Theta(g(N))\\) \\(\\lim=\\infty\\) ，即 \\(f(N)\\) 是 \\(g(N)\\) 的低阶无穷小，这意味着 \\(g(N) = \\mathcal{o}(f(N))\\) \\(\\lim=\\nexists\\) ，即极限不存在，这意味着二者无关 需要注意的是，我们不要说 \\(f(N) \\leq \\mathcal{O}(g(N))\\) ，因为 \\(\\mathcal{O}\\) 定义中已经蕴含了不等式；写作 \\(f(N) \\geq \\mathcal{O}(g(N))\\) 则是错误的，因为其没有意义。 ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:2:1","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#数学基础"},{"categories":["Algorithm⁄DataStructure"],"content":" 要分析的问题一般需要分析的最重要的资源是运行时间，有多个因素影响程序的运行时间。除了编译器与计算机等超出理论模型的范畴，主要因素是使用的 算法 以及对该算法的 输入。 我们需要明白，虽然实现方面我们可以使用不同编程语言，但是我们往往忽略编程语言的细节所带来的影响，虽然不同语言的实现存在着运行时间的差异。 典型情况下，输入的大小是主要的讨论因素。我们定义函数 \\(T_{avg}(N)\\) 与 \\(T_{worst}(N)\\) 分别表示对于输入 N 所花费的平均情形与最坏情形的运行时间，显然 \\(T_{avg}(N) \\leq T_{worst}(N)\\) 。一般最好结果不在分析范围内，因为其不代表典型结果。平均情形性能往往可以反应出该算法的典型结果，而最坏情形的性能则是算法对于任何输入在性能上的保证。 一般来说，在无特殊说明时，所需要的量就是最坏情况的运行时间，因为它对所有输出提供了一个界限，而平均情形并不提供这样的界。 ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:2:2","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#要分析的问题"},{"categories":["Algorithm⁄DataStructure"],"content":" 运行时间计算为了让我们更有效的分析算法，我们约定 不存在 特定的时间单位，因此我们只需要计算 \\(\\mathcal{O}\\) 运行时间。由于 \\(\\mathcal{O}\\) 是上界，我们不能低估程序的运行时间，这是对程序运行时间的保证。 一般法则 顺序语句的运行时间是各个语句运行时间求和 选择语句的运行时间是判断与分支中的最长运行时间之和 循环语句其运行时间至多是该 for 循环内语句的运行时间乘以迭代的次数 不过我们需要注意一点，就是 递归 (recursion)，虽然其中可能只使用到顺序与选择语句，但其是隐含的循环结构。如果你对递归的认识并不是很好，可以学习 SICP 的第一章 ~ 构造过程的抽象~ ，使用 Scheme 进行学习与构造的过程中是十分愉快的。 使用 SICP 中的例子，递归可以分为 线性递归 与 树形递归，在 recursion 应用中，前者的典型示例是阶乘，而后者的典型示例是 Fibonacci 数列。 以阶乘为示例，翻译为简单的数学表达式 \\(n! = n \\cdot [(n - 1) \\cdot (n - 2) \\cdots 3 \\cdot 2 \\cdot 1] = n \\cdot (n - 1)!\\) ，递归的进行阶乘的求解，构造起一个 推迟进行的操作 所形成的链条，收缩阶段表现为这些运算的实际执行。忽略程序运行时进行的函数调用开销，这个程序的时间复杂度为 \\(\\mathcal{O}(N)\\) ，保证对于任何输入都可以在关于 N 的线性时间完成。 (define (factorial n) (if (or (= n 1) (= n 0)) 1 (* n (factorial (- n 1))))) 以 Fibonacci 数列 \\(F(n) = F(n - 1) + F(n - 2), F(0) = F(1) = 1\\) 为示例，在求解第 n 个 fibonacci 数时，我们需要对第 \\(n - 1\\) 个和第 \\(n - 2\\) 个数进行分别求解，然后对第 \\(n - 1\\) 个数也如此求解。最终我们的递归构造起一个树形的推迟计算结构，并在收缩时进行了很多冗余计算。对于示例程序，其运行时间为 \\(T(N) = T(N - 1) + T(N - 2) + 2\\) ，利用数学归纳法可以得知 \\(fib_{N} \u003c (5/3)^{N}\\) 并且 \\(F_{N} \\geq (3/2)^{N}\\) ，这个运算的时间将随着 N 的增加而指数级增加。 (define (fib n) (cond ((= n 0) 1) ((= n 1) 1) (else (+ (fib (- n 1)) (fib (- n 2)))))) 示例：最大子数组和问题 给定序列 \\(A_{1}, A_{2}, \\dots, A_{N}\\) ，且 \\(A_{i} \\in \\mathbb{Z}\\) ，求 \\(\\sum_{k=i}^{j}{A_{k}}\\) 的最大值。 例如，对于输入 \\(-2, 11, -4, 13, -5, -2\\) ，其答案为 \\(20\\) 对于该问题有四种差异极大的解决方法，其时间复杂度分别为 \\(\\mathcal{O}{N^{3}}\\) 、 \\(\\mathcal{O}{N^{2}}\\) 、 \\(\\mathcal{O}{N \\log_{}{N}}\\) 以及 \\(\\mathcal{O}{N}\\) 。对于小输入来说，无论选取哪种方式，计算机总能很快完成其给定输入；但数据输入到达一定大的数量级时，其前两种算法的时间复杂度实在太大了，以至于它们十分缓慢，不再适合解决该问题。 朴素算法，时间复杂度为 \\(\\mathcal{O}(N^{3})\\) int max_subarray_sum(const int arr[], const int len) { int ans = 0; for (int i = 0; i \u003c len; i++) { for (int j = i; j \u003c len; j++) { int sum = 0; for (int k = i; k \u003c= j; k++) { sum += arr[k]; } if (sum \u003e ans) { ans = sum; } } } return ans; } 该算法在 6 ~ 8 行由一个隐含于三层 for 循环中的 \\(\\mathcal{O}(1)\\) 语句组成，循环大小为 N (虽然它们是 \\(N - i\\) 和 \\(j - i + 1\\) ，但最坏情况为 \\(N\\))，总开销为 \\(\\mathcal{O}(1 \\cdot N \\cdot N \\cdot N) = \\mathcal{O}(N^{3})\\) 。第 9 ~ 11 行语句开销 \\(\\mathcal{O}(N^{2})\\) 。因此我们可以忽略低阶表达式带来的影响，其最终的分析答案为 \\(\\Theta(N^{3})\\) 。 优化算法到 \\(\\mathcal{O}(N^{2})\\) 我们观察到第三层循环是 \\(\\sum_{k=i}^{j}{A_{k}} = A_{j} + \\sum_{k=i}^{j-1}{A_{k}}\\) ，而第二层循环即从 i 到 len 循环整个数组，而 \\(j = i\\) 时 \\(\\sum_{k=i}^{j}{A_{k}} = A_{j}\\) ，而在循环时完全可以将第三层循环去除，累加 \\(A_{i}\\) ~ \\(A_{len}\\) 并在累加过程中与当前最大结果进行比较。 int max_subarray_sum(const int arr[], const int len) { int ans = 0; for (int i = 0; i \u003c len; i++) { int sum = 0; for (int j = i; j \u003c len; j++) { sum += arr[j]; if (sum \u003e ans) { ans = sum; } } } return ans; } 时间复杂度为 \\(\\mathcal{O}(N)\\) 的 Recursion 解法 (分治算法) 对于一个区间 \\([l, r]\\) 我们取 \\(\\lfloor\\dfrac{l+r}{2}\\rfloor\\) 分治区间，直到区间长度为 1 后开始收缩。对于每段区间，需要定义四种变量来维护区间的最大子数组和 a. \\([l, r]\\) 内以 l 为左端点的最大子数组和，记作 lr_lsum b. \\([l, r]\\) 内以 r 为右端点的最大子数组和，记作 lr_rsum c. \\([l, r]\\) 内的最大子数组和，记作 lr_msum d. \\([l, r]\\) 的区间和，记作 lr_sum 假设 \\(m = \\lfloor\\dfrac{l+r}{2}\\rfloor\\) ，称区间 \\([l, m]\\) 为左区间，\\([m + 1, r]\\) 为右区间，在合并区间时 a. \\([l, r]\\) 的 lr_lsum 为 \\(max(lm\\_lsum, lm\\_sum + mr\\_lsum)\\) b. \\([l, r]\\) 的 lr_rsum 为 \\(max(mr\\_rsum, mr\\_sum + lm\\_rsum)\\) c. \\([l, r]\\) 的 lr_msum 为 \\(max(lm\\_msum, mr\\_msum, lm\\_rsum + mr\\_lsum)\\) 如此我们可以轻松获取到整个区间的最大值 struct State { int lsum, rsum, msum, sum; }; struct State push_up(struct State l, struct State r) { return (struct State) { .lsum = fmax(l.lsum, l.sum + r.lsum), .rsum = fmax(r.rsum, r.sum + l.rsum), .msum = fmax(fmax(l.msum, r.msum), l.rsum + r.lsum), .sum = fmax(l.sum, r.sum), }; } struct State get(const int arr[], const int l, const int r) { if (l == r) { return (struct State){arr[l], arr[l], arr[l], arr[l]}; } int m = (l + r) \u003e\u003e 1; return push_up(get(arr, l, m), get(arr, m + 1, r)); } int max_subarray_sum(const int arr[], const int len) { return get(arr, 0, len - 1); } ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:2:3","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#运行时间计算"},{"categories":["Algorithm⁄DataStructure"],"content":" 运行时间计算为了让我们更有效的分析算法，我们约定 不存在 特定的时间单位，因此我们只需要计算 \\(\\mathcal{O}\\) 运行时间。由于 \\(\\mathcal{O}\\) 是上界，我们不能低估程序的运行时间，这是对程序运行时间的保证。 一般法则 顺序语句的运行时间是各个语句运行时间求和 选择语句的运行时间是判断与分支中的最长运行时间之和 循环语句其运行时间至多是该 for 循环内语句的运行时间乘以迭代的次数 不过我们需要注意一点，就是 递归 (recursion)，虽然其中可能只使用到顺序与选择语句，但其是隐含的循环结构。如果你对递归的认识并不是很好，可以学习 SICP 的第一章 ~ 构造过程的抽象~ ，使用 Scheme 进行学习与构造的过程中是十分愉快的。 使用 SICP 中的例子，递归可以分为 线性递归 与 树形递归，在 recursion 应用中，前者的典型示例是阶乘，而后者的典型示例是 Fibonacci 数列。 以阶乘为示例，翻译为简单的数学表达式 \\(n! = n \\cdot [(n - 1) \\cdot (n - 2) \\cdots 3 \\cdot 2 \\cdot 1] = n \\cdot (n - 1)!\\) ，递归的进行阶乘的求解，构造起一个 推迟进行的操作 所形成的链条，收缩阶段表现为这些运算的实际执行。忽略程序运行时进行的函数调用开销，这个程序的时间复杂度为 \\(\\mathcal{O}(N)\\) ，保证对于任何输入都可以在关于 N 的线性时间完成。 (define (factorial n) (if (or (= n 1) (= n 0)) 1 (* n (factorial (- n 1))))) 以 Fibonacci 数列 \\(F(n) = F(n - 1) + F(n - 2), F(0) = F(1) = 1\\) 为示例，在求解第 n 个 fibonacci 数时，我们需要对第 \\(n - 1\\) 个和第 \\(n - 2\\) 个数进行分别求解，然后对第 \\(n - 1\\) 个数也如此求解。最终我们的递归构造起一个树形的推迟计算结构，并在收缩时进行了很多冗余计算。对于示例程序，其运行时间为 \\(T(N) = T(N - 1) + T(N - 2) + 2\\) ，利用数学归纳法可以得知 \\(fib_{N} \u003c (5/3)^{N}\\) 并且 \\(F_{N} \\geq (3/2)^{N}\\) ，这个运算的时间将随着 N 的增加而指数级增加。 (define (fib n) (cond ((= n 0) 1) ((= n 1) 1) (else (+ (fib (- n 1)) (fib (- n 2)))))) 示例：最大子数组和问题 给定序列 \\(A_{1}, A_{2}, \\dots, A_{N}\\) ，且 \\(A_{i} \\in \\mathbb{Z}\\) ，求 \\(\\sum_{k=i}^{j}{A_{k}}\\) 的最大值。 例如，对于输入 \\(-2, 11, -4, 13, -5, -2\\) ，其答案为 \\(20\\) 对于该问题有四种差异极大的解决方法，其时间复杂度分别为 \\(\\mathcal{O}{N^{3}}\\) 、 \\(\\mathcal{O}{N^{2}}\\) 、 \\(\\mathcal{O}{N \\log_{}{N}}\\) 以及 \\(\\mathcal{O}{N}\\) 。对于小输入来说，无论选取哪种方式，计算机总能很快完成其给定输入；但数据输入到达一定大的数量级时，其前两种算法的时间复杂度实在太大了，以至于它们十分缓慢，不再适合解决该问题。 朴素算法，时间复杂度为 \\(\\mathcal{O}(N^{3})\\) int max_subarray_sum(const int arr[], const int len) { int ans = 0; for (int i = 0; i \u003c len; i++) { for (int j = i; j \u003c len; j++) { int sum = 0; for (int k = i; k \u003c= j; k++) { sum += arr[k]; } if (sum \u003e ans) { ans = sum; } } } return ans; } 该算法在 6 ~ 8 行由一个隐含于三层 for 循环中的 \\(\\mathcal{O}(1)\\) 语句组成，循环大小为 N (虽然它们是 \\(N - i\\) 和 \\(j - i + 1\\) ，但最坏情况为 \\(N\\))，总开销为 \\(\\mathcal{O}(1 \\cdot N \\cdot N \\cdot N) = \\mathcal{O}(N^{3})\\) 。第 9 ~ 11 行语句开销 \\(\\mathcal{O}(N^{2})\\) 。因此我们可以忽略低阶表达式带来的影响，其最终的分析答案为 \\(\\Theta(N^{3})\\) 。 优化算法到 \\(\\mathcal{O}(N^{2})\\) 我们观察到第三层循环是 \\(\\sum_{k=i}^{j}{A_{k}} = A_{j} + \\sum_{k=i}^{j-1}{A_{k}}\\) ，而第二层循环即从 i 到 len 循环整个数组，而 \\(j = i\\) 时 \\(\\sum_{k=i}^{j}{A_{k}} = A_{j}\\) ，而在循环时完全可以将第三层循环去除，累加 \\(A_{i}\\) ~ \\(A_{len}\\) 并在累加过程中与当前最大结果进行比较。 int max_subarray_sum(const int arr[], const int len) { int ans = 0; for (int i = 0; i \u003c len; i++) { int sum = 0; for (int j = i; j \u003c len; j++) { sum += arr[j]; if (sum \u003e ans) { ans = sum; } } } return ans; } 时间复杂度为 \\(\\mathcal{O}(N)\\) 的 Recursion 解法 (分治算法) 对于一个区间 \\([l, r]\\) 我们取 \\(\\lfloor\\dfrac{l+r}{2}\\rfloor\\) 分治区间，直到区间长度为 1 后开始收缩。对于每段区间，需要定义四种变量来维护区间的最大子数组和 a. \\([l, r]\\) 内以 l 为左端点的最大子数组和，记作 lr_lsum b. \\([l, r]\\) 内以 r 为右端点的最大子数组和，记作 lr_rsum c. \\([l, r]\\) 内的最大子数组和，记作 lr_msum d. \\([l, r]\\) 的区间和，记作 lr_sum 假设 \\(m = \\lfloor\\dfrac{l+r}{2}\\rfloor\\) ，称区间 \\([l, m]\\) 为左区间，\\([m + 1, r]\\) 为右区间，在合并区间时 a. \\([l, r]\\) 的 lr_lsum 为 \\(max(lm\\_lsum, lm\\_sum + mr\\_lsum)\\) b. \\([l, r]\\) 的 lr_rsum 为 \\(max(mr\\_rsum, mr\\_sum + lm\\_rsum)\\) c. \\([l, r]\\) 的 lr_msum 为 \\(max(lm\\_msum, mr\\_msum, lm\\_rsum + mr\\_lsum)\\) 如此我们可以轻松获取到整个区间的最大值 struct State { int lsum, rsum, msum, sum; }; struct State push_up(struct State l, struct State r) { return (struct State) { .lsum = fmax(l.lsum, l.sum + r.lsum), .rsum = fmax(r.rsum, r.sum + l.rsum), .msum = fmax(fmax(l.msum, r.msum), l.rsum + r.lsum), .sum = fmax(l.sum, r.sum), }; } struct State get(const int arr[], const int l, const int r) { if (l == r) { return (struct State){arr[l], arr[l], arr[l], arr[l]}; } int m = (l + r) \u003e\u003e 1; return push_up(get(arr, l, m), get(arr, m + 1, r)); } int max_subarray_sum(const int arr[], const int len) { return get(arr, 0, len - 1); } ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:2:3","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#一般法则"},{"categories":["Algorithm⁄DataStructure"],"content":" 运行时间计算为了让我们更有效的分析算法，我们约定 不存在 特定的时间单位，因此我们只需要计算 \\(\\mathcal{O}\\) 运行时间。由于 \\(\\mathcal{O}\\) 是上界，我们不能低估程序的运行时间，这是对程序运行时间的保证。 一般法则 顺序语句的运行时间是各个语句运行时间求和 选择语句的运行时间是判断与分支中的最长运行时间之和 循环语句其运行时间至多是该 for 循环内语句的运行时间乘以迭代的次数 不过我们需要注意一点，就是 递归 (recursion)，虽然其中可能只使用到顺序与选择语句，但其是隐含的循环结构。如果你对递归的认识并不是很好，可以学习 SICP 的第一章 ~ 构造过程的抽象~ ，使用 Scheme 进行学习与构造的过程中是十分愉快的。 使用 SICP 中的例子，递归可以分为 线性递归 与 树形递归，在 recursion 应用中，前者的典型示例是阶乘，而后者的典型示例是 Fibonacci 数列。 以阶乘为示例，翻译为简单的数学表达式 \\(n! = n \\cdot [(n - 1) \\cdot (n - 2) \\cdots 3 \\cdot 2 \\cdot 1] = n \\cdot (n - 1)!\\) ，递归的进行阶乘的求解，构造起一个 推迟进行的操作 所形成的链条，收缩阶段表现为这些运算的实际执行。忽略程序运行时进行的函数调用开销，这个程序的时间复杂度为 \\(\\mathcal{O}(N)\\) ，保证对于任何输入都可以在关于 N 的线性时间完成。 (define (factorial n) (if (or (= n 1) (= n 0)) 1 (* n (factorial (- n 1))))) 以 Fibonacci 数列 \\(F(n) = F(n - 1) + F(n - 2), F(0) = F(1) = 1\\) 为示例，在求解第 n 个 fibonacci 数时，我们需要对第 \\(n - 1\\) 个和第 \\(n - 2\\) 个数进行分别求解，然后对第 \\(n - 1\\) 个数也如此求解。最终我们的递归构造起一个树形的推迟计算结构，并在收缩时进行了很多冗余计算。对于示例程序，其运行时间为 \\(T(N) = T(N - 1) + T(N - 2) + 2\\) ，利用数学归纳法可以得知 \\(fib_{N} \u003c (5/3)^{N}\\) 并且 \\(F_{N} \\geq (3/2)^{N}\\) ，这个运算的时间将随着 N 的增加而指数级增加。 (define (fib n) (cond ((= n 0) 1) ((= n 1) 1) (else (+ (fib (- n 1)) (fib (- n 2)))))) 示例：最大子数组和问题 给定序列 \\(A_{1}, A_{2}, \\dots, A_{N}\\) ，且 \\(A_{i} \\in \\mathbb{Z}\\) ，求 \\(\\sum_{k=i}^{j}{A_{k}}\\) 的最大值。 例如，对于输入 \\(-2, 11, -4, 13, -5, -2\\) ，其答案为 \\(20\\) 对于该问题有四种差异极大的解决方法，其时间复杂度分别为 \\(\\mathcal{O}{N^{3}}\\) 、 \\(\\mathcal{O}{N^{2}}\\) 、 \\(\\mathcal{O}{N \\log_{}{N}}\\) 以及 \\(\\mathcal{O}{N}\\) 。对于小输入来说，无论选取哪种方式，计算机总能很快完成其给定输入；但数据输入到达一定大的数量级时，其前两种算法的时间复杂度实在太大了，以至于它们十分缓慢，不再适合解决该问题。 朴素算法，时间复杂度为 \\(\\mathcal{O}(N^{3})\\) int max_subarray_sum(const int arr[], const int len) { int ans = 0; for (int i = 0; i \u003c len; i++) { for (int j = i; j \u003c len; j++) { int sum = 0; for (int k = i; k \u003c= j; k++) { sum += arr[k]; } if (sum \u003e ans) { ans = sum; } } } return ans; } 该算法在 6 ~ 8 行由一个隐含于三层 for 循环中的 \\(\\mathcal{O}(1)\\) 语句组成，循环大小为 N (虽然它们是 \\(N - i\\) 和 \\(j - i + 1\\) ，但最坏情况为 \\(N\\))，总开销为 \\(\\mathcal{O}(1 \\cdot N \\cdot N \\cdot N) = \\mathcal{O}(N^{3})\\) 。第 9 ~ 11 行语句开销 \\(\\mathcal{O}(N^{2})\\) 。因此我们可以忽略低阶表达式带来的影响，其最终的分析答案为 \\(\\Theta(N^{3})\\) 。 优化算法到 \\(\\mathcal{O}(N^{2})\\) 我们观察到第三层循环是 \\(\\sum_{k=i}^{j}{A_{k}} = A_{j} + \\sum_{k=i}^{j-1}{A_{k}}\\) ，而第二层循环即从 i 到 len 循环整个数组，而 \\(j = i\\) 时 \\(\\sum_{k=i}^{j}{A_{k}} = A_{j}\\) ，而在循环时完全可以将第三层循环去除，累加 \\(A_{i}\\) ~ \\(A_{len}\\) 并在累加过程中与当前最大结果进行比较。 int max_subarray_sum(const int arr[], const int len) { int ans = 0; for (int i = 0; i \u003c len; i++) { int sum = 0; for (int j = i; j \u003c len; j++) { sum += arr[j]; if (sum \u003e ans) { ans = sum; } } } return ans; } 时间复杂度为 \\(\\mathcal{O}(N)\\) 的 Recursion 解法 (分治算法) 对于一个区间 \\([l, r]\\) 我们取 \\(\\lfloor\\dfrac{l+r}{2}\\rfloor\\) 分治区间，直到区间长度为 1 后开始收缩。对于每段区间，需要定义四种变量来维护区间的最大子数组和 a. \\([l, r]\\) 内以 l 为左端点的最大子数组和，记作 lr_lsum b. \\([l, r]\\) 内以 r 为右端点的最大子数组和，记作 lr_rsum c. \\([l, r]\\) 内的最大子数组和，记作 lr_msum d. \\([l, r]\\) 的区间和，记作 lr_sum 假设 \\(m = \\lfloor\\dfrac{l+r}{2}\\rfloor\\) ，称区间 \\([l, m]\\) 为左区间，\\([m + 1, r]\\) 为右区间，在合并区间时 a. \\([l, r]\\) 的 lr_lsum 为 \\(max(lm\\_lsum, lm\\_sum + mr\\_lsum)\\) b. \\([l, r]\\) 的 lr_rsum 为 \\(max(mr\\_rsum, mr\\_sum + lm\\_rsum)\\) c. \\([l, r]\\) 的 lr_msum 为 \\(max(lm\\_msum, mr\\_msum, lm\\_rsum + mr\\_lsum)\\) 如此我们可以轻松获取到整个区间的最大值 struct State { int lsum, rsum, msum, sum; }; struct State push_up(struct State l, struct State r) { return (struct State) { .lsum = fmax(l.lsum, l.sum + r.lsum), .rsum = fmax(r.rsum, r.sum + l.rsum), .msum = fmax(fmax(l.msum, r.msum), l.rsum + r.lsum), .sum = fmax(l.sum, r.sum), }; } struct State get(const int arr[], const int l, const int r) { if (l == r) { return (struct State){arr[l], arr[l], arr[l], arr[l]}; } int m = (l + r) \u003e\u003e 1; return push_up(get(arr, l, m), get(arr, m + 1, r)); } int max_subarray_sum(const int arr[], const int len) { return get(arr, 0, len - 1); } ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:2:3","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#示例-最大子数组和问题"},{"categories":["Algorithm⁄DataStructure"],"content":" 对数级增长对数级增长通常发生在分治算法中，或者其他算法中。如果一个算法用 常数时间 (\\(\\mathcal{O}(1)\\)) 将问题的大小削减为其一部分 (通常为 \\(1/2\\))，则该算法就是 \\(\\mathcal{O}(\\log_{}{N})\\) 的。比如说在二分算法、欧几里得算法 (迭代法求最大公因数) 或快速幂算法。 ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:2:4","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#对数级增长"},{"categories":["Algorithm⁄DataStructure"],"content":" 抽象数据类型抽象数据类型 (Abstract Data Type, ADT) 是带有一组操作的一些对象的集合。ADT 是数学抽象，在 ADT 的定义中根本又有提到这组操作是如何实现的。对于不同的数据结构，其存储的数据都是抽象数据，可以是整数、浮点数、布尔数或其他符合 ADT 要求的数据类型。对于不同的 ADT 也有不同的操作，比如线性 ADT 可以有 insert (插入)、移除 (remove)、大小 (size) 等等，集合 ADT 还可以有其他操作，比如 并 (union)、查找 (find) 等。 对于适当地隐藏实现细节，如此程序中需要对 ADT 实施操作的任何其他部分可以通过调用适当的方法来进行。如果出于某些原因需要更改实现细节，那么通过仅仅改变执行这些 ADT 操作的例程是十分轻松的，而这些修改对于程序的其他部分是 透明的 (transparent)。 ADT 并不是必须实现这些操作，而是一种 设计决策 。错误处理和结构调整一般取决于程序的设计者。比如说 C++ 的 STL，标准中只定义了每种容器的接口，和每个接口的时间复杂度和要求。 ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:3:0","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#抽象数据类型"},{"categories":["Algorithm⁄DataStructure"],"content":" 容器与迭代器容器 (Container) 是一类特殊的类型，它是存放数据的集合，不同类型的 Container 有着不同的适用场景。容器主要分为四大类： 顺序容器 (sequence container)：实现能按顺序访问的数据结构 关联容器 (associative container)：实现能快速查找 (\\(\\mathcal{O}(\\log_{}{N})\\)) 的数据结构 无序关联容器 (unordered associative container)：实现能快速查找 (\\(\\mathcal{O}_{avg}(1), \\mathcal{O}_{worst}(N)\\)) 的无序数据结构 容器适配器 (container adaptor)：提供顺序容器的不同接口 容器其实是一组特殊的数据结构，为编程过程中提供便利。其中 associative container 主要使用 红黑树 (red-black tree) 作为底层实现，这是我们之后需要学习的树的一种； unordered associative container 底层使用 Hash (散列) 进行实现；container adaptor 则是对 sequence container 的接口进行再封装，所实现的一种受限容器。 为了更轻松的访问容器，实现容器元素的遍历 (traverse)，从而无需关心容器对象的内存分配的实现细节，从而引入 迭代器 (iterator) 的概念。iterator 依据功能的不同被分为了不同的种类，且约束 (constraint) 逐渐增强。 分类 名称 功能 遗留迭代器 LegacyIterator 描述可以用来标识和遍历容器中的元素的类型 向前迭代器 ForwardIterator 能从所指向的元素读取数据的 LegacyIterator 双向迭代器 BidirectionalIterator 能双向移动 (即自增自减) 的 ForwardIterator 随机访问迭代器 RandomAccessIterator 能在常数时间内移动到指向任何元素的 BidirectionalIterator 连续迭代器 ContiguousIterator 其所指向的逻辑相邻元素也在内存中物理上相邻的 RandomAccessIterator 虽然上述这些关于 Container 与 Iterator 的概念从 C++ 而来，但在不同编程语言中差别不大，是一种较为通用的概念。 ","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:3:1","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#容器与迭代器"},{"categories":["Algorithm⁄DataStructure"],"content":" 概念与约束这是一个 C++ 20 中添加的特性，可以与 constraint 关联，它指定对模板实参的一些要求，这些要求可被用于选择最恰当的函数重载和模板特化。这与 Haskell 的类型类相似，限制可以接受的对象的类型，并对其进行 constraint。在不同语言中有不同的类似概念，如果你想了解更多关于它们区别的内容，可以移步 这里。 对不住了，我能力有限啊！ 迭代器首先介绍 iterator 的 concept，你可以将 iterator 想象成一个指向元素的指针。 container 的 concept 依赖于 iterator，但 iterator 的具体实现依赖于 container。 LegacyIterator template \u003cclass I\u003e concept iterator = requires(I i) { { *i } -\u003e Referenceable; // 1 { ++i } -\u003e std::same_as\u003cI\u0026\u003e; // 2 { *i++ } -\u003e Referenceable; // 3 } \u0026\u0026 std::copyable\u003cI\u003e; // 4 LegacyIterator 要求： 对于 I 类型的对象 i 可以解引用并返回对数据的引用 对于 I 类型的对象 i 可以自增且返回的是对自身的引用 对于 I 类型的对象 i 可以返回对数据的引用并使其自增 必须是可复制的 ForwardIterator template \u003cclass I\u003e concept forward_iterator = input_iterator\u003cI\u003e \u0026\u0026 // 1 std::constructible_from\u003cI\u003e \u0026\u0026 // 2 std::is_lvalue_reference_v\u003cstd::iter_reference_t\u003cI\u003e\u003e \u0026\u0026 // 3 std::same_as\u003cstd::remove_cvref_t\u003cstd::iter_reference_t\u003cI\u003e\u003e, // 4 typename std::indirectly_readable_traits\u003cI\u003e::value_type\u003e \u0026\u0026 requires(I i) { { i++ } -\u003e std::convertible_to\u003cconst I\u0026\u003e; { *i++ } -\u003e std::same_as\u003cstd::iter_reference_t\u003cI\u003e\u003e; }; ForwardIterator 要求： I 是一个 LegacyInputIterator 可以从 I 构造 I 的引用的元素类型可被左值引用 I 的引用的元素类型可被读 BidirectionalIterator template \u003cclass I\u003e concept bidirectional_iterator = forward_iterator\u003cI\u003e \u0026\u0026 // 1 requires(I i) { { --i } -\u003e std::same_as\u003cI\u0026\u003e; // 2 { i-- } -\u003e std::convertible_to\u003cconst I\u0026\u003e; { *i-- } -\u003e std::same_as\u003cstd::iter_reference_t\u003cI\u003e\u003e; }; BidirectionalIterator 要求： I 是一个 ForwordIterator 对于 I 类型的对象 i 可以自减并返回自身的引用 RandomAccessIterator template \u003cclass I\u003e concept random_access_iterator = bidirectional_iterator\u003cI\u003e \u0026\u0026 // 1 std::totally_ordered\u003cI\u003e \u0026\u0026 // 2 requires(I i, typename std::incrementable_traits\u003cI\u003e::difference_type n) { // 3 { i += n } -\u003e std::same_as\u003cI\u0026\u003e; // 3.1 { i -= n } -\u003e std::same_as\u003cI\u0026\u003e; // 3.2 { i + n } -\u003e std::same_as\u003cI\u003e; // 3.3 { n + i } -\u003e std::same_as\u003cI\u003e; // 3.4 { i - n } -\u003e std::same_as\u003cI\u003e; // 3.5 { i - i } -\u003e std::same_as\u003cdecltype(n)\u003e; // 3.6 { i[n] } -\u003e std::convertible_to\u003cstd::iter_reference_t\u003cI\u003e\u003e; // 3.7 }; RandomAccessIterator 要求： I 是一个 BidirectionalIterator 对于 I 类型的对象进行比较，其结果符合 严格全序要求 对于 I 类型的对象 i 与 I 类型的关联差类型 n： i 以 \\(\\mathcal{O}(1)\\) 时间复杂度向前步进 n 并返回对其自身的引用 i 以 \\(\\mathcal{O}(1)\\) 时间复杂度向后步进 n 并返回对其自身的引用 i 的副本以 \\(\\mathcal{O}(1)\\) 时间复杂度向前步进 n 并返回 同 3 i 的副本以 \\(\\mathcal{O}(1)\\) 时间复杂度向后步进 n 并返回 \\(i_{1}\\) 与 \\(i_{2}\\) 的关联差，即计算 \\(i_{i} - i_{2}\\) 随机对 i 进行访问并返回元素的引用，即 \\(*(i + n)\\) ContiguousIterator template \u003cclass I\u003e concept contiguous_iterator = std::random_access_iterator\u003cI\u003e \u0026\u0026 requires(const I\u0026 i) { { std::to_address(i) } -\u003e std::same_as\u003cstd::add_pointer_t\u003cstd::iter_reference_t\u003cI\u003e\u003e\u003e; }; ContiguousIterator 要求：设 a 与 b 为 I 类型的可解引用迭代器，c 为 I 类型的不可解引用迭代器，使得 b 从 a 可抵达而 c 从 b 可抵达。类型 I 实现 contiguous_iterator 仅若其所蕴含的所有概念均被实现，且： \\[address_{a} = address_{*a} \\quad \\land \\quad address_{b} = address_{a} + (b - a) \\quad \\land \\quad address_{c} = address_{a} + (c - a)\\] 容器 template \u003cclass T\u003e concept container = requires(T a, const T b) { requires regular\u003cT\u003e; // 1 requires swappable\u003cT\u003e; // 2 requires erasable\u003ctypename T::value_type\u003e; // 3 requires same\u003ctypename T::reference, typename T::value_type\u0026\u003e; // 4 requires same\u003ctypename T::const_reference, const typename T::value_type\u0026\u003e; // 4 requires forward_iterator\u003ctypename T::iterator\u003e; // 5 requires forward_iterator\u003ctypename T::const_iterator\u003e; // 5 requires unsigned\u003ctypename T::size_type\u003e; // 6 requires signed\u003ctypename T::difference_type\u003e; // 7 requires same\u003ctypename T::difference_type, typename std::iterator_traits\u003ctypename T::iterator\u003e::difference_type\u003e; // 8 requires same\u003ctypename T::difference_type, typename std::iterator_traits\u003ctypename T::const_iterator\u003e::difference_type\u003e; // 8 { a.begin() } -\u003e typename T::iterator; { a.end() } -\u003e typename T::iterator; { b.begin() } -\u003e typename T::const_iterator; { b.end() } -\u003e typename T::const_iterator; { a.cbegin() } -\u003e typename T::const_iterator; { a.cend() } -\u003e typename T::const_iterator; { a.size() } -\u003e typename T::size_type; { a.max_size() } -\u003e typename T::size_type; { a.empty() } -\u003e boolean; a.clear(); a.swap(a); }; 对于容器类型 T，其中包含的元","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:3:2","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#概念与约束"},{"categories":["Algorithm⁄DataStructure"],"content":" 概念与约束这是一个 C++ 20 中添加的特性，可以与 constraint 关联，它指定对模板实参的一些要求，这些要求可被用于选择最恰当的函数重载和模板特化。这与 Haskell 的类型类相似，限制可以接受的对象的类型，并对其进行 constraint。在不同语言中有不同的类似概念，如果你想了解更多关于它们区别的内容，可以移步 这里。 对不住了，我能力有限啊！ 迭代器首先介绍 iterator 的 concept，你可以将 iterator 想象成一个指向元素的指针。 container 的 concept 依赖于 iterator，但 iterator 的具体实现依赖于 container。 LegacyIterator template concept iterator = requires(I i) { { *i } -\u003e Referenceable; // 1 { ++i } -\u003e std::same_as; // 2 { *i++ } -\u003e Referenceable; // 3 } \u0026\u0026 std::copyable; // 4 LegacyIterator 要求： 对于 I 类型的对象 i 可以解引用并返回对数据的引用 对于 I 类型的对象 i 可以自增且返回的是对自身的引用 对于 I 类型的对象 i 可以返回对数据的引用并使其自增 必须是可复制的 ForwardIterator template concept forward_iterator = input_iterator \u0026\u0026 // 1 std::constructible_from \u0026\u0026 // 2 std::is_lvalue_reference_v","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:3:2","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#迭代器"},{"categories":["Algorithm⁄DataStructure"],"content":" 概念与约束这是一个 C++ 20 中添加的特性，可以与 constraint 关联，它指定对模板实参的一些要求，这些要求可被用于选择最恰当的函数重载和模板特化。这与 Haskell 的类型类相似，限制可以接受的对象的类型，并对其进行 constraint。在不同语言中有不同的类似概念，如果你想了解更多关于它们区别的内容，可以移步 这里。 对不住了，我能力有限啊！ 迭代器首先介绍 iterator 的 concept，你可以将 iterator 想象成一个指向元素的指针。 container 的 concept 依赖于 iterator，但 iterator 的具体实现依赖于 container。 LegacyIterator template concept iterator = requires(I i) { { *i } -\u003e Referenceable; // 1 { ++i } -\u003e std::same_as; // 2 { *i++ } -\u003e Referenceable; // 3 } \u0026\u0026 std::copyable; // 4 LegacyIterator 要求： 对于 I 类型的对象 i 可以解引用并返回对数据的引用 对于 I 类型的对象 i 可以自增且返回的是对自身的引用 对于 I 类型的对象 i 可以返回对数据的引用并使其自增 必须是可复制的 ForwardIterator template concept forward_iterator = input_iterator \u0026\u0026 // 1 std::constructible_from \u0026\u0026 // 2 std::is_lvalue_reference_v","date":"08-15","objectID":"/2021/data_strucures_and_algorithm_analysis_001_introduction/:3:2","series":["数据结构与算法分析"],"tags":["Note"],"title":"数据结构与算法分析引论","uri":"/2021/data_strucures_and_algorithm_analysis_001_introduction/#容器"},{"categories":["OperatingSystem"],"content":"GinShio | 现代操作系统第六章读书笔记","date":"08-06","objectID":"/2021/operatingsystem_005/","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/"},{"categories":["OperatingSystem"],"content":"计算机中有很多独占 resource，在任一时刻它们只能被一个进程使用，因此 OS 需要授权一个进程临时地、排他地访问某一 resource 的能力。一般进程会排他性地访问若干资源，假设进程 A 在先使用扫描仪的情况请求蓝光光盘刻录机，而进程 B 在先使用蓝光光盘刻录机的情况下请求扫描仪，由于两个进程都占有一定 resouce 且不会释放，并且互相请求了对方的 Resource，而造成这两个进程无限阻塞下去，这样的状态被称为 死锁 (deadlock)。请不要单纯理解只有一台机器才会产生 deadlock，在多台机器同时访问局域网下的多个独占 resource 时也可能发生。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:0:0","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#"},{"categories":["OperatingSystem"],"content":" 资源我们将需要排他访问的对象称为 资源 (resource)，资源可以是硬件设备或一组信息，通常有多种资源同时存在，且一些类型的资源存在若干实例 (打印店在同一局域网下会存在多台打印机)。 还记得学习进程与线程时所说的，scheduling algorithm 分为两类： 抢占式 (preemptable) 与 非抢占式 (nonpreemptable)，这里我们也将 resource 分为这两类，且意义相同。比如说 RAM 就是 preemptable resource，一个进程可以在使用时被 OS 换出 RAM；而蓝光刻录机则是 nonpreemptable resource，将正在刻录的蓝光刻录机分配给另一个进程可能造成蓝光光盘的损坏。不过需要思考一个问题，如果这台计算机不支持交换和页面调度，那么内存也就变为了 nonpreemptable resource。因此区分 preemptable / nonpreemptable resource 取决于上下文环境。 preemptable resource 的 deadlock 可以通过 resource 的重新配分而化解，因此 deadlock 主要与 nonpreemptable resource 有关。若 resource 变得不可用，则请求进程将不可用，有的 OS 在这时会阻塞该进程，直到 resource 可用时再次唤醒；有些 OS 则会返回一个错误代码，请求进程自己处理这个错误。但是大部分进程会选择请求、休眠、再请求的方式进行循环，这与被阻塞没什么两样，因此假设 OS 在 resource 不可用时阻塞进程。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:1:0","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#资源"},{"categories":["OperatingSystem"],"content":" 死锁 如果一个进程集合中的每个进程都在等待只能由该进程集合中的其他进程才能引发的事件，那么该集合就是死锁的。 进程的数量、占有或请求的资源数量与种类都是无关紧要的，无论资源是何种类型，硬件还是软件，当该集合进程无法引发事件供集合使用时，他们就是 deadlock，当然这是 资源死锁 (resource deadlock)。resource deadlock 并不是唯一的死锁类型，但这是最常见的一种。 Coffman 等人在 1971 年发表的论文 System Deadlocks 中总结了 Resource Deadlock 发生的四个必要条件： 互斥，每个资源要么已分配给了一个进程，要么是可用的。 占有与等待，已经得到了某个资源的进程可以再次请求新的资源。 不可抢占，已分配的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。 环路等待，死锁发生时系统中必定存在两个或以上的进程组成的环路，该环路中每个进程都在等待下一个进程所占有的资源。 需要注意的是，deadlock 发生时这四个条件必定同时满足，如果有一个不成立那么死锁不会发生。Holt 则在 1972 年发表的论文 Some Deadlock Properties of Computer Systems 中描述了如何用有向图对上述四个条件进行建模。有向图中 圆形 表示进程，方形 表示资源，资源到进程的有向边则表示该资源已授权并被进程占用，进程到资源的有向边则表示请求且还未占有。 可以大胆的猜测，当进程集合中的所有进程都以严格的串行运行，那么该集合就不会发生死锁。但是问题时，没有并行的系统性能极低，是不可接受的。如果所有进程都不进行 IO，那么 SJF 比轮询更好，此时的串行运行是最优解，但现实不是这样。 既然死锁是四个必要条件，那么对其进行破坏不就可以解除死锁了吗。Holt 在建模的同时，提出了四种处理死锁的策略： 鸵鸟算法，即忽略死锁。如果你忽略死锁，死锁也会忽略你。 检测死锁并恢复。让死锁发生，检测它们是否发生，发生时采取行动解决问题。 仔细对资源进行分配，动态避免死锁。 通过破坏四个必要条件之一，防止死锁产生。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:2:0","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#死锁"},{"categories":["OperatingSystem"],"content":" 死锁的检测与恢复","date":"08-06","objectID":"/2021/operatingsystem_005/:3:0","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#死锁的检测与恢复"},{"categories":["OperatingSystem"],"content":" 每种类型一个资源的死锁检测我们可以为这样的系统构造一张资源分配图，这是一个有向图，当图中存在一个以上的环时即可说明死锁的存在。而在环上的资源与进程则是死锁所涉及的，最终我们将死锁检测转换为对有向环路的检测。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:3:1","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#每种类型一个资源的死锁检测"},{"categories":["OperatingSystem"],"content":" 每种类型多个资源的死锁检测如果一个类型有多个资源时，便不能使用有向图来检测是否死锁。现在提供一种基于矩阵的算法来检测 \\(P_1\\) 到 \\(P_n\\) 这 n 个进程的死锁。假设资源的类型数量为 m，资源 \\(E_1\\) 到 \\(E_m\\) 代表了不同的资源。E 是 现有资源向量 (Existing Resource Vector)，代表每种已存在资源的总数；A 是 可用资源向量 (Available Resource Vector)，代表每种资源当前可用的数量。现在有 C 为 当前分配矩阵 (Current Allocation Matrix)，R 为 请求矩阵 (Request Matrix)。C 中第 \\(i\\) 行代表进程 \\(P_i\\) 当前所持有的资源数量，所以 \\(C_{ij}\\) 代表了 \\(P_i\\) 持有资源 \\(E_j\\) 的数量，而 \\(R_{ij}\\) 代表了 \\(P_i\\) 所需资源 \\(R_j\\) 的数量。 这四种 data structure 之间有一个重要的恒等式，具体地说，某种资源要么已分配要么可用： \\[\\sum_{i=1}^{n} CAM_{ij} + ARV_{j} = ERV_{j}.\\] 换言之，如果我们将所有已分配的资源 j 的数量加起来再和所有可供使用的资源相加，结果就是该资源的总数。死锁检测算法就是基于向量的比较，当且仅当 \\(A_{i} \\leq B_{i} (0 \\leq i \\leq m)\\) 时 \\(A \\leq B\\)。当个进程起初都是没有标记过的，算法开始会对进程做标记，进程被标记后就表明它们能够被执行，不会进入死锁。当算法结束时，任何没有被标记的进程都是死锁的。 死锁检查算法如下： 寻找一个没有标记的进程 \\(P_{i}\\) ，对于它而言 \\(RM_{i} \\leq ARV\\)。 如果找到了一个这样的进程，那么 \\(ARV += CRM_{i}\\)，标记该进程并重新执行第一步。 如果没有这样的进程，那么算法停止。 你可能注意到了第二步中只是简单的将占有资源加到了可用资源中，没有做其他处理。因为我们这里只是对死锁进行检查，无需处理其他情况，因此该进程可以被满足时就认为不会死锁，并将其算为可用资源并计算其他进程的资源。 我们知道了如何检测死锁，现在的问题是何时检测死锁。一般可以选择在资源请求时进行死锁检测，这样可以更早地发现存在的死锁，但是也会消耗大量的 CPU。另一种方法是每隔一段时间进行一次检测，或者当 CPU 的使用率降到某一语支时进行检测，这样考虑到 CPU 使用率的问题，如果死锁的进程数达到一定数量后，就没有多少进程可以运行了，所以 CPU 会经常空闲。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:3:2","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#每种类型多个资源的死锁检测"},{"categories":["OperatingSystem"],"content":" 从死锁中恢复我们需要利用一些手段，将检测到的死锁恢复，从而使系统恢复正常。 利用抢占恢复 在某些情况下，可能会临时将某个资源从它的当前所有者那里转移给另一个进程。在不通知进程的情况下，将某一资源从一个进程强行取走给另一个进程使用接着再送回，这种做法是否可行主要取决于该资源本身的特性。用这种方法恢复通常比较困难或不太可能，若选择挂起某个进程，则在很大程度上取决于哪个进程拥有比较容易回收的资源。 利用回滚恢复 如果系统设计人员以及主机操作员了解到死锁有可能发生，他们就可以周期性地对进程进行检查点 (checkpointed) 检查。进程检查点检查就是将进程的状态写入一个文件以备以后重启。该检查点中不仅包括存储映像，还包括资源状态，即哪些资源分配给了该进程。为了使这一过程更有效，新的检查点不应覆盖原有的文件，而应写入新的文件。 当检测到死锁时，就很容易发现需要哪些资源。为了进行恢复，要从一个较早的检查点上开始，这样拥有所需要资源的进程会回滚到一个时间点，在此时间点之前该进程获得了一些其他的资源。在该检查点之后所做的所有工作都丢失。接着可以将这个资源分配给一个死锁进程，当该进程再次试图获取资源的控制时，就必须一直等待直到可用。 通过杀死进程恢复 最直接、简单且有效的方法就是杀死一个或若干个进程，这些进程都是死锁环路上的一个，若杀死进程依然不行的话则继续杀死其他进程，直到打破死锁环。当然还可以牺牲环外的进程用以释放相关资源，供死锁环上的进程使用。当然选择杀死的进程应该具有 幂等性，这样它再次运行时与第一次则会产生相同的结果。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:3:3","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#从死锁中恢复"},{"categories":["OperatingSystem"],"content":" 死锁避免还记着检测死锁的四个 structure 吗，我们假设的是一次请求所有的资源，但大多数系统一次只能请求一个资源。系统必须能够判断分配资源是否安全，且只能在保证安全的条件下进行资源分配。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:4:0","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#死锁避免"},{"categories":["OperatingSystem"],"content":" 资源轨迹图避免死锁的主要算法是基于一个安全状态的概念，在描述算法前，先讨论关于安全的概念。 假设现在我们有两个进程与绘图仪、打印机，我们使用一个轨迹图来描述它们。横轴表示进程 A 执行的指令，纵轴则为进程 B 执行的指令。进程 A 在 \\(I_1\\) 处请求一台打印机并在 \\(I_3\\) 处释放，在 \\(I_2\\) 处请求一台绘图仪，在 \\(I_4\\) 处释放；进程 B 则是 \\(I_5 \\sim I_7\\) 之间使用绘图仪，\\(I_6 \\sim I_8\\) 之间使用打印机。 资源轨迹图中的每个点代表两个进程的连接状态。初始点 p 表示没有进程执行任何指令。如果调度程序先选择 A 运行，那么 A 执行一段指令后到达 q，此时调度程序开始选中 B 执行。渐变部分是死锁状态，而 \\(I_1\\)、\\(I_2\\)、\\(I_5\\) 和 \\(I_6\\) 围成的矩形区域一旦进入，一定会进入渐变区造成死锁，因此整个矩形都是不安全区域。唯一的办法就是在 t 点开始调度程序一直让 A 运行直到 \\(I_4\\)。一旦过了不安全区域，调度程序即可以任意方式到达中断。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:4:1","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#资源轨迹图"},{"categories":["OperatingSystem"],"content":" 安全与不安全状态在任何时刻，系统状态包含了 E (Existing)、A (Available)、 C (Current) 和 R (Request)。如果没有死锁发生，且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。在其他序列中，进程可能先占有一个资源而使其他进程无法完成需求，但进程可能先释放一个资源，从而使其他进程可以优先完成，从而避免死锁。因此需要注意的是，不安全状态不意味着死锁，而是安全状态系统保证所有进程都可以完成，但不安全状态无此保证。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:4:2","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#安全与不安全状态"},{"categories":["OperatingSystem"],"content":" 银行家算法我们十分熟悉的大神 Dijkstra 于 1965 年发表的算法，该模型基于一个小镇的银行家，他向一群客户分别承诺了一定的贷款额度。算法要做的是判断对请求的满足是否会导致进入不安全状态。如果是则拒绝请求，反之满足请求后系统仍是安全的，就给予分配。当然我们可以将银行家算法推广到多个资源的死锁预防。 现在我们现在系统中有 6 台磁带机、3 台绘图仪、4 台打印机和 2 台蓝光光驱，5 个进程对其的占有与请求如下图。 检查一个状态是否安全的算法如下： 查找 Request 中是否有一行，请求的资源数小于或等于 Available Resource。如果不存在这样的行则认为系统将会死锁，因为任何进程都无法结束运行。 若找到满足条件的行，那么可以认为其获得所需的资源并运行结束，将该进程标记为终止，并将其资源加到 Available 上。 重复以上两步，或者直到所有进程都标记为终止，其初始状态是安全的；或者所有进程资源都无法得到满足，发生死锁。 虽然 Banker Algorithm 十分强大，但由于很少有进程在运行初期就知道所需资源的最大值，且进程数与资源数是动态变化的，因此实际上很少有系统使用该算法避免死锁。但一些系统会从该算法之类的启发式方法来避免死锁。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:4:3","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#银行家算法"},{"categories":["OperatingSystem"],"content":" 死锁预防虽然无法避免死锁，但我们还是可以从最初的四个必要条件说起，毕竟只需要破坏一个就可以保证死锁不会发生。 破坏互斥条件 如果一个资源不被进程所独占，那么死锁不会发生，但是允许两个进程同时使用某些资源，难免会发生混乱，比如打印机。因此反过来，如果任何时候都只有一个进程使用该资源，那么也不会造成 deadlock。想想之前所说过的 spooling 技术，仅允许 daemon 对资源进行请求，而其他进程以请求 daemon 的方式使用资源。并且 daemon 不再请求其他资源，因此不会造成 daedlock。 破坏占有等待条件 我们可以要求进程在开始时请求自己所需的全部资源，而在运行时将不再申请资源，这样的进程运行时一定不会发生死锁。当然我们还可以要求进程在请求资源时，首先释放当前已占有的所有资源，再请求所需的全部资源。 破坏不可抢占条件 : 破坏环路条件 我们可以对系统中的资源规定从小到大的统一编号，并且定义规则，所有请求必须按资源编号的升序提出。如果按照此规则进行实现，资源分配图中就不会出现环。当然在某些时刻，比如一些抽象资源会使编号变得很大，以至于根本无法使用编号。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:5:0","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#死锁预防"},{"categories":["OperatingSystem"],"content":" 其他问题虽然在一般情况下避免死锁和预防死锁并不是很有希望，但一些特殊应用会有很优秀的算法应对死锁。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:6:0","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#其他问题"},{"categories":["OperatingSystem"],"content":" 两阶段加锁在数据库系统中，一个需要经常锁住一些记录，然后更新所有锁住的记录。当同时有很多进程运行时，就会出现死锁的危险。在此最常用的方法是 两阶段加锁 (two-phase locking)。第一阶段进程试图对所有所需的记录进行加锁，一次锁一个记录。如果第一阶段加锁成功则进行第二阶段，完成更新后释放锁。如果需要加锁的记录已被加锁，则需要释放该进程刚刚所加的所有锁。第一阶段的全部记录都被加锁后，进行第二阶段，即真正的修改数据阶段。当然这在实时系统中是不可接受的。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:6:1","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#两阶段加锁"},{"categories":["OperatingSystem"],"content":" 通信死锁资源死锁是 竞争性同步 问题，这是最普遍但不是唯一一种死锁。而两个或以上进程利用发送消息来通信时也可能发生死锁。普遍的情况是：进程 A 想进程 B 发送请求信息然后等待进程 B 回复，当请求丢失后 A 将阻塞等待回复，而 B 会阻塞等待一个向其发送命令的请求。虽然进程 A 与 B 没有占用对方所需的资源，但它们依然发生了死锁，这种死锁一般被称为 通信死锁 (communication deadlock)。通信死锁是 协同同步 的异常情况，这种死锁中的进程如果是各自独立执行的，则无法完成服务。 由于没有资源，所以针对资源排序或安排调度来避免死锁是行不通的，幸运的是其可以使用 超时 来中断死锁。在大多数网络通信中，只要一个消息被发送且期待回信，通常会同时启动一个 Timer，如果 Timer 发生中断时回复消息还没有到达则认为发送的消息丢失。当然超时不仅可以用在通信死锁上，资源死锁也可以使用这种策略。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:6:2","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#通信死锁"},{"categories":["OperatingSystem"],"content":" 活锁在某些情况下，当进程意识到它不能获取所需要的下一个锁时，自主地释放已经获得的锁，然后等待 1ms 并再次尝试。从理论上来说这是一种检测并预防死锁的好方法，但如果另一个进程也在同时刻做相同的事情，那么两个进程就永远的相遇并为对方让步，导致双方都无法前进。这种情况被称之为 活锁 (livelock)。在处理 livelock 时，我们尝尝为进程等待一个可接受范围内的随机时间，这样极大程度地避免 livelock 的发生。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:6:3","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#活锁"},{"categories":["OperatingSystem"],"content":" 饥饿饥饿 (starvation) 与 deadlock / livelock 非常相似。在动态运行的系统中，任何时刻都有可能请求资源，在就需要策略来决定什么时候由谁获取资源。虽然这个策略表面上很合理，但某些进程可能永远得不到服务。 考虑进程调度的 SJF 算法，最短的作业总是被最先执行，而需要时间长的作业被放到之后完成。如果大量小型作业不停涌入，大作业将一直得不到服务。这个大作业就产生了 starvation。 当然我们可以在其上加入 FCFS 的思想，所有作业都会变老，而最老的作业将被服务，从而缓解系统的 starvation 现象。 ","date":"08-06","objectID":"/2021/operatingsystem_005/:6:4","series":["Operating System Note"],"tags":["Note","Concurrency","Deadlock"],"title":"死锁","uri":"/2021/operatingsystem_005/#饥饿"},{"categories":["OperatingSystem"],"content":"GinShio | 现代操作系统第五章读书笔记","date":"07-25","objectID":"/2021/operatingsystem_004/","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/"},{"categories":["OperatingSystem"],"content":"除了提供抽象外，操作系统还要控制计算机的所有 IO (输入/输出) 设备，必须向设备发送命令、捕获中断并处理设备的各种错误。它还应该在设备和其他部分之间提供简单且易于使用的接口，且这些接口应该尽可能的对所有设备都相同，即设备无关。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:0:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#"},{"categories":["OperatingSystem"],"content":" 单位在进行本篇之前，需要明确一下计算机领域常用单位，只有统一了单位，我们才能更好的交流。 计算机领域以 bit (b，位) 和 byte (B，字节) 作为基本单位，bit 是只能表示 1 或 0 的单位数据，\\(1 \\texttt{Byte} = 8 \\texttt{bit}\\)，即 1 Byte 可以表示 256 种不同的状态。另外在 IO 传输数据时，最常用的单位即 比特率 (单位：bit/s 或 bps)，即每秒传输的 bit 数量，当然也可以对其进行除以 8 运算转变为 Byte/s。 传统单位以 SI (10 进制) 前缀为词头，而计算机领域多以 IEC 60027 (2 进制) 前缀为词头。一般内存与 CD 使用 IEC-60027 词头，而磁盘、闪存、DVD 常用 SI 词头表示容量。至于 Windows 系统对于容量的显示问题，内部转换为 IEC-60027 但显示的词头却是 SI，这也是历史原因所造成的。 词头 prefix 符号 \\(10^{n}\\) prefix 符号 \\(2^{n}\\) \\(16^{n}\\) 尧 yotta Y \\(10^{24}\\) yobi Yi \\(2^{80}\\) \\(16^{20}\\) 泽 zetta Z \\(10^{21}\\) zebi Zi \\(2^{70}\\) \\(16^{17.5}\\) 艾 exa E \\(10^{18}\\) exbi Ei \\(2^{60}\\) \\(16^{15}\\) 拍 peta P \\(10^{15}\\) pebi Pi \\(2^{50}\\) \\(16^{12.5}\\) 太 tera T \\(10^{12}\\) tebi Ti \\(2^{40}\\) \\(16^{10}\\) 吉 giga G \\(10^{9}\\) gibi Gi \\(2^{30}\\) \\(16^{7.5}\\) 兆 mega M \\(10^{6}\\) mebi Mi \\(2^{20}\\) \\(16^{5}\\) 千 kilo k \\(10^{3}\\) kibi Ki \\(2^{10}\\) \\(16^{2.5}\\) ","date":"07-25","objectID":"/2021/operatingsystem_004/:1:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#单位"},{"categories":["OperatingSystem"],"content":" IO 硬件原理对于不同角度观察的人，IO 硬件的理解是不同的。对于电子工程师而言，IO 硬件就是芯片、导线、电源、电机和其他组成硬件的物理部件。对程序员而言，则只注意 IO 硬件提供给软件的接口，如硬件能够接收的命令、实现的功能以及能够报告的错误。因此这里所描述的 IO 设备仅限于对硬件的编程，而非其内部工作原理。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:2:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#io-硬件原理"},{"categories":["OperatingSystem"],"content":" IO 设备IO 设备大致可以分为两类 块设备 (block device) 与 字符设备 (character device)。 block device 将信息存储在固定大小的块中，每个块有自己的地址，通常块的大小在 512 字节到 65536 字节之间，所有传输以一个或多个完整的块为单位，其基本特征是每个块都能独立于其他块读写。硬盘、蓝光光盘、USB 盘等都是常见的 block device。character device 以字符为单位发送或接收一个字符流，而不考虑任何块结构，其是不可寻址的，也没有任何寻道操作。键鼠、打印机、网络接口都是常见的 character device。 时钟是相对特殊的一类 IO 设备，既不是 character 也不是 block，其工作就是按照预先设置好的时间间隔产生中断。但是 block 与 character 的分类具有足够的一般性，可以用作使处理 IO 设备的某些 OS 软件具有设备无关性的基础。另外在速度上 IO 设备有巨大的差异，虽然大部分设备变得越来越快，但软件在跨越如此多的数量级的数据率下保证性能优良，有十分大的压力。 设备 数据率 设备 数据率 设备 数据率 PS/2 Port 7~12 kbps IEEE 802.11n 600 Mbps IEEE 802.11ax 9.6 Gbps 2G (GSM) 14.4 kbps FireWire 800 786.432 Mbps USB 3.1 10 Gbps Modem 56 kbps ATA/100 800 Mbps HDMI 1.3 10.2 Gbps 3G (W-CDMA) 384 kbps 1GEthernet 1 Gbps SAS-3 12 Gbps CD 1x 1.2288 Mbps SATA 2.0 3 Gbps PCIe 4.0 1x 15.752 Gbps MD (PCM) 1.4112 Mbps DVI 1x 3.96 Gbps USB 3.2 20 Gbps DVD 1x 11 Mbps USB 3.0 5 Gbps PCIe 5.0 1x 31.504 Gbps Blu-ray 1x 36 Mbps SATA 3.0 6 Gbps DP 1.3 32.4 Gbps 3G (HSPA+) 42.2 Mbps SAS-2 6 Gbps USB 4.0 40 Gbps 4G (LTE-FDD) 150 Mbps IEEE 802.11ac 6.93 Gbps HDMI 2.0 48 Gbps USB 2.0 480 Mbps PCIe 3.0 1x 7.88 Gbps DP 2.0 80 Gbps ","date":"07-25","objectID":"/2021/operatingsystem_004/:2:1","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#io-设备"},{"categories":["OperatingSystem"],"content":" 设备控制器IO 设备一般由机械部件与电子部件两部分组成，通常可以将两部分分开处理，以提供更加模块化、通用的设计。电子部分称为 设备控制器 (device controller) 或 适配器 (adapter)，在个人计算机中，通常以主板上的芯片或插入 PCI 总线扩展槽的电路板的形式出现。机械部分则是其设备本身。 Controller 通常有一个连接器，通向设备本身的电缆可以插入连接器中，很多控制器可以操作多个相同的设备。controller 与 device 的接口通常是更低层次的接口，例如磁盘以每个磁道 2000000 个扇区、每个扇区 512 字节进行格式化，但是从驱动器出来的却是一个串行的比特流，它以前导符开始，接着是扇区中的数据，最后则是一个 错误校正码 (Error-Correcting Code, ECC)。前导符是对磁盘格式化时写入的，其中包括柱面数、扇区号、扇区大小以及类似数据，此外还包含同步信息。Controller 的任务是将串行的比特流转换为字节块并进行校验工作。字节块在 Controller 内按 bit 组装，进行校验保证没有错误，之后再将其复制到内存中。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:2:2","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#设备控制器"},{"categories":["OperatingSystem"],"content":" 内存映射 IO每个 Controller 有几个寄存器用来与 CPU 通信，通过写入寄存器，OS 可以命令设备发送数据、接收数据、开启或关闭，或执行其他某些操作。通过读取这些寄存器，OS 可以了解设备的状态、是否准备好接收新的命令等。除了这些寄存器外，许多设备还有一个 OS 可以读写的数据缓冲区，那么首先要解决的问题即如何与设备的控制器寄存器和缓冲区通信。 可以为控制器寄存器分配 IO 端口 (port) 号，形成 IO 端口空间，并且受到保护使得普通的用户程序不能对其进行访问。另一种方法是 PDP-11 引入的，将所有控制寄存器映射到内存空间中，这些内存空间是唯一且不会被其他程序分配到的，这样的系统称为 内存映射 IO (memory mapped IO)。在大部分系统中，分配给控制寄存器的地址位于或靠近地址空间的顶端。当 CPU 读取一个字的时候，不论从内存还是 IO port 读取，都需要将地址放入总线的地址线上，然后总线的控制线置为 READ，第二条信号线表明需要的是 IO 空间或内存空间。如果只有内存空间，那么每个内存模块和和每个 IO 设备都会将地址线和它所服务的地址范围进行比较，地址落入该范围才会响应请求。 对于 memory mapped IO 的 优点： 若需要特殊的 IO 指令读写设备控制寄存器，那么访问这些寄存器需要汇编代码，而使用 memory mapped 则可以完全使用 C 语言编写驱动 不需要特殊的保护机制阻止用户进程执行 IO 操作，OS 只需要避免将内存分配给其他进程即可。在用户需要控制特定的设备时，只需要将其页面添加进页表 可以引用内存的每一条指令也可以引用控制寄存器 但其也有相应的 缺点 需要为其选择性禁用高速缓存，但会为硬件与 OS 增添额外的复杂性 所有的内存模块与 IO 设备都必须检查所有的内存引用，以便了解哪个设备做出了响应。但这是复杂的，因为现代计算机更多的使用 CPU 与 内存之间的专用告诉总线，有的架构中甚至包含更多条总线。为此有一些解决方法： 先将全部内存引用发送到内存，若内存响应失败，再尝试其他总线 内存总线上放置一个探查设备，放过所有潜在地指向所关注的 IO 设备的地址，但是设备可能无法以内存所能达到的速度处理请求 内存控制器芯片中包含引导时预装载的范围寄存器，落在标记为非内存范围内的地址将被转发到设备 ","date":"07-25","objectID":"/2021/operatingsystem_004/:2:3","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#内存映射-io"},{"categories":["OperatingSystem"],"content":" 直接存储器存取无论是否具有 memory-mapped IO，有时都需要寻址设备控制器以便与它们交换数据，但每次请求读取一字节的数据将浪费 CPU 资源，因此经常用到 直接存储器存取 (Direct Memory Access, DMA) 这种方案。只有硬件存在 DMA 时 OS 才能使用它，有时 DMA 被集成到设备的控制器中，无论 DMA 控制器出于什么位置，其都能够独立于 CPU 访问系统总线。其包括若干可直接被 CPU 读写的寄存器，包含一个内存地址寄存器、一个字节计数寄存器和一到多个控制寄存器，控制寄存器指定要使用的 IO port、传输方向、传送单位以及传送大小。 假设 CPU 通过单一总线连接所有设备与内存，系统中拥有一个 DMA 控制器。以下图为传输数据操作为例，简单说明。 CPU 通过设置 DMA 控制器的寄存器对其编程 DMA Controller 根据 CPU 的编程，在总线上对设备发起请求 设备将请求的数据写入内存 写操作完成时，设备将在总线上发出一个应答信号给 DMA DMA 完成任务后将中断 CPU，以便告知其请求内容已存在于内存中 许多总线能够进行两种操作：字模式与块模式。DMA Controller 进行字模式操作时，请求传送一个字并得到这个字，如果 CPU 需要使用总线则这时必须等待，这被称为 周期窃取 (cycle stealing)，轻微地延迟 CPU。当 DMA 通知设备获得总线并发送一连串的传送，之后释放总线，这被称为 突发模式 (burst mode)。burst mode 相比 cycle stealing 总是效率更高，但需要阻塞更长时间的 CPU 与其他设备。之前所说的 – DMA 通知设备直接将数据写入 RAM，这种模式被称为 飞跃模式 (fly-by mode)。前两种相对灵活，因为可以将数据写到任何地方，但相应地需要更多的总线周期。 DMA 一般使用物理内存地址进行传输，这就需要 OS 将内存缓冲区的物理地址直接写入 DMA。有些使用虚拟地址的 DMA，需要使用 MMU 配合完成地址转换，MMU 是内存的组成部分才能完成这部分工作，但 MMU 往往存在于 CPU 中。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:2:4","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#直接存储器存取"},{"categories":["OperatingSystem"],"content":" 中断当一个 IO 设备完成交给它的工作时，它就产生一个中断 (假设 OS 已开放中断)，它通过在分配给它的一条总线信号线上置起信号而产生中断，该信号被主板上的中断控制器芯片检测到，由中断控制芯片决定做什么。 当用中断到来时，中断控制器将立刻对中断进行处理。当有另一个中断处理到来时，可能会被忽略，或者运行具有更高的优先级的中断。被忽略的中断请求会继续设置请求，直到 CPU 处理。 为了处理中断，中断控制器在地址线上放置一个数字表明哪个设备需要关注，并置起一个中断 CPU 的信号。中断信号导致 CPU 停止当前工作并开始新的工作。地址线上的数字被用作指向 中断向量 (interrupt vector)，以便读取新的程序计数器，新的 counter 指向相应服务过程的开始。interrupt vector 的位置可以硬布线到机器中，或在内存中的任何地方通过 OS 装载到 CPU 寄存器。 中断服务开始运行后，它立刻通过一个确定的值写入中断控制器的某个 IO port 来对中断做出应答，这一应答告诉中断控制器可以自由地发出另一中断。通过让 CPU 延迟应答直到它准备好处理下一中断，就可以避免与多个几乎同时发生的中断相牵扯的竞争状态。 在开始运行中断服务前，需要硬件保存一定信息，这样可以保证被中断的程序在中断结束后可以继续运行。而中断中需要保存哪些内容，各个 CPU 之间差别巨大，其中最少必须保存程序计数器，此外还可能保存寄存器的状态。将数据保存到内部寄存器中，在需要时由 OS 读出，但这样会导致中断控制器被延迟，直到所有相关数据被 OS 读出内部寄存器，防止中断重写内部寄存器。 大多数 CPU 在 堆栈 中保存信息，可以使用内核集中管理或保存在用户空间中。保存在用户空间时，可能造成堆栈指针不合法，也可能在指向末端的页面写入数据后造成越界。使用内核空间保存堆栈没有以上的缺点，但是会造成 MMU 切换上下文，导致 cache 与 TLB 失效，从而浪费 CPU 时间。 现代计算机使用 流水线 与 超标量 技术，可能导致在一条指令没有执行完毕时，发生未处理的中断，而之前的假设都是在指令完成时处理中断。而超标量技术带来了乱序执行，导致可能之前的指令还没有执行但最近的指令已经快要完成了。 将机器留在一个明确状态的中断称为 精确中断 (precise interrupr)，其具有以下特征： PC 保存在一个已知的地方 PC 所指向的指令之前的所有指令已经完成执行 PC 所指向的指令之后的所有指令都没有执行 PC 所指向的指令的执行状态是已知的 PC 所指向的指令之后的指令，并不会被禁止执行，而是要求在中断发生之前必须撤销它们对寄存器或内存做出的任何修改。PC 所指向的指令有可能已经执行了，也可能还没执行，必须清楚适用的是哪种情况。 不满足要求的中断被称为 不精确中断 (imprecise interrupt)，这种机器通常将大量的内部状态吐出到堆栈中，从而使 OS 可以判断出正在发出什么事情。重新启动机器所必需的代码通常极其复杂，在每次中断发生时将大量的信息保存在内存中使得中断响应十分缓慢，而恢复则更加糟糕。因此这样的系统，缓慢的中断使得非常快速的超标量 CPU 有时并不适合实时工作。 有些计算机设计成某些种类的中断和陷阱是 precise (如 IO 中断)，而其他的不是 (如除零中断)。计算机有一个位可以设置精确中断，它强迫所有中断都是精确的，这将强迫 CPU 仔细地将正在做的一切事情记入日志并维护寄存器的影子副本，但这样对开销都对性能具有较大的影响。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:2:5","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#中断"},{"categories":["OperatingSystem"],"content":" IO 软件原理IO 软件设计的关键概念即 设备独立性 (Device Independence)，编写的程序无需事先指定设备且可以访问任意 IO 设备。与 Device Independence 息息相关的是 统一命名 (uniform naming)，即文件或设备的名称应该是一个简单的字符串或整数，不应依赖于设备。例如 UNIX 下 mount 一个 USB 设备到一个 path，那么访问并修改这个 path 就可以将数据写入 USB 设备中，这样文件和设备都采用统一的 路径名寻址。 错误处理 (Error Handling) 是 IO 软件的重要组成，Error 应该尽可能地接近硬件层面处理，只有在底层软件处理不了的情况再交由高层处理。 同步 (Synchronous) 与 异步 (Asynchronous) 传输是 IO 软件需要关心的，大多数物理 IO 是 Asynchronous，CPU 请求 IO 之后就去完成其他任务，直到 IO 中断的发生。如果是 Synchronous IO 那么用户程序将变得容易编写，只需要调用 read 后程序就会挂起，直到缓冲区准备完毕。但是相应的，一些高性能的程序需要自己控制 IO 细节，所以需要 Asynchronous IO。 缓冲 (buffering) 是 IO 软件的一个重要问题，数据离开设备后通常并不能直接存放到最终目的地。但是缓冲区需要大量的复制，经常对 IO 性能有着重大影响。 IO 可以由三种完全不同的方式实现 程序控制 IO (programmed IO) OS 将数据复制到内核，然后进入密闭的循环，每次输出一个字符，在每次输出之后 CPU 会查询设备状态以了解其是否准备就绪接受下一个字符。这一行为被成为 轮询 (polling) 或 忙等待 (busy waiting)。programmed IO 十分简单，但直到 IO 完成之前都需要占用 CPU 的全部时间。 中断驱动 IO polling 会浪费 CPU 的时间，因此在每次提交 IO 请求后，CPU 将调用其他进程并阻塞当前进程，当设备完成 IO 后将为 OS 发送中断信号，OS 得以处理接下来的情况 — 继续打印还是完成 IO 就绪刚刚进行 IO 请求的进程。 DMA IO 中断需要浪费时间，因此可以使用 DMA 替代 CPU 处理中断，CPU 向 DMA Controller 请求一次 IO，DMA 将完成这次 IO 并在完成之前不会打扰 CPU，从而减少调度进程所消耗的 CPU 资源。但是当 DMA 无法全速调度设备或 CPU 需要等待 DMA 中断，则可能使用 programmed IO 或中断驱动 IO 会更好一些。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:3:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#io-软件原理"},{"categories":["OperatingSystem"],"content":" IO 软件层次","date":"07-25","objectID":"/2021/operatingsystem_004/:4:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#io-软件层次"},{"categories":["OperatingSystem"],"content":" 中断处理程序中断是令人不愉快且无法避免的，应当隐藏于 OS 内部，以便系统其他部分尽量不与其发生联系。隐藏的最好办法就是将启动一个 IO 操作的驱动程序阻塞，直到 IO 操作完成并产生中断信号。当中断发生时，中断处理程序将做它必须要做的全部工作以便对中断进行处理。然后它可以将启动中断的驱动程序解除阻塞。 真正的实现并不简单，对 OS 而言还涉及更多的工作。以下列出了通用的中断步骤，这是在硬件中断完成后必须在软件上执行的，由于中断处理依赖于不同平台，因此有些机器上并不需要某些步骤，或者顺序不同 保存没有被中断硬件保存的所有寄存器 (包含 PSW) 为中断服务过程设置上下文，可能包含 TLB、MMU 和页表 为中断服务过程设置堆栈 应答中断控制器，若不存在集中的中断控制器则再次开放中断 将寄存器从它们被保存的地方 (某个堆栈) 复制到进程表中 运行中断服务过程，从发出中断的设备控制器的寄存器中提取信息 选择下次运行的进程，若中断导致某个被阻塞的高优先级进程变为就绪，则可能选择这个进程运行 为下次运行的进程设置 MMU 上下文，可能需要设置 TLB 装入新进程的寄存器 (包含 PSW) 开始运行新进程 ","date":"07-25","objectID":"/2021/operatingsystem_004/:4:1","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#中断处理程序"},{"categories":["OperatingSystem"],"content":" 设备驱动程序某些设备控制器会有寄存器用来向设备发送命令，或读出设备状态的寄存器，亦或两者都有。这些寄存器的数量、命令的性质在不同设备之间差距极大。因此连接计算机的 IO 设备需要某些设备的特定代码对其控制，这种代码被称为 设备驱动 (device driver)，一般由设备制造商编写并交付。为了访问设备硬件 (即访问设备控制器的寄存器)，设备驱动程序通常必须是操作系统内核的一部分 (即 macro kernel)。不过也有可能处在用户空间并使用 syscall 读写设备寄存器 (即 micro kernel)，这样做可以隔离 OS/driver 以及 driver/driver，消除有问题的 driver 干扰 OS。 往往不同设备基于不同的底层技术，但 USB 设备是典型的不同 device 基于相同的底层技术的代表。底层硬件中，USB 链路层处理硬件事物，如发送信号以及将信号译码为 USB 包；较高层次中，处理数据包以及 USB 的通用功能；顶层 API 则是交由不同 driver 进行处理。这与 TCP/IP 协议栈类似，底层协议栈实现数据的传输，而在上层实现不同的协议以针对不同的场景。因此即使是 USB 设备，最上层的设备驱动程序也是分开实现的。 OS 一般定义一个明确的模型，以供 driver 被安装到 OS 中。通常 device 被分为 block 与 character 两大类 (当然还有其他类型设备)，OS 会针对这些类别的设备分别制订必须支持的标准接口，以便 kernel 调用它们让驱动程序工作。当今的 OS 往往支持动态地装载 driver，以便增添设备时添加新的 driver。 许多 driver 具有若干功能，最主要的即接收上方设备无关的软件发出的请求并执行，还会在必要的时候对设备进行初始化，或对电源需求与日志事件进行管理等等。driver 可能要检查当前是否使用，如果使用那么新的请求排入队列以备稍后处理，空闲则检查硬件状态以了解请求是否能够处理。传输开始之前可能需要接通设备或启动马达，当就绪后实际的控制就可以开始。控制设备意味着向设备发送一系列命令，根据控制设备必须要做的工作，由 driver 确定命令序列。driver 在获知哪些命令将要发出后，就开始将它们写入控制器的设备寄存器，并检测控制器是否已接收命令且准备好接收下一命令，直到所有命令被发送。某些设备控制器可以为其提供在内存中的命令链表，由控制器从命令链表中读取命令，而不再需要 OS 干涉。 大多数情况下，命令发出后 driver 都会阻塞自身直到中断到来，然而某些情况操作可以几乎无延迟地完成 (比如没有机械部件的 character device)，前者 driver 会被中断唤醒，而后者则不会阻塞。无论哪种情况操作完成后 driver 必须检查错误，如果一切正常那么 driver 将把数据传送给请求方，并为其返回用于错误报告的状态码。在一切完成后， driver 会检查队列中是否有待处理的请求，如果有则继续处理请求，反之则被阻塞等待新任务唤醒。 但是实际上，driver 正在处理数据时，另一个中断可能会中断其执行，也可能中断通知该 driver 新的数据到来，因此 driver 必须是 可重入的 (reentrant)，这意味着 driver 必须预料到在第一次调用完成之前第二次被调用。而热插拔设备在删除时，当前 IO 传送必须中止且不能破坏任何核心数据结构，并且相关没有处理的 IO 请求都应正确地从系统中删除，同时为这些请求向调用者返回错误信息。而添加新设备则需要 kernel 重新配置资源，从 driver 中删除旧资源，并在适当位置填入新资源。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:4:2","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#设备驱动程序"},{"categories":["OperatingSystem"],"content":" 设备无关的 IO 软件driver 与设备无关软件之间的确切界限依赖于具体的系统，可能由于性能等因素，本应设备无关的实现方式交由 driver 实现。但是我们依然可以确定设备无关的软件的基本功能，即执行对所有设备公共的 IO 功能，并向用户层软件提供一个统一的接口。 driver 的统一接口 如何使 IO 设备与驱动程序看起来或多或少相同，是 OS 中主要的设计问题，因为不能在新设备出现时都为其修改操作系统。 因此对于每种设备类型，OS 定义一组驱动程序必须支持的函数，如 read、write、init、 poweroff、poweron 等等。驱动程序通常包含这张表格，其中具有针对这些函数指向驱动程序自身的指针，当 OS 挂载 driver 时将记录下这个表格的地址，从而在统一接口被调用时查询实际需要调用的函数。 其次 OS 还会将符号化的设备名映射到合适的的 driver 上。以 UNIX 为例，设备名一般被映射到特殊文件的 index 上，其中包含了用于定位驱动程序的 主设备号 (major device number) 与作为参数传递给 driver 的 次设备号 (minor device number)，并且所有 device 都是通过 major 来选择 driver 而得到访问的。其次设备都是以特殊文件映射到系统的，因此文件系统上对文件的保护规则可以应用于 IO 设备。 缓冲 无论是 character 还是 block 设备都有可能需要 缓冲 (buffering)。每次都由小数据频繁引起中断，将导致进程运行效率的低下。 加入 buffer 是麻烦的，如果 buffer 位于用户空间，当数据到来时 buffer 页面可能会被调出 RAM；当 buffer 位于内核空间时，buffer 被填满并向用户空间 buffer 复制时，到来新的数据将无处容纳。因此常用的方法为 双缓冲 (double buffering)，在 kernel 中使用两个缓冲区，一个 buffer 被填满时改用另一个 buffer 接收数据。另外一种方案则是使用 环缓冲区 (circular buffer)，即一个内存区域与双指针的实现，一个指针指向该区域的数据的第一个字节，另一个指针指向最后一个数据的尾后字节。 但是需要注意的是，数据被缓冲的次数过多时，由于大量的复制操作，且赋值操作必须有序地发生，因此系统性能会有所降低。 错误报告 错误在 IO 上下文中比其他上下文中要常见的多，错误发生时，OS 必须尽最大努力处理。许多错误是设备特定的且必须由适当的驱动程序来处理，但错误处理的框架是设备无关的。因此错误可以被分为不同的大类： 编程错误 发生在一个进程请求某些不可能的事情时所发生的，面对这些错误可以直接返回一个错误码给调用者 读一个输入设备 (键鼠、扫描仪等) 或写一个输出设备 (打印机等) 指定了一个无效设备 提供了无效的缓冲区地址或参数 面对 实际 IO 错误 时应该由 driver 决定干什么，如果 driver 不知道该做什么，则应将问题向上传递，返回给与设备无关的软件 软件要做的事取决于环境与错误本身的本质，如果简单的读错误并存在一个交互式的用户可利用，那么可以显示交由用户选择需要做什么，没有交互用户时则以一个错误代码让系统调用中止。当然在重要的数据结构上，错误不能这样处理，比如根目录或空闲块列表被破坏，系统应该显示错误消息并终止进程。 分配与释放专有设备 某些设备 (如打印机) 只能在任意时刻由一个进程使用，这就要求 OS 对设备的请求进行检查，并根据被请求的设备是否可用来接受或拒绝请求。这里说明两种方式以解决专有设备的独占问题： 要求进程在代表设备的特殊文件上直接执行 open 操作，如果 open 失败则说明设备不可用，当使用结束则 close 将其释放 对于请求与释放专有设备实现特殊机制，试图得到不可用的设备可以将调用者阻塞而非失败，这一进程被放入一个队列，当设备可用时从该队列中取出进程并使其继续运行 提供与设备无关的块大小 由于不同设备可能有不同的交付数据的单位，但设备无关的软件应该隐藏这一事实，向高层提供一个统一的块大小。高层软件只需要处理抽象的设备，这些设备无论是 character 还是 block 全部使用相同的逻辑块大小 ","date":"07-25","objectID":"/2021/operatingsystem_004/:4:3","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#设备无关的-io-软件"},{"categories":["OperatingSystem"],"content":" 用户空间的 IO 软件大部分 syscall (包含 IO 系统调用) 由库过程实现，这些库过程的集合则是 IO 系统的组成部分。虽然这些过程是将参数放在合适的位置供系统调用使用，但的确有其他 IO 过程实际实现真正的操作，比如 C 语言中的标准 IO 库 stdio.h，它们作为用户程序的一部分运行。 并非所有的用户层 IO 软件都是由库过程组成的，一个重要的类型就是 假脱机 (spooling) 系统，这是多道程序设计系统中处理独占 IO 设备的一种方法。打印机就是典型的 spooling device，尽管技术上可以让任何用户进程打开表示打印机的字符特殊文件，但假如一个进程打开它并长时间不使用，那么其他进程也将无法打印。 关于 spooling 的解决方法很简单，创建一个特殊进程 守护进程 (daemon)，以及一个特殊目录 假脱机目录 (spooling directory)。当一个进程要打印时，首先生成要打印的整个文件，并将其放入 spooling directory，由 daemon 打印该目录下的文件，该进程是允许使用打印机特殊文件的唯一进程。通过保护特殊文件来防止用户直接使用，可以解决某些进程不必要地长时间空占 spooling device。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:4:4","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#用户空间的-io-软件"},{"categories":["OperatingSystem"],"content":" IO 系统的总结 用户进程 产生 IO 请求 对 IO 进行格式化 假脱机 与设备无关的软件 命名、保护 统一逻辑块大小 缓冲 分配与释放专有设备 设备驱动程序 设置设备寄存器 检查设备状态 中断处理程序 当 IO 完成时唤醒驱动程序 硬件 执行 IO 操作 ","date":"07-25","objectID":"/2021/operatingsystem_004/:4:5","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#io-系统的总结"},{"categories":["OperatingSystem"],"content":" 盘盘是概念简单且重要的 IO 设备，具有多种类型，最为常见的是磁盘，读写速度较快、容量大、断电数据不丢失，适合作为可靠的辅助存储器。对于程序、电影的发行光盘 (DVD 与 Blu-ray) 也是十分重要的存储介质。如今移动硬盘越来越成为主流，其中仅包含半导体元件，速度也十分快速。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:5:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#盘"},{"categories":["OperatingSystem"],"content":" 磁盘磁盘的物理结构一般由 磁头 (heads) 与 盘片 (platters)、电动机、主控芯片与排线等部件组成。磁盘被划分为以下部分： 磁道 (track)：当磁盘旋转时，head 若保持在一个位置上，则每个 head 都会在磁盘表面划出一个圆形的轨迹。 柱面 (cylinder)：在多个 platter 构成的盘组中，不同 platter 的面，但处于同一半径的多个磁道组成的圆柱面。cylinder 的数量与 head 的数量相同，一般为 platter 的二倍 (一般一个 platter 有正反两个盘面)。 扇区 (sector)：或称 磁道扇区 (track sector)，磁盘上的每个磁道被等分为若干个弧段中的一段。磁盘的第一个扇区为引导扇区。 几何扇区 (geometrical sector)：或称 扇面，是同一 platter 上，由拥有同心角的不同 sector 组成的扇面。 簇 (cluster)：或称 分配单元，是 OS 中文件系统存储管理的单位，即文件系统中的块大小。一个 cluster 可以由一个或多个 sector 组成，cluster size 一般在格式化文件系统时选定。这是一个 OS 逻辑概念，而非磁盘的物理特性。 集成驱动电子设备 (Integrated Drive Electronics, IDE) 是将控制器与盘体结合在一起的一种技术，这种技术使得数据传输的可靠性增加，磁盘制造变得简单，厂商无需担心自己产品与其他厂商的控制器之间的兼容性问题，也使得安装更为方便。高技术附件 (Advanced Technology Attachment, ATA) 是一种并行的、用于连接存储设备的标准接口，这是一种控制器技术，也被称为 并行高技术附件 (Parallel ATA)。由于并联易被干扰，速度缓慢，被之后出现的 串行高技术附件 (Serial ATA) 所替代。不过无论是 PATA 或 STAT，都是使用了 IDE 技术的磁盘，其本身包含一个微控制器，向实际的控制器发出一组高级指令，而控制器经常做磁道的高速缓存、坏块映射以及更多工作。 磁盘驱动程序有一个重要的设备特性：控制器是否可以同时控制两个或多个驱动器进行寻道，即 重叠寻道 (overlapped seek)。当控制器和软件等待一个驱动器完成寻道时，控制器同时可以启动另一个驱动器进行寻道；许多控制器可以在一个驱动器上进行读写，与此同时再对另一个或多个其他驱动器进行寻道，但是软盘控制器不能在两个驱动器上同时进行读写操作。overlapped seek 极大程度地降低了平均存取时间。 在实现上，磁盘物理几何规格和驱动程序软件的几何规格几乎总是不同，现代 HDD 外层的磁道的扇区数量比内层磁道的扇区数量更多，为了隐藏这些磁盘细节，大多数现代磁盘都有一个虚拟几何规格呈现给 OS。软件使用 cylinder、head 和 sector 定位数据，并由控制器将 Triple (C, H, S) 映射为真实的位置。Triple 的最大值为 (65535, 16, 63)，这与最初的 IBM PC 相兼容。在 IBM PC 上使用分别使用 (16 bit, 4 bit, 6 bit) 来蛇者这些参数，其中 cylinder 与 sector 的编号从 1 开始，head 的编号从 0 开始。为了突破 Triple 最大值的限制，现代 HDD 基本都支持名为 逻辑块寻址 (Logical Block Addressing, LBA) 的系统，sector 从 0 开始连续编号，而不管磁盘的几何规格。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:5:1","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#磁盘"},{"categories":["OperatingSystem"],"content":" RAID随着时间的推移，CPU 的速度越来越快，但是磁盘的寻道时间却依然难以以数量级的方式下降，CPU 与磁盘之间的速度差距越来越大。因此为提高性能，有人开始考虑并行 IO， Patterson 等人在 1988 年发表的 论文 中提出，使用六种特殊的磁盘组织可能会改进性能、可靠性或两者同时改进。这一思想在工业界很快采纳，并导致成为 RAID 这一新型 IO 设备诞生。RAID 技术被称为 Redundant Arrays of Inexpensive Disks (廉价磁盘冗余阵列)，工作界将其称之为 Redundant Arrays of Independent Disks (独立磁盘冗余阵列)，并且有其对应的「反派角色」 Signle Large Expensive Disks (SLED，单独大容量昂贵磁盘)。顺带一提，如今 RISC 一词最早出现于 Patterson 在 1980 年 UC 主持 Berkeley RISC 计划时期发表的 另一篇论文，尽管更早的由 Cocke 主持的 IBM 801 计划已经开始使用 RISC。 RAID 是将一个装满磁盘的盒子安装到计算机上，用 RAID 控制器替换磁盘控制器卡，讲数据复制到整个 RAID 上，然后继续进行常规操作。以用户的角度来说 RAID 就是一个 SLED，但具有更好的性能与可靠性。RAID 就是将数据分布在全部的驱动器上，因此为不同的阵列方案，Pattersion 等人定义了不同的等级，如今是 0 到 6 级这七个等级。需要注意的是 RAID 并不是层级结构，7 个等级只是 7 种不同的组织阵列的方式。 RAID0 它将 RAID 模拟的虚拟单个磁盘分成条带，每个条带具有 k 个扇区，其中 \\(0 ~ k - 1\\) 为条带 0，\\(k ~ 2k - 1\\) 为条带 1，以此类推。RAID0 将连续的条带以轮转的方式写入全部驱动器上。 RAID0 的性能很好，尤其在数据量大时。RAID 驱动器会自动拆分命令，让控制器并行的在驱动器上查找数据。但是对于每次读取一个条带的请求，RAID0 并没有增强其性能，因为其中不存在并行。 RAID1 将所有的数据都会完全写入到备份数据盘中，实现数据的冗余，保证数据的完整性，而其他数据盘则会像 RAID0 一样轮询写入数据。因此 RAID1 可用存储容量将会是真实容量的一半，但数据可靠性大大增强，如果驱动器崩溃需要恢复，只要完整拷贝备份数据即可。 RAID2 RAID2 不再以条带为单位，而是使用汉明码对数据进行编码分割为独立的 bit 然后再写入磁盘。 RAID3 这是 RAID2 的简化版本，采用奇偶校验技术，将数据分为位存储于各个磁盘之中，并将同比特单独存在一个硬盘当中。 RAID2 与 RAID3 都能提供非常高的数据率，但要求磁盘必须全部工作，但性能并不一定有 SLED 好。 RAID4 RAID4 重新采用条带为单位，使用块交织技术，将条带对条带的奇偶校验写入额外的磁盘上。如果一个驱动器崩溃了，则可以从奇偶校验驱动器中恢复。 RAID5 RAID5 在 RAID4 的基础上，轮询驱动器作为奇偶校验驱动器，将校验压力分摊到阵列的所有驱动器上。 RAID6 RAID6 更进一步使用了额外的奇偶校验块，带来了两块冗余空间。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:5:2","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#raid"},{"categories":["OperatingSystem"],"content":" 磁盘格式化磁盘是由一叠铝的、合金的或玻璃的盘片组成，典型的直径为 3.5 inch，在每个 platter 上沉积着薄薄的可磁化的金属氧化物，在制造出来后磁盘上不存在任何信息。 每个 sector 之间存在短的间隙，且具有一定格式，如下图。其中前导码以一定位模式开始，位模式使得硬件得以识别扇区的开始，前导码还包含 cylinder 与 sector number 以及其他信息。数据的大小是由低级格式化程序决定的。ECC 域包含冗余信息，可以用来恢复读错误。ECC Field 的大小和内容随生产商的不同而不同，它取决于设计者为了更高的可靠性原因放弃多少磁盘空间，以及控制器能够处理的 ECC 编码有多复杂。所有磁盘都分配有某些数目的备用 sector 来取代制造有瑕疵的 sector。 在磁盘能够使用之前，需要软件完成对每个 platter 的 低级格式化 (low-level format)。在格式化时每个磁道上 0th sector 的位置与前一个磁道存在偏移，这被称为 柱面斜进 (cylinder skew)，这样做主要是为了改进性能。比如读取最内层 track 后， head 需要寻道并移动到第二个 track，假设没有 skew，由于寻道时间的存在，head 错过了 0th sector 而不得不等待 platter 转到 0th sector；在 skew 的磁盘上，当 head 在前一个 track 的 0th sector 向下一个 track 寻道完成后，正好可以读取 0th sector，无需等待磁盘空转。因此 low-level format 与磁盘的物理格式相关，一般需要斜进的扇区数量为 \\[\\frac{T_{s} * Number_{seek} * RPM}{60000}.\\] 我们假设一个 7200 RPM 的磁盘驱动器，track 到 track 的寻道时间为 \\(750\\mu s\\)，每个 track 包含 sector 300 个，则需要斜进 sector 27 个。而 low-level format 的结果是磁盘容量减少，减少的量取决于前导码、扇区间间隙、ECC Field 大小以及保留的备用扇区数目。通常格式化比未格式化的容量低 \\(20\\%\\) 。 当然我们需要思考当 head 找到对应扇区后，读取数据并做 ECC 校验，之后讲数据送往内存，然后读取相邻的 sector。但是在读取之前相邻的 sector 从 head 划走，此时完成传输的 head 不得不再等磁盘空转直至第二个 sector 归来。面对这一问题，我们在 low-level format 时往往还会采取交错的方式编号 sector，以防止这种情况的发生。 low-level format 完成后，我们即将对磁盘进行分区操作，每个 partition 从逻辑上来讲就像是一个独立的磁盘。0 扇区一般为 MBR，它包含某些引导代码以及处于扇区末尾的分区表。最后一步我们将对 partition 进行 高级格式化 (high-level format)，这一操作用于设置引导块、空闲存储管理器、根目录和空文件系统，并且将一个代码设置在分区表项中以表明分区使用的文件系统。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:5:3","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#磁盘格式化"},{"categories":["OperatingSystem"],"content":" 磁盘臂调度算法在读写一个磁盘块时需要多长时间？一般由三个因素决定： 寻道时间 (将磁盘臂移动到适当的柱面上所需的时间) 旋转延迟 (等待适当扇区旋转到 head 下所需的时间) 实际数据传输时间 对于大多数磁盘而言，前两项是占主导地位的，所以减少平均寻道时间可以充分改善系统性能。 如果现在有一系列以三元组 (C,H,S) 进行磁盘 IO 请求，如何调度磁盘臂？最简单的方法就是队列的思想，即 先来先服务 (First-Come First-Served, FCFS)，但是如此难以优化寻道时间。那加入一点点 greedy 的思想，每次服务的请求是与磁盘臂当前 cylinder 最接近的请求，这是否可行呢？当然是没问题的，尽量让下一次的寻道时间最小化，这种算法被称为 最短寻道优先 (Shortest Seek Frist, SSF)。不得不说这是很优秀的算法，思考这是不是与之前进程调度中的 SJF (Shortest Job First) 算法类似，这类算法往往有着最短的平均寻道时间。但是回想一下 SJF 的缺点，距离当前 cylinder 较远的请求往往需要较长时间的等待。 那么针对每个请求与 head 寻道的动作，是否与电梯的运行很类似。它们都是在请求到来时，在一个方向上运行，直到那个方向上没有请求为止，然后改变方向。因此这个方法在磁盘世界也被称为 电梯算法 (elevator algorithm)。在该算法中需要一个标志位来指示磁盘臂当前是向上还是向下移动，当该方向上没有请求时则掉转磁盘臂可是寻道，如果没有请求那么磁盘臂可以停止寻道并等待请求。elevator 往往不如 SSF，但其对任意一组请求，磁盘臂的移动总次数具有一个固定上界：\\(2 \\times Number_{cylinder}\\)。当然电梯算法还可以总是按相同的方向进行扫描，即处理完最高 cylinder 的请求后，将 head 移动到请求的最低 cylinder，然后继续沿相同方向移动 head。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:5:4","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#磁盘臂调度算法"},{"categories":["OperatingSystem"],"content":" 错误处理磁盘制造商通过不断加大线性密度而持续推进技术的极限。一块 5.25 inch 的磁盘处于中间位置的磁道大约是 300 mm 周长，假设磁道存放 300 个 512 Byte 的 sector，考虑前导码、ECC 与扇区间隙损失部分空间的情况，线性记录密度大约是 \\(5000 \\texttt{bit}/\\texttt{mm}\\)，而这需要及其均匀的基片与非常精细的氧化物涂层。但是按照规范制作的磁盘也不可能没有瑕疵，即使如此密度能够做到没有瑕疵，但是在向更高密度制作时也可能引入瑕疵。 磁盘制造时的瑕疵会引入坏扇区，即 sector 不能正确地读回刚刚写入其上的值。如果只有几 bit 有瑕疵那么 ECC 即可校正错误，但瑕疵较大时便无法恢复。对于坏块一般有两种处理方法，但是控制器都必须知道每个扇区的编号，所以一般采用内部表跟踪扇区信息 (每个 track 一张表)，或为了更好的性能重写前导码来重新映射扇区。 在控制器或操作系统中对他们进行处理，磁盘在出场时进行测试并将坏扇区列表写入磁盘，每个坏扇区用一个备用扇区替换 将所有扇区向上移动以避开坏扇区 除了瑕疵的扇区外，驱动器在正常运行时也可能出错。当遇到 ECC 不可处理的错误时，可以尝试重读数据，因为某些错误是瞬时的，比如 head 下正好有一粒灰尘。当 Controller 遇到重复性错误时，可以在该 sector 完全坏掉之前切换到备用扇区，这样将不会丢失数据，用户也不会注意到此问题。但是回到之前的问题，如果硬件不具备内部表将扇区透明映射的能力，这将由操作系统来完成。OS 必须先获取到坏扇区列表，通过磁盘读出该表或自己测试整个磁盘。然后进行扇区的透明映射，OS 必须保证坏扇区不被任何文件包含，也不存在于空闲块列表中。 虽然一直在讨论坏扇区问题，但是这并不是唯一的错误来源，也可能是磁盘臂的机械故障引发的寻道错误。大多数 Controller 可以自动修复该错误，如果交由 driver 修复，driver 会发出一个 recalibrate (重新校准) 命令，让磁盘臂尽可能地想最外移动，并将 Controller 内部的 current cylinder 重置为 0。一般如此可以解决问题，否则你的磁盘需要修理一下了。 Recalibrate 会使磁盘发出古怪的噪音，但这不是大问题，最大的问题是实时约束系统需要磁盘的 bit stream 以均匀的速率到达 (比如播放视频或烧录 Blu-ray)，recalibrate 将会在均匀的 bit stream 中插入不可接受的间隙。因此一种称为 AV盘 (Audio Visual Disk) 的驱动器永远不会执行 recalibrate 操作。 还有一点必须说明，荷兰黑客 Jeroen Domburg 破解了一个现代控制器并使其运行定制代码，不得不说这个控制器是一个强大的 ARM 处理器，且有足够的资源运行 Linux。如果有人以这样的方式破坏你的计算机系统，并看到所有传入与传出磁盘的数据，这个磁盘被永久植入了后门。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:5:5","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#错误处理"},{"categories":["OperatingSystem"],"content":" 稳定存储器当我们不惜一切代价的保证磁盘的一致性时，这个磁盘系统应该在一个写命令时，要么正确的写入数据要么什么也不做，让现有数据完整无缺地留下，这种系统被称为 稳定存储器 (stable storage)。 我们首先需要确认一点，如果使用 16 Byte 的 ECC Field 校验 512 Byte 的 sector，在该 sector 出现错误但 ECC 没有出错的情况下，该错误是不会被检测出来的，即使这种错误的概率大约是 \\(2^{-144}\\)。假设一点，被正确写入的 sector 会自然地变为不可读状态，但是在一个合理的间隔内 (如 1 天) 让相同的扇区在独立的驱动器上变坏的概率可以小到忽略不计。最后假设 CPU 故障，这种情况下只能停机，因此进行中的写操作也会停止，这会使写入的数据检测到不正确 ECC。 现在所有假设成立的情况下，stable storage 使用完全相同的一对磁盘，对应的块一同工作以形成无差错的块。当不存在错误时，任意读取一块磁盘的数据即可，因为它们是完全相同的。为达到这一目的，定义以下三种操作： 稳定写 (stable write) 首先将块写入第一个驱动器，然后将其都会校验是否正确，如果不正确则会重新进行读写操作，直到 n 次正常为止。经过 n 次连续的失败后将会将块应设备备用块上，并再次重复连续的重复读写。在驱动器 1 的数据写入成功后，再对驱动器 2 进行写入。在不存在 CPU 崩溃的情况下，stable write 完成后两个驱动器上的数据将正确的被写入。 稳定读 (stable read) 从第一个驱动器读取数据，若 ECC 校验失败则重新尝试读取数据，直到 n 次后成功为止。若连续 n 次失败则使用另一个驱动器进行 stable read。 崩溃恢复 (crash recovery) 崩溃之后恢复程序扫描两个磁盘，对比对应的块，一对块如果都是好的且相同那就什么都不做；如果其中一个具有 ECC 错误，那么坏块就用对应的好块来覆盖；如果两个块都是好的，但不相同，那么将第一个驱动器的块写到第二个驱动器。 在某些计算机中拥有少量的 非易失性 RAM (nonvolatile RAM)，它是特殊的 CMOS 存储器，由锂电池供电，且可以维持很多年，计算机的每天的时间都会存储在此。如果 nonvolatile RAM 有几个字节可供 OS 使用，那么 stable write 前将准备更新的块编号存入，如此即使计算机崩溃也可以快速找到哪些块需要检查一致性。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:5:6","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#稳定存储器"},{"categories":["OperatingSystem"],"content":" 时钟时钟 (clock) 又称为 定时器 (timer)，由于各种各样的原因决定了它对于任何多道程序设计系统的操作都是至关重要的。clock 主要负责维护时间，并防止一个进程垄断 CPU。 clock 采用 driver 的形式存在并驱动，但它并不像 block 或 character device 一样。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:6:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#时钟"},{"categories":["OperatingSystem"],"content":" 时钟硬件可编程时钟由三个部分组成：晶体振荡器、计数器以及存储寄存器。当石英晶体通过适当的切割并安装在一定电压下时，就可以产生非常精确的周期信号，典型的频率范围是几百兆 Hz。使用电子器件可以将这一基础信号乘以一个小的整数来获得高达 1GHz 甚至更高的频率。 clock 通常给计算机的各种电路提供同步信号，该信号背诵道计数器使其递减计数，直到计数器为 0 时产生一个 CPU 中断。 可编程时钟一般具有几种操作模式 一次完成模式 (one-shot mode)：当时钟启动时，将存储寄存器的值复制到计数器中，然后在每个晶体脉冲到来时使计数器减 1。当计数器为 0 时产生一个中断并停止工作，直到下一次启动 方波模式 (square-wave mode)：当计数器变为 0 并产生中断后，存储寄存器的值自动复制到计数器中，并整个过程无限重复。这种周期性的中断被称为 时钟滴答 (clock tick)。 可编程时钟的优点是其中断频率可以由软件控制，例如 100 MHz 的晶体 (\\(10 \\texttt{ns}\\) 一次脉冲信号)，对于 32 bit 无符号寄存器来说，可以编程中断以 10 ns 时间间隔最长时间 42.950 s 发生一次，可编程时钟芯片通常还会包含 2 到 3 个独立的可编程时钟，并且还具有许多其他选项。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:6:1","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#时钟硬件"},{"categories":["OperatingSystem"],"content":" 时钟软件时钟硬件所做的工作即根据已知的时间间隔产生中断，但是设计时间的其他工作必须由软件完成，即时钟驱动器。clock driver 的确切任务因 OS 而有差异，但通常都包含以下大部分： 维护日时间。采用 timestamp (时间戳) 的形式以秒计时或记录 clock tick。 防止进程超时运行。由调度程序以 clock tick 为单位初始化时间片，driver 在每次时钟中断到来时为时间片计数器减一，知道为 0 时进行进程切换。 对 CPU 的使用情况记账。在进程启动时启动一个不同于主系统定时器的辅助定时器，进程终止时独处辅助定时器的值即可得知进程的运行时间。当中断发生时，辅助定时器应该被保存起来，中断结束后再恢复。 处理用户进程提出的 alarm 系统调用。 为系统本身的各个部分提供监视定时器 (watchdog timer)。wtachdog 通常用于对停止运行的系统进行复位，该定时器永远不会过期。watchdog 被用于 kernel 中，与普通的定时器类似，但它不会引发中断或发送信号，而是调用一个调用者提供的过程。由于 watchdog 调用的过程是调用者的一部分，因此必须与调用者处于相同的地址空间时才会起到作用。 完成概要剖析、监视和统计信息收集。 如果有无限多个物理时钟，那么 driver 可以为每个请求设置一个时钟，但是物理时钟是有限的，请求可能会溢出物理时钟的数量上限。面对这种情况，需要 driver 使用物理时钟模拟多个虚拟时钟，当请求到来时将其记录在表中，并在每次 tick 时检查是否有需要发出的信号。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:6:2","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#时钟软件"},{"categories":["OperatingSystem"],"content":" 软定时器一般而言，IO 一般采用中断和轮询的方式管理。中断具有较低的 latency (响应时间)，即中断在事件本身发生后立即发生，具有低 delay (延迟) 或根本没有 delay。但是由于现代 CPU 需要进程切换以及流水线、TLB、Cache 等影响使得中断有相当大的开销。为了避免中断那么应用程序需要自己对其期待的事件进行轮询，虽然避免了中断但是有相当长的等待时间，等待时间平均是轮询间隔的一半。 对于高性能应用来说，中断的开销与轮询的等待时间都是不能接受的，因此它需要一种避免中断的方法。这是一种被称为 软定时器 (soft timer) 的解决方案。无论因什么原因，只要内核在运行时，在它返回用户态之前，都对实时时钟进行检查，用以了解 soft timer 是否到期。如果 soft timer 到期则执行被调度时间，并在时间完成后复位 soft timer。 soft timer 随着其他原因进入内核的频率而发起脉冲，其中包含： 系统调用 TLB 未命中 页面故障 IO 中断 CPU 空闲 对于这些事件发生的频率，Aron 与 Druschel 对于几种 CPU 负载进行了测量，并于 2000 年发表了一篇 论文。他们测试了包含全负载 Web Server、具有计算约束后台作业的 Web Server、在 Internet 播放实时音频以及重编译 UNIX 内核，进入内核的平均进入率在 \\(1 \\sim 2 \\upmu\\texttt{s}\\) ，其中大约有一半是 syscall。当然如果很长一段时间不发生上述事件，可以将辅助定时器设置每隔一定时间中断一次，来强制进入内核态触发 Soft Timer。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:6:3","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#软定时器"},{"categories":["OperatingSystem"],"content":" 用户界面：键盘、鼠标和监视器为了人机交互，一台计算机往往具备键盘和监视器，现代的 GUI 系统还会配有一个鼠标，但是以前大型机上通常用户使用 Terminal (终端) 远程连接。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:7:0","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#用户界面-键盘-鼠标和监视器"},{"categories":["OperatingSystem"],"content":" 输入设备用户输入主要来自键鼠，当然这些年也有越来越多的触摸屏、数位板等，但是 我们还是以键鼠为主 这本书上只介绍了键鼠。 键盘包含一个 embedded microprocessor，该处理器通过一个特殊的串行端口与主板的控制芯片通信，当按下一个键时都会产生一个中断，并且在键被释放时产生第二个中断。每当发生这样的键盘中断时，键盘驱动程序都要从与键盘相关联的 IO port 提取信息。其他一切事情都是在软件中发生的，在相当大的程度上独立于硬件。在讨论输入设备时，请想象往 Terminal 输入命令时的场景。 键盘软件键盘发往 IO port 的数据是键的编号，称为 扫描码 (scan code)，请不要与 ASCII 混淆！键盘所拥有的键不超过 128 个，所以只需要 7 bit 即可编码所有键位，因此最后 1 bit 可以设置为键按下 / 释放。跟踪每个键的状态是 driver 的任务，硬件仅给出中断。 driver 的处理让键盘的输入十分灵活且不依赖于具体硬件。driver 一般向上层原封不动地传递输入的数据，这将输入的精准数据发送给上层应用 (比如文本编辑器和 Terminal，另外说一下书中使用 Emacs 举例好评！)。但是有些程序并不需要如此丰富的信息，它们只需要校正后的输入，即将行内编辑全部处理完成后再发送给上层应用。因此第一种是面向字符的，被称为 原始模式 (raw mode)；而第二种则是面向行的，被称为 加工模式 (cooked mode)。POSIX 这时候出现了，他们带着标准就这样来了！他们将 cooked mode 称为 术语规范模式 (canonical mode)，而 raw mode 等价 非规范模式 (noncanonical mode)，NOTWITHSTANDING 终端行为的许多细节可能被修改了。POSIX 还提供了相关 library。 如果键盘处于 Canonical Mode 则字符必须存储起来直到累积完整的一样，并对这行完成相应的编辑。即使处于 Noncanonical Mode，程序也可能尚未请求输入，所以 character 也可能被缓存起来以便允许用户提前键入。如果你无法理解为什么 Noncanonical Mode 也需要缓存字符，想想 Terminal 中，如果你正在运行一个命令，这时你在键盘上输入了一些字符并摁下了回车会发生什么呢？什么都没有发生！命令还在运行当中！但是但这条命令完成后，Terminal 会直接运行你刚刚键入的字符，因为它们被 buffering 并且你摁下了回车。 另外书中提到了一个很有意思的观点 不允许用户提前键入的系统设计者都应该被 涂柏油、粘羽毛，或者更加严重的惩罚，强迫他们使用自己设计的系统。 这里的涂柏油和粘羽毛是近代欧洲及其殖民地的一种严厉惩罚和公开羞辱对方的行为，意在伸张非官方认可的正义或报复，通常由暴民作为私刑实施。羽毛粘在灼热的尚未凝固的柏油上，难以去除。当代欧洲语言中 tarring and feathering 则是 Metaphor (隐喻) 惩罚或严厉批评。 言归正传，将刚刚键入的字符出现在屏幕上，这个过程被称为 回显 (echoing)。键入与正在写屏幕使 echoing 变得复杂，因为输出不应该覆盖掉输入的 echoing。另一方面限制行长度，而在需要的时候进行断行也是复杂的，为了简单起见某些 driver 将有长度限制的 echoing 直接根据限制截断。 如果你认为这些都还好，另一点麻烦的是空白符，水平制表 \\t 与空格 space 的显示， echoing 需要多少个正确的 space。而换行 LF (Line Feed, newline, end-of-line) \\n 与回车 CR (carriage return) \\r 的也同样是问题，由于行末换行符依赖于 OS，需要等效处理这些情况。 简单的说一下，LF 与 CR 都是有其历史意义的，这两个可以追溯到 typewriter 时期。LF 用于在一行结束时让 typewriter 将纸张向下移动一行，而移动这一行后 typewriter 的 head 位置不变，如果输出可能使前面纸张的空白浪费，为了使 head 回到最前端则需要使用 CR。如果不使用 LF 而直接使用 CR 则会回到当前行的开头部分。进入计算机时代，LF 与 CR 的传统也被保留了下来，BTW 如今主流的 QWERTY 键盘布局也是那个时代的产物。那么依赖于 OS 的换行符是如何选择的呢： LF：UNIX-like CR：Apple II 家族，以及 MacOS (before version X) CRLF: DOS 以及 Windows，还有部分非 UNIX-like Canonical Mode 下许多输入字符都具有特殊含义，以下列出了 POSIX 要求的所有特殊字符，这些字符一般不与程序所使用的文本输入或代码相冲突。并且处理 LF 与 CR，其它字符都可以在程序的控制下修改。PS 以下使用 C 代表 CTRL、S 代表 Shift、M 代表 Meta 或 Alt、s 代表 super。 字符 POSIX 名称 注释 C-H ERASE 退格一个字符 C-U KILL 擦除正在键入的整行 C-V LNEXT 按字面意义解释下一个字符 C-S STOP 停止输出 C-Q START 开始输出 DEL INTR 中断进程 (SIGINT) C-\\ QUIT 强制核心转储 (SIGQUIT) C-D EOF End-of-File C-M CR carriage return C-J NL NewLine (Line Feed) 鼠标软件老式的鼠标采用轨迹球获取移动数据，当鼠标在粗糙表面上移动时这个轨迹球会随着旋转。现代鼠标更多使用光学设备，其底部采用一个或多个发光二级管和光电探测器。早期的光电鼠标需要在特殊的鼠标垫上使用，其上游锯齿状网格，通过计算穿过的线数获得移动轨迹。现代光电鼠标主要通过图形处理芯片获取并处理下方连续的低分辨率图片，寻找图像之间的变化，从而获取移动数据。 无论是向哪个方向移动了一个确定的最小距离，或按钮被按下或释放时，都会有消息发送给计算机，而这个最小距离被称为 鼠标步 (mickey)。发送到计算机的消息一般都包含 \\(\\delta{}x\\) 、\\(\\delta{}y\\) 与 button，消息的格式取决于系统和鼠标所具有的 button 数目，通常占 3 Byte。大多数鼠标返回报告频率最大为 40 Hz，所以鼠标自上次报告之后移动了多个 mickey。有时需要区分双击与单击，这是 driver 的事情，用户可以自行设置足够接近的时间来区分单击与双击，也可以设置 mickey 的大小。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:7:1","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#输入设备"},{"categories":["OperatingSystem"],"content":" 输入设备用户输入主要来自键鼠，当然这些年也有越来越多的触摸屏、数位板等，但是 我们还是以键鼠为主 这本书上只介绍了键鼠。 键盘包含一个 embedded microprocessor，该处理器通过一个特殊的串行端口与主板的控制芯片通信，当按下一个键时都会产生一个中断，并且在键被释放时产生第二个中断。每当发生这样的键盘中断时，键盘驱动程序都要从与键盘相关联的 IO port 提取信息。其他一切事情都是在软件中发生的，在相当大的程度上独立于硬件。在讨论输入设备时，请想象往 Terminal 输入命令时的场景。 键盘软件键盘发往 IO port 的数据是键的编号，称为 扫描码 (scan code)，请不要与 ASCII 混淆！键盘所拥有的键不超过 128 个，所以只需要 7 bit 即可编码所有键位，因此最后 1 bit 可以设置为键按下 / 释放。跟踪每个键的状态是 driver 的任务，硬件仅给出中断。 driver 的处理让键盘的输入十分灵活且不依赖于具体硬件。driver 一般向上层原封不动地传递输入的数据，这将输入的精准数据发送给上层应用 (比如文本编辑器和 Terminal，另外说一下书中使用 Emacs 举例好评！)。但是有些程序并不需要如此丰富的信息，它们只需要校正后的输入，即将行内编辑全部处理完成后再发送给上层应用。因此第一种是面向字符的，被称为 原始模式 (raw mode)；而第二种则是面向行的，被称为 加工模式 (cooked mode)。POSIX 这时候出现了，他们带着标准就这样来了！他们将 cooked mode 称为 术语规范模式 (canonical mode)，而 raw mode 等价 非规范模式 (noncanonical mode)，NOTWITHSTANDING 终端行为的许多细节可能被修改了。POSIX 还提供了相关 library。 如果键盘处于 Canonical Mode 则字符必须存储起来直到累积完整的一样，并对这行完成相应的编辑。即使处于 Noncanonical Mode，程序也可能尚未请求输入，所以 character 也可能被缓存起来以便允许用户提前键入。如果你无法理解为什么 Noncanonical Mode 也需要缓存字符，想想 Terminal 中，如果你正在运行一个命令，这时你在键盘上输入了一些字符并摁下了回车会发生什么呢？什么都没有发生！命令还在运行当中！但是但这条命令完成后，Terminal 会直接运行你刚刚键入的字符，因为它们被 buffering 并且你摁下了回车。 另外书中提到了一个很有意思的观点 不允许用户提前键入的系统设计者都应该被 涂柏油、粘羽毛，或者更加严重的惩罚，强迫他们使用自己设计的系统。 这里的涂柏油和粘羽毛是近代欧洲及其殖民地的一种严厉惩罚和公开羞辱对方的行为，意在伸张非官方认可的正义或报复，通常由暴民作为私刑实施。羽毛粘在灼热的尚未凝固的柏油上，难以去除。当代欧洲语言中 tarring and feathering 则是 Metaphor (隐喻) 惩罚或严厉批评。 言归正传，将刚刚键入的字符出现在屏幕上，这个过程被称为 回显 (echoing)。键入与正在写屏幕使 echoing 变得复杂，因为输出不应该覆盖掉输入的 echoing。另一方面限制行长度，而在需要的时候进行断行也是复杂的，为了简单起见某些 driver 将有长度限制的 echoing 直接根据限制截断。 如果你认为这些都还好，另一点麻烦的是空白符，水平制表 \\t 与空格 space 的显示， echoing 需要多少个正确的 space。而换行 LF (Line Feed, newline, end-of-line) \\n 与回车 CR (carriage return) \\r 的也同样是问题，由于行末换行符依赖于 OS，需要等效处理这些情况。 简单的说一下，LF 与 CR 都是有其历史意义的，这两个可以追溯到 typewriter 时期。LF 用于在一行结束时让 typewriter 将纸张向下移动一行，而移动这一行后 typewriter 的 head 位置不变，如果输出可能使前面纸张的空白浪费，为了使 head 回到最前端则需要使用 CR。如果不使用 LF 而直接使用 CR 则会回到当前行的开头部分。进入计算机时代，LF 与 CR 的传统也被保留了下来，BTW 如今主流的 QWERTY 键盘布局也是那个时代的产物。那么依赖于 OS 的换行符是如何选择的呢： LF：UNIX-like CR：Apple II 家族，以及 MacOS (before version X) CRLF: DOS 以及 Windows，还有部分非 UNIX-like Canonical Mode 下许多输入字符都具有特殊含义，以下列出了 POSIX 要求的所有特殊字符，这些字符一般不与程序所使用的文本输入或代码相冲突。并且处理 LF 与 CR，其它字符都可以在程序的控制下修改。PS 以下使用 C 代表 CTRL、S 代表 Shift、M 代表 Meta 或 Alt、s 代表 super。 字符 POSIX 名称 注释 C-H ERASE 退格一个字符 C-U KILL 擦除正在键入的整行 C-V LNEXT 按字面意义解释下一个字符 C-S STOP 停止输出 C-Q START 开始输出 DEL INTR 中断进程 (SIGINT) C-\\ QUIT 强制核心转储 (SIGQUIT) C-D EOF End-of-File C-M CR carriage return C-J NL NewLine (Line Feed) 鼠标软件老式的鼠标采用轨迹球获取移动数据，当鼠标在粗糙表面上移动时这个轨迹球会随着旋转。现代鼠标更多使用光学设备，其底部采用一个或多个发光二级管和光电探测器。早期的光电鼠标需要在特殊的鼠标垫上使用，其上游锯齿状网格，通过计算穿过的线数获得移动轨迹。现代光电鼠标主要通过图形处理芯片获取并处理下方连续的低分辨率图片，寻找图像之间的变化，从而获取移动数据。 无论是向哪个方向移动了一个确定的最小距离，或按钮被按下或释放时，都会有消息发送给计算机，而这个最小距离被称为 鼠标步 (mickey)。发送到计算机的消息一般都包含 \\(\\delta{}x\\) 、\\(\\delta{}y\\) 与 button，消息的格式取决于系统和鼠标所具有的 button 数目，通常占 3 Byte。大多数鼠标返回报告频率最大为 40 Hz，所以鼠标自上次报告之后移动了多个 mickey。有时需要区分双击与单击，这是 driver 的事情，用户可以自行设置足够接近的时间来区分单击与双击，也可以设置 mickey 的大小。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:7:1","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#键盘软件"},{"categories":["OperatingSystem"],"content":" 输入设备用户输入主要来自键鼠，当然这些年也有越来越多的触摸屏、数位板等，但是 我们还是以键鼠为主 这本书上只介绍了键鼠。 键盘包含一个 embedded microprocessor，该处理器通过一个特殊的串行端口与主板的控制芯片通信，当按下一个键时都会产生一个中断，并且在键被释放时产生第二个中断。每当发生这样的键盘中断时，键盘驱动程序都要从与键盘相关联的 IO port 提取信息。其他一切事情都是在软件中发生的，在相当大的程度上独立于硬件。在讨论输入设备时，请想象往 Terminal 输入命令时的场景。 键盘软件键盘发往 IO port 的数据是键的编号，称为 扫描码 (scan code)，请不要与 ASCII 混淆！键盘所拥有的键不超过 128 个，所以只需要 7 bit 即可编码所有键位，因此最后 1 bit 可以设置为键按下 / 释放。跟踪每个键的状态是 driver 的任务，硬件仅给出中断。 driver 的处理让键盘的输入十分灵活且不依赖于具体硬件。driver 一般向上层原封不动地传递输入的数据，这将输入的精准数据发送给上层应用 (比如文本编辑器和 Terminal，另外说一下书中使用 Emacs 举例好评！)。但是有些程序并不需要如此丰富的信息，它们只需要校正后的输入，即将行内编辑全部处理完成后再发送给上层应用。因此第一种是面向字符的，被称为 原始模式 (raw mode)；而第二种则是面向行的，被称为 加工模式 (cooked mode)。POSIX 这时候出现了，他们带着标准就这样来了！他们将 cooked mode 称为 术语规范模式 (canonical mode)，而 raw mode 等价 非规范模式 (noncanonical mode)，NOTWITHSTANDING 终端行为的许多细节可能被修改了。POSIX 还提供了相关 library。 如果键盘处于 Canonical Mode 则字符必须存储起来直到累积完整的一样，并对这行完成相应的编辑。即使处于 Noncanonical Mode，程序也可能尚未请求输入，所以 character 也可能被缓存起来以便允许用户提前键入。如果你无法理解为什么 Noncanonical Mode 也需要缓存字符，想想 Terminal 中，如果你正在运行一个命令，这时你在键盘上输入了一些字符并摁下了回车会发生什么呢？什么都没有发生！命令还在运行当中！但是但这条命令完成后，Terminal 会直接运行你刚刚键入的字符，因为它们被 buffering 并且你摁下了回车。 另外书中提到了一个很有意思的观点 不允许用户提前键入的系统设计者都应该被 涂柏油、粘羽毛，或者更加严重的惩罚，强迫他们使用自己设计的系统。 这里的涂柏油和粘羽毛是近代欧洲及其殖民地的一种严厉惩罚和公开羞辱对方的行为，意在伸张非官方认可的正义或报复，通常由暴民作为私刑实施。羽毛粘在灼热的尚未凝固的柏油上，难以去除。当代欧洲语言中 tarring and feathering 则是 Metaphor (隐喻) 惩罚或严厉批评。 言归正传，将刚刚键入的字符出现在屏幕上，这个过程被称为 回显 (echoing)。键入与正在写屏幕使 echoing 变得复杂，因为输出不应该覆盖掉输入的 echoing。另一方面限制行长度，而在需要的时候进行断行也是复杂的，为了简单起见某些 driver 将有长度限制的 echoing 直接根据限制截断。 如果你认为这些都还好，另一点麻烦的是空白符，水平制表 \\t 与空格 space 的显示， echoing 需要多少个正确的 space。而换行 LF (Line Feed, newline, end-of-line) \\n 与回车 CR (carriage return) \\r 的也同样是问题，由于行末换行符依赖于 OS，需要等效处理这些情况。 简单的说一下，LF 与 CR 都是有其历史意义的，这两个可以追溯到 typewriter 时期。LF 用于在一行结束时让 typewriter 将纸张向下移动一行，而移动这一行后 typewriter 的 head 位置不变，如果输出可能使前面纸张的空白浪费，为了使 head 回到最前端则需要使用 CR。如果不使用 LF 而直接使用 CR 则会回到当前行的开头部分。进入计算机时代，LF 与 CR 的传统也被保留了下来，BTW 如今主流的 QWERTY 键盘布局也是那个时代的产物。那么依赖于 OS 的换行符是如何选择的呢： LF：UNIX-like CR：Apple II 家族，以及 MacOS (before version X) CRLF: DOS 以及 Windows，还有部分非 UNIX-like Canonical Mode 下许多输入字符都具有特殊含义，以下列出了 POSIX 要求的所有特殊字符，这些字符一般不与程序所使用的文本输入或代码相冲突。并且处理 LF 与 CR，其它字符都可以在程序的控制下修改。PS 以下使用 C 代表 CTRL、S 代表 Shift、M 代表 Meta 或 Alt、s 代表 super。 字符 POSIX 名称 注释 C-H ERASE 退格一个字符 C-U KILL 擦除正在键入的整行 C-V LNEXT 按字面意义解释下一个字符 C-S STOP 停止输出 C-Q START 开始输出 DEL INTR 中断进程 (SIGINT) C-\\ QUIT 强制核心转储 (SIGQUIT) C-D EOF End-of-File C-M CR carriage return C-J NL NewLine (Line Feed) 鼠标软件老式的鼠标采用轨迹球获取移动数据，当鼠标在粗糙表面上移动时这个轨迹球会随着旋转。现代鼠标更多使用光学设备，其底部采用一个或多个发光二级管和光电探测器。早期的光电鼠标需要在特殊的鼠标垫上使用，其上游锯齿状网格，通过计算穿过的线数获得移动轨迹。现代光电鼠标主要通过图形处理芯片获取并处理下方连续的低分辨率图片，寻找图像之间的变化，从而获取移动数据。 无论是向哪个方向移动了一个确定的最小距离，或按钮被按下或释放时，都会有消息发送给计算机，而这个最小距离被称为 鼠标步 (mickey)。发送到计算机的消息一般都包含 \\(\\delta{}x\\) 、\\(\\delta{}y\\) 与 button，消息的格式取决于系统和鼠标所具有的 button 数目，通常占 3 Byte。大多数鼠标返回报告频率最大为 40 Hz，所以鼠标自上次报告之后移动了多个 mickey。有时需要区分双击与单击，这是 driver 的事情，用户可以自行设置足够接近的时间来区分单击与双击，也可以设置 mickey 的大小。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:7:1","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#鼠标软件"},{"categories":["OperatingSystem"],"content":" 输出软件 文本窗口当输出是连续的、单一字体、大小和颜色的形式时，输出比输入更简单。大体上，程序将字符发送到当前窗口，而字符在那里显示出来，通常通过系统调用将字符写入窗口。 屏幕编辑器和许多其他复杂程序需要能够以更加复杂的方式更新屏幕，比如在屏幕的中间替换一行。为满足如此需求，大多数 output driver 支持一系列命令来移动光标，在光标处插入或删除字符或行。这些命令常被称为 转义序列 (escape sequence)。 在 25 * 80 ASCII 哑中断全盛时期，有数百种终端类型，每种都有自己的转义序列，因此编写可以在一种以上的终端类型的程序是十分困难的。此时出现了 termcap 的终端数据库，这是在 Berkeley UNIX 中引入的。该软件包定义了许多基本动作，比如移动光标到行、列或特殊位置，软件使用一般的转义序列，然后通过 termcap 转换成 terminal 实际定义的 escape sequence。之后由于标准化的需要 ANSI 标准出现了！当然下表就是 ANSI escape sequence，但是其中的 space 只是方便展示，实际不被使用。 escape sequence 含义 ESC [nA 向上移动 n 行 ESC [nB 向下移动 n 行 ESC [nC 向右移动 n 个间隔 ESC [nD 向左移动 n 个间隔 ESC [m;nH 将光标移动到 \\((m,n)\\) ESC [sJ 从光标清除屏幕 (0:结尾; 1:开始; 2:两者) ESC [sK 从光标清除行 (0:结尾; 1:开始; 2:两者) ESC [nL 在光标处插入 n 行 ESC [nM 在光标处删除 n 行 ESC [nP 在光标处删除 n 个字符 ESC [n@ 在光标处插入 n 个字符 ESC [nm 允许再现 n (0=常规; 4=粗体; 5=闪烁; 7=反白) ESC M 如果光标在顶行则向后滚动屏幕 X Window SystemX Window Ststem (简称为 X) 几乎是 UNIX-like 世界用户界面的基础，作为 Athena 计划的一部分于 20 世纪 80 年代在 MIT 被开发。X 具有非常好的可移植性，并完全运行在用户空间中。人们最初打算将其用于将大量的远程用户终端与中央计算服务器相连接，所以 X 在逻辑上分为 Client 与 Server 两部分，这样就有可能运行在不同的计算机上。在现代个人计算机上，这两部分可以运行在同一机器上，GNOME 与 KDE 都运行于 X 之上。 X 能为 GUI Environment 提供基本环境：在屏幕上描绘、呈现图像与移动程序窗口，同时也受理、运行以及管理键鼠的交互程序。X 没有管辖用户界面部分，由其他以 X 为基础的扩展来负责。此外由于 X 分为客户端 X Client 与服务端 X Server，Server 与 Client 之间的网络通信是透明的，且能够安全交换数据。 X 只是一个窗口系统，并不是完整的 GUI，为了实现完整的 GUI，则需要在 X 之上运行其他软件层。Xlib 则是一组在其之上的库过程，用于访问 X 的功能，这些功能是 X Window System 的基础。但是它们实在过于底层了，实现起来过于麻烦，以至于大部分程序并不会直接使用 Xlib。为了让 X 编程更加容易，X 提供了一个名为 X Toolkit Intrinsics (本征函数集) 的工具包，intrinsics 用于管理按钮、滚动条以及其他窗口小部件 (widget)。而图形库正是构建于 Intrinsics 之上，用以提供一致的外观、跨平台抽象。而真正的 窗口管理器 (Window Manager, WM)，则是顶层的用户空间软件，其主要控制着屏幕上窗口的创建、删除和移动，为了管理窗口，WM 要发送命令到 X Server 告诉它做什么。 当然 X 也有着一套完整的开发、改进规则 除非已有真正的应用程序，真的需要 X 为其修订、增订等支持，否则不会为 X 增加新功能。 决定「一个系统不是什么」和决定「它是什么」同样重要。与其适应整个世界的需要，宁可使得系统可以扩展，如此才能以持续兼容的方式来满足新增的需求。 只有完全没实例时，才会比只有一个实例来的糟。 如果问题没完全弄懂，最好不要去解决它。 如果可以通过 \\(10\\%\\) 的工作量得到 \\(90\\%\\) 的预期效果，应该用更简单的方法解决。 尽量避免复杂性。 提供机制而不是策略，有关用户界面的开发实现，交给实现应用者自主。 由于这一套完整的规则，X 已经将版本号稳定到了 11，因此也被称为 X11。对于在 X 上运行的不同 Desktop Environment，freedesktop.org 积极与努力地维持各种不同 X Desktop Environment 的兼容性，使相竞态势下仍不失X的兼容本色。对于 X 的性能、稳定性的缺点，有着其他全新的窗口系统来竞争，比如 freedesktop.org 主导的 Wayland。 对了！X 是一个开源软件，最早使用 X LICENSE 发布，如果你不知道这个协议，那你应该听说过 MIT LICENSE，没错它们是一个 LICENSE。Wayland 也使用 MIT 协议发布。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:7:2","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#输出软件"},{"categories":["OperatingSystem"],"content":" 输出软件 文本窗口当输出是连续的、单一字体、大小和颜色的形式时，输出比输入更简单。大体上，程序将字符发送到当前窗口，而字符在那里显示出来，通常通过系统调用将字符写入窗口。 屏幕编辑器和许多其他复杂程序需要能够以更加复杂的方式更新屏幕，比如在屏幕的中间替换一行。为满足如此需求，大多数 output driver 支持一系列命令来移动光标，在光标处插入或删除字符或行。这些命令常被称为 转义序列 (escape sequence)。 在 25 * 80 ASCII 哑中断全盛时期，有数百种终端类型，每种都有自己的转义序列，因此编写可以在一种以上的终端类型的程序是十分困难的。此时出现了 termcap 的终端数据库，这是在 Berkeley UNIX 中引入的。该软件包定义了许多基本动作，比如移动光标到行、列或特殊位置，软件使用一般的转义序列，然后通过 termcap 转换成 terminal 实际定义的 escape sequence。之后由于标准化的需要 ANSI 标准出现了！当然下表就是 ANSI escape sequence，但是其中的 space 只是方便展示，实际不被使用。 escape sequence 含义 ESC [nA 向上移动 n 行 ESC [nB 向下移动 n 行 ESC [nC 向右移动 n 个间隔 ESC [nD 向左移动 n 个间隔 ESC [m;nH 将光标移动到 \\((m,n)\\) ESC [sJ 从光标清除屏幕 (0:结尾; 1:开始; 2:两者) ESC [sK 从光标清除行 (0:结尾; 1:开始; 2:两者) ESC [nL 在光标处插入 n 行 ESC [nM 在光标处删除 n 行 ESC [nP 在光标处删除 n 个字符 ESC [n@ 在光标处插入 n 个字符 ESC [nm 允许再现 n (0=常规; 4=粗体; 5=闪烁; 7=反白) ESC M 如果光标在顶行则向后滚动屏幕 X Window SystemX Window Ststem (简称为 X) 几乎是 UNIX-like 世界用户界面的基础，作为 Athena 计划的一部分于 20 世纪 80 年代在 MIT 被开发。X 具有非常好的可移植性，并完全运行在用户空间中。人们最初打算将其用于将大量的远程用户终端与中央计算服务器相连接，所以 X 在逻辑上分为 Client 与 Server 两部分，这样就有可能运行在不同的计算机上。在现代个人计算机上，这两部分可以运行在同一机器上，GNOME 与 KDE 都运行于 X 之上。 X 能为 GUI Environment 提供基本环境：在屏幕上描绘、呈现图像与移动程序窗口，同时也受理、运行以及管理键鼠的交互程序。X 没有管辖用户界面部分，由其他以 X 为基础的扩展来负责。此外由于 X 分为客户端 X Client 与服务端 X Server，Server 与 Client 之间的网络通信是透明的，且能够安全交换数据。 X 只是一个窗口系统，并不是完整的 GUI，为了实现完整的 GUI，则需要在 X 之上运行其他软件层。Xlib 则是一组在其之上的库过程，用于访问 X 的功能，这些功能是 X Window System 的基础。但是它们实在过于底层了，实现起来过于麻烦，以至于大部分程序并不会直接使用 Xlib。为了让 X 编程更加容易，X 提供了一个名为 X Toolkit Intrinsics (本征函数集) 的工具包，intrinsics 用于管理按钮、滚动条以及其他窗口小部件 (widget)。而图形库正是构建于 Intrinsics 之上，用以提供一致的外观、跨平台抽象。而真正的 窗口管理器 (Window Manager, WM)，则是顶层的用户空间软件，其主要控制着屏幕上窗口的创建、删除和移动，为了管理窗口，WM 要发送命令到 X Server 告诉它做什么。 当然 X 也有着一套完整的开发、改进规则 除非已有真正的应用程序，真的需要 X 为其修订、增订等支持，否则不会为 X 增加新功能。 决定「一个系统不是什么」和决定「它是什么」同样重要。与其适应整个世界的需要，宁可使得系统可以扩展，如此才能以持续兼容的方式来满足新增的需求。 只有完全没实例时，才会比只有一个实例来的糟。 如果问题没完全弄懂，最好不要去解决它。 如果可以通过 \\(10\\%\\) 的工作量得到 \\(90\\%\\) 的预期效果，应该用更简单的方法解决。 尽量避免复杂性。 提供机制而不是策略，有关用户界面的开发实现，交给实现应用者自主。 由于这一套完整的规则，X 已经将版本号稳定到了 11，因此也被称为 X11。对于在 X 上运行的不同 Desktop Environment，freedesktop.org 积极与努力地维持各种不同 X Desktop Environment 的兼容性，使相竞态势下仍不失X的兼容本色。对于 X 的性能、稳定性的缺点，有着其他全新的窗口系统来竞争，比如 freedesktop.org 主导的 Wayland。 对了！X 是一个开源软件，最早使用 X LICENSE 发布，如果你不知道这个协议，那你应该听说过 MIT LICENSE，没错它们是一个 LICENSE。Wayland 也使用 MIT 协议发布。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:7:2","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#文本窗口"},{"categories":["OperatingSystem"],"content":" 输出软件 文本窗口当输出是连续的、单一字体、大小和颜色的形式时，输出比输入更简单。大体上，程序将字符发送到当前窗口，而字符在那里显示出来，通常通过系统调用将字符写入窗口。 屏幕编辑器和许多其他复杂程序需要能够以更加复杂的方式更新屏幕，比如在屏幕的中间替换一行。为满足如此需求，大多数 output driver 支持一系列命令来移动光标，在光标处插入或删除字符或行。这些命令常被称为 转义序列 (escape sequence)。 在 25 * 80 ASCII 哑中断全盛时期，有数百种终端类型，每种都有自己的转义序列，因此编写可以在一种以上的终端类型的程序是十分困难的。此时出现了 termcap 的终端数据库，这是在 Berkeley UNIX 中引入的。该软件包定义了许多基本动作，比如移动光标到行、列或特殊位置，软件使用一般的转义序列，然后通过 termcap 转换成 terminal 实际定义的 escape sequence。之后由于标准化的需要 ANSI 标准出现了！当然下表就是 ANSI escape sequence，但是其中的 space 只是方便展示，实际不被使用。 escape sequence 含义 ESC [nA 向上移动 n 行 ESC [nB 向下移动 n 行 ESC [nC 向右移动 n 个间隔 ESC [nD 向左移动 n 个间隔 ESC [m;nH 将光标移动到 \\((m,n)\\) ESC [sJ 从光标清除屏幕 (0:结尾; 1:开始; 2:两者) ESC [sK 从光标清除行 (0:结尾; 1:开始; 2:两者) ESC [nL 在光标处插入 n 行 ESC [nM 在光标处删除 n 行 ESC [nP 在光标处删除 n 个字符 ESC [n@ 在光标处插入 n 个字符 ESC [nm 允许再现 n (0=常规; 4=粗体; 5=闪烁; 7=反白) ESC M 如果光标在顶行则向后滚动屏幕 X Window SystemX Window Ststem (简称为 X) 几乎是 UNIX-like 世界用户界面的基础，作为 Athena 计划的一部分于 20 世纪 80 年代在 MIT 被开发。X 具有非常好的可移植性，并完全运行在用户空间中。人们最初打算将其用于将大量的远程用户终端与中央计算服务器相连接，所以 X 在逻辑上分为 Client 与 Server 两部分，这样就有可能运行在不同的计算机上。在现代个人计算机上，这两部分可以运行在同一机器上，GNOME 与 KDE 都运行于 X 之上。 X 能为 GUI Environment 提供基本环境：在屏幕上描绘、呈现图像与移动程序窗口，同时也受理、运行以及管理键鼠的交互程序。X 没有管辖用户界面部分，由其他以 X 为基础的扩展来负责。此外由于 X 分为客户端 X Client 与服务端 X Server，Server 与 Client 之间的网络通信是透明的，且能够安全交换数据。 X 只是一个窗口系统，并不是完整的 GUI，为了实现完整的 GUI，则需要在 X 之上运行其他软件层。Xlib 则是一组在其之上的库过程，用于访问 X 的功能，这些功能是 X Window System 的基础。但是它们实在过于底层了，实现起来过于麻烦，以至于大部分程序并不会直接使用 Xlib。为了让 X 编程更加容易，X 提供了一个名为 X Toolkit Intrinsics (本征函数集) 的工具包，intrinsics 用于管理按钮、滚动条以及其他窗口小部件 (widget)。而图形库正是构建于 Intrinsics 之上，用以提供一致的外观、跨平台抽象。而真正的 窗口管理器 (Window Manager, WM)，则是顶层的用户空间软件，其主要控制着屏幕上窗口的创建、删除和移动，为了管理窗口，WM 要发送命令到 X Server 告诉它做什么。 当然 X 也有着一套完整的开发、改进规则 除非已有真正的应用程序，真的需要 X 为其修订、增订等支持，否则不会为 X 增加新功能。 决定「一个系统不是什么」和决定「它是什么」同样重要。与其适应整个世界的需要，宁可使得系统可以扩展，如此才能以持续兼容的方式来满足新增的需求。 只有完全没实例时，才会比只有一个实例来的糟。 如果问题没完全弄懂，最好不要去解决它。 如果可以通过 \\(10\\%\\) 的工作量得到 \\(90\\%\\) 的预期效果，应该用更简单的方法解决。 尽量避免复杂性。 提供机制而不是策略，有关用户界面的开发实现，交给实现应用者自主。 由于这一套完整的规则，X 已经将版本号稳定到了 11，因此也被称为 X11。对于在 X 上运行的不同 Desktop Environment，freedesktop.org 积极与努力地维持各种不同 X Desktop Environment 的兼容性，使相竞态势下仍不失X的兼容本色。对于 X 的性能、稳定性的缺点，有着其他全新的窗口系统来竞争，比如 freedesktop.org 主导的 Wayland。 对了！X 是一个开源软件，最早使用 X LICENSE 发布，如果你不知道这个协议，那你应该听说过 MIT LICENSE，没错它们是一个 LICENSE。Wayland 也使用 MIT 协议发布。 ","date":"07-25","objectID":"/2021/operatingsystem_004/:7:2","series":["Operating System Note"],"tags":["Note","IuputOutput"],"title":"输入输出","uri":"/2021/operatingsystem_004/#x-window-system"},{"categories":["OperatingSystem"],"content":"GinShio | 现代操作系统第四章读书笔记","date":"07-19","objectID":"/2021/operatingsystem_003/","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/"},{"categories":["OperatingSystem"],"content":"对于长期存储的信息有三个基本要求： 能够存储大量信息 使用信息的进程终止时，信息依旧存在 必须能使多个进程并发访问相关信息 磁盘由于其长期存储的性质，已有多年的使用历史。今年固态硬盘因其没有易损坏的移动部件、可以提供高速的随即访问，而流行起来。但磁盘和光盘虽然性能较差但也广泛用于备份。磁盘可以看做一种大小固定的块的线性序列，且支持： 读块 k 写块 k 磁盘一般支持更多操作，但只要存在这两个操作，原则上就可以解决长期存储问题。当然这还远远不够，一些操作不便于实现，在思考时往往还产生一些问题： 如何找到信息 如何防止一个用户读取另一个用户的数据 如何知道哪些块是空闲的 就像 OS 提取处理器的概念来创建进程的抽象，以及提取 RAM 的概念来创建进程虚拟地址空间的抽象一样，使用 文件 (File) 来解决磁盘的问题。File 是 进程创建的信息逻辑单元，文件是对磁盘的建模而非 RAM，因此将文件看做地址空间就能理解了。 进程可以读取已存在的 File，并在需要时建立新 File，存储的文件必须是持久的，因此不会受到进程的创建与终止而受到影响，只有在其所有者明确删除它的情况下才会消失。File 受操作系统管理，有关的构造、命名、访问、使用、保护、实现和管理方法都是 OS 设计的主要内容。从总体上看，OS 处理文件的部分被称为 文件系统 (file system)，这才是 OS 的核心问题之一。从用户的角度看，File System 中最重要的就是其表现形式，即文件由什么组成的，如何命名、修改等操作。至于如何实现，和我这个用户有什么关系呢。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:0:0","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#"},{"categories":["OperatingSystem"],"content":" 文件首先先从用户的角度观察文件，文件是对磁盘上保存信息的一种抽象，用户不必关心磁盘如何存储、存储到哪里、实际的工作方式等细节。 文件系统是实现这些细节的程序，在 MS-DOS 中使用的是 FAT-16 文件系统，Windows98 对其进行了扩展，从而成为今天耳熟能详的 FAT-32 文件系统，Windows 如今是用一种更为先进的 NTFS 文件系统。微软还根据 FAT 文件系统开发出了 exFAT ，这是针对于闪存和大文件开发的系统，并且也是唯一能满足 OS X 读写操作的微软开发的文件系统。UNIX 的文件系统也有很多，目前主流的是 Linux 的 ext4 (其是 ext2 和 ext3 的升级版本)、 Solaris 的 ZFS 、Unix 传统的 FFS 以及 OS X 的 HFS+ 等，还有一些著名公司如 SGI 开发的 XFS 、RedHat 主导的 BtrFS 等。这些文件系统可能底层实现、适用环境不同，但大部分都遵循 POSIX 实现，对用户操作来说是透明的。 文件系统限制着文件的命名，但是我们可以在进程结束后并且文件依然存在时，根据名称访问文件。文件名一般用圆点隔开分文两部分，圆点后的部分被称为 文件扩展名 (file extension)，extension 往往是约定俗成的，表明了当前文件的内容。在 FAT-16 中限制文件名由 1 到 8 个字符以及 1 到 3 个字符的 extension 组成，而 UNIX 中则以约定而非强制使用 extension。比如 FAT-16 是不区分大小写的文件系统，但是 UNIX 会严格区分文件的大小写，所以一个 C++ 文件 a.C ，在 FAT-16 中被认为是一个 C 语言文件，而 UNIX 中则是一个 C++ 文件。虽然 UNIX 并不关心 extension，但是现代图形界面往往支持程序与 extension 注册绑定，方便用户使用。 文件系统支持多种文件类型，比如 UNIX 与 Windows 中都支持的 普通文件 (regular file) 与 目录 (directory)，UNIX 还支持 字符文件 (character file) 与 块文件 (block file)。regular 包含有用户信息的文件，有二进制文件或 ASCII 文件，ASCII 文件在每行结束有一个换行符 LF (有的操作系统采用回车 CR，或混合 CRLF)，ASCII 文件方便显示、打印并可以编辑。二进制文件人们往往无法理解，其打印也是一堆无意义数据，但二进制文件其格式正确，执行的程序才能精准的将其展示或打印为人类可懂的内容。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:1:0","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件"},{"categories":["OperatingSystem"],"content":" 文件结构文件可以有多种构造方式，最常用的是三种方式： 字节，即文件是由多个字节组成的，OS 并不关心文件的内容是什么，其内容的含义交由处理其的软件解释。这是 OS 中最主流的实现方式，有极大的灵活性，允许用户写入任何内容，OS 不提供帮助也不构成障碍。 记录，即文件是具有固定长度记录的序列，每个记录都有其内部结构。读操作将返回一个记录，写操作重写或追加一个记录。 树，即文件由一颗记录树构成，每个记录不必具有相同的长度，记录的固定位置上有一个 key 字段，树按照 key 进行排序并可以快速查找。采用这种方式时，用户可以为文件添加新的记录，并无需关心记录的存储位置，但用户不能决定记录添加在什么位置，这是 OS 的工作范围。这种方法主要被用语一些大型计算机中。 信息 当 80 列穿孔卡片还是主流的时候，很多大型机 OS 将文件系统建立在由 80 个字符的记录组成的文件基础上。这些 OS 也支持 132 个字符的记录组成文件，因为为了适应行式打印机。程序以 80 个字符读数据、以 132 个字符为单位写数据 (最后 52 个字符为空格)。因此这种方式流行于 80 列穿孔卡片的时代。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:1:1","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件结构"},{"categories":["OperatingSystem"],"content":" 文件属性文件除了文件名与数据，还有保存其他与文件相关的信息，如创建日期与时间、大小等，这些附加信息被称为 属性 (attribute) 或 元数据 (metadata)。但是 attribute 在不同 OS 中差别很大，这里列出其中一些作为举例。 属性 含义 创建者 创建文件者的 ID 所有者 文件当前的所有者 只读标志 文件处于只读、读写状态 隐藏标志 文件是否在列表中显示 系统标志 这是一个系统文件或普通文件 存档标志 该文件是否备份 临时标志 该文件在进程退出时是否删除 创建时间 创建文件的日期与时间 访问时间 上一次访问文件的日期与时间 修改时间 上一次修改文件的日期与时间 当前大小 文件的字节数 占用大小 文件实际占用的字节数 ","date":"07-19","objectID":"/2021/operatingsystem_003/:1:2","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件属性"},{"categories":["OperatingSystem"],"content":" 文件操作早期操作系统仅支持 顺序访问 (sequential access)，进程在这些系统中从头按顺序读取文件的全部字节或记录，不能跳过或随机读取内容，可以返回起点并根据需求多次读取文件，这是磁带的访问方式。当我们使用磁盘时，可以不按顺序读取文件，这种访问方式被称为 随机访问 (random access)。 使用文件的目的是储存信息并未了之后的检索，因此这里介绍一些常见的与文件相关的系统调用 create，创建不包含任何数据的文件，表明文件即将建立并设置一些属性 delete，当文件不再需要时，以删除文件并释放磁盘空间 open，将文件属性与磁盘地址装入内存，以供以后的使用 close，关闭文件并释放内部表空间 read，在文件中读取数据，读取的数据来自文件的当前位置 write，向文件的当前位置写入数据 append，向文件的末尾写入数据 seek，指定文件的位置，方便对文件进行随机访问 ","date":"07-19","objectID":"/2021/operatingsystem_003/:1:3","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件操作"},{"categories":["OperatingSystem"],"content":" 目录文件系统通常提供 目录 或 文件夹 来记录文件的位置，当然很多系统中目录也是文件。目录是层级结构的，最顶层的目录被称为 根 (root)，在 UNIX 系统中也被称为 / ，目录将文件自然而然的进行分组。 当要访问一个文件时，最简单的方法就是从 root 开始，将所有目录与该文件组成一个路径，这被称为 绝对路径 (absolute path)。由于可能有多个路径，因此系统会使用一种符号作为路径的分隔符，在有些系统是不同的，对于一个路径 \\(usr \\rightarrow GinShio \\rightarrow mailbox\\)，它的绝对路径有以下表示： Windows (\\): \\usr\\GinShio\\mailbox UNIX (/): /usr/GinShio/mailbox MULTICS (\u003e): \u003eusr\u003eGinShio\u003emailbox 另一种方式就是 相对路径 (relative path)，它与 工作目录 / 当前目录 (working / current directory) 一起使用，用户不再需要从 root 开始指定，在没有任何前置分隔符时系统会将其解释为 current 下的文件。这使得 relative 更加方便，且与 absolute 功能相同。如果需要声明 current 一般使用 . (dot) 表示，而需要声明当前目录的上一级目录则使用 .. 表示。 以上图为例，假设我们现在工作目录位于 init.d 中，则有以下表示 绝对路径表示当前路径：/etc/init.d 相对路径表示 X11 的路径：../X11 相对路径表示 boot.d 的路径：./boot.d ","date":"07-19","objectID":"/2021/operatingsystem_003/:1:4","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#目录"},{"categories":["OperatingSystem"],"content":" 文件系统的实现","date":"07-19","objectID":"/2021/operatingsystem_003/:2:0","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件系统的实现"},{"categories":["OperatingSystem"],"content":" 文件系统的布局文件系统存放于磁盘上，多数磁盘划分为一个或多个分区，每个分区是一个独立的文件系统。在传统的 BIOS 中需要将磁盘格式化为 主引导记录 (Master Boot Record, MBR) 格式，而 UEFI 则会将磁盘格式化为 全局唯一标识磁盘分区表 (GUID Partition Table, GPT) 格式，GPT 的最开始部分有一段兼容 MBR 的引导，不论是 MBR 还是 GPT 都为了引导计算机系统的启动。 引导记录结尾有一段分区表，该表给出了每个分区的起始和结束地址，表中的一个分区被标记为活动分区。在引导时 BIOS 会读入并执行 MBR，MBR 做的第一件事即确定活动分区并读入其第一个块，这个块被称为 引导块 (boot block)，并执行。boot block 中的程序将装载该分区中的操作系统。为了统一，每个分区都有从 boot block 开始，无论是否包含操作系统。磁盘的分区的布局是根据文件系统的不同而变化的。一般情况下第一个是 超级块 (superblock)，其中包含文件系统的所有关键参数，在计算机启动时或该文件系统首次使用时，superblock 被读入内存。其中的典型信息包括：确定文件系统类型用的魔数、文件系统中的块数量以及其他重要的管理信息。空闲空间管理是针对空闲块的信息，可以使用位图或指针列表的形式给出。index 组是记录文件信息的数组，说明了文件的方方面面。根目录存储着目录树的根部，之后则是其他所有文件和目录。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:2:1","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件系统的布局"},{"categories":["OperatingSystem"],"content":" 文件的实现文件存储实现的关键是记录各个文件分别用到哪些磁盘块。 连续分配 最简单的方案是将每个文件作为一串连续数据块存储到磁盘上，根据块大小的不同文件占用的块数量也不同。每个文件都从一个新的块开始存储，因此当最后一个块不被占满的时候，将造成一定的内部浪费，这与内存分页的内部浪费类似。这样的文件分配方式有两大优点： 实现简单，仅需记住磁盘地址与文件块数即可 读操作性能好 当删除文件时，文件所占用的块被释放，这时会出现外部的碎片化的空闲块。这与 RAM 的外部碎片相似，当文件无法找到足够容纳自己的空闲区时，这个文件将不能被写入磁盘。对于已满的磁盘来说，我们可以进行压缩操作，将磁盘中的空闲区合并。 链表分配 可以与链表的实现类似，将每个块的第一个字作为指向下一块的指针，块的其他部分存放数据。这样可以充分利用磁盘的每一个块，不会造成磁盘碎片进而浪费存储空间。并且 index 中只需要存储文件第一个块的地址即可，实现并不复杂。但是这个实现中，顺序读写并不是什么问题，当需要随机读写时该实现将造成很大困扰，链表并不能进行随即访问 (或者是十分缓慢)。 为了增强读效率，可以将链表存放于内存当中，使用表来记录文件使用的块，这个表被称为 文件分配表 (File Allocation Table, FAT)。按 FAT 的组织方式整个块都可以存放数据，但内存消耗极大，并不实用。 index 节点 i 节点 (index node) 中可以列出文件属性和文件块的地址，这样只需要 index 即可获取到文件需要哪些块，并且在打开文件的时候将 index node 读入 RAM 即可，不再需要 FAT 对文件进行常驻记录。但是问题也随之接踵而至，index node 只能固定存储一定量的磁盘地址，需要使用文件中一些特殊的块来存储额外的地址空间。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:2:2","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件的实现"},{"categories":["OperatingSystem"],"content":" 目录的实现用户利用路径名来寻找对应的文件，因此目录项提供了查找文件磁盘块所需的信息，因系统差异这些信息可能是指向整个文件的磁盘地址 (sequential allocation)、第一个块编号 (list allocation) 或 index node 编号。无论如何目录系统对会将路径名映射为定位文件数据所需的信息。另一个问题是文件的属性如何存放。显而易见的答案是，将属性与磁盘地址都存储于目录项之中。而采用 index node 的文件系统，则可以把属性存放于 index node 中，这样目录项的信息只需要包含文件名与 index 地址即可。 对于可变长的文件名，存储在目录项将会有一个问题，定长的目录项可能无法容纳足够长的文件名，或对目录项的空间造成浪费，而可变长的目录项也可能操作空间的浪费与碎片化。一个比较好的方法是定长的目录项，但是将文件名存储在一个 heap 数据结构中，如此需要额外对其进行管理。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:2:3","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#目录的实现"},{"categories":["OperatingSystem"],"content":" 共享文件共享文件对使用是十分方便的，文件 A 可以共享给目录 B，而不需要占用额外的空间，并且 A 与副本 A 是完全相同的，当 A 发生修改时副本 A 也被修改，这被称为 链接 (link)。文件系统本身是一个 有向无环图 (Directed Acyclic Graph, DAG) 而非树状，因此维护变得复杂。 共享文件带来了一个问题，如果目录中包含磁盘地址，链接文件时必须把目标目录的磁盘地址复制到源目录中，如果目录添加新文件时，只会修改当前添加文件的目录，其他共享的目录无法被修改，这与共享文件的目录相悖。这里提出两个解决方法： 磁盘块不列入目录，而是列入一个与文件本身相关的小型数据结构中，目录将指向这个数据结构，即 Index Node 方式 (也是 UNIX 所采用的方式) 让系统建立一个类型为 LINK 的新文件，并将其放入目标目录下，新的文件只包含它所链接文件的路径名。当读该共享文件时，操作系统发现其是链接文件，则根据记录的路径名找到原始文件。这个方法被称为 符号链接 (symbolic linking) 在共享文件时 index 记录文件的所有者，建立链接时并不会修改所有者，但将 index 的链接计数加 1，所以系统知道有多少目录项指向该文件。在决定删除文件时，只有当链接计数变为 0 时系统才会真正删除该文件。对于符号链接，当原始文件被删除时，链接文件将会访问错误，但是每个符号链接需要额外的 index 节点，并且额外的开销寻找真正的文件地址。但是符号链接也会带来一个好处：只要提供机器的网络地址与文件在该机器上的路径，就可以链接任意机器上的文件。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:2:4","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#共享文件"},{"categories":["OperatingSystem"],"content":" 日志结构文件系统在 CPU 性能快速提升、磁盘容量越来越大的背景下，磁盘的性能却没有太大提升。随着缓存的增大，越来越多的操作可以不需要访问操作，就有可能满足来自文件系统高速缓存的很大一部分读请求。在有些系统中采取的提前读机制并不能获取很好的性能，并且大多数写操作是零碎的 (必须写文件、文件的 index、目录块、目录的 index)，磁盘利用率很低。因此 Berkeley 设计了一种全新的文件系统 — 日志结构文件系统 (Log-structured File System, LFS)。 LFS 希望在面对一个大部分由零碎的随机写操作组成的任务，同样能够充分利用磁盘的带宽。其基本思路是将整个磁盘结构转化为一个日志，每隔一段时间或有特殊需要时，被缓冲在内存中的所有未决的写操作都被放到一个单独的段中，作为在日志末尾的一个邻接段写入磁盘。这个单独的段可能包含 index、目录块、数据块或全都有，每个段开始都是该段的摘要，说明该段都包含哪些内容。若所有的段平均在 1 MB 左右，那么几乎可以利用磁盘的完整带宽。 LFS 的设计中同样存在 index node，但 index node 分散在整个日志中而不是磁盘的某一固定位置。当 index node 被定位后，定位一个块就与通常的方式相同。但是这种设计中 index node 的寻找将变得困难，因为 index node 不再通过简单的计算得到。为了能够找到 index 节点，必须维护一个由 index node 编号索引组成的节点图，这个图中的表项 i 指向磁盘中第 i 个 index，这个图保存在磁盘上、高速缓存中，因此大多数情况下最常用的部分也在内存中。 由于磁盘并不是无限大的，日志可能会占满整个磁盘，但是幸运的是，有些块虽然被前面的日志使用着，但实际已经不再需要。因此 LFS 有一个清理线程，周期性地扫描日志进行磁盘压缩。该线程首先读入日志的第一个段的摘要，检查有哪些 index 节点与文件，并与当前 index 节点图进行对比，查看文件、index 是否有效，无效时信息将被丢弃，反之 index 与块将进入内存等待写回到下一个段中，原先的段将被标记为空闲以便存放新的日志。 日志结构文件系统的大量零碎写操作性能强于 UNIX 一个数量级，并且其读操作与大块写操作的性能并不比 UNIX 文件系统差，甚至更好。 由于 LFS 并不于现有文件系统匹配，但其面对出错的健壮性，及其思想 (保存系统下一步将要做什么的日志)，被其他文件系统所借鉴。这样系统即将完成一些任务时崩溃，重启后通过查看日志即可获取崩溃前的任务并完成。这样的文件系统被称为 日志文件系统， NTFS、ext3、ReiserFS 等都是这种系统的应用。 日志文件系统先写入一个日志项，列出将要完成的任务，当日志项被写入后将开始执行任务，所有的任务都完成后将擦除日志项。如果系统这时候崩溃，可以在系统恢复后通过检查日志项，确定是否有未完成的操作西药继续完成。为了让系统工作，被写入日志的操作必须是 幂等的，意味着只要有必要操作就可以执行多次，并且不会带来破坏。为了可靠性，系统可以添加 原子事务 的概念。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:2:5","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#日志结构文件系统"},{"categories":["OperatingSystem"],"content":" 虚拟文件系统在计算机的不同分区可能使用不同的文件系统，这在现实中很常用，因此操作系统使用 虚拟文件系统 (Virtual File System, VFS) 的概念将不同文件系统统一成一种有序的结构。关键思路是抽象出所有文件系统的共同部分，并将这部分代码放在单独的一层，VFS 调用底层实际的文件系统来管理数据。 VFS 并不关心底层如何存储数据或数据被存储到哪里，正如 Sun 建立 VFS 最初的目的是使用 网络文件系统 (Network File System, NFS) 协议的远程文件系统，因此 VFS 只关心实际的文件系统提供的功能。通常支持一些必要的对象类型，包括超块 (描述文件系统)、v 节点 (描述文件) 和目录，这些中的每一个都有实际文件系统必须支持的相关操作。VFS 也会有一些供自己使用的内部数据结构，包括用于跟踪用户进程中所打开文件的装载表和文件描述符的数组。 在实际运行时，文件系统需要先向 VFS 进行注册，将 VFS 所支持的功能，通过提交对应的功能函数表，告知 VFS 相关的真实操作。当用户通过调用 POSIX 接口进行文件操作时，由 VFS 管理用户打开的文件，并将 POSIX 接口转换为真正的文件系统调用。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:2:6","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#虚拟文件系统"},{"categories":["OperatingSystem"],"content":" 文件系统管理和优化","date":"07-19","objectID":"/2021/operatingsystem_003/:3:0","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件系统管理和优化"},{"categories":["OperatingSystem"],"content":" 磁盘管理文件通常存放在磁盘上，所以对磁盘空间的管理是设计者要考虑的一个主要问题。存储一个有 n 个字节的文件可以有两种策略，分配 n 个字节的连续空间，或把文件分成很多连续的块。在存储管理系统中，分段处理和分页处理也需要进行权衡。由于分段需要移动文件，但磁盘相对内存操作慢得多，因此文件系统经常采用分页的方式。 块大小一旦决定分页，最直观的问题即页大小。大的块尺寸意味着小文件将浪费大量磁盘空间，而小的块尺寸则会有大量文件跨越多个块，因此需要多次寻道与旋转延迟才能找到它们，从而降低性能。 一项关于 VU 计算机系研究数据表明，VU \\(59.13\\%\\) 的文件都小于等于 4 KB，而 \\(90.84\\%\\) 的文件都小于等于 64 KB，其中文件大小的中位数是 2474 字节。假设磁盘每道 1 MB，其旋转时间 8.33 ms，平均寻道时间 5 ms，那么读取一个 k 字节的块需要： \\[5 + 4.165 + \\frac{k}{1000000} * 8.33\\] 由于对一个块的访问主要由寻道时间与旋转延迟决定，因此在访问磁盘块时，取到的数据越多越好，因此数据率随着磁盘块的增大而线性增大。对于空间利用率来说，随着磁盘块的增大而降低。性能与利用率是天生矛盾的。在历史上文件系统的块大小设置在 1 ~ 4 KB 之间，随着磁盘的容量增加，块大小也可以考虑提升到 64 KB 以增加性能而接受空间浪费。 记录空闲块对于如何记录空闲块的方式，主要有两种： 磁盘块链表，每个块中包含尽可能多的空闲磁盘号，并采用空闲块存放空闲列表 位图 使用记录连续块的方式优化链表存储，但磁盘碎片严重时，这种方式将比单独记录的方法效率低。并且指针块可以存储在内存中，但在指针块快满时对文件的删除操作，可能使指针块溢出，从而交换新的指针块，导致大量的磁盘 IO。避免过多的磁盘 IO 可以将内存中始终保持半满的指针块，在释放与写入时都可以减少频繁的 IO。 对于位图的实现，内存中只保留一个块是可能的，在块满了或空了的情况下再进行磁盘 IO。另外通过位图在单一块上进行分配操作，磁盘块会较为紧密地聚集在一起，从而减少了磁盘臂的移动。并且如果内核是分页的，可以将固定大小的块读入虚拟内存，在需要时将位图页面调入。 磁盘配额多用户操作系统往往会为不同用户分配一定大小的空间，防止用户占用太多的磁盘空间。当用户修改文件时，文件属性中的所有者可以记录该文件属于谁，关于文件大小的变动都会记录到所有者的配额上。此外还会有一个指向所有者的配额指针，用以记录所有者配额的详细信息，并在文件变动时检查配额是否超出。配额中存在软配额与硬配额，前者可以超过但会发生警告，后者不可超出。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:1","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#磁盘管理"},{"categories":["OperatingSystem"],"content":" 磁盘管理文件通常存放在磁盘上，所以对磁盘空间的管理是设计者要考虑的一个主要问题。存储一个有 n 个字节的文件可以有两种策略，分配 n 个字节的连续空间，或把文件分成很多连续的块。在存储管理系统中，分段处理和分页处理也需要进行权衡。由于分段需要移动文件，但磁盘相对内存操作慢得多，因此文件系统经常采用分页的方式。 块大小一旦决定分页，最直观的问题即页大小。大的块尺寸意味着小文件将浪费大量磁盘空间，而小的块尺寸则会有大量文件跨越多个块，因此需要多次寻道与旋转延迟才能找到它们，从而降低性能。 一项关于 VU 计算机系研究数据表明，VU \\(59.13\\%\\) 的文件都小于等于 4 KB，而 \\(90.84\\%\\) 的文件都小于等于 64 KB，其中文件大小的中位数是 2474 字节。假设磁盘每道 1 MB，其旋转时间 8.33 ms，平均寻道时间 5 ms，那么读取一个 k 字节的块需要： \\[5 + 4.165 + \\frac{k}{1000000} * 8.33\\] 由于对一个块的访问主要由寻道时间与旋转延迟决定，因此在访问磁盘块时，取到的数据越多越好，因此数据率随着磁盘块的增大而线性增大。对于空间利用率来说，随着磁盘块的增大而降低。性能与利用率是天生矛盾的。在历史上文件系统的块大小设置在 1 ~ 4 KB 之间，随着磁盘的容量增加，块大小也可以考虑提升到 64 KB 以增加性能而接受空间浪费。 记录空闲块对于如何记录空闲块的方式，主要有两种： 磁盘块链表，每个块中包含尽可能多的空闲磁盘号，并采用空闲块存放空闲列表 位图 使用记录连续块的方式优化链表存储，但磁盘碎片严重时，这种方式将比单独记录的方法效率低。并且指针块可以存储在内存中，但在指针块快满时对文件的删除操作，可能使指针块溢出，从而交换新的指针块，导致大量的磁盘 IO。避免过多的磁盘 IO 可以将内存中始终保持半满的指针块，在释放与写入时都可以减少频繁的 IO。 对于位图的实现，内存中只保留一个块是可能的，在块满了或空了的情况下再进行磁盘 IO。另外通过位图在单一块上进行分配操作，磁盘块会较为紧密地聚集在一起，从而减少了磁盘臂的移动。并且如果内核是分页的，可以将固定大小的块读入虚拟内存，在需要时将位图页面调入。 磁盘配额多用户操作系统往往会为不同用户分配一定大小的空间，防止用户占用太多的磁盘空间。当用户修改文件时，文件属性中的所有者可以记录该文件属于谁，关于文件大小的变动都会记录到所有者的配额上。此外还会有一个指向所有者的配额指针，用以记录所有者配额的详细信息，并在文件变动时检查配额是否超出。配额中存在软配额与硬配额，前者可以超过但会发生警告，后者不可超出。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:1","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#块大小"},{"categories":["OperatingSystem"],"content":" 磁盘管理文件通常存放在磁盘上，所以对磁盘空间的管理是设计者要考虑的一个主要问题。存储一个有 n 个字节的文件可以有两种策略，分配 n 个字节的连续空间，或把文件分成很多连续的块。在存储管理系统中，分段处理和分页处理也需要进行权衡。由于分段需要移动文件，但磁盘相对内存操作慢得多，因此文件系统经常采用分页的方式。 块大小一旦决定分页，最直观的问题即页大小。大的块尺寸意味着小文件将浪费大量磁盘空间，而小的块尺寸则会有大量文件跨越多个块，因此需要多次寻道与旋转延迟才能找到它们，从而降低性能。 一项关于 VU 计算机系研究数据表明，VU \\(59.13\\%\\) 的文件都小于等于 4 KB，而 \\(90.84\\%\\) 的文件都小于等于 64 KB，其中文件大小的中位数是 2474 字节。假设磁盘每道 1 MB，其旋转时间 8.33 ms，平均寻道时间 5 ms，那么读取一个 k 字节的块需要： \\[5 + 4.165 + \\frac{k}{1000000} * 8.33\\] 由于对一个块的访问主要由寻道时间与旋转延迟决定，因此在访问磁盘块时，取到的数据越多越好，因此数据率随着磁盘块的增大而线性增大。对于空间利用率来说，随着磁盘块的增大而降低。性能与利用率是天生矛盾的。在历史上文件系统的块大小设置在 1 ~ 4 KB 之间，随着磁盘的容量增加，块大小也可以考虑提升到 64 KB 以增加性能而接受空间浪费。 记录空闲块对于如何记录空闲块的方式，主要有两种： 磁盘块链表，每个块中包含尽可能多的空闲磁盘号，并采用空闲块存放空闲列表 位图 使用记录连续块的方式优化链表存储，但磁盘碎片严重时，这种方式将比单独记录的方法效率低。并且指针块可以存储在内存中，但在指针块快满时对文件的删除操作，可能使指针块溢出，从而交换新的指针块，导致大量的磁盘 IO。避免过多的磁盘 IO 可以将内存中始终保持半满的指针块，在释放与写入时都可以减少频繁的 IO。 对于位图的实现，内存中只保留一个块是可能的，在块满了或空了的情况下再进行磁盘 IO。另外通过位图在单一块上进行分配操作，磁盘块会较为紧密地聚集在一起，从而减少了磁盘臂的移动。并且如果内核是分页的，可以将固定大小的块读入虚拟内存，在需要时将位图页面调入。 磁盘配额多用户操作系统往往会为不同用户分配一定大小的空间，防止用户占用太多的磁盘空间。当用户修改文件时，文件属性中的所有者可以记录该文件属于谁，关于文件大小的变动都会记录到所有者的配额上。此外还会有一个指向所有者的配额指针，用以记录所有者配额的详细信息，并在文件变动时检查配额是否超出。配额中存在软配额与硬配额，前者可以超过但会发生警告，后者不可超出。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:1","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#记录空闲块"},{"categories":["OperatingSystem"],"content":" 磁盘管理文件通常存放在磁盘上，所以对磁盘空间的管理是设计者要考虑的一个主要问题。存储一个有 n 个字节的文件可以有两种策略，分配 n 个字节的连续空间，或把文件分成很多连续的块。在存储管理系统中，分段处理和分页处理也需要进行权衡。由于分段需要移动文件，但磁盘相对内存操作慢得多，因此文件系统经常采用分页的方式。 块大小一旦决定分页，最直观的问题即页大小。大的块尺寸意味着小文件将浪费大量磁盘空间，而小的块尺寸则会有大量文件跨越多个块，因此需要多次寻道与旋转延迟才能找到它们，从而降低性能。 一项关于 VU 计算机系研究数据表明，VU \\(59.13\\%\\) 的文件都小于等于 4 KB，而 \\(90.84\\%\\) 的文件都小于等于 64 KB，其中文件大小的中位数是 2474 字节。假设磁盘每道 1 MB，其旋转时间 8.33 ms，平均寻道时间 5 ms，那么读取一个 k 字节的块需要： \\[5 + 4.165 + \\frac{k}{1000000} * 8.33\\] 由于对一个块的访问主要由寻道时间与旋转延迟决定，因此在访问磁盘块时，取到的数据越多越好，因此数据率随着磁盘块的增大而线性增大。对于空间利用率来说，随着磁盘块的增大而降低。性能与利用率是天生矛盾的。在历史上文件系统的块大小设置在 1 ~ 4 KB 之间，随着磁盘的容量增加，块大小也可以考虑提升到 64 KB 以增加性能而接受空间浪费。 记录空闲块对于如何记录空闲块的方式，主要有两种： 磁盘块链表，每个块中包含尽可能多的空闲磁盘号，并采用空闲块存放空闲列表 位图 使用记录连续块的方式优化链表存储，但磁盘碎片严重时，这种方式将比单独记录的方法效率低。并且指针块可以存储在内存中，但在指针块快满时对文件的删除操作，可能使指针块溢出，从而交换新的指针块，导致大量的磁盘 IO。避免过多的磁盘 IO 可以将内存中始终保持半满的指针块，在释放与写入时都可以减少频繁的 IO。 对于位图的实现，内存中只保留一个块是可能的，在块满了或空了的情况下再进行磁盘 IO。另外通过位图在单一块上进行分配操作，磁盘块会较为紧密地聚集在一起，从而减少了磁盘臂的移动。并且如果内核是分页的，可以将固定大小的块读入虚拟内存，在需要时将位图页面调入。 磁盘配额多用户操作系统往往会为不同用户分配一定大小的空间，防止用户占用太多的磁盘空间。当用户修改文件时，文件属性中的所有者可以记录该文件属于谁，关于文件大小的变动都会记录到所有者的配额上。此外还会有一个指向所有者的配额指针，用以记录所有者配额的详细信息，并在文件变动时检查配额是否超出。配额中存在软配额与硬配额，前者可以超过但会发生警告，后者不可超出。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:1","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#磁盘配额"},{"categories":["OperatingSystem"],"content":" 文件系统备份文件系统的损坏将造成数据的大量丢失，最直接的方法就是备份。备份有两个好处，从 意外的灾难 或 错误的操作 中恢复数据，但备份可能耗时又浪费空间，因此快又好的备份是十分必要的。 对于临时文件与设备文件是不需要备份的，大部分的可执行文件可以再获取也是不需要备份的，因此只备份特定目录与其下的文件即可。 对从没有修改过的文件进行多次备份也是一种浪费，因此我们需要用到增量备份的思想。在一次全量备份上只记录最新修改的部分，这样加快了备份的时间、减少了空间的浪费，但对恢复来说是一种挑战，恢复时必须从全量备份开始逐次进行恢复。 备份是否需要压缩算法，也是需要考量的。因为备份设备上的一个坏点即可破坏整个文件，导致数据无法读取。 对活动的文件系统进行备份是很难的，极有可能发生文件的不一致性。因此人们修改了转储算法，记录下文件系统的瞬时快照 (复制关键的数据结构)，然后需要把将来对文件和目录所做的修改复制到块中，而非处处更新。 转储分为 物理转储 与 逻辑转储。前者从磁盘的第 0 块开始，将全部的磁盘块按序输出到磁带，直到最后一块。这种方案很简单，可以确保万无一失。但是物理转储无法跳过选定目录，也无法增量转储。后者从一个或多个指定的目录开始，递归地转储其给定的基准日期后所更改的全部文件和目录。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:2","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件系统备份"},{"categories":["OperatingSystem"],"content":" 文件系统的一致性如果在修改过的磁盘块全部写回之前系统崩溃，则文件系统将处于不一致状态，如果未被写回的块是 index、目录块或含有空闲表的块时，这个问题尤为严重。因此在解决不一致性的问题上，很多系统都带有检验系统一致性的程序。在系统启动 (或因崩溃重启) 时可以运行该程序。一致性检测一般分为两种：块的一致性检测与文件的一致性检测。 在检测块的一致性时，程序构造两张表，每张表中为每个块设立一个计数器，都初始化为 0。接着程序使用原始设备读取全部的 index 节点，忽略文件结构，只返回从零开始的所有磁盘块。以下展示了块一致性检测中可能出现的问题。b 情况 块丢失，即块没有出现在任意一张表中，虽然没有危害，但是造成了资源的浪费，解决方案也很简单，将其加入到空闲表中即可。c 情况出现了重复的空闲块，此时只要重新建立空闲表即可。d 情况最为复杂，在两个或多个数据中都出现一同一个块，如果删除其中一个文件则会出现块处于一种使用与空闲的状态，而删除所有相关文件则会让其在空闲表中重复多次。一种解决方案是复制该块，并让引用这个块的文件得到相同的一份副本，消除文件系统的不一致性，并将错误报告给用户。 除了文件外目录也需要检查，同样使用计数器记录文件 (并非块) 的使用次数。在检测完成后得到一张由 index 节点号索引的表，说明每个文件被多少个目录包含，并与 index node 中的链接数目比较。index 中链接数目大于计数器的数目，则会导致释放完所有文件后 index 依然不能释放相关磁盘块，造成磁盘空间浪费。而小于计数器数目时，index 为 0 后会立即释放磁盘块，index 可能被很快用作其他文件，导致原先的文件内容错误。这两种方法都可以通过设置正确的 index node 链接数目解决。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:3","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件系统的一致性"},{"categories":["OperatingSystem"],"content":" 文件系统性能 高速缓存最常用的减少磁盘直接访问的方案即 块高速缓存 (block cache)，block cache 逻辑上属于磁盘，但实际被存储于内存中。在读请求中首先检测是否在 cache 中，命中 cache 时将不需要进行磁盘访问，从而提升性能。 cache 的使用，需要与磁盘对块进行交换，因此在内存所涉及的页面置换算法都可以使用，与分页相比 cache 的引用很不频繁，因此 LRU 顺序链表是可行的。但关键问题是，一个关键块被修改并保存在 cache 中没有写回，当系统崩溃时将造成数据不一致性。为解决这一问题，可以将块分类为 index、目录项、数据等不同的类型，将最近不再使用的块放在 LRU 链表的头部，对于很快将要再次使用的块 (如正在写入的「部分满数据块」) 放入链表的尾部，让其在高速缓存中保存更长的时间。除了数据块外的其他类型块，都应在修改后立即写回磁盘，保证文件系统的一致性。数据的一致性，可以通过定时的强制将修改的块写回磁盘，保证一定时间间隔内的数据不会丢失。而立即将所有修改的块写回磁盘被称为 通写高速缓存 (write-through cache)，这种技术相比非通写需要更多的磁盘 IO。 块提前读将要使用到的块提前读入 cache，从而提高 cache 命中率。特别地，很多文件是顺序读取的，因此系统提前读取到数据，以便提高 cache 命中率，从而减少磁盘 IO。但是对于随机访问的文件，提前读并不能优化性能，甚至可能降低性能。 减少磁盘臂运动另一项提升文件系统性能的重要技术是将有可能顺序读取的块放在一起，最好是同一柱面上，减少磁盘臂移动次数。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:4","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#文件系统性能"},{"categories":["OperatingSystem"],"content":" 文件系统性能 高速缓存最常用的减少磁盘直接访问的方案即 块高速缓存 (block cache)，block cache 逻辑上属于磁盘，但实际被存储于内存中。在读请求中首先检测是否在 cache 中，命中 cache 时将不需要进行磁盘访问，从而提升性能。 cache 的使用，需要与磁盘对块进行交换，因此在内存所涉及的页面置换算法都可以使用，与分页相比 cache 的引用很不频繁，因此 LRU 顺序链表是可行的。但关键问题是，一个关键块被修改并保存在 cache 中没有写回，当系统崩溃时将造成数据不一致性。为解决这一问题，可以将块分类为 index、目录项、数据等不同的类型，将最近不再使用的块放在 LRU 链表的头部，对于很快将要再次使用的块 (如正在写入的「部分满数据块」) 放入链表的尾部，让其在高速缓存中保存更长的时间。除了数据块外的其他类型块，都应在修改后立即写回磁盘，保证文件系统的一致性。数据的一致性，可以通过定时的强制将修改的块写回磁盘，保证一定时间间隔内的数据不会丢失。而立即将所有修改的块写回磁盘被称为 通写高速缓存 (write-through cache)，这种技术相比非通写需要更多的磁盘 IO。 块提前读将要使用到的块提前读入 cache，从而提高 cache 命中率。特别地，很多文件是顺序读取的，因此系统提前读取到数据，以便提高 cache 命中率，从而减少磁盘 IO。但是对于随机访问的文件，提前读并不能优化性能，甚至可能降低性能。 减少磁盘臂运动另一项提升文件系统性能的重要技术是将有可能顺序读取的块放在一起，最好是同一柱面上，减少磁盘臂移动次数。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:4","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#高速缓存"},{"categories":["OperatingSystem"],"content":" 文件系统性能 高速缓存最常用的减少磁盘直接访问的方案即 块高速缓存 (block cache)，block cache 逻辑上属于磁盘，但实际被存储于内存中。在读请求中首先检测是否在 cache 中，命中 cache 时将不需要进行磁盘访问，从而提升性能。 cache 的使用，需要与磁盘对块进行交换，因此在内存所涉及的页面置换算法都可以使用，与分页相比 cache 的引用很不频繁，因此 LRU 顺序链表是可行的。但关键问题是，一个关键块被修改并保存在 cache 中没有写回，当系统崩溃时将造成数据不一致性。为解决这一问题，可以将块分类为 index、目录项、数据等不同的类型，将最近不再使用的块放在 LRU 链表的头部，对于很快将要再次使用的块 (如正在写入的「部分满数据块」) 放入链表的尾部，让其在高速缓存中保存更长的时间。除了数据块外的其他类型块，都应在修改后立即写回磁盘，保证文件系统的一致性。数据的一致性，可以通过定时的强制将修改的块写回磁盘，保证一定时间间隔内的数据不会丢失。而立即将所有修改的块写回磁盘被称为 通写高速缓存 (write-through cache)，这种技术相比非通写需要更多的磁盘 IO。 块提前读将要使用到的块提前读入 cache，从而提高 cache 命中率。特别地，很多文件是顺序读取的，因此系统提前读取到数据，以便提高 cache 命中率，从而减少磁盘 IO。但是对于随机访问的文件，提前读并不能优化性能，甚至可能降低性能。 减少磁盘臂运动另一项提升文件系统性能的重要技术是将有可能顺序读取的块放在一起，最好是同一柱面上，减少磁盘臂移动次数。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:4","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#块提前读"},{"categories":["OperatingSystem"],"content":" 文件系统性能 高速缓存最常用的减少磁盘直接访问的方案即 块高速缓存 (block cache)，block cache 逻辑上属于磁盘，但实际被存储于内存中。在读请求中首先检测是否在 cache 中，命中 cache 时将不需要进行磁盘访问，从而提升性能。 cache 的使用，需要与磁盘对块进行交换，因此在内存所涉及的页面置换算法都可以使用，与分页相比 cache 的引用很不频繁，因此 LRU 顺序链表是可行的。但关键问题是，一个关键块被修改并保存在 cache 中没有写回，当系统崩溃时将造成数据不一致性。为解决这一问题，可以将块分类为 index、目录项、数据等不同的类型，将最近不再使用的块放在 LRU 链表的头部，对于很快将要再次使用的块 (如正在写入的「部分满数据块」) 放入链表的尾部，让其在高速缓存中保存更长的时间。除了数据块外的其他类型块，都应在修改后立即写回磁盘，保证文件系统的一致性。数据的一致性，可以通过定时的强制将修改的块写回磁盘，保证一定时间间隔内的数据不会丢失。而立即将所有修改的块写回磁盘被称为 通写高速缓存 (write-through cache)，这种技术相比非通写需要更多的磁盘 IO。 块提前读将要使用到的块提前读入 cache，从而提高 cache 命中率。特别地，很多文件是顺序读取的，因此系统提前读取到数据，以便提高 cache 命中率，从而减少磁盘 IO。但是对于随机访问的文件，提前读并不能优化性能，甚至可能降低性能。 减少磁盘臂运动另一项提升文件系统性能的重要技术是将有可能顺序读取的块放在一起，最好是同一柱面上，减少磁盘臂移动次数。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:4","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#减少磁盘臂运动"},{"categories":["OperatingSystem"],"content":" 磁盘碎片整理最初安装操作系统时，从磁盘开始位置，一个接一个的安装程序与文件，所有的空闲磁盘空间都被存放在一个大的空闲区域内。随着系统的使用，文件的删除与创建，磁盘上会产生很多碎片，此时创建一个新文件可能会使其块散布在整个磁盘上，造成性能下降。不过可以通过一种方式恢复：移动文件使他们相邻，并把大部分空闲块放到一个或多个大的连续区域中。 不过碎片整理的过程中，有些文件并不能被移动，包含页文件、休眠文件和日志，移动这些文件所需的管理成本大于移动他们所获得的收益。这些文件使用固定大小的连续区域，因此不需要进行碎片整理。如果这些文件在分区的末尾，此时用户想缩小分区的时候，将会把它们删除，改变分区大小后再进行重建。 由于 ext2 与 ext3 的选择磁盘块的方式，在磁盘碎片整理上一般不会遭受像 Windows 那样的困难，因此很少需要手动进行碎片整理。此外，SSD 并不受磁盘碎片的影响，并且频繁的碎片整理可能影响其寿命。 ","date":"07-19","objectID":"/2021/operatingsystem_003/:3:5","series":["Operating System Note"],"tags":["Note","FileSystem"],"title":"文件系统","uri":"/2021/operatingsystem_003/#磁盘碎片整理"},{"categories":["OperatingSystem"],"content":"GinShio | 现代操作系统第三章读书笔记","date":"07-14","objectID":"/2021/operatingsystem_002/","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/"},{"categories":["OperatingSystem"],"content":"内存 (随机访问存储器，RAM) 是计算机中一种需要认真管理的重要资源。不管存储器有多大，程序都能把它填满。经过多年的探索，我们有了 分层存储器体系 (memory hierarchy) 的概念，即计算机拥有若干 MiB 快速、昂贵且易失性的 Cache，数 GiB 速度与价格适中的易失性内存，以及数 TiB 快速、廉价但非易失性的磁盘存储。计算机中管理分层存储器体系的部分被称为 存储管理器 (memory manager)。它的任务是有效地管理内存，记录哪些内存正在使用，哪些内存是空闲的，在进程需要时为其分配内存，在进程使用完后释放内存。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:0:0","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#"},{"categories":["OperatingSystem"],"content":" 无存储器抽象最简单的存储器抽象是不使用抽象。早期的大型机 (60 年代以前)、小型机 (70 年代以前) 以及个人计算机 (80 年代以前) 都是没有存储器抽象的，每一个程序都直接访问物理内存，这种模型中系统每次仅运行一个进程。 虽然直接使用物理内存，但还是有不同的模型，下图展示了三种模型。a 模型中操作系统位于 RAM 底部，这种模型曾被用于大型机与小型机；b 模型中操作系统位于内存顶端的 ROM (只读存储器) 中，这种模型被用于掌上电脑或嵌入式系统中；c 模型中设备驱动程序位于顶部的 ROM 中，而操作系统的其他部分位于 RAM 的底部，该方案被用于早期的个人计算机中 (如运行 MS-DOS 的计算机)，在 ROM 中的系统部分被称为 BIOS (基本输入输出系统， Basic Input Output System)。a 和 c 模型当用户程序出错时，可能会摧毁操作系统，引发灾难性后果。 在无存储器抽象的系统中实现并行的方法是采用多线程编程。由于引入线程时假设一个进程中的所有线程对同一内存映像都可见，如此实现并行也就不是问题。虽然方法行得通，但没有被广泛使用，因为人们通常希望能够在同一时间运行没有关联的程序，而这正是线程抽象所不能提供的。因此一个无存储器抽象的系统也不大可能提供线程抽象的功能。 由于使用无存储器抽象时并发进程，可以在一个进程运行一段时间后，从磁盘中加载其他进程到 RAM 中。但由于两个进程都引用的绝对地址，因此可能会引用到第一个进程的私有地址，导致进程崩溃。IBM 360 对上述问题的补救方案就是在第二个进程装载到内存的时候，使用静态重定位的技术修改它。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:1:0","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#无存储器抽象"},{"categories":["OperatingSystem"],"content":" 一种存储器抽象：地址空间当物理地址暴露给进程会带来下面一些严重问题： 如果用户程序可以寻址内存的每个字节，它们就可以很容易地 (故意地或偶然地) 破坏操作系统，从而使整个系统慢慢地停止运行，除非有特殊的硬件保护 (IBM 360 的锁键模式) 使用这种模型，想要同时运行多个程序是很困难的 ","date":"07-14","objectID":"/2021/operatingsystem_002/:2:0","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#一种存储器抽象-地址空间"},{"categories":["OperatingSystem"],"content":" 地址空间的概念要使多个应用程序同时处于内存中并且不互相影响，需要解决两个问题：保护 和 重定位 。 对于保护的解决方法，IBM 360 上使用过一种方法：给内存块标记上一个保护键，并且比较执行进程的键和其访问的内存字的保护键。然而，这种方法本身并没有解决后一个问题，虽然这个问题可以通过在程序被装载时重定位程序来解决，但这是一个缓慢且复杂的解决方法。 一个更好的办法是创造一个新的存储器抽象：地址空间。就像进程的概念创造了一类抽象的 CPU 以运行程序一样，地址空间为程序创造了一种抽象的内存。地址空间 是一个进程可用于寻址内存的一套地址结合，每个进程都有一个自己的地址空间，并且这个地址空间独立于其他进程的地址空间。以域名为例，以 .com 结尾的网络域名的集合是一种地址空间，这个地址空间是由所有包含 2 ~ 63 个字符并且后面跟着 .com 的字符串组成的，组成这些字符串的字符可以是字母、数字和连字符。比较难的是给每个进程一个自己独有的地址空间，使得一个程序中的地址 28 所对应的物理地址与另一个程序中的地址 28 多对应的物理地址有所不同。 一个简单的方式是使用 动态重定位，简单地把每个进程的地址空间映射到物理内存的不同部分，这个方法会为 CPU 配置两个特殊的硬件寄存器 基址寄存器 与 界限寄存器 。当使用基址寄存器与界限寄存器时，程序装载到内存中连续的空闲位置且装载期间无须重定位。当程序开始运行时，将程序的起始物理地址装载到基址寄存器中，程序的长度装载到界限寄存器中。当每次进程访问内存、取一条指令、读或写一个数据字，CPU 硬件会在把地址发送到内存总先前，自动将基址值加到进程发出地址值上。同时检查程序提供的地址是否大于或等于界限寄存器中的值，如果访问的地址超过界限则会产生错误并中止访问。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:2:1","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#地址空间的概念"},{"categories":["OperatingSystem"],"content":" 交换技术如果计算机物理内存足够大，可以保存所有进程，那么之前提及的所有方案或多或少都是可行的。但实际上，所有进程所需的 RAM 数量总和通常会远超出存储器能够支持的范围。有两种处理内存超载的通用方法，最简单的策略是 交换 (swapping) 技术，即将一个进程完整调入内存，使该进程运行一段时间，然后把它存回磁盘。空闲进程主要存储在磁盘之上，所以当它们不运行时将不占用内存。另一种策略是 虚拟内存 (virtual memory)，该策略甚至能使程序在只有一部分被调入内存的情况下运行。 如下图所示一个交换系统。开始时内存中仅存在进程 A，之后创建进程 B、C，在 A 被交换到磁盘后载入 D 的内存映像，之后 B 被调出，最后 A 再次被调入。由于 A 的位置发生改变，所以在它换入的时候通过软件或程序运行期间通过硬件，对其地址进行重定位。 交换在内存中产生了多个空闲区 (hole，或称 空洞)，通过把所有进程尽可能向下移动，有可能将这些小的 hole 合并成一大块，该技术被称为 内存紧缩 (memory compaction)。通常不进行这个操作，因为将要消耗大量的 CPU 时间。 在进程创建时应该分配多大的内存是一个重要问题。若进程创建时大小固定并不再改变，那么操作系统准确按其大小进行分配。但如果进程数据段可以增长 (多数编程语言允许从堆中动态分配内存)，那么进程增长时就会出现问题。假设大部分进程在运行时都会增长，为减少因内存区域不足而引起的进程交换和移动所产生的开销，可以在换入或移动进程时为其分配一些额外的内存。当进程被换出到磁盘上时，仅交换进程实际上使用的内存内容。如果进程有两个可增长的段 (堆所使用的数据段以及存放局部变量和返回地址的堆栈段)，则可以预留一部分区域，堆栈段占据进程内存的顶部并向下增长，数据段占据程序底部并向上增长，其间的内存为预留的空闲内存。如下图所示。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:2:2","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#交换技术"},{"categories":["OperatingSystem"],"content":" 空闲内存管理在动态分配内存时，操作系统必须对其进行管理。一般而言，有两种管理方法 位图 与 空闲区链表。 使用位图的存储管理该方法中，内存可能被划分成小到几个字或大到几千字的分配单元，每个分配单元对应位图中的一位。 分配单元的大小是一个重要设计因素：分配单元越小，位图越大。然而即使只有 4 个字节大小的分配单元，32 位的内存需要位图中的 1 位，因此位图占用了 \\(1 / 32\\) 的内存。如果选择较大的分配单元，若进程的大小不是分配单元的整数倍，将极易浪费内存。 使用链表的存储管理维护一个记录已分配内存段和空闲内存段的链表，其中每个结点包含域：空闲区 (H) 或进程 (P) 的指示标志、起始地址、长度和指向下一个结点的指针。进程表中表示终止进程的结点通常含有指向对应于其段链表结点的指针，因此段链表使用双向链表更加方便，易于找到上一个结点并检查是否可以合并。 当使用地址顺序在链表中存放进程和空闲区时，有几种算法可以用来为进程交换分配内存。假设存储管理器知道进程需要多少内存。 最简单的方法即 首次适配 (first fit) 算法，存储器沿着段链表搜索，直到找到足够大的空闲区为止 下次适配 (next fit) 算法是通过 first fit 修改而来，在找到足够大的空闲区时， next fit 会记录下当前位置，供下次搜索使用 最佳适配 (best fit) 算法搜索整个链表，找出能够容纳进程的最小空闲区。best fit 以最好地匹配请求可用空闲区，而不是拆分一个以后可能用到的大空闲区。但是遗憾的是，best fit 将会产生大量的无用小空闲区，导致内存的浪费 最差适配 (worst fit) 算法是 best fit 的改进，总是分配最大的空用空闲区，使新的空闲区保持较大的程度，方便以后使用。但这并不是一个好的方法 如果将空闲区链表与进程链表分开实现，可以增加分配内存的速度，但内存释放速度会变慢。但是可以将空闲区由小到大排序，以此提高 best fit 的性能，但 next fit 将变得毫无意义。另一个优化是，可以不使用空闲区链表，在每个空闲区的第一个字存储空闲区大小，第二个字指向下一空闲区，这样将大幅降低存储管理器的内存使用。 快速适配 (quick fit) 算法为那些常用大小的空闲区维护单独的链表。比如一个 N 项表中，第一项是指向大小 4KB 的空闲区链表表头指针，第二项是指向大小 8KB 的空闲区链表表头指针，以此类推。而 21 KB 的空闲区，可以放在专门的大小比较特别的空闲区链表中。该算法在搜索指定大小的空闲区时十分快速，但进程终止或换出时，寻找相邻空闲区并合并是十分费时的。如果不进行合并，内存将很快分裂出大量无用的小空闲区。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:2:3","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#空闲内存管理"},{"categories":["OperatingSystem"],"content":" 空闲内存管理在动态分配内存时，操作系统必须对其进行管理。一般而言，有两种管理方法 位图 与 空闲区链表。 使用位图的存储管理该方法中，内存可能被划分成小到几个字或大到几千字的分配单元，每个分配单元对应位图中的一位。 分配单元的大小是一个重要设计因素：分配单元越小，位图越大。然而即使只有 4 个字节大小的分配单元，32 位的内存需要位图中的 1 位，因此位图占用了 \\(1 / 32\\) 的内存。如果选择较大的分配单元，若进程的大小不是分配单元的整数倍，将极易浪费内存。 使用链表的存储管理维护一个记录已分配内存段和空闲内存段的链表，其中每个结点包含域：空闲区 (H) 或进程 (P) 的指示标志、起始地址、长度和指向下一个结点的指针。进程表中表示终止进程的结点通常含有指向对应于其段链表结点的指针，因此段链表使用双向链表更加方便，易于找到上一个结点并检查是否可以合并。 当使用地址顺序在链表中存放进程和空闲区时，有几种算法可以用来为进程交换分配内存。假设存储管理器知道进程需要多少内存。 最简单的方法即 首次适配 (first fit) 算法，存储器沿着段链表搜索，直到找到足够大的空闲区为止 下次适配 (next fit) 算法是通过 first fit 修改而来，在找到足够大的空闲区时， next fit 会记录下当前位置，供下次搜索使用 最佳适配 (best fit) 算法搜索整个链表，找出能够容纳进程的最小空闲区。best fit 以最好地匹配请求可用空闲区，而不是拆分一个以后可能用到的大空闲区。但是遗憾的是，best fit 将会产生大量的无用小空闲区，导致内存的浪费 最差适配 (worst fit) 算法是 best fit 的改进，总是分配最大的空用空闲区，使新的空闲区保持较大的程度，方便以后使用。但这并不是一个好的方法 如果将空闲区链表与进程链表分开实现，可以增加分配内存的速度，但内存释放速度会变慢。但是可以将空闲区由小到大排序，以此提高 best fit 的性能，但 next fit 将变得毫无意义。另一个优化是，可以不使用空闲区链表，在每个空闲区的第一个字存储空闲区大小，第二个字指向下一空闲区，这样将大幅降低存储管理器的内存使用。 快速适配 (quick fit) 算法为那些常用大小的空闲区维护单独的链表。比如一个 N 项表中，第一项是指向大小 4KB 的空闲区链表表头指针，第二项是指向大小 8KB 的空闲区链表表头指针，以此类推。而 21 KB 的空闲区，可以放在专门的大小比较特别的空闲区链表中。该算法在搜索指定大小的空闲区时十分快速，但进程终止或换出时，寻找相邻空闲区并合并是十分费时的。如果不进行合并，内存将很快分裂出大量无用的小空闲区。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:2:3","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#使用位图的存储管理"},{"categories":["OperatingSystem"],"content":" 空闲内存管理在动态分配内存时，操作系统必须对其进行管理。一般而言，有两种管理方法 位图 与 空闲区链表。 使用位图的存储管理该方法中，内存可能被划分成小到几个字或大到几千字的分配单元，每个分配单元对应位图中的一位。 分配单元的大小是一个重要设计因素：分配单元越小，位图越大。然而即使只有 4 个字节大小的分配单元，32 位的内存需要位图中的 1 位，因此位图占用了 \\(1 / 32\\) 的内存。如果选择较大的分配单元，若进程的大小不是分配单元的整数倍，将极易浪费内存。 使用链表的存储管理维护一个记录已分配内存段和空闲内存段的链表，其中每个结点包含域：空闲区 (H) 或进程 (P) 的指示标志、起始地址、长度和指向下一个结点的指针。进程表中表示终止进程的结点通常含有指向对应于其段链表结点的指针，因此段链表使用双向链表更加方便，易于找到上一个结点并检查是否可以合并。 当使用地址顺序在链表中存放进程和空闲区时，有几种算法可以用来为进程交换分配内存。假设存储管理器知道进程需要多少内存。 最简单的方法即 首次适配 (first fit) 算法，存储器沿着段链表搜索，直到找到足够大的空闲区为止 下次适配 (next fit) 算法是通过 first fit 修改而来，在找到足够大的空闲区时， next fit 会记录下当前位置，供下次搜索使用 最佳适配 (best fit) 算法搜索整个链表，找出能够容纳进程的最小空闲区。best fit 以最好地匹配请求可用空闲区，而不是拆分一个以后可能用到的大空闲区。但是遗憾的是，best fit 将会产生大量的无用小空闲区，导致内存的浪费 最差适配 (worst fit) 算法是 best fit 的改进，总是分配最大的空用空闲区，使新的空闲区保持较大的程度，方便以后使用。但这并不是一个好的方法 如果将空闲区链表与进程链表分开实现，可以增加分配内存的速度，但内存释放速度会变慢。但是可以将空闲区由小到大排序，以此提高 best fit 的性能，但 next fit 将变得毫无意义。另一个优化是，可以不使用空闲区链表，在每个空闲区的第一个字存储空闲区大小，第二个字指向下一空闲区，这样将大幅降低存储管理器的内存使用。 快速适配 (quick fit) 算法为那些常用大小的空闲区维护单独的链表。比如一个 N 项表中，第一项是指向大小 4KB 的空闲区链表表头指针，第二项是指向大小 8KB 的空闲区链表表头指针，以此类推。而 21 KB 的空闲区，可以放在专门的大小比较特别的空闲区链表中。该算法在搜索指定大小的空闲区时十分快速，但进程终止或换出时，寻找相邻空闲区并合并是十分费时的。如果不进行合并，内存将很快分裂出大量无用的小空闲区。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:2:3","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#使用链表的存储管理"},{"categories":["OperatingSystem"],"content":" 虚拟内存软件对存储器容量的需求增长极快，需要运行的程序往往大到 RAM 无法容纳，而系统必须支持多个程序同时运行，即使 RAM 可以满足一个软件的内存需求，但需求总和必然超出了 RAM 大小。在 20 世纪 60 年代采用的解决方法是：把程序分割成许多片段，称为 覆盖 (overlay)。overlay 块存放于磁盘上，在需要时由操作系统动态地换入换出。虽然系统可以自动的进行换入换出工作，但还需要程序员主动将程序分割为多个片段。将一个大工程分割为小的、模块化的片段是费时且枯燥的，并且极易出错。在之后提出了一个可以由系统自动进行的方法，被称为 虚拟内存 (virtual memory)。 VM 的基本思想是：每个进程拥有自己的地址空间，这个空间被分割为多个块，每个块称作 页面 (page) 或一页。page 拥有连续的地址范围，被映射到物理内存，但并不是所有 page 都必须在内存中才能运行程序。当程序引用到一部分在物理内存中的地址空间时，由硬件立刻执行必要的映射；而引用到不在物理内存中的地址空间，由操作系统负责将缺失的部分装入物理内存用重新执行失败的指令。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:0","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#虚拟内存"},{"categories":["OperatingSystem"],"content":" 分页大多 VM 系统中都使用一种称为 分页 (paging) 技术。地址可以通过索引、基址寄存器、段寄存器或其他方式产生。由程序产生的这些地址称为 虚拟地址 (virtual address)，它们构成可一个 虚拟地址空间 (virtual address space)。在没有 VM 的计算机上，系统直接将虚拟地址送到内存总线上，读写操作使用具有同样地址的物理内存字；存在 VM 的情况下，虚拟地址被送往 内存管理单元 (Memory Management Unit, MMU) 中，MMU 将虚拟地址转换为物理地址。 虚拟空间地址按照固定的大小划分为 page 的若干单元，在物理内存中对应的单元被称为 * 页框* (page frame)，page 与 frame 的大小通常一样。page 的大小可以是 512B 到 1GB 不等，RAM 与磁盘的交换总是以整个 page 为单位交换的，在某些处理器上根据操作系统认为合适的方式，支持对不同大小 page 的混合使用与匹配。 由于虚拟内存大于物理内存，因此硬件会使用一个标志位用于记录 page 是否存在于 RAM 中。当 MMU 注意到访问的虚拟内存不存在于 RAM 中，将会使 CPU 陷入到内核中，这个陷阱称为 缺页中断 (page fault)。操作系统根据一定的策略将一个页面换出 RAM，并将需要访问的页面换入页框中，修改映射关系并重新启动引起中断的指令。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:1","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#分页"},{"categories":["OperatingSystem"],"content":" 页表页表用于虚拟地址与物理内存之间的索引，以得出应用于该虚拟页面的页框号。虚拟地址被分为高位部分的虚拟页号和低位部分的偏移量，对于 16 位地址和 4KB 的 page，高四位可以指定 16 个虚拟页面中的 page，而低 12 位可以确定 page 内的字节偏移量。由页表项找到页框号，然后把页框号拼接到偏移量的高位端，以替换掉虚拟页号，形成送往内存的物理地址。 页表项的结果与机器紧密相关，但存储的信息大致相同，示例如下图所示。保护 (protection) 域指出 page 允许什么类型的访问，最简单的 protection 只有一位，其值一般为 0 (可读写) 和 1 (只读)；更复杂的情况是三位 protection，各位分别对应了读、写、执行标识。修改 (modified) 域与 访问 (referenced) 域记录 page 的使用情况，在写入 page 时由硬件自动设置 modified，该 field 在在操作系统重新分配页框时是非常有用的：如果 page 被修改过 (即脏内存) 则必须写回磁盘，而没有被修改过的 page 可以直接丢弃。modified 又是也被称为 脏位 (dirty bit)。无论读写系统都会在 page 被访问时都会设置 referenced，其会在系统发生缺页中断时帮助管理策略淘汰无用 page。最后一位是用于禁用高速缓存的 field，对于映射到设备寄存器而非 RAM 的页面十分重要。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:2","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#页表"},{"categories":["OperatingSystem"],"content":" 加速分页过程在任何分页系统中，都需要考虑两个问题： 虚拟地址到物理地址的映射必须非常快 如果虚拟地址空间很大，那么页表也会很大 最简单的方法是使用完整的硬件页表，无需访问内存，但代价高昂，且进程切换时需要替换整个页表。另一种方法是使用 RAM 存储页表，仅使用页表寄存器指向内存页表的表头，在进程切换时仅需加载新的表头地址，但每条指令都需要访问 RAM 来读取对应页框。 大多数程序总是对少量 page 进行大量访问，而不是相反。因此可以设置一个小型硬件设备，将虚拟地址直接映射到物理地址，不再访问页表。这种设备被称为 转换检测缓冲区 (Translation Lookaside Buffer, TLB)，或称为 相联存储器 (associate memory) 或 快表。TLB 通常直接置于 MMU 中，包含少量的表项，每个表项记录了一个 page 的相关信息，这些表项与页表中的表项类似。 当一个虚拟地址输入 MMU 时，硬件先将虚拟页号与 TLB 中的表项进行并行匹配，判断是否存在。若存在有效的匹配，进行的操作不违反 protection，则将页框号直接输出而不需要访问页表；当违反 protection 操作时，与对页表进行非法操作一样；当虚拟页号不再 TLB 时，就会进行页表的查询，并使用一个表项替换这个页表。 使用软件实现 TLB 时，需要处理失效问题。当 page 的访问在内存而不在 TLB 中被称为 软失效 (soft miss)，此时只需要更新 TLB 而不用产生磁盘 I/O。如果本身不在内存中，则产生 硬失效，此时需要从磁盘中装入 page。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:3","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#加速分页过程"},{"categories":["OperatingSystem"],"content":" 对大内存的页表采用多级页表，可以轻松将大内存的页表拆分存储，避免全部页表一直保存在内存中，多级页表的级数越多其灵活性越强。 倒排页表 (inverted page table) 是为解决多级页表层级不断增长的一种解决方法，主要在 PowerPC 中使用，将物理内存中的页框对应一个表项而不再是虚拟页号。表项记录了哪个进程、虚拟页面对定位于该页框中。虽然极大减小了空间，但使虚拟地址到物理地址的转换变得困难。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:4","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#对大内存的页表"},{"categories":["OperatingSystem"],"content":" 页面置换算法当发生缺页中断时，操作系统必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。如果要换出的页面在内存驻留期间已经被修改过，就必须把它写回磁盘已更新该页面在磁盘上的副本；如果没有被修改过，那将直接调入页面覆盖被淘汰。 最优页面置换算法有些页面在内存中将会很快被用到，而一些将会很长时间之后被用到，每个页面都可用该页面将被访问前所需要执行的指令数作为标记，在缺页中断发生时将标记最大的页面换出。 最优置换算法 (OPT) 唯一的问题是其无法实现，缺页中断发生时，操作系统无法得知各个页面下一次被访问的时间。唯一的解决方法是仿真程序上，第一次运行用以跟踪页面访问情况，之后运行中利用第一次的结果实现最优页面置换算法。 最近未使用页面置换算法为使操作系统能够收集有用的统计信息，会为每个 page 设置两个状态位 modified 与 referenced，每次访问内存时将由硬件直接设置。如果硬件没有这两个状态位，则会使用缺页中断与时钟中断模拟：启动进程时将所有 page 标记为不在内存中；当访问任何一个页面时都会引发一次缺页中断，此时操作系统可以设置 referenced (由操作系统实现的内部表)，修改页表项使其指向正确的页面并设置为只读模式，然后重新启动引起缺页中断的指令；如果随后对该页面的修改又引发一次缺页中断，则操作系统设置这个页面的 modified 并将其改为读写模式。 可以利用 modified 与 referenced 构造一个简单的置换算法：referenced 被定期地清零，以标记这是一个最近没有被访问的 page。由此置换算法将 page 分为了四类 没有被访问过，没有被修改 没有被访问过，已被修改 已被访问，没有被修改 已被访问，已被修改 最近未被使用 (Not Recently Used, NRU) 易于理解和能够有效地实现实现。NRU 在淘汰 page 时，淘汰一个第二类 page 可能比第一类 page 要好一些。 先进先出页面置换算法先进先出 (First-In First-Out, FIFO) 算法类似于队列的实现，当一个 page 被换入的时候，加入到 page 对队尾，当需要换出一个页面时将队首的页面换出。FIFO 有一个显而易见的问题，一个常用的 page 到达队首时将被换出 RAM，不久之后又会产生缺页中断将其换入 RAM，因此很少使用纯粹的 FIFO 算法。 第二次机会 (Second Chance) 算法是对 FIFO 的一种优化，防止常用 page 被换出 RAM。第二次机会算法检查队首页面的 referenced 标志，如果标志是 0 那么这个 page 是最先进入 RAM 且没有被使用的，那么应该被换出 RAM；如果是 1 则进行清零，并将 page 加入到队尾。第二次机会算法即寻找一个在最近的时钟间隔内没有被访问过的页面。如果所有页面都被访问过，那该算法将退化为 FIFO 算法。 第二次机会尽管比较合理，但需要频繁在链表中移动 page，既降低了效率又不是很有必要。一个更好的方法是将链表改为循环链表，即可变为 时钟 (Clock) 页面置换算法。时钟算法与第二次机会算法一样，检查当前结点 page 的 referenced 标志，如果标识是 0 时将从链表中移除当前结点，如果是 1 时则清零标志。时钟算法不再需要实现结点 page 在链表中的移动，只需要移除或者清零标志位就行。 最近最少使用页面置换算法基于对软件指令执行的观察，在前面几条指令频繁使用的页面很可能在后面的几条指令中被使用。反过来说，已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。这样可以实现一个方法，在缺页中断时置换未使用时间最长的页面，这个策略称为 最近最少使用 (Least Recently Used, LRU) 页面置换算法。 虽然 LRU 理论上可以实现，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有 page 的链表，最近最多使用的 page 在表头，最近最少使用的 page 在表尾。困难的是在每次访问内存时都必须要更新整个链表，在链表中找到一个 page 并删除，然后把它移动到表头是十分费时的操作。 一种软件的方案被称为 最不常用 (Least Frequently Used, LFU) 算法，也被称为 NFU (Not Frequently Used)，该算法将每个 page 与一个软件计数器相关联，初始值为 0，当时钟中断时由 OS 扫描所有 page 并将每个 page 的 referenced 加到对应的计数器上。这个计数器跟踪了每个页面被访问的频繁程度，发生缺页中断时则置换计数器值最小的 page。 LFU 从来不会忘记任何事情，但是幸运的是将 LFU 做些小小的修改即可模拟 LRU 算法：首先将 referenced 被加到计数器前先将计数器右移一位，然后将 referenced 加到计数器的最左端而不是最右端。这种算法被成为 老化 (aging) 算法，aging 会使 LFU 忘记一些 page 的计数从而使计数器为 0，这些为 0 的 page 将是被换出 RAM 的被选项，以此模拟了 最近 的限制。 LFU 模拟的一个问题是，aging 不知道在两次时钟中断之间，如果有两个 page 被访问，将不得知哪个 page 被先访问。另一个问题，计数器位数将会限制对页面淘汰的策略，如果 8 位计数器能记录 8 个时钟中断内的情况，如果 page1 在 9 个时钟中断前被访问过，page2 在 1000 个时钟中断前被访问过，那么淘汰时将会从这两个 page 中随机选取一个淘汰，因为它们的计数器都为 0。 页面置换算法小结 算法 注释 OPT (最优) 不可实现，但可作为性能基准 NRU (最近未被访问) LRU 的很粗糙近似 FIFO (先进先出) 可能换出重要页面 第二次机会算法 防止重要页面被换出 时钟算法 使用循环链表，减少结点修改 LRU (最近最少使用) 优秀但难实现的算法 LFU (最不常用) LRU 的相对粗略近似 LFU aging 非常近似 LRU 的有效算法 我们可以利用缺页中断的次数比总页面调度次数，得出缺页率，缺页率可以简单直观地衡量一个置换算法的好坏。其中最初 n 个空物理块的页面调用也算作缺页。 假设现在有分配的物理块 4 个，页面 4,3,2,1,4,3,5,4,3,2,1,5，那么以 OPT、FIFO、 LFU 为例 OPT 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 4 4 4 4 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 1 1 1 5 5 5 5 5 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) 缺页次数 6，总访问次数 12，缺页率 \\(\\frac{1}{2}\\) FIFO 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 3 2 1 5 4 3 2 3 3 3 3 3 2 1 5 4 3 2 3 2 2 2 2 1 5 4 3 2 1 4 1 1 1 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) 缺页次数 10，总访问次数 12，缺页率 \\(\\frac{5}{6}\\) LFU 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 3 2 1 1 1 5 4 3 2 3 3 3 2 1 4 3 5 4 3 2 3 2 2 1 4 3 5 4 3 2 1 4 1 4 3 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) LFU 算法可以使用链表实现，将刚刚访问过的结点置于表尾，表头即是将被淘汰的 page，这与上面所介绍到计数器的方案有所不同。缺页次数 8，总访问次数 12，缺页率 \\(\\frac{2}{3}\\) ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:5","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#页面置换算法"},{"categories":["OperatingSystem"],"content":" 页面置换算法当发生缺页中断时，操作系统必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。如果要换出的页面在内存驻留期间已经被修改过，就必须把它写回磁盘已更新该页面在磁盘上的副本；如果没有被修改过，那将直接调入页面覆盖被淘汰。 最优页面置换算法有些页面在内存中将会很快被用到，而一些将会很长时间之后被用到，每个页面都可用该页面将被访问前所需要执行的指令数作为标记，在缺页中断发生时将标记最大的页面换出。 最优置换算法 (OPT) 唯一的问题是其无法实现，缺页中断发生时，操作系统无法得知各个页面下一次被访问的时间。唯一的解决方法是仿真程序上，第一次运行用以跟踪页面访问情况，之后运行中利用第一次的结果实现最优页面置换算法。 最近未使用页面置换算法为使操作系统能够收集有用的统计信息，会为每个 page 设置两个状态位 modified 与 referenced，每次访问内存时将由硬件直接设置。如果硬件没有这两个状态位，则会使用缺页中断与时钟中断模拟：启动进程时将所有 page 标记为不在内存中；当访问任何一个页面时都会引发一次缺页中断，此时操作系统可以设置 referenced (由操作系统实现的内部表)，修改页表项使其指向正确的页面并设置为只读模式，然后重新启动引起缺页中断的指令；如果随后对该页面的修改又引发一次缺页中断，则操作系统设置这个页面的 modified 并将其改为读写模式。 可以利用 modified 与 referenced 构造一个简单的置换算法：referenced 被定期地清零，以标记这是一个最近没有被访问的 page。由此置换算法将 page 分为了四类 没有被访问过，没有被修改 没有被访问过，已被修改 已被访问，没有被修改 已被访问，已被修改 最近未被使用 (Not Recently Used, NRU) 易于理解和能够有效地实现实现。NRU 在淘汰 page 时，淘汰一个第二类 page 可能比第一类 page 要好一些。 先进先出页面置换算法先进先出 (First-In First-Out, FIFO) 算法类似于队列的实现，当一个 page 被换入的时候，加入到 page 对队尾，当需要换出一个页面时将队首的页面换出。FIFO 有一个显而易见的问题，一个常用的 page 到达队首时将被换出 RAM，不久之后又会产生缺页中断将其换入 RAM，因此很少使用纯粹的 FIFO 算法。 第二次机会 (Second Chance) 算法是对 FIFO 的一种优化，防止常用 page 被换出 RAM。第二次机会算法检查队首页面的 referenced 标志，如果标志是 0 那么这个 page 是最先进入 RAM 且没有被使用的，那么应该被换出 RAM；如果是 1 则进行清零，并将 page 加入到队尾。第二次机会算法即寻找一个在最近的时钟间隔内没有被访问过的页面。如果所有页面都被访问过，那该算法将退化为 FIFO 算法。 第二次机会尽管比较合理，但需要频繁在链表中移动 page，既降低了效率又不是很有必要。一个更好的方法是将链表改为循环链表，即可变为 时钟 (Clock) 页面置换算法。时钟算法与第二次机会算法一样，检查当前结点 page 的 referenced 标志，如果标识是 0 时将从链表中移除当前结点，如果是 1 时则清零标志。时钟算法不再需要实现结点 page 在链表中的移动，只需要移除或者清零标志位就行。 最近最少使用页面置换算法基于对软件指令执行的观察，在前面几条指令频繁使用的页面很可能在后面的几条指令中被使用。反过来说，已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。这样可以实现一个方法，在缺页中断时置换未使用时间最长的页面，这个策略称为 最近最少使用 (Least Recently Used, LRU) 页面置换算法。 虽然 LRU 理论上可以实现，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有 page 的链表，最近最多使用的 page 在表头，最近最少使用的 page 在表尾。困难的是在每次访问内存时都必须要更新整个链表，在链表中找到一个 page 并删除，然后把它移动到表头是十分费时的操作。 一种软件的方案被称为 最不常用 (Least Frequently Used, LFU) 算法，也被称为 NFU (Not Frequently Used)，该算法将每个 page 与一个软件计数器相关联，初始值为 0，当时钟中断时由 OS 扫描所有 page 并将每个 page 的 referenced 加到对应的计数器上。这个计数器跟踪了每个页面被访问的频繁程度，发生缺页中断时则置换计数器值最小的 page。 LFU 从来不会忘记任何事情，但是幸运的是将 LFU 做些小小的修改即可模拟 LRU 算法：首先将 referenced 被加到计数器前先将计数器右移一位，然后将 referenced 加到计数器的最左端而不是最右端。这种算法被成为 老化 (aging) 算法，aging 会使 LFU 忘记一些 page 的计数从而使计数器为 0，这些为 0 的 page 将是被换出 RAM 的被选项，以此模拟了 最近 的限制。 LFU 模拟的一个问题是，aging 不知道在两次时钟中断之间，如果有两个 page 被访问，将不得知哪个 page 被先访问。另一个问题，计数器位数将会限制对页面淘汰的策略，如果 8 位计数器能记录 8 个时钟中断内的情况，如果 page1 在 9 个时钟中断前被访问过，page2 在 1000 个时钟中断前被访问过，那么淘汰时将会从这两个 page 中随机选取一个淘汰，因为它们的计数器都为 0。 页面置换算法小结 算法 注释 OPT (最优) 不可实现，但可作为性能基准 NRU (最近未被访问) LRU 的很粗糙近似 FIFO (先进先出) 可能换出重要页面 第二次机会算法 防止重要页面被换出 时钟算法 使用循环链表，减少结点修改 LRU (最近最少使用) 优秀但难实现的算法 LFU (最不常用) LRU 的相对粗略近似 LFU aging 非常近似 LRU 的有效算法 我们可以利用缺页中断的次数比总页面调度次数，得出缺页率，缺页率可以简单直观地衡量一个置换算法的好坏。其中最初 n 个空物理块的页面调用也算作缺页。 假设现在有分配的物理块 4 个，页面 4,3,2,1,4,3,5,4,3,2,1,5，那么以 OPT、FIFO、 LFU 为例 OPT 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 4 4 4 4 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 1 1 1 5 5 5 5 5 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) 缺页次数 6，总访问次数 12，缺页率 \\(\\frac{1}{2}\\) FIFO 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 3 2 1 5 4 3 2 3 3 3 3 3 2 1 5 4 3 2 3 2 2 2 2 1 5 4 3 2 1 4 1 1 1 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) 缺页次数 10，总访问次数 12，缺页率 \\(\\frac{5}{6}\\) LFU 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 3 2 1 1 1 5 4 3 2 3 3 3 2 1 4 3 5 4 3 2 3 2 2 1 4 3 5 4 3 2 1 4 1 4 3 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) LFU 算法可以使用链表实现，将刚刚访问过的结点置于表尾，表头即是将被淘汰的 page，这与上面所介绍到计数器的方案有所不同。缺页次数 8，总访问次数 12，缺页率 \\(\\frac{2}{3}\\) ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:5","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#最优页面置换算法"},{"categories":["OperatingSystem"],"content":" 页面置换算法当发生缺页中断时，操作系统必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。如果要换出的页面在内存驻留期间已经被修改过，就必须把它写回磁盘已更新该页面在磁盘上的副本；如果没有被修改过，那将直接调入页面覆盖被淘汰。 最优页面置换算法有些页面在内存中将会很快被用到，而一些将会很长时间之后被用到，每个页面都可用该页面将被访问前所需要执行的指令数作为标记，在缺页中断发生时将标记最大的页面换出。 最优置换算法 (OPT) 唯一的问题是其无法实现，缺页中断发生时，操作系统无法得知各个页面下一次被访问的时间。唯一的解决方法是仿真程序上，第一次运行用以跟踪页面访问情况，之后运行中利用第一次的结果实现最优页面置换算法。 最近未使用页面置换算法为使操作系统能够收集有用的统计信息，会为每个 page 设置两个状态位 modified 与 referenced，每次访问内存时将由硬件直接设置。如果硬件没有这两个状态位，则会使用缺页中断与时钟中断模拟：启动进程时将所有 page 标记为不在内存中；当访问任何一个页面时都会引发一次缺页中断，此时操作系统可以设置 referenced (由操作系统实现的内部表)，修改页表项使其指向正确的页面并设置为只读模式，然后重新启动引起缺页中断的指令；如果随后对该页面的修改又引发一次缺页中断，则操作系统设置这个页面的 modified 并将其改为读写模式。 可以利用 modified 与 referenced 构造一个简单的置换算法：referenced 被定期地清零，以标记这是一个最近没有被访问的 page。由此置换算法将 page 分为了四类 没有被访问过，没有被修改 没有被访问过，已被修改 已被访问，没有被修改 已被访问，已被修改 最近未被使用 (Not Recently Used, NRU) 易于理解和能够有效地实现实现。NRU 在淘汰 page 时，淘汰一个第二类 page 可能比第一类 page 要好一些。 先进先出页面置换算法先进先出 (First-In First-Out, FIFO) 算法类似于队列的实现，当一个 page 被换入的时候，加入到 page 对队尾，当需要换出一个页面时将队首的页面换出。FIFO 有一个显而易见的问题，一个常用的 page 到达队首时将被换出 RAM，不久之后又会产生缺页中断将其换入 RAM，因此很少使用纯粹的 FIFO 算法。 第二次机会 (Second Chance) 算法是对 FIFO 的一种优化，防止常用 page 被换出 RAM。第二次机会算法检查队首页面的 referenced 标志，如果标志是 0 那么这个 page 是最先进入 RAM 且没有被使用的，那么应该被换出 RAM；如果是 1 则进行清零，并将 page 加入到队尾。第二次机会算法即寻找一个在最近的时钟间隔内没有被访问过的页面。如果所有页面都被访问过，那该算法将退化为 FIFO 算法。 第二次机会尽管比较合理，但需要频繁在链表中移动 page，既降低了效率又不是很有必要。一个更好的方法是将链表改为循环链表，即可变为 时钟 (Clock) 页面置换算法。时钟算法与第二次机会算法一样，检查当前结点 page 的 referenced 标志，如果标识是 0 时将从链表中移除当前结点，如果是 1 时则清零标志。时钟算法不再需要实现结点 page 在链表中的移动，只需要移除或者清零标志位就行。 最近最少使用页面置换算法基于对软件指令执行的观察，在前面几条指令频繁使用的页面很可能在后面的几条指令中被使用。反过来说，已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。这样可以实现一个方法，在缺页中断时置换未使用时间最长的页面，这个策略称为 最近最少使用 (Least Recently Used, LRU) 页面置换算法。 虽然 LRU 理论上可以实现，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有 page 的链表，最近最多使用的 page 在表头，最近最少使用的 page 在表尾。困难的是在每次访问内存时都必须要更新整个链表，在链表中找到一个 page 并删除，然后把它移动到表头是十分费时的操作。 一种软件的方案被称为 最不常用 (Least Frequently Used, LFU) 算法，也被称为 NFU (Not Frequently Used)，该算法将每个 page 与一个软件计数器相关联，初始值为 0，当时钟中断时由 OS 扫描所有 page 并将每个 page 的 referenced 加到对应的计数器上。这个计数器跟踪了每个页面被访问的频繁程度，发生缺页中断时则置换计数器值最小的 page。 LFU 从来不会忘记任何事情，但是幸运的是将 LFU 做些小小的修改即可模拟 LRU 算法：首先将 referenced 被加到计数器前先将计数器右移一位，然后将 referenced 加到计数器的最左端而不是最右端。这种算法被成为 老化 (aging) 算法，aging 会使 LFU 忘记一些 page 的计数从而使计数器为 0，这些为 0 的 page 将是被换出 RAM 的被选项，以此模拟了 最近 的限制。 LFU 模拟的一个问题是，aging 不知道在两次时钟中断之间，如果有两个 page 被访问，将不得知哪个 page 被先访问。另一个问题，计数器位数将会限制对页面淘汰的策略，如果 8 位计数器能记录 8 个时钟中断内的情况，如果 page1 在 9 个时钟中断前被访问过，page2 在 1000 个时钟中断前被访问过，那么淘汰时将会从这两个 page 中随机选取一个淘汰，因为它们的计数器都为 0。 页面置换算法小结 算法 注释 OPT (最优) 不可实现，但可作为性能基准 NRU (最近未被访问) LRU 的很粗糙近似 FIFO (先进先出) 可能换出重要页面 第二次机会算法 防止重要页面被换出 时钟算法 使用循环链表，减少结点修改 LRU (最近最少使用) 优秀但难实现的算法 LFU (最不常用) LRU 的相对粗略近似 LFU aging 非常近似 LRU 的有效算法 我们可以利用缺页中断的次数比总页面调度次数，得出缺页率，缺页率可以简单直观地衡量一个置换算法的好坏。其中最初 n 个空物理块的页面调用也算作缺页。 假设现在有分配的物理块 4 个，页面 4,3,2,1,4,3,5,4,3,2,1,5，那么以 OPT、FIFO、 LFU 为例 OPT 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 4 4 4 4 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 1 1 1 5 5 5 5 5 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) 缺页次数 6，总访问次数 12，缺页率 \\(\\frac{1}{2}\\) FIFO 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 3 2 1 5 4 3 2 3 3 3 3 3 2 1 5 4 3 2 3 2 2 2 2 1 5 4 3 2 1 4 1 1 1 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) 缺页次数 10，总访问次数 12，缺页率 \\(\\frac{5}{6}\\) LFU 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 3 2 1 1 1 5 4 3 2 3 3 3 2 1 4 3 5 4 3 2 3 2 2 1 4 3 5 4 3 2 1 4 1 4 3 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) LFU 算法可以使用链表实现，将刚刚访问过的结点置于表尾，表头即是将被淘汰的 page，这与上面所介绍到计数器的方案有所不同。缺页次数 8，总访问次数 12，缺页率 \\(\\frac{2}{3}\\) ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:5","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#最近未使用页面置换算法"},{"categories":["OperatingSystem"],"content":" 页面置换算法当发生缺页中断时，操作系统必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。如果要换出的页面在内存驻留期间已经被修改过，就必须把它写回磁盘已更新该页面在磁盘上的副本；如果没有被修改过，那将直接调入页面覆盖被淘汰。 最优页面置换算法有些页面在内存中将会很快被用到，而一些将会很长时间之后被用到，每个页面都可用该页面将被访问前所需要执行的指令数作为标记，在缺页中断发生时将标记最大的页面换出。 最优置换算法 (OPT) 唯一的问题是其无法实现，缺页中断发生时，操作系统无法得知各个页面下一次被访问的时间。唯一的解决方法是仿真程序上，第一次运行用以跟踪页面访问情况，之后运行中利用第一次的结果实现最优页面置换算法。 最近未使用页面置换算法为使操作系统能够收集有用的统计信息，会为每个 page 设置两个状态位 modified 与 referenced，每次访问内存时将由硬件直接设置。如果硬件没有这两个状态位，则会使用缺页中断与时钟中断模拟：启动进程时将所有 page 标记为不在内存中；当访问任何一个页面时都会引发一次缺页中断，此时操作系统可以设置 referenced (由操作系统实现的内部表)，修改页表项使其指向正确的页面并设置为只读模式，然后重新启动引起缺页中断的指令；如果随后对该页面的修改又引发一次缺页中断，则操作系统设置这个页面的 modified 并将其改为读写模式。 可以利用 modified 与 referenced 构造一个简单的置换算法：referenced 被定期地清零，以标记这是一个最近没有被访问的 page。由此置换算法将 page 分为了四类 没有被访问过，没有被修改 没有被访问过，已被修改 已被访问，没有被修改 已被访问，已被修改 最近未被使用 (Not Recently Used, NRU) 易于理解和能够有效地实现实现。NRU 在淘汰 page 时，淘汰一个第二类 page 可能比第一类 page 要好一些。 先进先出页面置换算法先进先出 (First-In First-Out, FIFO) 算法类似于队列的实现，当一个 page 被换入的时候，加入到 page 对队尾，当需要换出一个页面时将队首的页面换出。FIFO 有一个显而易见的问题，一个常用的 page 到达队首时将被换出 RAM，不久之后又会产生缺页中断将其换入 RAM，因此很少使用纯粹的 FIFO 算法。 第二次机会 (Second Chance) 算法是对 FIFO 的一种优化，防止常用 page 被换出 RAM。第二次机会算法检查队首页面的 referenced 标志，如果标志是 0 那么这个 page 是最先进入 RAM 且没有被使用的，那么应该被换出 RAM；如果是 1 则进行清零，并将 page 加入到队尾。第二次机会算法即寻找一个在最近的时钟间隔内没有被访问过的页面。如果所有页面都被访问过，那该算法将退化为 FIFO 算法。 第二次机会尽管比较合理，但需要频繁在链表中移动 page，既降低了效率又不是很有必要。一个更好的方法是将链表改为循环链表，即可变为 时钟 (Clock) 页面置换算法。时钟算法与第二次机会算法一样，检查当前结点 page 的 referenced 标志，如果标识是 0 时将从链表中移除当前结点，如果是 1 时则清零标志。时钟算法不再需要实现结点 page 在链表中的移动，只需要移除或者清零标志位就行。 最近最少使用页面置换算法基于对软件指令执行的观察，在前面几条指令频繁使用的页面很可能在后面的几条指令中被使用。反过来说，已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。这样可以实现一个方法，在缺页中断时置换未使用时间最长的页面，这个策略称为 最近最少使用 (Least Recently Used, LRU) 页面置换算法。 虽然 LRU 理论上可以实现，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有 page 的链表，最近最多使用的 page 在表头，最近最少使用的 page 在表尾。困难的是在每次访问内存时都必须要更新整个链表，在链表中找到一个 page 并删除，然后把它移动到表头是十分费时的操作。 一种软件的方案被称为 最不常用 (Least Frequently Used, LFU) 算法，也被称为 NFU (Not Frequently Used)，该算法将每个 page 与一个软件计数器相关联，初始值为 0，当时钟中断时由 OS 扫描所有 page 并将每个 page 的 referenced 加到对应的计数器上。这个计数器跟踪了每个页面被访问的频繁程度，发生缺页中断时则置换计数器值最小的 page。 LFU 从来不会忘记任何事情，但是幸运的是将 LFU 做些小小的修改即可模拟 LRU 算法：首先将 referenced 被加到计数器前先将计数器右移一位，然后将 referenced 加到计数器的最左端而不是最右端。这种算法被成为 老化 (aging) 算法，aging 会使 LFU 忘记一些 page 的计数从而使计数器为 0，这些为 0 的 page 将是被换出 RAM 的被选项，以此模拟了 最近 的限制。 LFU 模拟的一个问题是，aging 不知道在两次时钟中断之间，如果有两个 page 被访问，将不得知哪个 page 被先访问。另一个问题，计数器位数将会限制对页面淘汰的策略，如果 8 位计数器能记录 8 个时钟中断内的情况，如果 page1 在 9 个时钟中断前被访问过，page2 在 1000 个时钟中断前被访问过，那么淘汰时将会从这两个 page 中随机选取一个淘汰，因为它们的计数器都为 0。 页面置换算法小结 算法 注释 OPT (最优) 不可实现，但可作为性能基准 NRU (最近未被访问) LRU 的很粗糙近似 FIFO (先进先出) 可能换出重要页面 第二次机会算法 防止重要页面被换出 时钟算法 使用循环链表，减少结点修改 LRU (最近最少使用) 优秀但难实现的算法 LFU (最不常用) LRU 的相对粗略近似 LFU aging 非常近似 LRU 的有效算法 我们可以利用缺页中断的次数比总页面调度次数，得出缺页率，缺页率可以简单直观地衡量一个置换算法的好坏。其中最初 n 个空物理块的页面调用也算作缺页。 假设现在有分配的物理块 4 个，页面 4,3,2,1,4,3,5,4,3,2,1,5，那么以 OPT、FIFO、 LFU 为例 OPT 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 4 4 4 4 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 1 1 1 5 5 5 5 5 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) 缺页次数 6，总访问次数 12，缺页率 \\(\\frac{1}{2}\\) FIFO 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 3 2 1 5 4 3 2 3 3 3 3 3 2 1 5 4 3 2 3 2 2 2 2 1 5 4 3 2 1 4 1 1 1 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) 缺页次数 10，总访问次数 12，缺页率 \\(\\frac{5}{6}\\) LFU 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 3 2 1 1 1 5 4 3 2 3 3 3 2 1 4 3 5 4 3 2 3 2 2 1 4 3 5 4 3 2 1 4 1 4 3 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) LFU 算法可以使用链表实现，将刚刚访问过的结点置于表尾，表头即是将被淘汰的 page，这与上面所介绍到计数器的方案有所不同。缺页次数 8，总访问次数 12，缺页率 \\(\\frac{2}{3}\\) ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:5","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#先进先出页面置换算法"},{"categories":["OperatingSystem"],"content":" 页面置换算法当发生缺页中断时，操作系统必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。如果要换出的页面在内存驻留期间已经被修改过，就必须把它写回磁盘已更新该页面在磁盘上的副本；如果没有被修改过，那将直接调入页面覆盖被淘汰。 最优页面置换算法有些页面在内存中将会很快被用到，而一些将会很长时间之后被用到，每个页面都可用该页面将被访问前所需要执行的指令数作为标记，在缺页中断发生时将标记最大的页面换出。 最优置换算法 (OPT) 唯一的问题是其无法实现，缺页中断发生时，操作系统无法得知各个页面下一次被访问的时间。唯一的解决方法是仿真程序上，第一次运行用以跟踪页面访问情况，之后运行中利用第一次的结果实现最优页面置换算法。 最近未使用页面置换算法为使操作系统能够收集有用的统计信息，会为每个 page 设置两个状态位 modified 与 referenced，每次访问内存时将由硬件直接设置。如果硬件没有这两个状态位，则会使用缺页中断与时钟中断模拟：启动进程时将所有 page 标记为不在内存中；当访问任何一个页面时都会引发一次缺页中断，此时操作系统可以设置 referenced (由操作系统实现的内部表)，修改页表项使其指向正确的页面并设置为只读模式，然后重新启动引起缺页中断的指令；如果随后对该页面的修改又引发一次缺页中断，则操作系统设置这个页面的 modified 并将其改为读写模式。 可以利用 modified 与 referenced 构造一个简单的置换算法：referenced 被定期地清零，以标记这是一个最近没有被访问的 page。由此置换算法将 page 分为了四类 没有被访问过，没有被修改 没有被访问过，已被修改 已被访问，没有被修改 已被访问，已被修改 最近未被使用 (Not Recently Used, NRU) 易于理解和能够有效地实现实现。NRU 在淘汰 page 时，淘汰一个第二类 page 可能比第一类 page 要好一些。 先进先出页面置换算法先进先出 (First-In First-Out, FIFO) 算法类似于队列的实现，当一个 page 被换入的时候，加入到 page 对队尾，当需要换出一个页面时将队首的页面换出。FIFO 有一个显而易见的问题，一个常用的 page 到达队首时将被换出 RAM，不久之后又会产生缺页中断将其换入 RAM，因此很少使用纯粹的 FIFO 算法。 第二次机会 (Second Chance) 算法是对 FIFO 的一种优化，防止常用 page 被换出 RAM。第二次机会算法检查队首页面的 referenced 标志，如果标志是 0 那么这个 page 是最先进入 RAM 且没有被使用的，那么应该被换出 RAM；如果是 1 则进行清零，并将 page 加入到队尾。第二次机会算法即寻找一个在最近的时钟间隔内没有被访问过的页面。如果所有页面都被访问过，那该算法将退化为 FIFO 算法。 第二次机会尽管比较合理，但需要频繁在链表中移动 page，既降低了效率又不是很有必要。一个更好的方法是将链表改为循环链表，即可变为 时钟 (Clock) 页面置换算法。时钟算法与第二次机会算法一样，检查当前结点 page 的 referenced 标志，如果标识是 0 时将从链表中移除当前结点，如果是 1 时则清零标志。时钟算法不再需要实现结点 page 在链表中的移动，只需要移除或者清零标志位就行。 最近最少使用页面置换算法基于对软件指令执行的观察，在前面几条指令频繁使用的页面很可能在后面的几条指令中被使用。反过来说，已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。这样可以实现一个方法，在缺页中断时置换未使用时间最长的页面，这个策略称为 最近最少使用 (Least Recently Used, LRU) 页面置换算法。 虽然 LRU 理论上可以实现，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有 page 的链表，最近最多使用的 page 在表头，最近最少使用的 page 在表尾。困难的是在每次访问内存时都必须要更新整个链表，在链表中找到一个 page 并删除，然后把它移动到表头是十分费时的操作。 一种软件的方案被称为 最不常用 (Least Frequently Used, LFU) 算法，也被称为 NFU (Not Frequently Used)，该算法将每个 page 与一个软件计数器相关联，初始值为 0，当时钟中断时由 OS 扫描所有 page 并将每个 page 的 referenced 加到对应的计数器上。这个计数器跟踪了每个页面被访问的频繁程度，发生缺页中断时则置换计数器值最小的 page。 LFU 从来不会忘记任何事情，但是幸运的是将 LFU 做些小小的修改即可模拟 LRU 算法：首先将 referenced 被加到计数器前先将计数器右移一位，然后将 referenced 加到计数器的最左端而不是最右端。这种算法被成为 老化 (aging) 算法，aging 会使 LFU 忘记一些 page 的计数从而使计数器为 0，这些为 0 的 page 将是被换出 RAM 的被选项，以此模拟了 最近 的限制。 LFU 模拟的一个问题是，aging 不知道在两次时钟中断之间，如果有两个 page 被访问，将不得知哪个 page 被先访问。另一个问题，计数器位数将会限制对页面淘汰的策略，如果 8 位计数器能记录 8 个时钟中断内的情况，如果 page1 在 9 个时钟中断前被访问过，page2 在 1000 个时钟中断前被访问过，那么淘汰时将会从这两个 page 中随机选取一个淘汰，因为它们的计数器都为 0。 页面置换算法小结 算法 注释 OPT (最优) 不可实现，但可作为性能基准 NRU (最近未被访问) LRU 的很粗糙近似 FIFO (先进先出) 可能换出重要页面 第二次机会算法 防止重要页面被换出 时钟算法 使用循环链表，减少结点修改 LRU (最近最少使用) 优秀但难实现的算法 LFU (最不常用) LRU 的相对粗略近似 LFU aging 非常近似 LRU 的有效算法 我们可以利用缺页中断的次数比总页面调度次数，得出缺页率，缺页率可以简单直观地衡量一个置换算法的好坏。其中最初 n 个空物理块的页面调用也算作缺页。 假设现在有分配的物理块 4 个，页面 4,3,2,1,4,3,5,4,3,2,1,5，那么以 OPT、FIFO、 LFU 为例 OPT 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 4 4 4 4 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 1 1 1 5 5 5 5 5 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) 缺页次数 6，总访问次数 12，缺页率 \\(\\frac{1}{2}\\) FIFO 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 3 2 1 5 4 3 2 3 3 3 3 3 2 1 5 4 3 2 3 2 2 2 2 1 5 4 3 2 1 4 1 1 1 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) 缺页次数 10，总访问次数 12，缺页率 \\(\\frac{5}{6}\\) LFU 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 3 2 1 1 1 5 4 3 2 3 3 3 2 1 4 3 5 4 3 2 3 2 2 1 4 3 5 4 3 2 1 4 1 4 3 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) LFU 算法可以使用链表实现，将刚刚访问过的结点置于表尾，表头即是将被淘汰的 page，这与上面所介绍到计数器的方案有所不同。缺页次数 8，总访问次数 12，缺页率 \\(\\frac{2}{3}\\) ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:5","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#最近最少使用页面置换算法"},{"categories":["OperatingSystem"],"content":" 页面置换算法当发生缺页中断时，操作系统必须在内存中选择一个页面将其换出内存，以便为即将调入的页面腾出空间。如果要换出的页面在内存驻留期间已经被修改过，就必须把它写回磁盘已更新该页面在磁盘上的副本；如果没有被修改过，那将直接调入页面覆盖被淘汰。 最优页面置换算法有些页面在内存中将会很快被用到，而一些将会很长时间之后被用到，每个页面都可用该页面将被访问前所需要执行的指令数作为标记，在缺页中断发生时将标记最大的页面换出。 最优置换算法 (OPT) 唯一的问题是其无法实现，缺页中断发生时，操作系统无法得知各个页面下一次被访问的时间。唯一的解决方法是仿真程序上，第一次运行用以跟踪页面访问情况，之后运行中利用第一次的结果实现最优页面置换算法。 最近未使用页面置换算法为使操作系统能够收集有用的统计信息，会为每个 page 设置两个状态位 modified 与 referenced，每次访问内存时将由硬件直接设置。如果硬件没有这两个状态位，则会使用缺页中断与时钟中断模拟：启动进程时将所有 page 标记为不在内存中；当访问任何一个页面时都会引发一次缺页中断，此时操作系统可以设置 referenced (由操作系统实现的内部表)，修改页表项使其指向正确的页面并设置为只读模式，然后重新启动引起缺页中断的指令；如果随后对该页面的修改又引发一次缺页中断，则操作系统设置这个页面的 modified 并将其改为读写模式。 可以利用 modified 与 referenced 构造一个简单的置换算法：referenced 被定期地清零，以标记这是一个最近没有被访问的 page。由此置换算法将 page 分为了四类 没有被访问过，没有被修改 没有被访问过，已被修改 已被访问，没有被修改 已被访问，已被修改 最近未被使用 (Not Recently Used, NRU) 易于理解和能够有效地实现实现。NRU 在淘汰 page 时，淘汰一个第二类 page 可能比第一类 page 要好一些。 先进先出页面置换算法先进先出 (First-In First-Out, FIFO) 算法类似于队列的实现，当一个 page 被换入的时候，加入到 page 对队尾，当需要换出一个页面时将队首的页面换出。FIFO 有一个显而易见的问题，一个常用的 page 到达队首时将被换出 RAM，不久之后又会产生缺页中断将其换入 RAM，因此很少使用纯粹的 FIFO 算法。 第二次机会 (Second Chance) 算法是对 FIFO 的一种优化，防止常用 page 被换出 RAM。第二次机会算法检查队首页面的 referenced 标志，如果标志是 0 那么这个 page 是最先进入 RAM 且没有被使用的，那么应该被换出 RAM；如果是 1 则进行清零，并将 page 加入到队尾。第二次机会算法即寻找一个在最近的时钟间隔内没有被访问过的页面。如果所有页面都被访问过，那该算法将退化为 FIFO 算法。 第二次机会尽管比较合理，但需要频繁在链表中移动 page，既降低了效率又不是很有必要。一个更好的方法是将链表改为循环链表，即可变为 时钟 (Clock) 页面置换算法。时钟算法与第二次机会算法一样，检查当前结点 page 的 referenced 标志，如果标识是 0 时将从链表中移除当前结点，如果是 1 时则清零标志。时钟算法不再需要实现结点 page 在链表中的移动，只需要移除或者清零标志位就行。 最近最少使用页面置换算法基于对软件指令执行的观察，在前面几条指令频繁使用的页面很可能在后面的几条指令中被使用。反过来说，已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。这样可以实现一个方法，在缺页中断时置换未使用时间最长的页面，这个策略称为 最近最少使用 (Least Recently Used, LRU) 页面置换算法。 虽然 LRU 理论上可以实现，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有 page 的链表，最近最多使用的 page 在表头，最近最少使用的 page 在表尾。困难的是在每次访问内存时都必须要更新整个链表，在链表中找到一个 page 并删除，然后把它移动到表头是十分费时的操作。 一种软件的方案被称为 最不常用 (Least Frequently Used, LFU) 算法，也被称为 NFU (Not Frequently Used)，该算法将每个 page 与一个软件计数器相关联，初始值为 0，当时钟中断时由 OS 扫描所有 page 并将每个 page 的 referenced 加到对应的计数器上。这个计数器跟踪了每个页面被访问的频繁程度，发生缺页中断时则置换计数器值最小的 page。 LFU 从来不会忘记任何事情，但是幸运的是将 LFU 做些小小的修改即可模拟 LRU 算法：首先将 referenced 被加到计数器前先将计数器右移一位，然后将 referenced 加到计数器的最左端而不是最右端。这种算法被成为 老化 (aging) 算法，aging 会使 LFU 忘记一些 page 的计数从而使计数器为 0，这些为 0 的 page 将是被换出 RAM 的被选项，以此模拟了 最近 的限制。 LFU 模拟的一个问题是，aging 不知道在两次时钟中断之间，如果有两个 page 被访问，将不得知哪个 page 被先访问。另一个问题，计数器位数将会限制对页面淘汰的策略，如果 8 位计数器能记录 8 个时钟中断内的情况，如果 page1 在 9 个时钟中断前被访问过，page2 在 1000 个时钟中断前被访问过，那么淘汰时将会从这两个 page 中随机选取一个淘汰，因为它们的计数器都为 0。 页面置换算法小结 算法 注释 OPT (最优) 不可实现，但可作为性能基准 NRU (最近未被访问) LRU 的很粗糙近似 FIFO (先进先出) 可能换出重要页面 第二次机会算法 防止重要页面被换出 时钟算法 使用循环链表，减少结点修改 LRU (最近最少使用) 优秀但难实现的算法 LFU (最不常用) LRU 的相对粗略近似 LFU aging 非常近似 LRU 的有效算法 我们可以利用缺页中断的次数比总页面调度次数，得出缺页率，缺页率可以简单直观地衡量一个置换算法的好坏。其中最初 n 个空物理块的页面调用也算作缺页。 假设现在有分配的物理块 4 个，页面 4,3,2,1,4,3,5,4,3,2,1,5，那么以 OPT、FIFO、 LFU 为例 OPT 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 4 4 4 4 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 1 1 1 5 5 5 5 5 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) 缺页次数 6，总访问次数 12，缺页率 \\(\\frac{1}{2}\\) FIFO 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 4 4 3 2 1 5 4 3 2 3 3 3 3 3 2 1 5 4 3 2 3 2 2 2 2 1 5 4 3 2 1 4 1 1 1 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) 缺页次数 10，总访问次数 12，缺页率 \\(\\frac{5}{6}\\) LFU 物理页面 4 3 2 1 4 3 5 4 3 2 1 5 1 4 4 4 4 3 2 1 1 1 5 4 3 2 3 3 3 2 1 4 3 5 4 3 2 3 2 2 1 4 3 5 4 3 2 1 4 1 4 3 5 4 3 2 1 5 是否缺页 \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\times\\) \\(\\times\\) \\(\\checkmark\\) \\(\\checkmark\\) \\(\\checkmark\\) LFU 算法可以使用链表实现，将刚刚访问过的结点置于表尾，表头即是将被淘汰的 page，这与上面所介绍到计数器的方案有所不同。缺页次数 8，总访问次数 12，缺页率 \\(\\frac{2}{3}\\) ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:5","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#页面置换算法小结"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#分页系统中的设计问题"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#局部与全局分配策略"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#页面大小"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#共享页面"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#共享库"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#内存映射文件"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#清除策略"},{"categories":["OperatingSystem"],"content":" 分页系统中的设计问题 局部与全局分配策略如何在相互竞争的进程之间分配内存，在进程 A 发生缺页中断时只考虑分配给 A 的页面还是需要考虑所有内存页面。前者被称为 局部 (local) 页面置换算法，后者则称为 全局 (global) 页面置换算法。local 可以有效地为每个进程分配固定的内存片段，global 在运行进程之间动态地分配页框，因此每个进程的页框数是随时间变化的。 global 通常情况下优于 local，当工作集的大小随进程运行时间发生变化时，变得更加明显。即使有大量空闲页框存在时，local 算法也会随着工作集的增长导致颠簸，如果工作集缩小则会导致浪费内存。global 算法则需要系统不停地确定应该为每个进程分配多少页框。一般可以平均将页框分配给正在运行的 n 个进程，但这并不合理，一个 300KB 的进程应该得到 10KB 进程的 30 倍份额，而不是一样的份额。另一个方案是规定一个最小页框数，这样无论多小的进程都可以运行。因为某些机器上，一条两个操作数的指令可能会用到多达 6 个 page，因为指令本身、源操作数、目标操作数可能跨越 page 边界。 管理内存动态分配的一种方案是使用 缺页中断率 (Page Fault Frequently, PFF) 算法， PFF 指出了何时增加或减少分配给一个进程的页面，但完全没有说明在发生缺页中断时应该替换掉哪个 page，仅仅控制分配集的大小。PFF 也假定 PFF 随着分配的 page 的增加而降低。 页面大小页面大小是操作系统可以选择的一个参数，要确定最佳的页面大小需要在几个互相矛盾的因素之间进行权衡，从结果来看不存在全局最优解。 正文段、数据段或堆栈段很可能不会恰好装满整个 page，最后一个页面通常是半空的，多余的空间就如此被浪费掉了，这种浪费称为 内部碎片 (internal fragmentation)。在内存中有 n 个段，页面大小为 p 字节时，会有 \\(np / 2\\) 字节被内部碎片浪费。从这方面考虑使用小页面会更好，但使用小页面程序需要更多的页面，意味着需要更大的页表。而多余的页面被存放于磁盘上，更多的 page 意味着需要频繁与磁盘交换页面，大量时间被浪费于此。此外小页面能更充分的利用 TLB 空间，但占据更多的 TLB 表项，由于 TLB 表项是稀缺资源，在这种情况下使用大页面是值得的。而每次的进程切换，页面越小意味着页表越大，装入时间就会更长。 假设进程平均大小是 s 字节，页面大小 p 字节，每个页表项 e 字节，则每个进程大约需要 \\(s / p\\) 个页面，占用 \\(se / p\\) 字节页表空间，内部碎片在最后一页的浪费为 \\(p / 2\\) 字节，因此页表和内部碎片造成的全部开销为 \\[\\frac{se}{p} + \\frac{p}{2}\\] 字节。由于最优值一定在页面大小处于某个值时取得，通过对 p 进行一次求导并令表达式为 0 得到 \\[-\\frac{se}{p^{2}} + \\frac{1}{2} = 0\\]，解得 \\[P = \\sqrt{2se}.\\] 共享页面几个不同的用户同时运行同一个程序是很常见的，但内存中如果存在同一个程序的两份副本将会浪费内存，共享页面效率更高。但是并不是所有 page 都适合共享，那些只读的 page 可以共享，但数据页面是不能共享的。 如果个进程共享代码时，共享页面将会出现一些问题。如果调度程序决定从内存中换出 A 程序，撤销其所有页面并用其他程序来填充空页面，则会引起 B 产生大量缺页中断。因此在换出或结束一个有共享页面的进程时，检查页面是否仍在使用是必要的。如果查找所有页表而考察一个 page 是否共享，其代价是十分巨大的，所以需要一个专门的数据结构来记录这些共享页面。 共享数据将比共享代码更为麻烦，但也不是不可能。当两个进程共享同一数据页面时，要求进程不进行写操作而只读操作；当写操作发生时，就触发只读保护并陷入操作系统内核，然后生成一个该页的副本，这时每个进程都拥有了自己的副本。随后每份副本都是可读写的，再次地写操作将不会陷入内核。这种方法被称为 写时复制 (Copy-on-Write, COW)，它通过减少复制而提高了性能。 共享库可以使用其他的粒度取代单个页面来实现共享。如果一个程序被启动两次，多数操作系统会自动共享所有的代码页面，在内存中只保留一份代码页面的副本。由于代码页面总是只读的，因此这样做不存在任何问题。每个进程都拥有一份数据页面的私有副本，或者利用写时复制技术创建共享的数据页面。 当多个代码库被不同的进程使用时，如果将每个程序与这些库静态的绑定在一起，将会使程序变得更加庞大，一个通用的技术是使用 共享库 来解决程序膨胀。在链接程序与共享库时，链接器没有加载被调用的函数，而是加载了一小段能够在运行时绑定被调用函数的存根例程 (stub routine)。依赖于系统实现和配置信息，共享库和程序一起被加载，或在所包含函数第一次被调用时加载。如果其他程序已加载了共享库，那么将不会再次装载它。当共享库被装载时，并不是一次性读入内存的，而是根据需要以 page 为单位进行装载，没有调用的函数是不会被装载进内存。除了使可执行程序的文件大小减小、节省内存空间外，共享库还有一个优点：如果共享库中的一个函数因修复 bug 而更新，那么并不需要重新编译调用了这个函数的程序，旧的二进制文件依旧可以运行。 但是共享库需要解决一个问题，共享库加载到 RAM 中被不同进程定位到不同地址上。这个库如果没有被共享可以在装载的过程中重定位，但共享库在装载时再进行重定位就行不通了。可以使用 COW 解决这个问题，对调用共享库的进程创建新页面，在创建新页面的过程中进行重定位，但这与使用共享库的目的相悖。一个更好的方法是：在编译共享库时让编译器不使用绝对地址，而是只能产生使用相对地址的指令。这样无论共享库被放在虚拟地址空间的什么位置，都可以正常工作。只是用相对偏移量的代码被称为 位置无关代码 (position-independent code)。 内存映射文件有一种通用机制 内存映射文件 (memory-mapped file)，共享库实际上是其一个特例。这种机制的思想是：进程可以通过发起一个 syscall，将一个文件映射到其虚拟地址空间的一部分。多数的系统实现中，映射共享的页面不会实际读入页面的内容，而是在访问页面时才会被每次一页的读入内存，磁盘文件则被当作后备存储。当进程退出或显示地解除文件映射时，所有被改动的页面会被写回到磁盘文件中。 内存映射文件提供了一种 I/O 的可选模型，可以把一个文件当作一个内存中一个内存中的大字符数组来访问，而不用通过读写操作来访问这个文件。如果两个或两个以上的进程同时映射了同一个文件，它们就可以通过共享内存通信。一个进程在共享内存上完成了写操作，此刻当另一个进程在映射到这个文件的虚拟地址上进行读操作时，它就可以立即看到写操作的修改结果。因此这个机制类似于一个进程之间的高带宽通道，并且这种机制很成熟、实用。 清除策略如果发生缺页中断时系统中有大量的空闲页框，此时分页系统工作在最佳状态。如果每个页框都被占用，并且被修改过的话，再换入新页面时就页面应首先被写回磁盘。为保证有足够的空闲页框，很多分页系统都会有一个称为 分页守护进程 (paging daemon) 的后台进程，它在大多数时候睡眠，但定期被唤醒检查内存状态。当空闲页框过少时，daemon 通过预订的页面置换算法选择换出内存，保证一定数量的页框供给比使用所有内存，在需要使用时搜索一个页框有更好的性能。daemon 至少保证了所有的空闲页框是干净的，所以空闲页框在被分配时不必再着急写回磁盘。 虚拟内存接口当前讨论的所有对上层程序员来说是透明的，使用者无需了解详细情况就能正常使用内存。但在一些高级系统中，程序员可以对内存映射进行控制，并通过非常贵的方法来增强程序的行为。 允许程序员对内存映射进行控制的一个原因是，为了允许两个或多个进程共享同一部分内存。如果程序员可以对内存区域进行命名，那么就有可能实现共享内存：通过让一个进程把一片内存区域的名称通知给另一个进程，使得其他进程可以将这片区域映射到它的虚拟空间中去。页面共享可以实现高性能的消息传递系统，传递消息的时候，数据被从一个地址空间复制到另一个地址空间，将会有很大开销；当进程可以控制他们的页面映射时，就可以只复制页面的名称，而不需要复制所有数据。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:3:6","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#虚拟内存接口"},{"categories":["OperatingSystem"],"content":" 分段目前所有有关虚拟内存的讨论都是一维的，对于许多问题来说，有多个独立的地址空间可能会优于一个。例如编译器在编译过程中建立许多表，可能包含： 被保存起来供打印清单用的程序正文 符号表，包含变量的名称与属性 包含用到的所有整型量和浮点常量的表 语法分析树，包含程序语法分析的结果 编译器内部过程调用使用的堆栈 前四个表随着编译的进行在不断地增长，最后一个表在编译过程中以一种不可预计的方式增长和缩小。在一维存储器中，这五张表只能被分配到虚拟地址空间中连续的块中。若一个程序中变量的的数量远多于其他部分的数量多时，地址空间中分配给符号表的块可能会被装满，但其他表还有大量的空间。这时更好的方法是机器上提供多个相互独立的 段 (segment) 的地址空间，各个 segment 的长度可以时 0 到某个允许的最大值中的任何值。不同的 segment 长度可以不同，通常情况下也都不相同。segment 的长度在运行时可以动态改变，在数据压入时增长、弹出时减小，并且不会影响其他段。 segment 是一个逻辑实体，一个段可能包括一个过程、一个数组、一个堆栈、一个数值变量，但一般不会同时包含多种不同类型的内容。除了能简化对长度经常变动的数据结构的管理外，分段存储管理的每个过程都位于一个独立的段中并且起始位置为 0，那么把单独编译好的过程链接起来的操作就可以得到很大的简化。当组成一个程序的所有过程都被编译和链接好之后，一个对段 n 中过程的调用将使用由两个部分组成的地址 \\((n, 0)\\) 来寻址。在重新编译时，对比一维地址，不会受到过程大小的改变而影响其他无关过程的起始地址。 由于 segment 可以时不同类型的内容，因此可以有不同种类的保护。一个过程段可以被指明为只执行的，从而禁止读写；读点数组可以读写但不允许执行，任何试图向这个段内的转跳都将被截获。这样的保护容易找到编程中的错误。 考察点 分页 分段 需要了解具体技术 \\(\\times\\) \\(\\checkmark\\) 存在多少线性空间 1 多个 超出存储器大小 \\(\\checkmark\\) \\(\\checkmark\\) 过程和数据区分并保护 \\(\\times\\) \\(\\checkmark\\) 大小可变的表更易提供 \\(\\times\\) \\(\\checkmark\\) 用户间过程共享方便 \\(\\times\\) \\(\\checkmark\\) 发明的原因 更大的地址空间 数据、过程逻辑独立，有助于共享、保护 page 是定长的但 segment 不是。系统运行一段时间后，内存会被分为许多块，一些块包含着 segment，而一些成了空闲区，这种现象称为 棋盘形碎片 或 外部碎片 (external fragmentation)。空闲区的存被浪费，而这可以通过内存紧缩来解决。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:4:0","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#分段"},{"categories":["OperatingSystem"],"content":" 段页式实现如果一个段比较大，那么将它整个保存在内存中可能很不方便甚至是不可能的，因此产生了对它进行分页的想法。这样只需要在真正需要的页面才会被调入内存。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:5:0","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#段页式实现"},{"categories":["OperatingSystem"],"content":" MULTICSMULTICS 是有史以来最具影响力的操作系统之一，对 UNIX 系统、x86 存储器体系结构、快表以及运计算均有过深刻的影响。MULTICS 始于 MIT 的一个研究项目，1969 年上线，最后一个 MULTICS 系统在运行了 31 年后于 2000 年关闭，几乎没有其他操作系统能像 MULTICS 一样几乎没有修改地持续运行那么长时间。更重要的是基于 MULTICS 形成的观点和理论在现在仍同 1965 年第一篇 相关论文 发表时产生的效果一样的。MULTICS 最具有创新性的一面 虚拟存储架构。 MULTICS 运行在 Honeywell 6000 及后续机型上，为每个程序提供最多 \\(2^{18}\\) 个 segment，每个 segment 的虚拟地址空间最长为 65536 个字长。为了实现它，MULTICS 的设计者决定把每个 segment 都看作一个虚拟内存并对其进行分页，以结合分页的优点 (固定的页大小和只需调用部分 page) 和分段的优点 (易于编程、模块化、保护和共享)。 每个程序都有一个段表，每个 segment 对应一个描述符。因为段表可能有 25 万多表项，因此段表本身也是一个 segment 并被分页。一个段描述符包含了一个段是否在内存中的标志，只要 segment 的任一部分在内存中就认为其在内存中。如果 segment 在内存中，它的描述符将包含一个 18 位的指向它的页表指针。由于物理内存是 24 bit 并且页面按 64 字节的边界对齐 (页面地址的低 6 位使用 0 填充)，所以描述符只需要 18 bit 来存储页表地址。段描述符还包含了段大小、保护位以及其他一些条目。每个 segment 都是一个普通的虚拟地址空间，一般页面大小位 1024 字，MULTICS 自己使用的段可能不分页或以 64 字为长度分页。 MULTICS 中一个地址由两部分构成：段和段内地址。段内地址又进一步分为页号和页内字。在进行内存访问时执行以下算法： 根据段号找到段描述符 检查该段的页表是否在内存中。如果在则找到它的位置，反之则产生一个段错误，如果访问违反了段的保护要求就发出一个越界错误。 检查所请求的虚拟页面的页表项，若该页面不在内存中则产生一个缺页中断，如果在内存就从页表项中取出这个页面在内存中的起始地址。 将偏移量加到页面的起始地址得到目标地址 进行读或写操作 这里简单地展示了 MULTICS 虚拟地址到物理地址的转换，这里忽略了描述符段也需要分页的事实。实际上通过一个寄存器找到描述符段的页表，这个页表指向描述符段的页面。但是由操作系统实现以上算法将非常缓慢，MULTICS 使用了包含 16 个字的告诉 TLB 并行搜索所有表项。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:5:1","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#multics"},{"categories":["OperatingSystem"],"content":" Intel x86x86 处理器的虚拟内存与 MULTICS 类似，其中包括分页分段机制，其支持最多 \\(2^{14}\\) 个 segment，每个 segment 最长 10 亿个 32 位字。虽然段数量较少，但相比之下 x86 的段大小特征比更多的段数目要重要的多，因为几乎没有程序需要 1000 个以上的段，但很多需要大型的段。自从 x86-64 起分段机制被认为是过时且不再支持的，但在本机模式下依然存在分段机制的某些痕迹。 x86 处理器中虚拟内存的核心是两张表：局部描述表 (Local Descriptor Table, LDT) 和 全局描述表 (Global Descriptor Table, GDT)，每个程序都有自己的 LDT，但同一台计算机上的所有程序共享一个 GDT。LDT 描述局部每个程序的 segment，包括其代码、数据、堆栈等；GDT 描述系统段，包含操作系统本身。 为了访问一个段，x86 程序必须把这个段的选择子 (selector) 装入机器的 6 个寄存器的某一个中，在运行过程中 CS 寄存器保存代码段的 selector，DS 寄存器保存数据段的 selector，其他的段寄存器不太重要，每个 selector 是一个 16 bit 数。selector 中的 1 bit 指出这个这个段是局部还是全局的，其他 13 bit 是 LDT 或 GDT 的表项编号。因此，这些表的长度被限制在最多容纳 8K 个段描述符。剩余的 2 bit 则是保护位，以后讨论。描述符 0 是禁止使用的，它可以被安全地装入一个段寄存器中用来表示这个段寄存器目前不可用，如果使用则会引发一次中断。selector 经过合理设计，使得根据选择字定位描述符十分方便。首先根据 selector 选择 LDT / GDT，随后 selector 被复制进一个内部擦除寄存器中并将其低 3 位清零，最后 LDT / GDT 表的地址被加到索引上，得出一个直接指向 descriptor 的指针。 被 selector 装入段寄存器时，对应的描述符被从 LDT 或 GDT 中取出装入微程序寄存器中，以便快速访问。一个描述符由 8 Byte 构成，包含了段的基址、大小和其他信息。下图描述了 x86 代码段的段描述符。在 descriptor 中应该有一个简单的 32 bit 域给出段大小，但实际上生育 20 bit 可用，因此采取了一种截然不同的方法：用粒度位域标明使用字节为单位还是页面为单位。处理器会将 32 bit 基址与偏移量相加形成 线性地址 (liner address)，为了和只有 24 bit 基址的 286 兼容，基址被分为 3 片分布在 descriptor 上。实际上基址运行每个 segment 位于 32 bit 线性地址空间内的任何位置。 如果禁止分页，线性地址将被解释为物理地址并送往存储器用于读写操作，因此禁止操作将是一个纯分段方案。另外段之间允许相互覆盖，这可能是由于验证段不重叠开销太大。如果允许分页，线性地址就被解释为与虚拟地址，并通过页表映射到物理地址。由于 32 位虚拟地址与 4 KB 页的情况下，segment 可能包含多达 100万个页面，因此使用两级映射以便在段较小时减小页表大小。每个进程都有一个 1024 个 32 bit 表项组成的 页目录 (page directory)，其通过全局寄存器来定位。page directory 中的每个目录项都指向一个也包含 1024 个 32 bit 表项的页表，页表项指向页框。线性地址被分为三个域：目录、 页面 和 偏移量。目录域被作为索引在 page directory 中找到指向正确的页表的指针，随后页面域被用于索引在页表中找到页框的物理地址，最后偏移量被加到页框地址上得到需要的物理地址。 ","date":"07-14","objectID":"/2021/operatingsystem_002/:5:2","series":["Operating System Note"],"tags":["Note","Memory"],"title":"内存管理","uri":"/2021/operatingsystem_002/#intel-x86"},{"categories":["OperatingSystem"],"content":"GinShio | 现代操作系统第二章读书笔记","date":"07-11","objectID":"/2021/operatingsystem_001/","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/"},{"categories":["OperatingSystem"],"content":" 进程进程 (Process) 是操作系统中的核心概念，是对正在运行的程序的抽象。即使只有一个可用的 CPU，也可以启动多个进程，让操作系统具有并发能力。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:1:0","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#进程"},{"categories":["OperatingSystem"],"content":" 进程模型一个进程就是一个正在执行的程序实例，每个进程都拥有一个自己的虚拟 CPU、程序计数器、寄存器、内存地址空间等，这些是一个进程私有的，不可被其他进程所访问、修改，真正的 CPU 在各个进程之间来回切换。 假设现在有 4 个程序，它们在运行时装入自己虚拟的程序计数器、寄存器，并有物理 CPU 运行程序。当程序被切换时，物理程序计数器等数据被保存到内存中。在观察足够长的时间时，所有的进程都运行了，但每个瞬间仅有一个程序在执行。需要注意的是，如果一个程序运行了两遍，那将被算作两个进程。 进程的核心思想即：一个进程是某种类型的一个活动，它有程序、输入、输出以及状态。单个处理器可以被若干进程共享，它使用调度算法决定何时停止一个进程的工作，并转而为另一个进程提供服务。 当进程创建了一个新进程后，其被称为父进程，新进程被称为子进程，这些进程组成了层次结构。在 UNIX 中一个称为 init 的特殊进程出现在启动映像中，init 运行时读入终端数量的文件，并为每一个终端创建一个新进程。这些终端等待用户登陆，登陆成功时便会执行 shell 进程来接收用户命令。整个系统中，所有进程都属于以 init 为根的进程树。 Windows 中没有层次结构概念，所有进程地位相同。创建进程时父进程得到 句柄 用于控制子进程，但也可以将句柄传送给其他进程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:1:1","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#进程模型"},{"categories":["OperatingSystem"],"content":" 进程的创建与终止操作系统往往需要一种方式来创建进程，一般由 4 种主要事件来创建进程： 系统初始化 正在运行的程序执行了创建进程的系统调用 (syscall) 用户请求创建新进程 批处理作业的初始化 新进程都是由于一个已存在的进程执行了一个用于创建进程的 syscall 用而创建的。这个系统调用通知操作系统创建一个新进程，并且直接或间接地指定在该进程中运行的程序。 在 UNIX 系统，往往采用 syscall fork 来创建一个与调用进程相同的副本。调用成功后，两个进程拥有相同的内存映像、相同的环境字符串与打开文件。通常子进程执行 exec 系列 syscall 来修改其内存映像，并运行一个新程序。在执行 exec 之前，允许子进程处理其文件描述符等操作。 UNIX 中子进程的初始地址空间是父进程的一个副本，但是两个不同的地址空间，不可写的部分则是共享的。而某些 UNIX 实现中，子进程共享父进程的所有内存，内存通过 写时复制 (Copy-On-Write, COW) 技术实现共享，即当两者之一修改内存时，这块内存被明确的复制，以保证修改发生在进程的私有区域。 进程在创建之后开始运行，完成其工作，进程可能在未来的某个时刻完成任务并被终止。通常终止进程由以下条件引起： 正常退出 (自愿) 错误退出 (自愿) 严重错误 (非自愿) 被其他进程杀死 (非自愿) 前两种即进程自己所处理的终止，完成工作或者在运行时遇到了一些可处理的错误，这时进程通过 exit 调用终止，并向父进程返回状态值。 严重错误往往会导致进程直接崩溃，终止进程，比如发生 零除错误 或 引用错误内存 等。不过在 UNIX 中，进程可以通知操作系统自行处理一部分严重错误，当发生错误时收到信号 (被中断) 而非终止。 进程可以被其他进程杀死，需要采用 syscall kill 来完成这个操作。kill 采用发送信号的方式告知进程执行操作，进程可以捕获一部分信号自行处理，但是「杀死」信号 SIGKILL 无法被捕获，进程接收到 KILL 信号后将直接被杀死。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:1:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#进程的创建与终止"},{"categories":["OperatingSystem"],"content":" 进程的状态进程虽然是独立的实体，但有时需要在进程之间相互作用。比如进程等待其他进程的输入时，则会被阻塞，进程的状态也变为阻塞态。正在运行的进程则是运行态进程，当一个进程被剥夺 CPU 的使用权而等待 CPU 的调度时，被称为就绪态。 阻塞态与运行态相似，都是进程不在使用 CPU，但其完全不同。前者是需要一定条件才能继续运行，比如等待输入就绪或等待硬盘读取数据，这时进程阻塞并主动让出 CPU 以供其他进程使用。而后者则是可以继续运行，但操作系统将 CPU 调度给了其他进程，因此进入等待 CPU 的状态。因此阻塞态进程必须满足一定条件才能继续进行，而就绪态进程只需要等待操作系统的调度，CPU 暂时没有分配给它。 这三种状态可以互相转换，如下图所示。当操作系统发现进程需要等待某些任务而不能继续执行时，将运行态转换为阻塞态；运行态与就绪态的转换，是由操作系统针对进程的调度引起的，对于进程来说无法察觉调度的存在；如果进程阻塞时，其等待的任务已完成，进程可以继续运行，则会转换为就绪态等待操作系统调度。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:1:3","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#进程的状态"},{"categories":["OperatingSystem"],"content":" 进程的实现操作系统使用进程表 (process table) 来维护进程模型，每个进程占用一个进程控制块。该控制块包含了进程状态的重要信息，包括程序计数器、堆栈指针、内存分配情况、打开的文件状态、调度信息等，从而保证该进程可以在之后调度中能再次启动，就像没有被中断过一样。下表展示了程序控制块中的关键字段。 进程管理 存储管理 文件管理 寄存器 正文段指针 根目录 程序计数器 数据段指针 工作目录 程序状态字 堆栈段指针 文件描述符 堆栈指针 用户 ID 进程状态 组 ID 优先级 调度参数 进程 ID 父进程 ID 进程组 信号 进程开始时间 使用的 CPU 时间 子进程的 CPU 时间 下次定时器时间 与每一 I/O 类关联的是一个称作 中断向量 (interrupt vector) 的位置，这是靠近内存底部的固定区域，这些内存包含中断服务程序的入口地址。在中断发生时，操作系统会做一系列操作，保存当前进程的上下文，并执行中断程序服务。以下是中断的执行步骤： 将程序计数器、程序状态字压入堆栈 (硬件) 从中断向量装入新的程序计数器 (硬件) 保存寄存器的值 (汇编语言) 设置新的堆栈 (汇编语言) 中断服务程序 (通常为 C 语言) 调度下一个运行的进程 中断返回 开始运行新的当前进程 一个进程在执行过程中可能被中断数千次，但关键是每次中断后，被中断的进程都返回到与中断发生前完全相同的状态。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:1:4","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#进程的实现"},{"categories":["OperatingSystem"],"content":" 线程在传统操作系统中，每个进程有一个地址空间和一个控制线程。经常存在在同一个地址空间中，并行运行多个控制线程的情形，这些线程就像分离的进程，但彼此共享地址空间。 我们引入线程往往有以下理由： 并行实例拥有共享同一个地址空间和所有数据的能力，这对于多进程模型是难以表达的 线程相对于进程更加轻量，创建更加容易、更加快速，也更容易撤销，有利于大量线程的动态、快速修改 若多个线程都是 CPU 密集型的，那么并不能获得性能上的增强，但是如果存在大量计算和大量 I/O 处理，拥有多个线程允许这些活动彼此重叠进行，从而会加快应用程序执行的速度 在多 CPU 系统中，多线程是有益的，真正的并行有了实现的可能 ","date":"07-11","objectID":"/2021/operatingsystem_001/:2:0","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#线程"},{"categories":["OperatingSystem"],"content":" 线程模型进程用某种方法将相关的资源集中在一起，包含程序正文、数据以及地址空间、打开的文件、定时器等。进程拥有一个执行线程 (thread)，thread 中存在一个程序计数器、寄存器和堆栈，用于记录指令、变量等信息。线程与传统进程一样，拥有若干个状态用于 CPU 的调度，线程的状态转换与进程是一致的。进程用于把资源集中到一起，线程则是 CPU 上被调度执行的实体。 进程独立的内容 线程独立的内容 地址空间 程序计数器 全局变量 寄存器 打开文件 堆栈 子进程 状态 即将发生的定时器 信号与信号处理程序 账户信息 同一个进程环境中，允许彼此之间有较大独立性的多个线程同时执行，这是对多进程模型的一种模拟。由于多线程模型中，所有线程都具有完全相同的地址空间，意味着他们也共享同样的全局变量。线程之间是没有保护的，一个线程可以读、写、清除另一个线程的堆栈。这样便于线程为完成某一任务而共同工作。 为实现可移植的线程程序，IEEE 1003.1c 中定义线程的标准，其被成为 pthread，大部分 UNIX 都支持该标准。所有 pthread 线程都有一些特性，包含一个 标识符 、一组 * 寄存器* (包含程序计数器) 和一组存储于结构中的属性，包括堆栈大小、调度参数等。 POSIX 函数名称 描述 pthread_create 创建一个新线程 pthread_exit 结束调用的线程 pthread_join 等待一个特定的线程退出 pthread_yield 主动释放 CPU pthread_attr_init 创建并初始化一个线程的属性结构 pthread_attr_destroy 删除一个线程的属性结构 创建一个新线程时使用 pthread_create 调用，线程的标识符会作为函数返回值返回。这么做看起来像 fork 调用，方便其他线程引用该线程。当线程完成它的工作时，可以通过调用 pthread_exit 来终止，这个调用类似于 exit 调用，这会终止线程并释放线程的栈。线程可以使用 pthread_yield 调用，主动地为其他线程让出 CPU。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:2:1","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#线程模型"},{"categories":["OperatingSystem"],"content":" 内核态线程由内核实现线程以及调度、管理操作，当需要创建或操作一个线程时，会使用 syscall 完成相关的操作。内核使用线程表对系统中的所有线程进行记录，其中保存了每个线程的寄存器、状态与其他信息。 由于内核中创建或撤销线程的代价较大，某些系统会采用 回收线程 的方式，减小开销。当某个线程被撤销时，将其标记为不可运行，但其内核数据结构完全不受影响，在需要创建新线程时就重新启用某个旧线程即可。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:2:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#内核态线程"},{"categories":["OperatingSystem"],"content":" 用户态线程用户态线程可以将整个线程包放在用户空间中，内核对线程包一无所知，内核仅需要管理单线程进程即可。可以在不支持线程的操作系统上以这种方式实现线程。在用户空间管理线程时，每个进程都需要专用的线程表，用以跟踪进程中的线程，该进程表及调度方法由运行时系统进行管理。 用户态线程模型进行调度时，保存线程状态的过程与调度程序都是在本地进行，不需要陷入内核、上下文切换等操作，相较于内核态线程要快很多 (一个数量级或更多)。并且针对不同的进程，允许用户制定不同的调度算法。 用户态线程有一个明显的问题，即如何实现 阻塞系统调用。当线程进行一个阻塞的系统调用时，将会阻塞整个进程直到等待就绪，导致其他线程也被迫停止运行。解决方法即：在运行时系统中使用 IO 多路复用进行阻塞系统调用，当阻塞时不进行调用并运行另一个线程，直到当前线程可以安全运行。这种方法需要改写一部分系统调用，对其进行包装，以保证用户态线程的正确运行。 用户态线程的另一个问题，如果一个线程开始运行，那么在该进程中的其他线程就不能运行，除非第一个线程自动放弃 CPU。在一个单独的进程内部，没有时钟中断，所以不能使用轮转调度的方式调用线程。除非某个线程能够按照自己的意志进入运行时系统，否则调度程序就没有任何机会。可以考虑让运行时系统请求每秒一次的时钟信号 (中断)，但高频率的发生周期性的时钟中断开销客观，如果线程使用时钟中断时可能扰乱运行时系统的时钟。 人们已经研究了各种试图将用户级线程的优点和内核级线程的优点结合起来的方法，其中一种方法即将用户态线程与一些/全部内核态线程多路复用起来，由编程人员决定使用多少内核线程与多少用户线程。内核只识别内核线程，并对内核线程进行调度；内核线程被用户线程多路复用，每个内核线程有一个可以轮流使用的用户线程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:2:3","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#用户态线程"},{"categories":["OperatingSystem"],"content":" 进程间通信进程间有时需要通信，被称为 进程间通信 (Inter Process Communication, IPC)。同时我们需要考虑三个问题： 进程如何将信息传递给另一个进程 确保两个或更多进程在关键活动中不会出现交叉 如果进程间顺序关联的话，确保顺序正确 除了第一问题在线程中很好解决，因为它们共享内存地址，但后两个问题对线程同样适用。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:0","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#进程间通信"},{"categories":["OperatingSystem"],"content":" 竞争条件与临界区协作的线程可能通过共享公共的存储区来完成通信，两个或多个的进程 (或线程) 读写某些共享数据，而最后的结果取决于进程 (或线程) 运行的精确时序，被称为 竞争条件 (race condition)。 以最简单的循环加一程序举例，两个线程同时对一个内存变量进行加一操作，各自循环 10000000 次，最终的结果可能不为 20000000。其中加一条件即是该段程序的竞争条件，它们竞争同一内存地址。 int sum = 0; // 可能的结果 13049876 void* add(void* atgs) { for (int i = 0; i \u003c 10000000; i++) ++sum; } int main(void) { pthread_t t1; pthread_t t2; pthread_create(\u0026t1, NULL, add, NULL); // 创建线程 1 pthread_create(\u0026t2, NULL, add, NULL); // 创建线程 2 pthread_join(t1, NULL); // 等待线程 1 结束 pthread_join(t2, NULL); // 等待线程 2 结束 printf(\"%d\\n\", sum); } 实际上，凡是涉及共享内存、共享文件或共享任何资源时，都会由竞争条件引发错误，要避免这种错误，必须阻止多个进程同时读写共享数据。换言之即 互斥 (mutual exclusion)，即以某种手段确保当一个进程使用共享资源时，其他进程无法进行同样的操作。实现互斥而选择的原语是操作系统的主要设计内容之一。 避免竞争条件的问题也可以用一种抽象的方式进行描述。一个进程的一部分时间做内部计算或另外一些不会引发竞争条件的操作。在某些时候进程可能需要访问共享数据或执行另外一些导致竞争的操作。我们把对共享内存进行访问的程序片段称为 临界区域 (critical region) 或 临界区 (critical section)。如果我们能够适当地安排，使得两个进程不可能同时处于 临界区中，就能够避免竞争条件。 尽管这样的要求避免了竞争条件，但它还不能保证使用共享数据的并发进程能够正确和高效地进行协作。对于一个好的解决方案，需要满足以下 4 个条件： 任何两个进程不能同时处于临界区 不应对 CPU 的速度和数量做任何假设 临界区外运行的进程不得阻塞其他进程 不得使进程无限期等待进入临界区 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:1","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#竞争条件与临界区"},{"categories":["OperatingSystem"],"content":" 忙等待的互斥 屏蔽中断 在单处理器系统中，最简单的方法为每个进程进入临界区时立即屏蔽所有中断，并在离开前再次打开中断。但是屏蔽中断也会导致时钟中断被屏蔽。CPU 只有发生时钟中断或其他中断时才会进行进程切换，屏蔽之后 CPU 将不会切换到其他进程。 对于内核来说，当更新变量或列表的几条指令期间，将中断屏蔽是方便的。但是用户可以自由屏蔽中断对系统来说是不安全的，如果用户屏蔽中断并不再打开中断，那么整个系统将会因此终止。而系统如果有多个处理器，屏蔽中断仅对当前关闭中断的 CPU 有效，其他 CPU 依然会继续运行。因此这是一种在用户进程中不合适的互斥机制。 锁变量 设想有一个共享 (锁) 变量，其初始值为0，当进程想进入临界区时，如果锁的值为 0 则设置为 1 并进入临界区，反之则等待。这可能在设置时，被 CPU 调度而导致有多个进程同时位于临界区内。 严格轮换法 设置一个共享内存记录当前可以进入临界区的进程 ID，当即将进入临界区时，检查该变量是否与自己的进程 ID 相等，不相等时则一直循环空转检测，直到可以进入临界区为止。当即将离开临界区时，将共享内存设置为下一个进程 ID，轮询每个竞争进程。连续测试一个变量，直到某个值出现为止，被称为 忙等待 (busy waiting)。由于这种方式浪费 CPU 时间，所以通常应该避免，只有在有理由认为等待时间非常短的情况下进行忙等待。用于忙等待的锁被称为 自旋锁 (spin lock)。 Peterson 解法 这是一种简单的不需要严格轮换的软件互斥算法。当前进程准备进入临界区时，标志数组对应的元素为 TRUE，并将共享变量设置为当前进程。循环判断共享变量是否为当前进程，是否有其他进程在标志数组中被设置为 TRUE。如果检查成功，则会继续循环，直到条件不成立，代码将进入临界区。在离开临界区时，将当前进程所对应的标志元素设置为 FALSE 即可。 int turn; int intersted[N]; // N = 2 void enter_region(int process) { int other = 1 - process; intersted[process] = TRUE; turn = process; while (turn == process \u0026\u0026 interested[other] == TRUE); } void leave_region(int process) { intersted[process] = FALSE; } TSL 指令 TSL 是一种硬件支持的解决方案，指令为 TSL RX, LOCK，称为 测试并加锁 (test and set lock)，它将内存字 lock 读入寄存器 RX 中，然后在该内存地址上存一个非零值。读字与写字操作是不可分割的，即该指令结束之前其他处理器均不允许访问该内存字。执行 TSL 指令的 CPU 将锁住内存总线，以禁止其他 CPU 在本指令结束之前访问内存。 在进入临界区时，将 LOCK 变量通过 TSL 指令设置为 1，并进入临界区。如果已经被设置为 1，则表示已经有进程进入临界区，则循环检测条件是否达成。当离开临界区时，将 0 写入 LOCK 即可。所以在请求进入临界区时将导致忙等待，直到锁空闲为止。 XCHG 是 TSL 的一种可替代指令，原子性地交换两个位置的内容。所有的 Intel x86 CPU 在底层同步中使用 XCHG 指令。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#忙等待的互斥"},{"categories":["OperatingSystem"],"content":" 睡眠与唤醒Peterson 与 TSL 都是正确的，但都有着忙等待的缺点，即进程准备进入临界区时，先检查是否允许进入，如果不被允许进程将原地等待，直到允许为止。原地等待将造成 CPU 空转，浪费 CPU 资源。 忙等待的另一个问题为 优先级反转问题 (priority inversion problem)：在两个进程 H (高优先级进程) 与 L (低优先级进程)，调度时当高优先级人物就绪时就可以运行。此时 L 处于临界区中，此时 H 就绪准备运行。当 H 开始忙等待，但由于调度关系导致 L 不会被调度，因此 L 无法离开临界区而 H 也会永远地等待下去。 我们讨论以下进程间通信原语，这些原语在无法进入临界区时将阻塞进程，而非忙等待。 sleep 是一个引起调用进程阻塞的系统调用，直到其他进程将其唤醒，wakeup 将参数指定的进程唤醒。 以下讨论 生产者-消费者问题 (又称 有界缓冲区 (bounded-buffer) 问题) 问题的实际应用。生产-消费模型即两个进程共享一个公共的固定大小的缓冲区，这两个进程一个是生产者，将消息放入公共缓冲区中；一个是消费者，从缓冲区中读取消息。在缓冲区满的情况下，生产者如果再次生产消息，则通过 sleep 对进程进行阻塞，直到消费者消费消息时再次唤醒，产生消息。同样地，消费者消费没有消息的缓冲区时，也被阻塞，直到生产者为缓冲区填入消息时被唤醒。 int size; const int capcity = 100; void producer(void) { while (true) { int item = product_item(); if (size == capcity) sleep(); insert_item(item); ++size; if (1 == size) wakeup(consumer); } } void consumer(void) { while (true) { if (size == 0) sleep(); int item = remove_item(); --size; if (size == capcity - 1) wakeup(producer); consume_item(item); } } 以上代码虽然展示了生产-消费模型，但其中存在数据竞争，即对缓冲区大小 size 的访问没有加以限制。有可能出现：缓冲区为空时消费者读取 size 等于 0 时的值，但此时调度程序启用生产者而暂停消费者。生产者产生数据并将 size + 1，此时生产者使用 wakeup 唤醒一个消费者，由于消费者还未被阻塞，因此 wakeup 丢失。当消费者下次被调度时，由于上次读取到 0 值，因此被阻塞。当缓冲区被生产者填满时，生产者与消费者都会被阻塞。 问题的实质是向一个未被阻塞的进程发送的 wakeup 信号丢失，如果信号不丢失那么将不会产生任何问题。我们可以为进程加上一个 唤醒等待位，当发送一个 wakeup 信号给清醒的进程时，将唤醒等待位设置为 1，之后如果进程要被阻塞时，检查唤醒等待位，如果为 1 则清除掉继续运行。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:3","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#睡眠与唤醒"},{"categories":["OperatingSystem"],"content":" 信号量信号量 (Semaphore) 由 E.W.Dijkstra 于 1965 年提出，使用整型变量来累计唤醒次数，供以后使用。一个 semaphore 的取值可以为 0 (没有保存下来的唤醒操作) 与正整数 (表示多次唤醒操作)。当 Semaphore 取值仅有 0 和 1 时，被称为 二元信号量 (binary semaphore)。信号量支持两种操作： down (消费一次唤醒直到 0 阻塞) 与 up (增加一次唤醒操作)。但是在 Dijkstra 的论文中，分别使用 P (荷兰语 Proberen，尝试，表示 down 操作) 与 V (荷兰语 Verhogen，增加，表示 up 操作) 来表示 Semaphore 的两种操作。Semaphore 另一个重要的用途是实现 同步 (synchronization)，保证某种事件的顺序发生或不发生，在程序设计语言 Algol 68 中首次引入。 在 Semaphore 进行操作时，所有动作是 原子性 的。原子 (Atom) 是从希腊语 ἄτομος (atomos，不可切分的) 转化而来，一个原子操作表示一组相关联的操作要么都不间断的执行，要么都不执行，整个过程是不可分割的。原子操作在计算机科学领域与解决同步、竞争问题是非常重要的。为了完成原子操作，引号量采用 TSL 或 XCHG 指令保证只有一个 CPU 对信号量进行操作。这与忙等待不同，忙等待可能在一个任意长的时间内进行，而 Semaphore 只需要几毫秒。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:4","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#信号量"},{"categories":["OperatingSystem"],"content":" 互斥量当不需要信号量的计数能力时，可以简化为二元版本，称为 互斥量 (mutex)。互斥量仅适用于管理共享资源或一小段代码，但由于其实现容易、有效，在用户空间线程包的实现时非常有用。 随着并行的增加，有效的同步和锁机制对性能而言十分重要。如果等待时间很短，自旋锁将会很快，但随着等待时间的增长，将会有大量 CPU 周期被浪费。如果存在很多竞争时，那么阻塞此进程，并仅当锁释放的时候让内核解除阻塞会很有效果，但只有很小的竞争时，频繁的陷入内核与切换线程的开销将变得十分巨大，并且锁竞争的数量不是很容易预测的。 Linux 实现了 快速用户空间互斥 (futex)，它实现了基础的锁，但避免陷入内核。futex 包含两个部分，一个内核服务和一个用户库。内核服务提供一个等待队列，它允许多个进程在一个锁上等待。它们将不会运行，除非内核明确地对它们解除阻塞。将一个进程放到等待队列需要系统调用，因此我们尽可能地避免这么做。在没有竞争时，futex 完全工作在用户空间。 pthread 提供了锁设施用于数据竞争，同时提供了同步机制：条件变量。在允许或阻塞对临界区的访问时互斥量是很有用的，而条件变量则是允许线程由于一些未达成的条件而阻塞。绝大多数情况下，这两种方法是合作使用的。 线程调用 描述 pthread_mutex_init 创建一个互斥量 pthread_mutex_destroy 撤销一个已存在的互斥量 pthread_mutex_lock 对互斥量加锁或阻塞 pthread_mutex_trylock 对互斥量加锁或失败 pthread_mutex_unlock 对互斥量解锁 pthread_cond_init 创建一个条件变量 pthread_cond_destroy 撤销一个已存在的条件变量 pthread_cond_wait 阻塞当前线程以等待一个信号 pthread_cond_signal 向一个线程发送信号来唤醒 pthread_cond_broadcast 向多个线程发送信号来唤醒 需要注意的是，条件变量与信号量不同的是，不会存在与内存中。因此一个将一个信号传递给一个没有线程在等待的条件变量，那么这个信号就会消失。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:5","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#互斥量"},{"categories":["OperatingSystem"],"content":" 管程由于在编写多进程、多线程代码时，极易出现问题，并且这些错误都是竞争条件、死锁以及一些不可预测、不可复现的行为。为了更易于编写正确的程序，出现了一种高级同步原语 管程 (monitor)。一个 monitor 是由一个过程、变量以及数据结构等组成的集合，它们组成了一个特殊的模块或者说软件包。进程可以在任何需要的时候调用 monitor 中的过程，但它们不能在 monitor 之外直接访问内部的数据结构。 管程有一种很重要的特性，即任一时刻管程中只能有一个活跃的进程，这是一种有效的互斥原语。管程是编程语言的一部分，编译器可以对其进行特殊处理，来保证其互斥特性。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:6","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#管程"},{"categories":["OperatingSystem"],"content":" 消息传递和屏障信号量是一种低级的进程间通信方式，而管程存在于少数语言当中，因此它们无法用于跨计算机的进程通信。操作系统提供了一种跨机器的进程通信原语，即消息传递 (message passing)，它包含两个调用 send (将消息发送给特定目标) 和 receive (从给定源中接收消息)。如果当前没有消息可用，接收者可能会被阻塞 (直到消息达到) 或返回错误代码。 由于在网络上传输数据，因此消息传递必须考虑在网络不佳时，导致的消息丢失。并且还需要解决进程命名问题，在消息传递中的进程必须是没有二义性的。其次身份认证也是一个问题。 在消息传递中，可以为每个进程分配一个唯一地址，让消息按照进程的地址进行编址，或者引入全新的数据结构 信箱 (mailbox)，对一定数量的消息进行缓存。使用有缓存的信箱时，当信箱被填满时发送者将被阻塞，当信箱为空时，接收者会被阻塞。而没有缓存的信箱，发送者会被阻塞直到接收者调用 receive 接收消息，反之也会被阻塞，这种情况被称为 会合 (rendezvous)。 屏障 (barrier) 是用于进程组的一种同步机制。在一些应用中划分了若干阶段，并规定除非所有进程都准备就绪，否则任何进程都不会进入下一阶段，此时可以在每阶段的结尾安置 barrier。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:3:7","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#消息传递和屏障"},{"categories":["OperatingSystem"],"content":" 调度通常有多个进程或线程同时竞争 CPU，只要有两个或更多的进程处于就绪状态，调度就会发生。如果只有一个 CPU 可用，那么必须选择下一个要运行的进程。在操作系统中，完成选择工作的这一部分称为 调度程序 (scheduler)，该程序使用的算法被称为 调度算法 (scheduling algorithm)。 在早期以磁带上的卡片作为输入的批处理系统时代，调度算法很简单，依次运行磁带上的每个作业。对于多道程序设计系统，调度算法要复杂一些，因为经常有多个用户等待服务。有些大型机系统仍将批处理和分时服务结合使用，需要调度程序决定下一个运行的是一个批处理任务还是终端上的一个交互用户。CPU 是稀缺资源，所以好的调度程序可以在提高性能与用户满意度方面取得很大的成果。对于个人计算机多数时间内只有一个活动进程，且主要限制的是用户当前的输入速率而不是 CPU 处理速率，因此调度程序在个人计算机中并不是很重要。目前个人计算机主要在绘制高精度视频时需要大量且高强度的计算。在网络服务器上，多个进程经常竞争 CPU，因此调度功能变得十分重要。在移动设备上资源并不充足，CPU 薄弱并且电量也是重要限制，因此调度算法主要在于优化电量损耗。 为了选择正确的进程运行，调度程序还要考虑 CPU 的利用率，因为进程切换的代价是比较高的。首先用户态必须切换到内核态，然后要保存当前进程的状态，包括在进程表中存储的寄存器，在很多系统中内存映像 (页表内的内存访问位) 也会保存，接着将心进程的内存映像装入 MMU，最后新进程开始运行。进程切换还会使整个 Cache 失败，强迫缓存从内存中动态装入两次。 在进程具有较长时间的 CPU 集中使用和较小的频度的 I/O 等待，这种进程被称为 CPU 密集型 进程。反之具有较短时间的 CPU 集中使用和频繁的 I/O 等待的进程，被称为 I/O 密集型 进程，这些进程并不是拥有特别长的 I/O 时间，而是它们的 I/O 请求频繁。在 I/O 开始后无论处理数据多还是少，它们都花费同样的时间提出硬件请求。随着 CPU 的速度越来越快，更多的进程开始偏向于 I/O 密集型，为此未来对 I/O 密集型进程的调度变得尤为重要。 有关调度处理的一个关键问题，在于何时进行调度决策。 在创建新进程后需要决定是运行父进程还是子进程。由于两个进程都是就绪状态，因此调度程序可以任意决定运行的进程。 在一个进程退出时必须做调度决策。进程退出后，调度程序必须从就绪进程集中选择一个进程运行，如果没有可运行的进程，通常会运行一个系统提供的空闲进程。 当一个进程阻塞在 I/O 或 Semaphore 等原因的阻塞时，必须选择一个进程运行。有时阻塞的原因会成为选择的因素。 在一个 I/O 中断发生时，必须做出调度决策。如果中断来自 I/O 设备，而该设备现在完成了工作，某些被阻塞的等待该 I/O 的进程就成为可运行的进程了。是否让新就绪的进程来运行，或中断发生时正在运行的进程运行，这取决于调度程序。 如果硬件时钟提供 50 、 60 Hz 或其他频率的周期性中断，可以在每个或每 k 个时钟中断时做出调度决策。根据如何处理时钟中断，可以把调度算法分为两类。 非抢占式 调度算法会挑选一个进程，并让该进程运行直至被阻塞，或直到进程主动放弃 CPU 时。因此非抢占式调度不会强迫进程挂起，即使该进程已运行了几个小时。这种调度方式是没有时钟时的唯一选择。 抢占式 调度算法会挑选一个进程，并让该进程运行某个固定时段的最大值。如果该进程在时段结束后仍在运行，则会被挂起并调度其他进程。这种调度方式需要在时间间隔末端发生时钟中断，以便 CPU 的使用权可以分配给调度程序。 由于不同的应用领域与操作系统有不同的目标，不同的环境中需要不同的调度算法，调度程序的优化也是不同的。主要将环境分为三种： 批处理 交互式 实时 调度算法的目标也会根据环境的不同而有所不同。 所有系统 公平。每个进程公平的 CPU 份额 策略强制执行。保证规定的策略被执行 平衡。保持系统的所有部分都忙碌 批处理系统 吞吐量。每小时最大作业数 周转时间。从提交到终止的最小时间 CPU 利用率。保持 CPU 始终忙碌 交互式系统 响应时间。快速响应请求 均衡性。满足用户的期望 实时系统 满足截止时间。避免丢失数据 可预测性。在多媒体系统中避免品质降低 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:0","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#调度"},{"categories":["OperatingSystem"],"content":" 批处理系统中的调度 先来先服务在所有的调度算法中，最简单的非抢占式的即是 先来先服务 (First-Come First-Served, FCFS) 算法。使用该算法，即采用队列数据结构，将最先请求使用 CPU 的进程最先调度。 FCFS 最主要的优点即 易于理解 且 便于运行。这个算法中，单链表记录了所有就绪进程，当需要选取进程时只需要从表头选择即可；当添加一个新的作业或阻塞一个进程时，只需要将进程添加到链表的尾部。但是其缺点很明显，即 平均等待时间过长。当前面的进程有着相当大的 CPU 执行时间时，排在后面的进程就会显著增加其等待时间，最终导致平均等待时间的增长。 比如 3 个进程，\\(P_1\\) 进程执行 24 ms，\\(P_2\\) 和 \\(P_3\\) 进程各执行 3 ms，按 FCFS 执行时平均等待时间为 \\((0 + 24 + 27) / 3 = 17 ms\\)。 最短作业优先当运行时间可以预知时，可以采用非抢占式算法 最短作业优先 (Shortest Job First， SJF)。即使用优先队列以进程运行时间对将要调度的进程进行排序，运行时间最短的进程最先被调度，运行时间最长的进程最后被调度。当一组给定且已知时间的进程，可以求得 SJF 算法是最优的，其平均等待时间最短。 以 FCFS 中的例子计算，使用 SJF 算法的平均等待时间为 \\((0 + 3 + 6) / 3 = 3 ms\\)。 SJF 的抢占式版本即最短剩余时间优先 (Shortest Remaining Time Next，SRTN) 算法，调度程序总是选择剩余运行时间最短的那个进程运行。这种方式可以使新的短作业获得良好的服务。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:1","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#批处理系统中的调度"},{"categories":["OperatingSystem"],"content":" 批处理系统中的调度 先来先服务在所有的调度算法中，最简单的非抢占式的即是 先来先服务 (First-Come First-Served, FCFS) 算法。使用该算法，即采用队列数据结构，将最先请求使用 CPU 的进程最先调度。 FCFS 最主要的优点即 易于理解 且 便于运行。这个算法中，单链表记录了所有就绪进程，当需要选取进程时只需要从表头选择即可；当添加一个新的作业或阻塞一个进程时，只需要将进程添加到链表的尾部。但是其缺点很明显，即 平均等待时间过长。当前面的进程有着相当大的 CPU 执行时间时，排在后面的进程就会显著增加其等待时间，最终导致平均等待时间的增长。 比如 3 个进程，\\(P_1\\) 进程执行 24 ms，\\(P_2\\) 和 \\(P_3\\) 进程各执行 3 ms，按 FCFS 执行时平均等待时间为 \\((0 + 24 + 27) / 3 = 17 ms\\)。 最短作业优先当运行时间可以预知时，可以采用非抢占式算法 最短作业优先 (Shortest Job First， SJF)。即使用优先队列以进程运行时间对将要调度的进程进行排序，运行时间最短的进程最先被调度，运行时间最长的进程最后被调度。当一组给定且已知时间的进程，可以求得 SJF 算法是最优的，其平均等待时间最短。 以 FCFS 中的例子计算，使用 SJF 算法的平均等待时间为 \\((0 + 3 + 6) / 3 = 3 ms\\)。 SJF 的抢占式版本即最短剩余时间优先 (Shortest Remaining Time Next，SRTN) 算法，调度程序总是选择剩余运行时间最短的那个进程运行。这种方式可以使新的短作业获得良好的服务。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:1","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#先来先服务"},{"categories":["OperatingSystem"],"content":" 批处理系统中的调度 先来先服务在所有的调度算法中，最简单的非抢占式的即是 先来先服务 (First-Come First-Served, FCFS) 算法。使用该算法，即采用队列数据结构，将最先请求使用 CPU 的进程最先调度。 FCFS 最主要的优点即 易于理解 且 便于运行。这个算法中，单链表记录了所有就绪进程，当需要选取进程时只需要从表头选择即可；当添加一个新的作业或阻塞一个进程时，只需要将进程添加到链表的尾部。但是其缺点很明显，即 平均等待时间过长。当前面的进程有着相当大的 CPU 执行时间时，排在后面的进程就会显著增加其等待时间，最终导致平均等待时间的增长。 比如 3 个进程，\\(P_1\\) 进程执行 24 ms，\\(P_2\\) 和 \\(P_3\\) 进程各执行 3 ms，按 FCFS 执行时平均等待时间为 \\((0 + 24 + 27) / 3 = 17 ms\\)。 最短作业优先当运行时间可以预知时，可以采用非抢占式算法 最短作业优先 (Shortest Job First， SJF)。即使用优先队列以进程运行时间对将要调度的进程进行排序，运行时间最短的进程最先被调度，运行时间最长的进程最后被调度。当一组给定且已知时间的进程，可以求得 SJF 算法是最优的，其平均等待时间最短。 以 FCFS 中的例子计算，使用 SJF 算法的平均等待时间为 \\((0 + 3 + 6) / 3 = 3 ms\\)。 SJF 的抢占式版本即最短剩余时间优先 (Shortest Remaining Time Next，SRTN) 算法，调度程序总是选择剩余运行时间最短的那个进程运行。这种方式可以使新的短作业获得良好的服务。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:1","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#最短作业优先"},{"categories":["OperatingSystem"],"content":" 交互式系统中的调度 轮转调度轮转调度 (round robin) 是一种 最古老、最简单、最公平 且 使用最广 的调度算法，每个进程被分配一个时间段，称为 时间片 (quantum)，即允许该进程在该时间段中运行。如果在 quantum 结束时该进程依然在运行，则剥夺 CPU 并分配给其他进程。如果该进程在 quantum 结束前阻塞或结束，则立即切换进程。 轮转调度中最有趣的即是时间片的大小。从一个进程切换到另一个进程是需要一定时间进行事务处理的，即我们之前说的保存、装入寄存器值与内存映像等。假设 进程切换 (process switch) 或称 上下文切换 (context switch) 需要 1 ms，时间片大小为 4 ms，那么 CPU 将会有 \\(20\\%\\) 的时间被浪费在进程切换中。如果将时间片调整为 100 ms，那么 CPU 仅浪费 \\(1\\%\\) 的时间，但是如果有 50 个可运行进程且每个进程都会使用完整的时间片，那么最后一个进程需要等待 5 秒才可以使用到 CPU。如果时间片的大小长于平均的 CPU 突发时间，那么不会经常发生抢占。相反，在时间片耗费完之前多数进程会完成一个阻塞操作引起进程切换。抢占的消失改善了性能，因为进程切换只会发生在确实逻辑上有需要的时候，即进程被阻塞不能继续运行。 简单来说，太短的 quantum 会导致频繁的进程切换，降低 CPU 效率；太长的 quantum 可能引起对短交互请求的响应时间变长。因此 quantum 一般设置为 20 ~ 50 ms 一个比较折中的长度。 优先级调度轮转调度做了一个隐含的假设，即所有的进程同等重要，而拥有和操作多用户计算机系统的人对此常有不同的看法，将外部因素考虑在内的需要就导致了 优先级调度 。其基本思想很清楚，每个进程被赋予一个优先级，允许优先级最高的可运行进程先运行。为了防止高优先级进程无休止的运行，低优先级进程可能出现饥饿现象，调度程序可能在每个时钟中断降低当前进程的优先级，如果当前进程的优先级低于次高级优先级的进程，则进行进程切换。另一种方法是，每个进程拥有一个运行的最大时间片，当时间片耗尽时，调度次高优先级的进程。 优先级可以是静态赋予或动态赋予的，也可以由系统动态确定。例如有些 I/O 密集型进程，多数时间用来等待 I/O 结束，当这样的进程需要 CPU 时，应立即分配 CPU 以便启动下一个 I/O 请求，这样就可以在另一个进程执行计算时执行 I/O 操作。使这类 I/O 密集型进程长时间等待 CPU 只会造成其无谓地长时间占用内存。使 I/O 密集型进程获得较好服务的一种简单方法是，将其优先级设置为 \\(1/f\\)，其中 \\(f\\) 为该进程在上一时间片中所占用的部分。另一种方式是让用户进程主动的调整进程的优先级，如此可以使调度机制采用适用于当前环境的调度策略。 相容分时系统相容分时系统 (Compatible Time Sharing System，CTSS) 是 MIT 在 IBM 7094 上开发的最早使用优先级调度的系统之一。CTSS 中存在进程切换速度太慢的问题，这与 IBM 7094 内存中只能存在一个进程有关。CTSS 的设计者很快便认识到，为 CPU 密集型进程设置较长的时间片比频繁分给它们时间片更加高效，但长时间片又会影响到响应时间。其解决方法即建立优先级类，最高优先级的进程运行 1 个 quantum，次高优先级进程运行 2 个 quantum，再次一级运行 4 个 quantum，以此类推。 当一个进程用完分配的 quantum 后，被移动到下一级。如此，以前可能需要 100 次调度的进程，可以减少到 7 次调度，并且随着优先级的不断降低，它的运行频率会不断放慢，从而为短的交互进程让出 CPU。对于刚刚运行过长时间但又需要交互的进程，为防止其永远处于惩罚状态，可以将其调整到高优先级队伍，提高其响应速度。 最短进程优先SJF 算法往往有着最短响应时间，所以如果能够把其用于交互式进程是最好的。当前的问题是如何找出当前可运行进程中最短的那个进程。 可以根据进程过去的行为进行推测，并执行估计运行时间最短的那个。假设某终端上每条命令的估计运行时间为 \\(T_0\\) ，现在假设测量到其下一次运行时间为 \\(T_1\\) 。可以用这两个值的加权和 \\(\\alpha T_0 + (1-\\alpha) T_1\\) 来改进估计时间。通过选择 \\(\\alpha\\) 的值，可以决定是尽快忘掉老的运行时间，还是在一段长时间内始终记住他们。当 \\(\\alpha = 1 / 2\\) 时，可以得到如下序列： \\[T_0, \\quad \\frac{T_0}{2}+\\frac{T_1}{2}, \\quad \\frac{T_0}{4}+\\frac{T_1}{4}+\\frac{T_2}{2}, \\quad \\frac{T_0}{8}+\\frac{T_1}{8}+\\frac{T_2}{4}+\\frac{T_3}{2}, \\quad \\dots\\] 可以看到，三轮过后，\\(T_0\\) 在新的估计值中所占的比重下降到了 \\(1 / 8\\)。将这种通过当前测量值与先前估计值进行加权平均而得到下一个估计值的技术，称为 老化 (aging)，它适用于许多测量值必须基于先前值的情况。 其他调度算法 保证调度 向用户作出明确的性能保证并实现它，是一种独特的调度算法。如现在有 n 个用户登陆终端，可以保证每个用户将会获得 CPU 处理能力的 \\(\\frac{1}{n}\\)。或者说，在单用户系统中，帮证运行的 n 个进程，每个都可以获得 \\(\\frac{1}{n}\\) 的 CPU 时间。为了实现所做的保证，系统必须跟踪各个进程自创建以来，已使用的 CPU 时间，然后计算各个进程应获得的时间。 彩票调度 为用户做出承诺并兑现，是一个好方法，但难以实现。有一个既可以给出类似预测结果又可以简单实现的算法，即 彩票调度 (lottery scheduling)。其基本思想是：为进程提供各个系统资源的彩票，一旦需要做出一项调度决策时，就随机抽出一张彩票，拥有该彩票的进程获得资源。比如系统可以掌握每秒 50 次的彩票，作为奖励每个获奖者可以获得 20 ms 的 CPU 时间。所有进程是平等的，但某些进程更平等一些，对于重要的进程可以分配额外的彩票，以便增加它们的获胜机会。比如一共 100 张彩票，一个进程持有其中的 20 张，那么该进程在较长的运行中会得到 \\(20\\%\\) 的系统资源。即拥有彩票 f 份额的进程大约得到系统资源的 f 份额。 公平分享调度 我们假设被调度的都是进程自身，并不关注其所有者是谁。这样的结果就是，如果用户 A 拥有 9 个进程，而用户 2 拥有 1 个进程，那么轮转调度或同优先级调度算法中，用户 2 仅能使用 CPU 的 \\(90\\%\\) 。为避免这样的情况，可以在调度处理前考虑谁拥有该进程，让每个用户可以公平的分配 CPU 时间，而调度程序以强制的方式选择进程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#交互式系统中的调度"},{"categories":["OperatingSystem"],"content":" 交互式系统中的调度 轮转调度轮转调度 (round robin) 是一种 最古老、最简单、最公平 且 使用最广 的调度算法，每个进程被分配一个时间段，称为 时间片 (quantum)，即允许该进程在该时间段中运行。如果在 quantum 结束时该进程依然在运行，则剥夺 CPU 并分配给其他进程。如果该进程在 quantum 结束前阻塞或结束，则立即切换进程。 轮转调度中最有趣的即是时间片的大小。从一个进程切换到另一个进程是需要一定时间进行事务处理的，即我们之前说的保存、装入寄存器值与内存映像等。假设 进程切换 (process switch) 或称 上下文切换 (context switch) 需要 1 ms，时间片大小为 4 ms，那么 CPU 将会有 \\(20\\%\\) 的时间被浪费在进程切换中。如果将时间片调整为 100 ms，那么 CPU 仅浪费 \\(1\\%\\) 的时间，但是如果有 50 个可运行进程且每个进程都会使用完整的时间片，那么最后一个进程需要等待 5 秒才可以使用到 CPU。如果时间片的大小长于平均的 CPU 突发时间，那么不会经常发生抢占。相反，在时间片耗费完之前多数进程会完成一个阻塞操作引起进程切换。抢占的消失改善了性能，因为进程切换只会发生在确实逻辑上有需要的时候，即进程被阻塞不能继续运行。 简单来说，太短的 quantum 会导致频繁的进程切换，降低 CPU 效率；太长的 quantum 可能引起对短交互请求的响应时间变长。因此 quantum 一般设置为 20 ~ 50 ms 一个比较折中的长度。 优先级调度轮转调度做了一个隐含的假设，即所有的进程同等重要，而拥有和操作多用户计算机系统的人对此常有不同的看法，将外部因素考虑在内的需要就导致了 优先级调度 。其基本思想很清楚，每个进程被赋予一个优先级，允许优先级最高的可运行进程先运行。为了防止高优先级进程无休止的运行，低优先级进程可能出现饥饿现象，调度程序可能在每个时钟中断降低当前进程的优先级，如果当前进程的优先级低于次高级优先级的进程，则进行进程切换。另一种方法是，每个进程拥有一个运行的最大时间片，当时间片耗尽时，调度次高优先级的进程。 优先级可以是静态赋予或动态赋予的，也可以由系统动态确定。例如有些 I/O 密集型进程，多数时间用来等待 I/O 结束，当这样的进程需要 CPU 时，应立即分配 CPU 以便启动下一个 I/O 请求，这样就可以在另一个进程执行计算时执行 I/O 操作。使这类 I/O 密集型进程长时间等待 CPU 只会造成其无谓地长时间占用内存。使 I/O 密集型进程获得较好服务的一种简单方法是，将其优先级设置为 \\(1/f\\)，其中 \\(f\\) 为该进程在上一时间片中所占用的部分。另一种方式是让用户进程主动的调整进程的优先级，如此可以使调度机制采用适用于当前环境的调度策略。 相容分时系统相容分时系统 (Compatible Time Sharing System，CTSS) 是 MIT 在 IBM 7094 上开发的最早使用优先级调度的系统之一。CTSS 中存在进程切换速度太慢的问题，这与 IBM 7094 内存中只能存在一个进程有关。CTSS 的设计者很快便认识到，为 CPU 密集型进程设置较长的时间片比频繁分给它们时间片更加高效，但长时间片又会影响到响应时间。其解决方法即建立优先级类，最高优先级的进程运行 1 个 quantum，次高优先级进程运行 2 个 quantum，再次一级运行 4 个 quantum，以此类推。 当一个进程用完分配的 quantum 后，被移动到下一级。如此，以前可能需要 100 次调度的进程，可以减少到 7 次调度，并且随着优先级的不断降低，它的运行频率会不断放慢，从而为短的交互进程让出 CPU。对于刚刚运行过长时间但又需要交互的进程，为防止其永远处于惩罚状态，可以将其调整到高优先级队伍，提高其响应速度。 最短进程优先SJF 算法往往有着最短响应时间，所以如果能够把其用于交互式进程是最好的。当前的问题是如何找出当前可运行进程中最短的那个进程。 可以根据进程过去的行为进行推测，并执行估计运行时间最短的那个。假设某终端上每条命令的估计运行时间为 \\(T_0\\) ，现在假设测量到其下一次运行时间为 \\(T_1\\) 。可以用这两个值的加权和 \\(\\alpha T_0 + (1-\\alpha) T_1\\) 来改进估计时间。通过选择 \\(\\alpha\\) 的值，可以决定是尽快忘掉老的运行时间，还是在一段长时间内始终记住他们。当 \\(\\alpha = 1 / 2\\) 时，可以得到如下序列： \\[T_0, \\quad \\frac{T_0}{2}+\\frac{T_1}{2}, \\quad \\frac{T_0}{4}+\\frac{T_1}{4}+\\frac{T_2}{2}, \\quad \\frac{T_0}{8}+\\frac{T_1}{8}+\\frac{T_2}{4}+\\frac{T_3}{2}, \\quad \\dots\\] 可以看到，三轮过后，\\(T_0\\) 在新的估计值中所占的比重下降到了 \\(1 / 8\\)。将这种通过当前测量值与先前估计值进行加权平均而得到下一个估计值的技术，称为 老化 (aging)，它适用于许多测量值必须基于先前值的情况。 其他调度算法 保证调度 向用户作出明确的性能保证并实现它，是一种独特的调度算法。如现在有 n 个用户登陆终端，可以保证每个用户将会获得 CPU 处理能力的 \\(\\frac{1}{n}\\)。或者说，在单用户系统中，帮证运行的 n 个进程，每个都可以获得 \\(\\frac{1}{n}\\) 的 CPU 时间。为了实现所做的保证，系统必须跟踪各个进程自创建以来，已使用的 CPU 时间，然后计算各个进程应获得的时间。 彩票调度 为用户做出承诺并兑现，是一个好方法，但难以实现。有一个既可以给出类似预测结果又可以简单实现的算法，即 彩票调度 (lottery scheduling)。其基本思想是：为进程提供各个系统资源的彩票，一旦需要做出一项调度决策时，就随机抽出一张彩票，拥有该彩票的进程获得资源。比如系统可以掌握每秒 50 次的彩票，作为奖励每个获奖者可以获得 20 ms 的 CPU 时间。所有进程是平等的，但某些进程更平等一些，对于重要的进程可以分配额外的彩票，以便增加它们的获胜机会。比如一共 100 张彩票，一个进程持有其中的 20 张，那么该进程在较长的运行中会得到 \\(20\\%\\) 的系统资源。即拥有彩票 f 份额的进程大约得到系统资源的 f 份额。 公平分享调度 我们假设被调度的都是进程自身，并不关注其所有者是谁。这样的结果就是，如果用户 A 拥有 9 个进程，而用户 2 拥有 1 个进程，那么轮转调度或同优先级调度算法中，用户 2 仅能使用 CPU 的 \\(90\\%\\) 。为避免这样的情况，可以在调度处理前考虑谁拥有该进程，让每个用户可以公平的分配 CPU 时间，而调度程序以强制的方式选择进程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#轮转调度"},{"categories":["OperatingSystem"],"content":" 交互式系统中的调度 轮转调度轮转调度 (round robin) 是一种 最古老、最简单、最公平 且 使用最广 的调度算法，每个进程被分配一个时间段，称为 时间片 (quantum)，即允许该进程在该时间段中运行。如果在 quantum 结束时该进程依然在运行，则剥夺 CPU 并分配给其他进程。如果该进程在 quantum 结束前阻塞或结束，则立即切换进程。 轮转调度中最有趣的即是时间片的大小。从一个进程切换到另一个进程是需要一定时间进行事务处理的，即我们之前说的保存、装入寄存器值与内存映像等。假设 进程切换 (process switch) 或称 上下文切换 (context switch) 需要 1 ms，时间片大小为 4 ms，那么 CPU 将会有 \\(20\\%\\) 的时间被浪费在进程切换中。如果将时间片调整为 100 ms，那么 CPU 仅浪费 \\(1\\%\\) 的时间，但是如果有 50 个可运行进程且每个进程都会使用完整的时间片，那么最后一个进程需要等待 5 秒才可以使用到 CPU。如果时间片的大小长于平均的 CPU 突发时间，那么不会经常发生抢占。相反，在时间片耗费完之前多数进程会完成一个阻塞操作引起进程切换。抢占的消失改善了性能，因为进程切换只会发生在确实逻辑上有需要的时候，即进程被阻塞不能继续运行。 简单来说，太短的 quantum 会导致频繁的进程切换，降低 CPU 效率；太长的 quantum 可能引起对短交互请求的响应时间变长。因此 quantum 一般设置为 20 ~ 50 ms 一个比较折中的长度。 优先级调度轮转调度做了一个隐含的假设，即所有的进程同等重要，而拥有和操作多用户计算机系统的人对此常有不同的看法，将外部因素考虑在内的需要就导致了 优先级调度 。其基本思想很清楚，每个进程被赋予一个优先级，允许优先级最高的可运行进程先运行。为了防止高优先级进程无休止的运行，低优先级进程可能出现饥饿现象，调度程序可能在每个时钟中断降低当前进程的优先级，如果当前进程的优先级低于次高级优先级的进程，则进行进程切换。另一种方法是，每个进程拥有一个运行的最大时间片，当时间片耗尽时，调度次高优先级的进程。 优先级可以是静态赋予或动态赋予的，也可以由系统动态确定。例如有些 I/O 密集型进程，多数时间用来等待 I/O 结束，当这样的进程需要 CPU 时，应立即分配 CPU 以便启动下一个 I/O 请求，这样就可以在另一个进程执行计算时执行 I/O 操作。使这类 I/O 密集型进程长时间等待 CPU 只会造成其无谓地长时间占用内存。使 I/O 密集型进程获得较好服务的一种简单方法是，将其优先级设置为 \\(1/f\\)，其中 \\(f\\) 为该进程在上一时间片中所占用的部分。另一种方式是让用户进程主动的调整进程的优先级，如此可以使调度机制采用适用于当前环境的调度策略。 相容分时系统相容分时系统 (Compatible Time Sharing System，CTSS) 是 MIT 在 IBM 7094 上开发的最早使用优先级调度的系统之一。CTSS 中存在进程切换速度太慢的问题，这与 IBM 7094 内存中只能存在一个进程有关。CTSS 的设计者很快便认识到，为 CPU 密集型进程设置较长的时间片比频繁分给它们时间片更加高效，但长时间片又会影响到响应时间。其解决方法即建立优先级类，最高优先级的进程运行 1 个 quantum，次高优先级进程运行 2 个 quantum，再次一级运行 4 个 quantum，以此类推。 当一个进程用完分配的 quantum 后，被移动到下一级。如此，以前可能需要 100 次调度的进程，可以减少到 7 次调度，并且随着优先级的不断降低，它的运行频率会不断放慢，从而为短的交互进程让出 CPU。对于刚刚运行过长时间但又需要交互的进程，为防止其永远处于惩罚状态，可以将其调整到高优先级队伍，提高其响应速度。 最短进程优先SJF 算法往往有着最短响应时间，所以如果能够把其用于交互式进程是最好的。当前的问题是如何找出当前可运行进程中最短的那个进程。 可以根据进程过去的行为进行推测，并执行估计运行时间最短的那个。假设某终端上每条命令的估计运行时间为 \\(T_0\\) ，现在假设测量到其下一次运行时间为 \\(T_1\\) 。可以用这两个值的加权和 \\(\\alpha T_0 + (1-\\alpha) T_1\\) 来改进估计时间。通过选择 \\(\\alpha\\) 的值，可以决定是尽快忘掉老的运行时间，还是在一段长时间内始终记住他们。当 \\(\\alpha = 1 / 2\\) 时，可以得到如下序列： \\[T_0, \\quad \\frac{T_0}{2}+\\frac{T_1}{2}, \\quad \\frac{T_0}{4}+\\frac{T_1}{4}+\\frac{T_2}{2}, \\quad \\frac{T_0}{8}+\\frac{T_1}{8}+\\frac{T_2}{4}+\\frac{T_3}{2}, \\quad \\dots\\] 可以看到，三轮过后，\\(T_0\\) 在新的估计值中所占的比重下降到了 \\(1 / 8\\)。将这种通过当前测量值与先前估计值进行加权平均而得到下一个估计值的技术，称为 老化 (aging)，它适用于许多测量值必须基于先前值的情况。 其他调度算法 保证调度 向用户作出明确的性能保证并实现它，是一种独特的调度算法。如现在有 n 个用户登陆终端，可以保证每个用户将会获得 CPU 处理能力的 \\(\\frac{1}{n}\\)。或者说，在单用户系统中，帮证运行的 n 个进程，每个都可以获得 \\(\\frac{1}{n}\\) 的 CPU 时间。为了实现所做的保证，系统必须跟踪各个进程自创建以来，已使用的 CPU 时间，然后计算各个进程应获得的时间。 彩票调度 为用户做出承诺并兑现，是一个好方法，但难以实现。有一个既可以给出类似预测结果又可以简单实现的算法，即 彩票调度 (lottery scheduling)。其基本思想是：为进程提供各个系统资源的彩票，一旦需要做出一项调度决策时，就随机抽出一张彩票，拥有该彩票的进程获得资源。比如系统可以掌握每秒 50 次的彩票，作为奖励每个获奖者可以获得 20 ms 的 CPU 时间。所有进程是平等的，但某些进程更平等一些，对于重要的进程可以分配额外的彩票，以便增加它们的获胜机会。比如一共 100 张彩票，一个进程持有其中的 20 张，那么该进程在较长的运行中会得到 \\(20\\%\\) 的系统资源。即拥有彩票 f 份额的进程大约得到系统资源的 f 份额。 公平分享调度 我们假设被调度的都是进程自身，并不关注其所有者是谁。这样的结果就是，如果用户 A 拥有 9 个进程，而用户 2 拥有 1 个进程，那么轮转调度或同优先级调度算法中，用户 2 仅能使用 CPU 的 \\(90\\%\\) 。为避免这样的情况，可以在调度处理前考虑谁拥有该进程，让每个用户可以公平的分配 CPU 时间，而调度程序以强制的方式选择进程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#优先级调度"},{"categories":["OperatingSystem"],"content":" 交互式系统中的调度 轮转调度轮转调度 (round robin) 是一种 最古老、最简单、最公平 且 使用最广 的调度算法，每个进程被分配一个时间段，称为 时间片 (quantum)，即允许该进程在该时间段中运行。如果在 quantum 结束时该进程依然在运行，则剥夺 CPU 并分配给其他进程。如果该进程在 quantum 结束前阻塞或结束，则立即切换进程。 轮转调度中最有趣的即是时间片的大小。从一个进程切换到另一个进程是需要一定时间进行事务处理的，即我们之前说的保存、装入寄存器值与内存映像等。假设 进程切换 (process switch) 或称 上下文切换 (context switch) 需要 1 ms，时间片大小为 4 ms，那么 CPU 将会有 \\(20\\%\\) 的时间被浪费在进程切换中。如果将时间片调整为 100 ms，那么 CPU 仅浪费 \\(1\\%\\) 的时间，但是如果有 50 个可运行进程且每个进程都会使用完整的时间片，那么最后一个进程需要等待 5 秒才可以使用到 CPU。如果时间片的大小长于平均的 CPU 突发时间，那么不会经常发生抢占。相反，在时间片耗费完之前多数进程会完成一个阻塞操作引起进程切换。抢占的消失改善了性能，因为进程切换只会发生在确实逻辑上有需要的时候，即进程被阻塞不能继续运行。 简单来说，太短的 quantum 会导致频繁的进程切换，降低 CPU 效率；太长的 quantum 可能引起对短交互请求的响应时间变长。因此 quantum 一般设置为 20 ~ 50 ms 一个比较折中的长度。 优先级调度轮转调度做了一个隐含的假设，即所有的进程同等重要，而拥有和操作多用户计算机系统的人对此常有不同的看法，将外部因素考虑在内的需要就导致了 优先级调度 。其基本思想很清楚，每个进程被赋予一个优先级，允许优先级最高的可运行进程先运行。为了防止高优先级进程无休止的运行，低优先级进程可能出现饥饿现象，调度程序可能在每个时钟中断降低当前进程的优先级，如果当前进程的优先级低于次高级优先级的进程，则进行进程切换。另一种方法是，每个进程拥有一个运行的最大时间片，当时间片耗尽时，调度次高优先级的进程。 优先级可以是静态赋予或动态赋予的，也可以由系统动态确定。例如有些 I/O 密集型进程，多数时间用来等待 I/O 结束，当这样的进程需要 CPU 时，应立即分配 CPU 以便启动下一个 I/O 请求，这样就可以在另一个进程执行计算时执行 I/O 操作。使这类 I/O 密集型进程长时间等待 CPU 只会造成其无谓地长时间占用内存。使 I/O 密集型进程获得较好服务的一种简单方法是，将其优先级设置为 \\(1/f\\)，其中 \\(f\\) 为该进程在上一时间片中所占用的部分。另一种方式是让用户进程主动的调整进程的优先级，如此可以使调度机制采用适用于当前环境的调度策略。 相容分时系统相容分时系统 (Compatible Time Sharing System，CTSS) 是 MIT 在 IBM 7094 上开发的最早使用优先级调度的系统之一。CTSS 中存在进程切换速度太慢的问题，这与 IBM 7094 内存中只能存在一个进程有关。CTSS 的设计者很快便认识到，为 CPU 密集型进程设置较长的时间片比频繁分给它们时间片更加高效，但长时间片又会影响到响应时间。其解决方法即建立优先级类，最高优先级的进程运行 1 个 quantum，次高优先级进程运行 2 个 quantum，再次一级运行 4 个 quantum，以此类推。 当一个进程用完分配的 quantum 后，被移动到下一级。如此，以前可能需要 100 次调度的进程，可以减少到 7 次调度，并且随着优先级的不断降低，它的运行频率会不断放慢，从而为短的交互进程让出 CPU。对于刚刚运行过长时间但又需要交互的进程，为防止其永远处于惩罚状态，可以将其调整到高优先级队伍，提高其响应速度。 最短进程优先SJF 算法往往有着最短响应时间，所以如果能够把其用于交互式进程是最好的。当前的问题是如何找出当前可运行进程中最短的那个进程。 可以根据进程过去的行为进行推测，并执行估计运行时间最短的那个。假设某终端上每条命令的估计运行时间为 \\(T_0\\) ，现在假设测量到其下一次运行时间为 \\(T_1\\) 。可以用这两个值的加权和 \\(\\alpha T_0 + (1-\\alpha) T_1\\) 来改进估计时间。通过选择 \\(\\alpha\\) 的值，可以决定是尽快忘掉老的运行时间，还是在一段长时间内始终记住他们。当 \\(\\alpha = 1 / 2\\) 时，可以得到如下序列： \\[T_0, \\quad \\frac{T_0}{2}+\\frac{T_1}{2}, \\quad \\frac{T_0}{4}+\\frac{T_1}{4}+\\frac{T_2}{2}, \\quad \\frac{T_0}{8}+\\frac{T_1}{8}+\\frac{T_2}{4}+\\frac{T_3}{2}, \\quad \\dots\\] 可以看到，三轮过后，\\(T_0\\) 在新的估计值中所占的比重下降到了 \\(1 / 8\\)。将这种通过当前测量值与先前估计值进行加权平均而得到下一个估计值的技术，称为 老化 (aging)，它适用于许多测量值必须基于先前值的情况。 其他调度算法 保证调度 向用户作出明确的性能保证并实现它，是一种独特的调度算法。如现在有 n 个用户登陆终端，可以保证每个用户将会获得 CPU 处理能力的 \\(\\frac{1}{n}\\)。或者说，在单用户系统中，帮证运行的 n 个进程，每个都可以获得 \\(\\frac{1}{n}\\) 的 CPU 时间。为了实现所做的保证，系统必须跟踪各个进程自创建以来，已使用的 CPU 时间，然后计算各个进程应获得的时间。 彩票调度 为用户做出承诺并兑现，是一个好方法，但难以实现。有一个既可以给出类似预测结果又可以简单实现的算法，即 彩票调度 (lottery scheduling)。其基本思想是：为进程提供各个系统资源的彩票，一旦需要做出一项调度决策时，就随机抽出一张彩票，拥有该彩票的进程获得资源。比如系统可以掌握每秒 50 次的彩票，作为奖励每个获奖者可以获得 20 ms 的 CPU 时间。所有进程是平等的，但某些进程更平等一些，对于重要的进程可以分配额外的彩票，以便增加它们的获胜机会。比如一共 100 张彩票，一个进程持有其中的 20 张，那么该进程在较长的运行中会得到 \\(20\\%\\) 的系统资源。即拥有彩票 f 份额的进程大约得到系统资源的 f 份额。 公平分享调度 我们假设被调度的都是进程自身，并不关注其所有者是谁。这样的结果就是，如果用户 A 拥有 9 个进程，而用户 2 拥有 1 个进程，那么轮转调度或同优先级调度算法中，用户 2 仅能使用 CPU 的 \\(90\\%\\) 。为避免这样的情况，可以在调度处理前考虑谁拥有该进程，让每个用户可以公平的分配 CPU 时间，而调度程序以强制的方式选择进程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#相容分时系统"},{"categories":["OperatingSystem"],"content":" 交互式系统中的调度 轮转调度轮转调度 (round robin) 是一种 最古老、最简单、最公平 且 使用最广 的调度算法，每个进程被分配一个时间段，称为 时间片 (quantum)，即允许该进程在该时间段中运行。如果在 quantum 结束时该进程依然在运行，则剥夺 CPU 并分配给其他进程。如果该进程在 quantum 结束前阻塞或结束，则立即切换进程。 轮转调度中最有趣的即是时间片的大小。从一个进程切换到另一个进程是需要一定时间进行事务处理的，即我们之前说的保存、装入寄存器值与内存映像等。假设 进程切换 (process switch) 或称 上下文切换 (context switch) 需要 1 ms，时间片大小为 4 ms，那么 CPU 将会有 \\(20\\%\\) 的时间被浪费在进程切换中。如果将时间片调整为 100 ms，那么 CPU 仅浪费 \\(1\\%\\) 的时间，但是如果有 50 个可运行进程且每个进程都会使用完整的时间片，那么最后一个进程需要等待 5 秒才可以使用到 CPU。如果时间片的大小长于平均的 CPU 突发时间，那么不会经常发生抢占。相反，在时间片耗费完之前多数进程会完成一个阻塞操作引起进程切换。抢占的消失改善了性能，因为进程切换只会发生在确实逻辑上有需要的时候，即进程被阻塞不能继续运行。 简单来说，太短的 quantum 会导致频繁的进程切换，降低 CPU 效率；太长的 quantum 可能引起对短交互请求的响应时间变长。因此 quantum 一般设置为 20 ~ 50 ms 一个比较折中的长度。 优先级调度轮转调度做了一个隐含的假设，即所有的进程同等重要，而拥有和操作多用户计算机系统的人对此常有不同的看法，将外部因素考虑在内的需要就导致了 优先级调度 。其基本思想很清楚，每个进程被赋予一个优先级，允许优先级最高的可运行进程先运行。为了防止高优先级进程无休止的运行，低优先级进程可能出现饥饿现象，调度程序可能在每个时钟中断降低当前进程的优先级，如果当前进程的优先级低于次高级优先级的进程，则进行进程切换。另一种方法是，每个进程拥有一个运行的最大时间片，当时间片耗尽时，调度次高优先级的进程。 优先级可以是静态赋予或动态赋予的，也可以由系统动态确定。例如有些 I/O 密集型进程，多数时间用来等待 I/O 结束，当这样的进程需要 CPU 时，应立即分配 CPU 以便启动下一个 I/O 请求，这样就可以在另一个进程执行计算时执行 I/O 操作。使这类 I/O 密集型进程长时间等待 CPU 只会造成其无谓地长时间占用内存。使 I/O 密集型进程获得较好服务的一种简单方法是，将其优先级设置为 \\(1/f\\)，其中 \\(f\\) 为该进程在上一时间片中所占用的部分。另一种方式是让用户进程主动的调整进程的优先级，如此可以使调度机制采用适用于当前环境的调度策略。 相容分时系统相容分时系统 (Compatible Time Sharing System，CTSS) 是 MIT 在 IBM 7094 上开发的最早使用优先级调度的系统之一。CTSS 中存在进程切换速度太慢的问题，这与 IBM 7094 内存中只能存在一个进程有关。CTSS 的设计者很快便认识到，为 CPU 密集型进程设置较长的时间片比频繁分给它们时间片更加高效，但长时间片又会影响到响应时间。其解决方法即建立优先级类，最高优先级的进程运行 1 个 quantum，次高优先级进程运行 2 个 quantum，再次一级运行 4 个 quantum，以此类推。 当一个进程用完分配的 quantum 后，被移动到下一级。如此，以前可能需要 100 次调度的进程，可以减少到 7 次调度，并且随着优先级的不断降低，它的运行频率会不断放慢，从而为短的交互进程让出 CPU。对于刚刚运行过长时间但又需要交互的进程，为防止其永远处于惩罚状态，可以将其调整到高优先级队伍，提高其响应速度。 最短进程优先SJF 算法往往有着最短响应时间，所以如果能够把其用于交互式进程是最好的。当前的问题是如何找出当前可运行进程中最短的那个进程。 可以根据进程过去的行为进行推测，并执行估计运行时间最短的那个。假设某终端上每条命令的估计运行时间为 \\(T_0\\) ，现在假设测量到其下一次运行时间为 \\(T_1\\) 。可以用这两个值的加权和 \\(\\alpha T_0 + (1-\\alpha) T_1\\) 来改进估计时间。通过选择 \\(\\alpha\\) 的值，可以决定是尽快忘掉老的运行时间，还是在一段长时间内始终记住他们。当 \\(\\alpha = 1 / 2\\) 时，可以得到如下序列： \\[T_0, \\quad \\frac{T_0}{2}+\\frac{T_1}{2}, \\quad \\frac{T_0}{4}+\\frac{T_1}{4}+\\frac{T_2}{2}, \\quad \\frac{T_0}{8}+\\frac{T_1}{8}+\\frac{T_2}{4}+\\frac{T_3}{2}, \\quad \\dots\\] 可以看到，三轮过后，\\(T_0\\) 在新的估计值中所占的比重下降到了 \\(1 / 8\\)。将这种通过当前测量值与先前估计值进行加权平均而得到下一个估计值的技术，称为 老化 (aging)，它适用于许多测量值必须基于先前值的情况。 其他调度算法 保证调度 向用户作出明确的性能保证并实现它，是一种独特的调度算法。如现在有 n 个用户登陆终端，可以保证每个用户将会获得 CPU 处理能力的 \\(\\frac{1}{n}\\)。或者说，在单用户系统中，帮证运行的 n 个进程，每个都可以获得 \\(\\frac{1}{n}\\) 的 CPU 时间。为了实现所做的保证，系统必须跟踪各个进程自创建以来，已使用的 CPU 时间，然后计算各个进程应获得的时间。 彩票调度 为用户做出承诺并兑现，是一个好方法，但难以实现。有一个既可以给出类似预测结果又可以简单实现的算法，即 彩票调度 (lottery scheduling)。其基本思想是：为进程提供各个系统资源的彩票，一旦需要做出一项调度决策时，就随机抽出一张彩票，拥有该彩票的进程获得资源。比如系统可以掌握每秒 50 次的彩票，作为奖励每个获奖者可以获得 20 ms 的 CPU 时间。所有进程是平等的，但某些进程更平等一些，对于重要的进程可以分配额外的彩票，以便增加它们的获胜机会。比如一共 100 张彩票，一个进程持有其中的 20 张，那么该进程在较长的运行中会得到 \\(20\\%\\) 的系统资源。即拥有彩票 f 份额的进程大约得到系统资源的 f 份额。 公平分享调度 我们假设被调度的都是进程自身，并不关注其所有者是谁。这样的结果就是，如果用户 A 拥有 9 个进程，而用户 2 拥有 1 个进程，那么轮转调度或同优先级调度算法中，用户 2 仅能使用 CPU 的 \\(90\\%\\) 。为避免这样的情况，可以在调度处理前考虑谁拥有该进程，让每个用户可以公平的分配 CPU 时间，而调度程序以强制的方式选择进程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#最短进程优先"},{"categories":["OperatingSystem"],"content":" 交互式系统中的调度 轮转调度轮转调度 (round robin) 是一种 最古老、最简单、最公平 且 使用最广 的调度算法，每个进程被分配一个时间段，称为 时间片 (quantum)，即允许该进程在该时间段中运行。如果在 quantum 结束时该进程依然在运行，则剥夺 CPU 并分配给其他进程。如果该进程在 quantum 结束前阻塞或结束，则立即切换进程。 轮转调度中最有趣的即是时间片的大小。从一个进程切换到另一个进程是需要一定时间进行事务处理的，即我们之前说的保存、装入寄存器值与内存映像等。假设 进程切换 (process switch) 或称 上下文切换 (context switch) 需要 1 ms，时间片大小为 4 ms，那么 CPU 将会有 \\(20\\%\\) 的时间被浪费在进程切换中。如果将时间片调整为 100 ms，那么 CPU 仅浪费 \\(1\\%\\) 的时间，但是如果有 50 个可运行进程且每个进程都会使用完整的时间片，那么最后一个进程需要等待 5 秒才可以使用到 CPU。如果时间片的大小长于平均的 CPU 突发时间，那么不会经常发生抢占。相反，在时间片耗费完之前多数进程会完成一个阻塞操作引起进程切换。抢占的消失改善了性能，因为进程切换只会发生在确实逻辑上有需要的时候，即进程被阻塞不能继续运行。 简单来说，太短的 quantum 会导致频繁的进程切换，降低 CPU 效率；太长的 quantum 可能引起对短交互请求的响应时间变长。因此 quantum 一般设置为 20 ~ 50 ms 一个比较折中的长度。 优先级调度轮转调度做了一个隐含的假设，即所有的进程同等重要，而拥有和操作多用户计算机系统的人对此常有不同的看法，将外部因素考虑在内的需要就导致了 优先级调度 。其基本思想很清楚，每个进程被赋予一个优先级，允许优先级最高的可运行进程先运行。为了防止高优先级进程无休止的运行，低优先级进程可能出现饥饿现象，调度程序可能在每个时钟中断降低当前进程的优先级，如果当前进程的优先级低于次高级优先级的进程，则进行进程切换。另一种方法是，每个进程拥有一个运行的最大时间片，当时间片耗尽时，调度次高优先级的进程。 优先级可以是静态赋予或动态赋予的，也可以由系统动态确定。例如有些 I/O 密集型进程，多数时间用来等待 I/O 结束，当这样的进程需要 CPU 时，应立即分配 CPU 以便启动下一个 I/O 请求，这样就可以在另一个进程执行计算时执行 I/O 操作。使这类 I/O 密集型进程长时间等待 CPU 只会造成其无谓地长时间占用内存。使 I/O 密集型进程获得较好服务的一种简单方法是，将其优先级设置为 \\(1/f\\)，其中 \\(f\\) 为该进程在上一时间片中所占用的部分。另一种方式是让用户进程主动的调整进程的优先级，如此可以使调度机制采用适用于当前环境的调度策略。 相容分时系统相容分时系统 (Compatible Time Sharing System，CTSS) 是 MIT 在 IBM 7094 上开发的最早使用优先级调度的系统之一。CTSS 中存在进程切换速度太慢的问题，这与 IBM 7094 内存中只能存在一个进程有关。CTSS 的设计者很快便认识到，为 CPU 密集型进程设置较长的时间片比频繁分给它们时间片更加高效，但长时间片又会影响到响应时间。其解决方法即建立优先级类，最高优先级的进程运行 1 个 quantum，次高优先级进程运行 2 个 quantum，再次一级运行 4 个 quantum，以此类推。 当一个进程用完分配的 quantum 后，被移动到下一级。如此，以前可能需要 100 次调度的进程，可以减少到 7 次调度，并且随着优先级的不断降低，它的运行频率会不断放慢，从而为短的交互进程让出 CPU。对于刚刚运行过长时间但又需要交互的进程，为防止其永远处于惩罚状态，可以将其调整到高优先级队伍，提高其响应速度。 最短进程优先SJF 算法往往有着最短响应时间，所以如果能够把其用于交互式进程是最好的。当前的问题是如何找出当前可运行进程中最短的那个进程。 可以根据进程过去的行为进行推测，并执行估计运行时间最短的那个。假设某终端上每条命令的估计运行时间为 \\(T_0\\) ，现在假设测量到其下一次运行时间为 \\(T_1\\) 。可以用这两个值的加权和 \\(\\alpha T_0 + (1-\\alpha) T_1\\) 来改进估计时间。通过选择 \\(\\alpha\\) 的值，可以决定是尽快忘掉老的运行时间，还是在一段长时间内始终记住他们。当 \\(\\alpha = 1 / 2\\) 时，可以得到如下序列： \\[T_0, \\quad \\frac{T_0}{2}+\\frac{T_1}{2}, \\quad \\frac{T_0}{4}+\\frac{T_1}{4}+\\frac{T_2}{2}, \\quad \\frac{T_0}{8}+\\frac{T_1}{8}+\\frac{T_2}{4}+\\frac{T_3}{2}, \\quad \\dots\\] 可以看到，三轮过后，\\(T_0\\) 在新的估计值中所占的比重下降到了 \\(1 / 8\\)。将这种通过当前测量值与先前估计值进行加权平均而得到下一个估计值的技术，称为 老化 (aging)，它适用于许多测量值必须基于先前值的情况。 其他调度算法 保证调度 向用户作出明确的性能保证并实现它，是一种独特的调度算法。如现在有 n 个用户登陆终端，可以保证每个用户将会获得 CPU 处理能力的 \\(\\frac{1}{n}\\)。或者说，在单用户系统中，帮证运行的 n 个进程，每个都可以获得 \\(\\frac{1}{n}\\) 的 CPU 时间。为了实现所做的保证，系统必须跟踪各个进程自创建以来，已使用的 CPU 时间，然后计算各个进程应获得的时间。 彩票调度 为用户做出承诺并兑现，是一个好方法，但难以实现。有一个既可以给出类似预测结果又可以简单实现的算法，即 彩票调度 (lottery scheduling)。其基本思想是：为进程提供各个系统资源的彩票，一旦需要做出一项调度决策时，就随机抽出一张彩票，拥有该彩票的进程获得资源。比如系统可以掌握每秒 50 次的彩票，作为奖励每个获奖者可以获得 20 ms 的 CPU 时间。所有进程是平等的，但某些进程更平等一些，对于重要的进程可以分配额外的彩票，以便增加它们的获胜机会。比如一共 100 张彩票，一个进程持有其中的 20 张，那么该进程在较长的运行中会得到 \\(20\\%\\) 的系统资源。即拥有彩票 f 份额的进程大约得到系统资源的 f 份额。 公平分享调度 我们假设被调度的都是进程自身，并不关注其所有者是谁。这样的结果就是，如果用户 A 拥有 9 个进程，而用户 2 拥有 1 个进程，那么轮转调度或同优先级调度算法中，用户 2 仅能使用 CPU 的 \\(90\\%\\) 。为避免这样的情况，可以在调度处理前考虑谁拥有该进程，让每个用户可以公平的分配 CPU 时间，而调度程序以强制的方式选择进程。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#其他调度算法"},{"categories":["OperatingSystem"],"content":" 实时系统中的调度实时系统是一种时间起主导作用的系统。典型地，一种或多种外部物理设备发给计算机一个服务请求，计算机必须在一个确定的时间范围内恰当地做出反应。比如音乐播放器必须在非常短的时间间隔内将 bit 流转为音乐，否则听起来就会十分的诡异。因此在这类系统中 正确但迟到的应答往往比没有更加糟糕。 实时系统通常可以分为 硬实时 (hard real time) 与 软实时 (soft real time)，前者是必须满足绝对的截止时间，后者是虽然不希望偶尔错失截止时间但可以容忍。实时性能都是通过把程序分为一组进程而实现的，每个进程的行为是可预测和提前掌握的。这些进程一般寿命较短，且极快完成。在检测到一个外部信号时，调度程序的任务就是按照满足所有截止时间的要求调度程序。 实时系统中的时间可以按照效应方式进一步分为 周期性 与 非周期性 事件，一个系统可能要响应多个周期事件流。根据每个事件要处理时间的长短，系统甚至有可能无法处理完所有事件。如果有 m 个周期事件，事件 i 以周期 \\(P_i\\) 发生，并需要 \\(C_i\\) 秒 CPU 时间处理，那么可以处理负载的条件是 \\[\\sum_{i=1}^{m} \\frac{C_i}{P_i} \\leq 1\\] 满足这个条件的实时系统称为是 可调度的，这意味着它实际能够被实现的。一个不满足此检测标准的进程不能被调度，因为这些进程共同需要的 CPU 时间总和大于 CPU 能够提供的时间。这里假设进程切换的开销极小，可以忽略不计。 实时系统的调度算法可以是静态或动态的，前者在系统开始运行之前做出调度决策，后者在运行过程中进行调度决策。只有在可以提前掌握所完成的工作以及必须满足的截止时间等全部信息时，静态调度才能工作，而动态算法并不需要这些限制。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:4:3","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#实时系统中的调度"},{"categories":["OperatingSystem"],"content":" 经典的 IPC 问题","date":"07-11","objectID":"/2021/operatingsystem_001/:5:0","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#经典的-ipc-问题"},{"categories":["OperatingSystem"],"content":" 哲学家就餐问题Dijkstra 于 1965 年提出并解决了 哲学家就餐 的同步问题，该问题属于互斥访问有限资源的竞争问题，自那时起每个发明新的同步原语的人都希望通过解决哲学家就餐问题来展示其原语的精妙之处。 最直观的解决方法，即指定哲学家可用的叉子，并在指定的叉子可用时取用。但是有一个明显的问题，即所有哲学家同时拿起左面的叉子，就没有人可以拿到他们右面的叉子。如此情况就会发生 死锁 (deadlock)。稍加修改，可以在拿起左叉后检查另一把叉子是否可用，不可用则放下叉子。这也有一个显著的问题，即所有哲学家同时拿到左叉，发现右面的叉子不可用，又都放下手中的叉子，一段时间之后如此重复下去。对于这种情况，所有程序都在不停的运行，但又无法取得进展，如此被称为 饥饿 (starvation)。 当使用 binary semaphore 时既不会 deadlock 也不会 starvation，在拿起叉子之前对 semaphore 进行 P 操作，并在结束用餐时进行 V 操作。此方法可行，但只能有一位哲学家进餐。对其进行扩展，采用一个 binary semaphore 数组表示哲学家的进餐状态，即可最大程度的并行。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:5:1","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#哲学家就餐问题"},{"categories":["OperatingSystem"],"content":" 读者-写者问题读写者问题为数据库访问建立了一个模型，多个读进程可以同时进行，但读进程与写进程互斥，当写进程进入临界区后将与任何进程互斥。 ","date":"07-11","objectID":"/2021/operatingsystem_001/:5:2","series":["Operating System Note"],"tags":["Note","Process","Thread"],"title":"进程与线程","uri":"/2021/operatingsystem_001/#读者-写者问题"},{"categories":["Applications"],"content":"GinShio | KMS 服务器的搭建和使用","date":"05-09","objectID":"/2021/kms/","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/"},{"categories":["Applications"],"content":"微软的软件主要可以通过以下三个渠道获取: 零售 原始设备制造商 (OEM) 批量许可协议 OEM 在工厂执行激活，比如说新买的笔记本电脑自带的系统就是这种方式。零售主要通过联机、电话或 VAMT 代理激活。批量激活产品主要选择 MAK (多次激活密钥) 、 KMS (密钥管理服务) 以及 AD (Active Directory) 进行激活。 KMS 可以在本地网络完成激活，而无需将个别计算机连接到 Microsoft 进行产品激活。KMS 不需要专用系统的轻型服务，可以轻易地将其联合托管在提供其他服务的系统上。 KMS 服务器可以为局域网内所有连接的产品进行周期性激活，激活周期一般为 180 天，产品激活后会定期连接 KMS 服务器进行验证、续期，如果不能连接到服务器在激活周期过后，产品将过期而需要重新激活。 KMS 服务激活的是 VL 版，而我们常用的 RTL (零售版) 是无法激活的。自己搭建 KMS 服务器激活产品，虽然可以正常使用，但是不能算正版软件，请支持正版！ ","date":"05-09","objectID":"/2021/kms/:0:0","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#"},{"categories":["Applications"],"content":" 部署 KMS 服务器常用的 Microsoft KMS 服务器是开源的 Vlmcsd，它可以部署到不同平台上提供服务。 Vlmcsd 的使用很简单，下载下来启动即可提供服务，默认端口号是 1688 ","date":"05-09","objectID":"/2021/kms/:1:0","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#部署-kms-服务器"},{"categories":["Applications"],"content":" Windows对于 Windows 的下载，可以选择 官方渠道 或通过 MSDN, I tell you 进行下载，安装的专业版均可以 KMS 激活 ","date":"05-09","objectID":"/2021/kms/:2:0","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#windows"},{"categories":["Applications"],"content":" 激活 Windows相对来说激活 Windows 也很简单，以管理员身份打开 Powershell 或命令提示符，并输入命令即可激活 设置 GVLK，这里我们以 Windows Server 2016 标准版为例 slmgr /ipk WC2BQ-8NRM3-FDDYY-2BFGV-KHKQY 设置 KMS 服务器 slmgr /skms example.com 如果 KMS 服务端口号不是 1688 需要显示指定端口号，比如端口号是 1024 slmgr /skms example.com:1024 激活 Windows slmgr /ato 这是一个可选项，查看激活情况 slmgr /xpr ","date":"05-09","objectID":"/2021/kms/:2:1","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#激活-windows"},{"categories":["Applications"],"content":" Windows GVLK在此列出各个版本对应的 GVLK，所有的 Key 也可以前往 官方文档 查看 Windows Server Semi-Annual Channel (Windows Server 半年版) [自 1809 起] 操作系统版本 GVLK Datacenter 6NMRW-2C8FM-D24W7-TQWMY-CWH2D Standard N2KJX-J94YW-TQVFB-DG9YT-724CC Windows Server LTSC/LTSB 操作系统版本 GVLK 2022 Datacenter WX4NM-KYWYW-QJJR4-XV3QB-6VM33 2022 Standard VDYBN-27WPP-V4HQT-9VMD4-VMK7H 2019 Datacenter WMDGN-G9PQG-XVVXX-R3X43-63DFG 2019 Standard N69G4-B89J2-4G8F4-WWYCC-J464C 2019 Essentials WVDHN-86M7X-466P6-VHXV7-YY726 2016 Datacenter CB7KF-BWN84-R7R2Y-793K2-8XDDG 2016 Standard WC2BQ-8NRM3-FDDYY-2BFGV-KHKQY 2016 Essentials JCKRF-N37P4-C2D82-9YXRT-4M63B Windows 10 / Windows 11 Semi-Annual Channel (Windows 10 / Windows 11 半年版) 操作系统版本 中译 GVLK Pro 专业版 W269N-WFGWX-YVC9B-4J6C9-T83GX Pro N 专业版 N MH37W-N47XK-V7XM9-C7227-GCQG9 Pro for Workstations 专业工作站版 NRG8B-VKK3Q-CXVCJ-9G2XF-6Q84J Pro for Workstations N 专业工作站版 N 9FNHH-K3HBT-3W4TD-6383H-6XYWF Pro Education 专业教育版 6TP4R-GNPTD-KYYHQ-7B7DP-J447Y Pro Education N 专业教育版 N YVWGF-BXNMC-HTQYQ-CPQ99-66QFC Education 教育版 NW6C2-QMPVW-D7KKK-3GKT6-VCFB2 Education N 教育版 N 2WH4N-8QGBV-H22JP-CT43Q-MDWWJ Enterprise 企业版 NPPR9-FWDCX-D2C8J-H872K-2YT43 Enterprise N 企业版 N DPH2V-TTNVB-4X9Q3-TJR4H-KHJW4 Enterprise G 企业版 G YYVX9-NTFWV-6MDM3-9PT4T-4M68B Enterprise G N 企业版 G N 44RPN-FTY23-9VTTB-MP9BX-T84FV Windows 10 LTSC/LTSB 操作系统版本 中译 GVLK Enterprise LTSC 2019 / 2021 企业版 LTSC 2019 / 2021 M7XTQ-FN8P6-TTKYV-9D4CC-J462D Enterprise N LTSC 2019 / 2021 企业版 N LTSC 2019 / 2021 92NFX-8DJQP-P6BBQ-THF9C-7CG2H Enterprise LTSB 2016 企业版 LTSB 2016 DCPHK-NFMTC-H88MJ-PFHPY-QJ4BJ Enterprise N LTSB 2016 企业版 N LTSB 2016 QFFDN-GRT3P-VKWWX-X7T3R-8B639 Enterprise LTSB 2015 企业版 LTSB 2015 WNMTR-4C88C-JK8YV-HQ7T2-76DF9 Enterprise N LTSB 2015 企业版 N LTSB 2015 2F77B-TNFGY-69QQF-B8YKP-D69TJ Windows 8.1 操作系统版本 中译 GVLK Pro 专业版 GCRJD-8NW9H-F2CDX-CCM8D-9D6T9 Pro N 专业版 N HMCNV-VVBFX-7HMBH-CTY9B-B4FXY Enterprise 企业版 MHF9N-XY6XB-WVXMC-BTDCT-MKKG7 Enterprise N 企业版 N TT4HM-HN7YT-62K67-RGRQJ-JFFXW Windows 8 操作系统版本 中译 GVLK Pro 专业版 NG4HW-VH26C-733KW-K6F98-J8CK4 Pro N 专业版 N XCVCF-2NXM9-723PB-MHCB7-2RYQQ Enterprise 企业版 32JNW-9KQ84-P47T8-D8GGY-CWCK7 Enterprise N 企业版 N JMNMF-RHW7P-DMY6X-RF3DR-X2BQT Windows 7 操作系统版本 中译 GVLK Professional 专业版 FJ82H-XT6CR-J8D7P-XQJJ2-GPDD4 Professional N 专业版 N MRPKT-YTG23-K7D7T-X2JMM-QY7MG Professional E 专业版 E W82YF-2Q76Y-63HXB-FGJG9-GF7QX Enterprise 企业版 33PXH-7Y6KF-2VJC9-XBBR8-HVTHH Enterprise N 企业版 N YDRBP-3D83W-TY26F-D46B2-XCKRJ Enterprise E 企业版 E C29WB-22CC8-VJ326-GHFJW-H9DH4 ","date":"05-09","objectID":"/2021/kms/:2:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#windows-gvlk"},{"categories":["Applications"],"content":" Office由于 Office 的 RTL 转 VL 比较麻烦，这里我们使用 VL 版进行激活操作 ","date":"05-09","objectID":"/2021/kms/:3:0","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#office"},{"categories":["Applications"],"content":" 安装 Office VL我们首先下载 Office Deployment Tool (ODT)，即 Office 部署工具，运行 ODT 将会获取到一个 setup 和示例的配置文件。setup 是用于安装 Office 的可执行程序，而配置文件是指示 setup 如何安装 Office 的配置文件。 我们将使用配置文件 config.xml 进行安装，以管理员身份打开 Powershell 或命令提示符，并输入命令进行安装。至于 ODT 配置文件的内容，将在下节介绍。 根据配置文件下载数据 setup.exe /download config.xml 根据配置文件安装 Office setup.exe /configure config.xml ","date":"05-09","objectID":"/2021/kms/:3:1","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#安装-office-vl"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 \u003cConfiguration\u003e \u003cAdd OfficeClientEdition=\"64\" Channel=\"PerpetualVL2019\"\u003e \u003cProduct ID=\"ProPlus2019Volume\"\u003e \u003cLanguage ID=\"zh-CN\" /\u003e \u003c/Product\u003e \u003c/Add\u003e \u003c/Configuration\u003e 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 \u003cConfiguration\u003e \u003cAdd OfficeClientEdition=\"64\" Channel=\"PerpetualVL2019\"\u003e \u003cProduct ID=\"ProPlus2019Volume\"\u003e \u003cLanguage ID=\"zh-CN\" /\u003e \u003cLanguage ID=\"en-US\" /\u003e \u003c/Product\u003e \u003cProduct ID=\"VisioStd2019Volume\"\u003e \u003cLanguage ID=\"zh-CN\" /\u003e \u003cLanguage ID=\"en-US\" /\u003e \u003c/Product\u003e \u003cProduct ID=\"ProjectStd2019Volume\"\u003e \u003cLan","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#odt-配置文件"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#add-元素"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#product-元素"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#language-元素"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#excludeapp-元素"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#remove-元素"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#updates-元素"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#removemsi-元素"},{"categories":["Applications"],"content":" ODT 配置文件配置文件采用 XML 格式，我们详细说明一下文件中的标签元素，也可以查看 官方文档 Add 元素Add 用于定义要下载的 产品 和 语言 SourcePath (可选) 用于定义 安装文件的位置 ，这是下载的数据文件的位置，而非最终的安装位置。如果没有定义该属性，则会在 ODT 所处的文件夹下下载数据。如果 SourcePath 下包含相同版本的 Office 数据文件，那么 ODT 会增量下载文件以节省网络带宽。示例值 \\server\\share、c:\\preload\\office Version (可选) 用于指定 安装的版本 ，默认为可用的最新版本，推荐与 Channel 属性一同使用。可以选择 MatchInstalled 作为值，即使有新的可用版本，也将下载与已安装的 Office 版本相同的数据，此选项在添加语言包、Visio、Project 时十分有用。示例值 16.0.8201.2193、MatchInstalled OfficeClientEdition (可选) 指定 Office 的位版本，默认安装 64 位 Office，如果 Windows 版本为 64 位或内存小于 4 GB 则安装 32 位。如果已安装 Office 时，默认会与已安装的版本匹配，Office 不支持混合体系，请小心设置该属性 Channel (可选) 用于定义安装 Office 的更新通道，如果未安装 Office 则默认为 Current ，已安装的情况下会自动匹配相同的频道。Office 2019 受支持的更新频道为 PerpetualVL2019 ，2021 则是 PerpetualVL2021 Product 元素Product 定义要下载或安装的产品。如果定义了多个产品，这些产品会按照配置文件中的顺序进行安装。指定产品的 ID，更多 ID 请参阅 官方文档 遗憾的是我没有找到 Office 2016 的 KEY，但是可以通过 Volume License Pack 获取安装包，如果你知道如何用 ODT 安装请告诉我 Office 2021 产品 中译 ID Access 2021 Access2021Volume Excel 2021 Excel2021Volume OneNote 2021 OneNote2021Volume Outlook 2021 Outlook2021Volume PowerPoint 2021 PowerPoint2021Volume Project Professional 2021 Project 专业版 2021 ProjectPro2021Volume Project Standard 2021 Project 标准版 2021 ProjectStd2021Volume Office Pro Plus 2021 Office 专业增强版 2021 ProPlus2021Volume Office Standard 2021 Office 标准版 2021 Standard2021Volume Publisher 2021 Publisher2021Volume Visio Professional 2021 Visio 专业版 2021 VisioPro2021Volume Visio Standard 2021 Visio 标准版 2021 VisioStd2021Volume Word 2021 Word2021Volume Skype for Business LTSC 2021 Skype 商业版 LTSC 2021 SkypeforBusiness2021Volume Office 2019 产品 中译 ID Access 2019 Access2019Volume Excel 2019 Excel2019Volume Outlook 2019 Outlook2019Volume PowerPoint 2019 PowerPoint2019Volume Project Professional 2019 Project 专业版 2019 ProjectPro2019Volume Project Standard 2019 Project 标准版 2019 ProjectStd2019Volume Office Pro Plus 2019 Office 专业增强版 2019 ProPlus2019Volume Office Standard 2019 Office 标准版 2019 Standard2019Volume Publisher 2019 Publisher2019Volume Visio Professional 2019 Visio 专业版 2019 VisioPro2019Volume Visio Standard 2019 Visio 标准版 2019 VisioStd2019Volume Word 2019 Word2019Volume Skype for Business 2019 Skype 商业版 2019 SkypeforBusiness2019Volume Office 2016 Language 元素Language 定义要下载或安装的语言。如果定义了多个语言，首个语言决定了 UI 区域性、快捷方式、工具提示。 ID 属性的决定了具体下载、安装哪种语言。值 MatchInstalled 可以选择匹配已安装的 Office，MatchOS 可以选择匹配操作系统，也可以参考 官方文档 直接指定区域性代码 Table 1: 支持的常见语言列表 语言 英文名 区域性代码 (ll-CC) 校对语言 阿拉伯语 Arabic ar-SA 阿拉伯语、英语、法语 中文 (简体) Chinese (Simplified) zh-CN 中文 (简体)、英语 中文 (繁体) Chinese (Traditional) zh-TW 中文 (繁体)、英语 英语 English en-US 英语、法语、西班牙语 英语 (英国) English UK en-GB 英语、爱尔兰语、苏格兰盖尔语、威尔士语 法语 French fr-FR 法语、英语、德语、荷兰语、阿拉伯语、西班牙语 德语 German de-DE 德语、英语、法语、意大利语 日语 Japanese jp-JP 日语、英语 韩语 Korean ko-KR 韩语、英语 俄语 Russian ru-RU 俄语、英语、乌克兰语、德语 西班牙语 Spanish es-ES 西班牙语、英语、法语、加泰罗尼亚语、加利西亚语、葡萄牙语 (巴西) ExcludeApp 元素ExcludeApp 定义了不希望安装的应用程序，ID 属性为不安装的软件的 ID 产品 ID Access Access Excel Excel OneDrive OneDrive OneDrive for Business Groove OneNote OneNote Outlook Outlook PowerPoint PowerPoint Publisher Publisher Skype for Business Lync Teams Teams Word Word Remove 元素Remove 指定从旧的安装中删除哪些产品与语言，如果要删除指定语言必须同时指定子元素 Product 和 Language。如果要删除所有语言则不必指定子元素 Language。该元素不能作为 Add 的子元素 All 指定是否删除所有已安装的 Office (包含 Project 与 Visio)，此属性接受 Boolean Updates 元素Updates 定义如何在安装后更新 Office Enabled (可选) 默认值为 TRUE，设置 Office 是否检查更新 Channel (可选) 与 Add 元素的 Channel 属性相同 RemoveMSI 元素可选元素，在安装指定产品前，是否删除 MSI 安装的任何 Office 、 Visio 、 Project IgnoreProduct (可选) 指定忽略卸载的产品 ID 示例配置我们在此给出一些示例配置文件，以方便理解这些属性 安装 64 位 Office 专业增强版 安装 64 位 Office 专业增强版、Visio 标准版、Project 标准版，并支持中文与英文，并移除之前的 MSI 安装 ","date":"05-09","objectID":"/2021/kms/:3:2","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#示例配置"},{"categories":["Applications"],"content":" 激活 Office以管理员身份打开 Powershell 或命令提示符，并输入命令即可激活 进入 Office 目录，32 位安装在 C:\\Program Files (x86)\\Microsoft Office\\Office?? ，64 位安装在 C:\\Program Files\\Microsoft Office\\Office?? ，最后两位是数字，Office 这个数字好像是 16 cd \"C:\\Program Files\\Microsoft Office\\Office16\" 设置对应 Office 版本的 GVLK，这里以 Word 2016 为例 cscript ospp.vbs /inpkey:WXY84-JN2Q9-RBCCQ-3Q3J3-3PFJ6 注册 KMS 服务器，sethst 设置的是地址，setprt 设置的是端口，如果端口是 1688 则不需要设置 rem 设置地址 cscript ospp.vbs /sethst:example.com rem 设置端口 cscript ospp.vbs /setprt:1688 激活 cscript ospp.vbs /act 如果你希望查看 Office 的许可证状态，可以使用这条命令 cscript ospp.vbs /dstatus Office GVLKOffice 的 GVLK 可以在 官方文档 中查看 Office LTSC 2021 产品 中译 GVLK Office LTSC Professional Plus Office LTSC 专业增强版 FXYTK-NJJ8C-GB6DW-3DYQT-6F7TH Office LTSC Standard Office LTSC 标准版 KDX7X-BNVR8-TXXGX-4Q7Y8-78VT3 Project Professional Project 专业版 FTNWT-C6WBT-8HMGF-K9PRX-QV9H8 Project Standard Project 标准版 J2JDC-NJCYY-9RGQ4-YXWMH-T3D4T Visio LTSC Professional Visio LTSC 专业版 KNH8D-FGHT4-T8RK3-CTDYJ-K2HT4 Visio LTSC Standard Visio LTSC 标准版 MJVNY-BYWPY-CWV6J-2RKRT-4M8QG Access LTSC WM8YG-YNGDD-4JHDC-PG3F4-FC4T4 Excel LTSC NWG3X-87C9K-TC7YY-BC2G7-G6RVC Outlook LTSC C9FM6-3N72F-HFJXB-TM3V9-T86R9 PowerPoint LTSC TY7XF-NFRBR-KJ44C-G83KF-GX27K Publisher LTSC 2MW9D-N4BXM-9VBPG-Q7W6M-KFBGQ Skype for Business LTSC Skype 商业版 LTSC HWCXN-K3WBT-WJBKY-R8BD9-XK29P Word LTSC TN8H9-M34D3-Y64V9-TR72V-X79KV Office 2019 产品 中译 GVLK Office Professional Plus Office 专业增强版 NMMKJ-6RK4F-KMJVX-8D9MJ-6MWKP Office Standard Office 标准版 6NWWJ-YQWMR-QKGCB-6TMB3-9D9HK Project Professional Project 专业版 B4NPR-3FKK7-T2MBV-FRQ4W-PKD2B Project Standard Project 标准版 C4F7P-NCP8C-6CQPT-MQHV9-JXD2M Visio Professional Visio 专业版 9BGNQ-K37YR-RQHF2-38RQ3-7VCBB Visio Standard Visio 标准版 7TQNQ-K3YQQ-3PFH7-CCPPM-X4VQ2 Access 9N9PT-27V4Y-VJ2PD-YXFMF-YTFQT Excel TMJWT-YYNMB-3BKTF-644FC-RVXBD Outlook 7HD7K-N4PVK-BHBCQ-YWQRW-XW4VK PowerPoint RRNCX-C64HY-W2MM7-MCH9G-TJHMQ Publisher G2KWX-3NW6P-PY93R-JXK2T-C9Y9V Skype for Business Skype 商业版 NCJ33-JHBBY-HTK98-MYCV8-HMKHJ Word PBX3G-NWMT6-Q7XBW-PYJGG-WXD33 Office 2016 产品 中译 GVLK Office Professional Plus Office 专业增强版 XQNVK-8JYDB-WJ9W3-YJ8YR-WFG99 Office Standard Office 标准版 JNRGM-WHDWX-FJJG3-K47QV-DRTFM Project Professional Project 专业版 YG9NW-3K39V-2T3HJ-93F3Q-G83KT Project Standard Project 标准版 GNFHQ-F6YQM-KQDGJ-327XX-KQBVC Visio Professional Visio 专业版 PD3PC-RHNGV-FXJ29-8JK7D-RJRJK Visio Standard Visio 标准版 7WHWN-4T7MP-G96JF-G33KR-W8GF4 Access GNH9Y-D2J4T-FJHGG-QRVH7-QPFDW Excel 9C2PK-NWTVB-JMPW8-BFT28-7FTBF OneNote DR92N-9HTF2-97XKM-XW2WJ-XW3J6 Outlook R69KK-NTPKF-7M3Q4-QYBHW-6MT9B PowerPoint J7MQP-HNJ4Y-WJ7YM-PFYGF-BY6C6 Publisher F47MM-N3XJP-TQXJ9-BP99D-8K837 Skype for Business Skype 商业版 869NQ-FJ69K-466HW-QYCP2-DDBV6 Word WXY84-JN2Q9-RBCCQ-3Q3J3-3PFJ6 ","date":"05-09","objectID":"/2021/kms/:3:3","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#激活-office"},{"categories":["Applications"],"content":" 激活 Office以管理员身份打开 Powershell 或命令提示符，并输入命令即可激活 进入 Office 目录，32 位安装在 C:\\Program Files (x86)\\Microsoft Office\\Office?? ，64 位安装在 C:\\Program Files\\Microsoft Office\\Office?? ，最后两位是数字，Office 这个数字好像是 16 cd \"C:\\Program Files\\Microsoft Office\\Office16\" 设置对应 Office 版本的 GVLK，这里以 Word 2016 为例 cscript ospp.vbs /inpkey:WXY84-JN2Q9-RBCCQ-3Q3J3-3PFJ6 注册 KMS 服务器，sethst 设置的是地址，setprt 设置的是端口，如果端口是 1688 则不需要设置 rem 设置地址 cscript ospp.vbs /sethst:example.com rem 设置端口 cscript ospp.vbs /setprt:1688 激活 cscript ospp.vbs /act 如果你希望查看 Office 的许可证状态，可以使用这条命令 cscript ospp.vbs /dstatus Office GVLKOffice 的 GVLK 可以在 官方文档 中查看 Office LTSC 2021 产品 中译 GVLK Office LTSC Professional Plus Office LTSC 专业增强版 FXYTK-NJJ8C-GB6DW-3DYQT-6F7TH Office LTSC Standard Office LTSC 标准版 KDX7X-BNVR8-TXXGX-4Q7Y8-78VT3 Project Professional Project 专业版 FTNWT-C6WBT-8HMGF-K9PRX-QV9H8 Project Standard Project 标准版 J2JDC-NJCYY-9RGQ4-YXWMH-T3D4T Visio LTSC Professional Visio LTSC 专业版 KNH8D-FGHT4-T8RK3-CTDYJ-K2HT4 Visio LTSC Standard Visio LTSC 标准版 MJVNY-BYWPY-CWV6J-2RKRT-4M8QG Access LTSC WM8YG-YNGDD-4JHDC-PG3F4-FC4T4 Excel LTSC NWG3X-87C9K-TC7YY-BC2G7-G6RVC Outlook LTSC C9FM6-3N72F-HFJXB-TM3V9-T86R9 PowerPoint LTSC TY7XF-NFRBR-KJ44C-G83KF-GX27K Publisher LTSC 2MW9D-N4BXM-9VBPG-Q7W6M-KFBGQ Skype for Business LTSC Skype 商业版 LTSC HWCXN-K3WBT-WJBKY-R8BD9-XK29P Word LTSC TN8H9-M34D3-Y64V9-TR72V-X79KV Office 2019 产品 中译 GVLK Office Professional Plus Office 专业增强版 NMMKJ-6RK4F-KMJVX-8D9MJ-6MWKP Office Standard Office 标准版 6NWWJ-YQWMR-QKGCB-6TMB3-9D9HK Project Professional Project 专业版 B4NPR-3FKK7-T2MBV-FRQ4W-PKD2B Project Standard Project 标准版 C4F7P-NCP8C-6CQPT-MQHV9-JXD2M Visio Professional Visio 专业版 9BGNQ-K37YR-RQHF2-38RQ3-7VCBB Visio Standard Visio 标准版 7TQNQ-K3YQQ-3PFH7-CCPPM-X4VQ2 Access 9N9PT-27V4Y-VJ2PD-YXFMF-YTFQT Excel TMJWT-YYNMB-3BKTF-644FC-RVXBD Outlook 7HD7K-N4PVK-BHBCQ-YWQRW-XW4VK PowerPoint RRNCX-C64HY-W2MM7-MCH9G-TJHMQ Publisher G2KWX-3NW6P-PY93R-JXK2T-C9Y9V Skype for Business Skype 商业版 NCJ33-JHBBY-HTK98-MYCV8-HMKHJ Word PBX3G-NWMT6-Q7XBW-PYJGG-WXD33 Office 2016 产品 中译 GVLK Office Professional Plus Office 专业增强版 XQNVK-8JYDB-WJ9W3-YJ8YR-WFG99 Office Standard Office 标准版 JNRGM-WHDWX-FJJG3-K47QV-DRTFM Project Professional Project 专业版 YG9NW-3K39V-2T3HJ-93F3Q-G83KT Project Standard Project 标准版 GNFHQ-F6YQM-KQDGJ-327XX-KQBVC Visio Professional Visio 专业版 PD3PC-RHNGV-FXJ29-8JK7D-RJRJK Visio Standard Visio 标准版 7WHWN-4T7MP-G96JF-G33KR-W8GF4 Access GNH9Y-D2J4T-FJHGG-QRVH7-QPFDW Excel 9C2PK-NWTVB-JMPW8-BFT28-7FTBF OneNote DR92N-9HTF2-97XKM-XW2WJ-XW3J6 Outlook R69KK-NTPKF-7M3Q4-QYBHW-6MT9B PowerPoint J7MQP-HNJ4Y-WJ7YM-PFYGF-BY6C6 Publisher F47MM-N3XJP-TQXJ9-BP99D-8K837 Skype for Business Skype 商业版 869NQ-FJ69K-466HW-QYCP2-DDBV6 Word WXY84-JN2Q9-RBCCQ-3Q3J3-3PFJ6 ","date":"05-09","objectID":"/2021/kms/:3:3","series":null,"tags":["Windows","Server","KMS"],"title":"使用 KMS 激活 Microsoft 软件","uri":"/2021/kms/#office-gvlk"},{"categories":["API"],"content":"GinShio | Elixir Phoenix Framework - Introduction","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/"},{"categories":["API"],"content":"Phoenix Framework 是一个 MVC web 框架，与 Ruby 的 Rails 和 Python 的 Django 类似，是整个 Elixir 社区的核心项目之一，推荐阅读 Phoenix 文档 ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:0:0","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#"},{"categories":["API"],"content":" 安装我们使用 Phoenix (v1.5.7) 前，需要安装相关依赖与 mix Elixir (\u003e= v1.6) Erlang (\u003e= 20) node.js [optional] (\u003e= 5.0.0) Database [default=PostgreSQL] inotify-tools [linux] erlang 与 elixir 是运行时环境，数据库方面使用同为社区维护的 Ecto 来操作，Phoenix 使用 node.js 的原因是使用 webpack 编译静态资源，当然你可以只开发 API 不使用静态资源 Phoenix 提供了非常有用的实时重新加载功能，不过 Linux 用户需要安装 inotify-tools 才能使用 ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:1:0","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#安装"},{"categories":["API"],"content":" 创建新项目我们使用 mix 来创建一个 Phoenix 项目 mix phx.new awesome 如果你没有 phx.new 这个命令，你需要先使用 mix 安装一下 mix archive.install hex phx_new 项目创建完成之后，我们可以看到终端中有提示，在 config/dev.exs 中配置程序并执行 mix ecto.create，配置文件中的 Repo 是数据库设置，Endpoint 配置的是网站相关的内容 这时 mix 会为我们创建一个 Phoenix 项目，默认的数据库是 PostgreSQL，如果应用不使用数据库则加上tag --no-ecto 即可，如果要使用其他数据库只需要加上 tag --database db webpack 用于管理静态资源，添加 tag --no-webpack 即可禁止使用 webpack， --no-html 则会不生成HTML视图层，我们在完成 Rest API 程序时可以使用到这两个参数 配置完成之后，使用 mix phx.server 即可运行服务器了！ 我们要写这个网站，首先要搞清楚目录关系，有关这个网站的所有的代码将在 lib/awesome_web 中实现，简单的做一个了解 endpoint.ex 是 HTTP 请求的入口 router.ex 是 路由 定义 gettext.ex 通过 Gettext 提供了国际化支持 telemeter.ex 是一个 Supervisor 相关的程序 controllers 是 控制器 实现，请求将通过路由分派到控制器上 views 是 视图 实现 templates 是 模板 实现 我们来看看路由情况，可以看到 scope \"/\" 这个空间下有一个 API，绑定的函数是 PageController.index，即浏览器发送请求 GET / 时将会执行绑定的函数。那看看控制器吧，就一句 render(conn, \"index.html\") 渲染页面，渲染的是哪里的 index.html，还得看视图层 page_view.ex，这会默认去找 templates/page 下的文件进行渲染，整个最简单的流程就是这样 ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:2:0","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#创建新项目"},{"categories":["API"],"content":" 仿照一个页面我们实现一个页面，通过请求 GET /hello 展示 Hello, Phoenix! # router.ex get(\"/hello\", HelloController, :index) # controllers/hello_controller.ex defmodule AwesomeWeb.HelloController do use AwesomeWeb, :controller def index(conn, _params), do: render(conn, \"index.html\") end # views/hello_view.ex defmodule AwesomeWeb.HelloView do use AwesomeWeb, :view end # templates/hello/index.html.eex \u003cdiv class=\"phx-hero\"\u003e \u003ch2\u003eHello, Phoenix!\u003c/h2\u003e \u003c/div\u003e 效果还不错，不过太麻烦了，写页面岂不是要累死！好消息！Phoenix 提供了一些 phx.gen.* 的工具可以生成相关代码，减少重复的工作 我们先简单的生成一个 HTML 页面，然后按照提示添加路由并迁移数据库 mix phx.gen.html User User users username:string:unique email:string:unique password:string 之后访问 /users 就好，就完成了，简直暴力！！！最暴力的就是，后台开启的服务可以不需要关闭！！！你就能直接访问新加的页面了！！！ 稍微说一说 resources(\"/users\", UserController) 的作用，这其实是实现了一堆 CRUD API，然后用一条命令路由所有 CRUD 方法，也可以将它等价替换为： get(\"/users\", UserController, :index) get(\"/users/:id/edit\", UserController, :edit) get(\"/users/new\", UserController, :new) get(\"/users/:id\", UserController, :show) post(\"/users\", UserController, :create) patch(\"/users/:id\", UserController, :update) put(\"/users/:id\", UserController, :update) delete(\"/users/:id\", UserController, :delete) ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:2:1","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#仿照一个页面"},{"categories":["API"],"content":" 限制字段刚刚那个用户界面，我们只限制了所有字段必填，显然这种限制是不够的，我们应该清楚都需要哪些限制，如果添加这些限制，接下来我们以 username 为例 必填与唯一 英文字母、数字及下划线 长度 3 ~ 15 保留 admin, administrator 和 root 需求搞定了，我们知道这些需求第一个已经完成了，是 Phoenix 帮我们完成的，那看看源代码是怎么完成的吧。请打开 lib/awesome/account/user.ex 并查看源码，这是一个 Etco 所定义的模型，可以直接与数据库交互，而查文档的话也应该查 Ecto 的文档 @doc false def changeset(user, attrs) do user |\u003e cast(attrs, [:username, :email, :password]) |\u003e validate_required([:username, :email, :password]) |\u003e unique_constraint(:username) |\u003e unique_constraint(:email) end changeset/2 是 Ecto 的回调函数，一般对数据进行 过滤、验证 与 约束 操作，调用方式 changeset = User.changeset(%User{}, %{username: \"Example\", email: \"i@example.com\"}) {:error, changeset} = Repo.insert(changeset) changeset.errors #=\u003e [password: {\"can't be blank\", []}] OK，下来我们就开始仔细解读下这个函数，validate_required 是指明哪些字段必填，而 unique_constraint 则是约束字段唯一，那现在我们需要再添加一些代码来实现需求 |\u003e validate_exclusion(:username, ~w(admin administrator root)) # 保留用户名 |\u003e validate_format(:username, ~r/^[a-zA-Z0-9_]+$/) # 对格式进行要求 |\u003e validate_length(:username, [min: 3, max: 15]) # 限制长度 ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:2:2","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#限制字段"},{"categories":["API"],"content":" 基础","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:3:0","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#基础"},{"categories":["API"],"content":" PlugPlug 运行在 Phoenix 的 HTTP 层，所有链接都与 Plug 打交道，并且 Endpoint 、 Router 、 Controller 都属于 Plug function plug一个符合 plug 的函数需要接受一个连接，和相关的选项作为参数，并且最终返回这个连接 def put_headers(conn, kvs) do Enum.reduce(kvs, conn, fn ({k, v}, conn) -\u003e Plug.Conn.put_resp_header(conn, k, v) end) end 我们可以用 plug 将操作流式串联起来，将一个请求所需要的操作流水式的串联起来 defmodule AwesomeWeb.HelloController do use AwesomeWeb, :controller plug :put_haeders, %{content_encoding: \"gzip\", cache_control: \"max-age=3600\"} plug :put_layout, \"bare.html\" # Other Operators end module plugPlug 的另一种类型是 module plug，它被定义在 module 中，可以将整个 module 当作一个 plug 放入处理流程中，因此这个 module 需要符合一定的规范 init/1：初始化传递给 call/2 的参数或选项 call/2：处理链接，与 function plug 差不多 我们可以试一试写一个模块 plug，功能是把 :locale 键值对放到连接流里，以便让后面的其他 plugs 控制器和页面等也能使用 defmodule HelloPhoenixWeb.Plugs.Locale do import Plug.Conn @locales [\"en\", \"fr\", \"de\"] def init(default), do: default def call(%Plug.Conn{params: %{\"locale\" =\u003e loc}} = conn, _default) when loc in @locales do assign(conn, :locale, loc) end def call(conn, default), do: assign(conn, :locale, default) end 我们将 Locale Plug 串入 router 中即可 defmodule HelloPhoenixWeb.Router do use HelloPhoenixWeb, :router pipeline :browser do ... plug HelloPhoenixWeb.Plugs.Locale \"en\" end ... end ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:3:1","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#plug"},{"categories":["API"],"content":" PlugPlug 运行在 Phoenix 的 HTTP 层，所有链接都与 Plug 打交道，并且 Endpoint 、 Router 、 Controller 都属于 Plug function plug一个符合 plug 的函数需要接受一个连接，和相关的选项作为参数，并且最终返回这个连接 def put_headers(conn, kvs) do Enum.reduce(kvs, conn, fn ({k, v}, conn) -\u003e Plug.Conn.put_resp_header(conn, k, v) end) end 我们可以用 plug 将操作流式串联起来，将一个请求所需要的操作流水式的串联起来 defmodule AwesomeWeb.HelloController do use AwesomeWeb, :controller plug :put_haeders, %{content_encoding: \"gzip\", cache_control: \"max-age=3600\"} plug :put_layout, \"bare.html\" # Other Operators end module plugPlug 的另一种类型是 module plug，它被定义在 module 中，可以将整个 module 当作一个 plug 放入处理流程中，因此这个 module 需要符合一定的规范 init/1：初始化传递给 call/2 的参数或选项 call/2：处理链接，与 function plug 差不多 我们可以试一试写一个模块 plug，功能是把 :locale 键值对放到连接流里，以便让后面的其他 plugs 控制器和页面等也能使用 defmodule HelloPhoenixWeb.Plugs.Locale do import Plug.Conn @locales [\"en\", \"fr\", \"de\"] def init(default), do: default def call(%Plug.Conn{params: %{\"locale\" =\u003e loc}} = conn, _default) when loc in @locales do assign(conn, :locale, loc) end def call(conn, default), do: assign(conn, :locale, default) end 我们将 Locale Plug 串入 router 中即可 defmodule HelloPhoenixWeb.Router do use HelloPhoenixWeb, :router pipeline :browser do ... plug HelloPhoenixWeb.Plugs.Locale \"en\" end ... end ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:3:1","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#function-plug"},{"categories":["API"],"content":" PlugPlug 运行在 Phoenix 的 HTTP 层，所有链接都与 Plug 打交道，并且 Endpoint 、 Router 、 Controller 都属于 Plug function plug一个符合 plug 的函数需要接受一个连接，和相关的选项作为参数，并且最终返回这个连接 def put_headers(conn, kvs) do Enum.reduce(kvs, conn, fn ({k, v}, conn) -\u003e Plug.Conn.put_resp_header(conn, k, v) end) end 我们可以用 plug 将操作流式串联起来，将一个请求所需要的操作流水式的串联起来 defmodule AwesomeWeb.HelloController do use AwesomeWeb, :controller plug :put_haeders, %{content_encoding: \"gzip\", cache_control: \"max-age=3600\"} plug :put_layout, \"bare.html\" # Other Operators end module plugPlug 的另一种类型是 module plug，它被定义在 module 中，可以将整个 module 当作一个 plug 放入处理流程中，因此这个 module 需要符合一定的规范 init/1：初始化传递给 call/2 的参数或选项 call/2：处理链接，与 function plug 差不多 我们可以试一试写一个模块 plug，功能是把 :locale 键值对放到连接流里，以便让后面的其他 plugs 控制器和页面等也能使用 defmodule HelloPhoenixWeb.Plugs.Locale do import Plug.Conn @locales [\"en\", \"fr\", \"de\"] def init(default), do: default def call(%Plug.Conn{params: %{\"locale\" =\u003e loc}} = conn, _default) when loc in @locales do assign(conn, :locale, loc) end def call(conn, default), do: assign(conn, :locale, default) end 我们将 Locale Plug 串入 router 中即可 defmodule HelloPhoenixWeb.Router do use HelloPhoenixWeb, :router pipeline :browser do ... plug HelloPhoenixWeb.Plugs.Locale \"en\" end ... end ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:3:1","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#module-plug"},{"categories":["API"],"content":" 路由路由是 Phoenix 应用的重要组成部分，可以将对应的 HTTP 请求映射到 controller/action, 处理实时 channel，还为路由之前的中间件定义了一系列的转换功能 pipeline 可以定义一种类型的操作，这种操作可以定义一系列 plug 为以后定义的 API 使用。scope 操作可以定义一组 API 的作用域，并且可以指定 pipe 类型来获取 plug 操作 pipeline :browser do plug :accepts, [\"html\"] plug :fetch_session plug :fetch_flash plug :protect_from_forgery plug :put_secure_browser_headers end scope \"/\", HelloPhoenixWeb do pipe_through :browser get(\"/\", PageController, :index) end Phoenix 可以使用命令 mix phx.routes 查看所有已定义的 API page_path GET / HelloPhoenixWeb.PageController :index hello_path GET /hello HelloPhoenixWeb.HelloController :index Path helpers 是 Router.Helpers 模块动态产生的函数，命名规则根据 Controller 生成，我们可以执行命令 iex -S mix 运行项目并中查看 Helps HelloPhoenixWeb.Router.Helpers.page_path(HelloPhoenixWeb.Endpoint, :index) # \"/\" HelloPhoenixWeb.Router.Helpers.hello_path(HelloPhoenixWeb.Endpoint, :index) # \"/hello\" Helps 也可以用在 eex 中，示例 page_path(@conn, :index) 的输出即为 \"/\" \u003ca href=\"\u003c%= page_path(@conn, :index) %\u003e\"\u003eTo the Welcome Page!\u003c/a\u003e 我们添加一系列 / 与 /admin 下的 API，phx.routes 查看 API，可以发现 uri、 Controller、function 都是没问题的，但是 helps 都是 review_path，这会引起 helps 函数调用错误 review_path GET /reviews HelloPhoenixWeb.ReviewController :index review_path GET /reviews/:id/edit HelloPhoenixWeb.ReviewController :edit review_path GET /reviews/new HelloPhoenixWeb.ReviewController :new review_path GET /reviews/:id HelloPhoenixWeb.ReviewController :show review_path POST /reviews HelloPhoenixWeb.ReviewController :create review_path PATCH /reviews/:id HelloPhoenixWeb.ReviewController :update PUT /reviews/:id HelloPhoenixWeb.ReviewController :update review_path DELETE /reviews/:id HelloPhoenixWeb.ReviewController :delete review_path GET /admin/reviews HelloPhoenixWeb.Admin.ReviewController :index review_path GET /admin/reviews/:id/edit HelloPhoenixWeb.Admin.ReviewController :edit review_path GET /admin/reviews/new HelloPhoenixWeb.Admin.ReviewController :new review_path GET /admin/reviews/:id HelloPhoenixWeb.Admin.ReviewController :show review_path POST /admin/reviews HelloPhoenixWeb.Admin.ReviewController :create review_path PATCH /admin/reviews/:id HelloPhoenixWeb.Admin.ReviewController :update PUT /admin/reviews/:id HelloPhoenixWeb.Admin.ReviewController :update review_path DELETE /admin/reviews/:id HelloPhoenixWeb.Admin.ReviewController :delete 我们需要添加 as: :admin 来解决 helps 冲突 scope \"/\", HelloPhoenixWeb do pipe_through :browser resources \"/reviews\", ReviewController end scope \"/admin\", HelloPhoenixWeb.Admin, as: :admin do resources \"/reviews\", ReviewController end ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:3:2","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#路由"},{"categories":["API"],"content":" 控制器Phoenix 控制器是一个类似中间人的角色，里面的函数称为 atcion，它响应路由的 HTTP 请求。action 可以命名为任意名称，但我们一般遵循一些约定 index：按照给定的数据渲染一组条目 show：渲染一个给定的 id 的独立条目 new：渲染一个创建新条目所需的表单 create：接收创建的新条并将其存储 edit：接收给定 id 的条目，并将其显示在 form 中用以编辑 update：接收修改过的 item 并存储 delete：接收给定 id 的条目并将其删除 控制器有一些方法渲染内容，最简单的一种是使用 Phoenix 提供的 text/2 方法渲染纯文本，当然也可以有一些其他方法来渲染 json/2 或 html/2 def show(conn, %{\"id\" =\u003e id}) do text(conn, \"Showing id ${id}\") # text: Showing id 15 json(conn, %{id, id}) # json: {\"id\": \"15\"} html(conn, \"\"\" \u003chtml\u003e \u003chead\u003e \u003ctitle\u003ePassing an Id\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eYou sent in id #{id}\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e \"\"\") end render/3 是 Phoenix 提供的渲染 View 的方法，View 与 Controller 需要使用相同的名称，且在 Templates 下有对应的模板目录，目录下对应着 Elixir 模板 HTML 文件 defmodule HelloPhoenixWeb.HelloConttoller do use HelloPhoenixWeb, :controller def show(conn, %{\"messenger\" =\u003e messenger}), do: render(conn, \"show.html\", messenger: messenger) end 控制器还可以直接返回响应状态 def index(conn, _params), do: conn |\u003e send_resp(201, \"\") ","date":"02-23","objectID":"/2021/elixir_phoenix_framework_notes_001/:3:3","series":["Elixir Phoenix Framework"],"tags":["Elixir","Network","Web","Phoenix","Framework"],"title":"初识 Phoenix Framework","uri":"/2021/elixir_phoenix_framework_notes_001/#控制器"},{"categories":["Applications"],"content":"GinShio | 使用 SteamCMD 搭建游戏服务器","date":"02-22","objectID":"/2021/steam_apps/","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/"},{"categories":["Applications"],"content":"和好友联机的时候本地服务器实在是不爽，一个人起飞，其他人都是高PING战士，最开始主要是 L4D2 时各种 RPG 服务器有些不爽，为了纯净的服务器只好自己建了 事先声明，我们所有的操作在 Debian / Ubuntu 下操作，有些操作系统可能会不一样，不过大同小异，我们还是定义一些等等可能用到的变量 (主要是路径和密码之类的 steam=/home/steam group_id=123456789 l4d2=${steam}/l4d2 l4d2_id=222860 l4d2_server_name=\"L4D2 Server\" l4d2_port=1024 valheim=${steam}/valheim valheim_id=896660 valheim_server_name=\"Valheim Server\" valheim_world=\"World\" valheim_port=1024 valheim_passwd=valheim_password dst=${steam}/dst dst_id=343050 ","date":"02-22","objectID":"/2021/steam_apps/:0:0","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#"},{"categories":["Applications"],"content":" SteamCMD顾名思义，steamcmd 是一个命令行工具，同时支持 linux，是我们搭建服务器的好帮手，然而我不会用，不过这不重要，安装跑起来就好 add-apt-repository multiverse dpkg --add-architecture i386 apt update \u0026\u0026 apt upgrade apt install -y lib32gcc1 steamcmd 我们不仅要安装一个 steamcmd，还要将所有游戏服务器，存放在 ~steam 下，使用 steam 这个用户来运行游戏 adduser --disabled-login --gecos 'Steam' steam sudo -u steam -H ln -s /usr/games/steamcmd ${steam}/steamcmd ","date":"02-22","objectID":"/2021/steam_apps/:1:0","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#steamcmd"},{"categories":["Applications"],"content":" 语法这里说的语法并不是 SteamCMD 的语法，而是 Steam 中所使用的文本标记语法，这些标记标签允许您为您的留言及发帖文字添加格式，类似于 HTML，官方展示在这里 标签 释义 示例 h 标题 [h1]一级标题[/h1] b 粗体 [b]粗体文本[/b] u 下划线 [u]下划线文本[/u] l 斜体 [l]斜体[/l] strike 删除线 [strike]删除线文本[/strike] spoiler 隐藏 [spoiler]隐藏文本[/spoiler] noparse 不解析 [noparse]不解析[b]标签[/b][/noparse] url 网络链接 [url=blog.ginshio.org]博客[/url] Youtube / Shop / Workshop Widget https://store.steampowered.com/app/570 list 无序列表 [list] [*] 列表 [*] 列表 [/list] olist 有序列表 [olist] [*] 列表 [*] 列表 [/olist] quote 引用 [quote=author]引用文本[/quote] code 等宽字体 (保留空格) [code]code[/code] 表格语法有点难度，tr 表示一行，th 表示一个单元格 [table] [tr] [th][/th] [th]DST Memorandum[/th] [th]Status Announcements[/th] [th]Global Positions[/th] [/tr] [tr] [th]DST Memorandum[/th] [th][/th] [th]Crash[/th] [th]Crash[/th] [/tr] [tr] [th]Status Announcements[/th] [th]Crash[/th] [th][/th] [th]OK[/th] [/tr] [tr] [th]Global Positions[/th] [th]Crash[/th] [th]OK[/th] [th][/th] [/tr] 可以做出一个这样的表格 DST Memorandum Status Announcements Global Positions DST Memorandum Crash Crash Status Announcements Crash OK Global Positions Crash OK ","date":"02-22","objectID":"/2021/steam_apps/:1:1","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#语法"},{"categories":["Applications"],"content":" 遇到的问题如果遇到服务器无法启动，比如缺少 sdk32/libsteam.so 或 sdk32/steamclient.so 之类的错误，需要完成以下操作 sudo -u steam -H mkdir -p /home/steam/.steam/sdk32 sudo -u steam -H ln -s /home/steam/.steam/steamcmd/linux32/steamclient.so /home/steam/.steam/sdk32/steamclient.so ","date":"02-22","objectID":"/2021/steam_apps/:1:2","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#遇到的问题"},{"categories":["Applications"],"content":" L4D2 警告 L4D2 每次搭建都被 DDOS，已经不再维护 在搭建服务器之前，为了你的服务器着想，请先创建一个 Steam 组，将你们一起开黑的人都拉入组中，我们需要将服务器设置为组私有，只有组中的人才能使用 安装并对服务器进行配置，配置文件是 ${l4d2}/left4dead2/cfg/server.cfg sudo -u steam -H ${steam}/steamcmd +force_install_dir ${l4d2} +login anonymous +app_update ${l4d2_id} validate +quit sudo -u steam -H cat \u003c\u003c- EOF \u003e ${l4d2}/left4dead2/cfg/server.cfg hostname \"${l4d2_server_name}\" // 服务器名 hostport ${l4d2_port} // 服务器端口 sv_tags \"hidde,GinShio\" // 隐藏服务器 sv_gametypes \"versus,survival,coop,realism,teamversus,teamscavenge\" // 游戏类型 mp_gamemode \"coop\" sv_cheats 0 // 允许作弊 sv_voiceenable 1 // 语音服务 sv_pausable 0 // 暂停 sv_consistency 0 // 一致性 sv_lan 0 // 局域网游戏 sv_allow_lobby_connect_only 0 // 大厅连接 sv_region 4 // 区域：亚洲 sv_visiblemaxplayers 4 // 最多人数上线：4 (4~32) mp_disable_autokick 0 // 闲置踢出 sv_steamgroup \"${group_id}\" // 根据自己的steam组ID绑定服务器 sv_steamgroup_exclusive 1 // 设置组私有化 exec banned_user.cfg exec banned_ip.cfg heartbeat EOF 对于服务器公告，${l4d2}/left4dead2/host.txt 可以修改服务器公告的标题，而 ${l4d2}/left4dead2/motd.txt 修改的是公告的内容，我们可以在公告中使用之前列出的文本标记语法。对于第三方地图，我们只需要将地图存放在 {l4d2}/left4dead2/addons/workshop 中即可，不过记得将地图文件的权限转到 steam 个人喜欢使用 systemctl 来管理服务，这样觉得更安全些，所以我们完成一个 service 管理服务器，当然如果你想用 screen 可以自行搜索它的用法 [Unit] Description=Left 4 Dead 2 Server Documentation=https://left4dead.fandom.com/wiki/Left_4_Dead_Wiki After=network.target [Service] Type=simple User=steam WorkingDirectory=/home/steam/l4d2 ExecStart=/home/steam/l4d2/srcds_run -game left4dead2 -autorestart +ip 0.0.0.0 +exec server.cfg ExecReload=/bin/kill -HUP Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 到这里，纯净的 l4d2 服务器就搭建好了，我并没有弄插件，也没有弄管理员，所以就到这里！ ","date":"02-22","objectID":"/2021/steam_apps/:2:0","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#l4d2"},{"categories":["Applications"],"content":" Valheim: 英灵神殿 警告 该章节没有真正部署维护，且没有更新过，酌情参考 首先我们配置服务器，不过和 L4D2 的方式差不多，毕竟都是 SteamCMD，不废话，直接上 Shell 指令。需要注意的是，Valheim 将占用三个端口，即 valheim_port 到 valheim_port + 2，请在防火墙开启需要的全部三个端口 sudo -u steam -H ${steam}/steamcmd +force_install_dir ${valheim} +login anonymous +app_update ${valheim_id} validate +quit sudo -u steam -H cp ${valheim}/start_server.sh ${valheim}/start_server.sh.bkp sudo -u steam -H cat \u003c\u003c- EOF \u003e ${valheim}/start_server.sh export templdpath=${LD_LIBRARY_PATH} export LD_LIBRARY_PATH=${valheim}/linux64:${LD_LIBRARY_PATH} export SteamAppId=892970 echo \"Starting server PRESS CTRL-C to exit\" ${valheim}/valheim_server.x86_64 -name \"${valheim_server_name}\" -port ${valheim_port} -world \"${valheim_world}\" -password \"${valheim_passwd}\" export LD_LIBRARY_PATH=$templdpath EOF 搞定，整一个 service 让 systemd 帮我们管理服务器 [Unit] Description=Valheim Server After=network.target [Service] Type=simple User=steam WorkingDirectory=/home/steam/valheim ExecStart=bash /home/steam/valheim/start_server.sh ExecReload=/bin/kill -HUP Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 好了，现在问题就是，你的服务器配置，请务必 2C4G5Mbps 及以上配置，要求真tm高，听说开发者只有5个人，，， ","date":"02-22","objectID":"/2021/steam_apps/:3:0","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#valheim-英灵神殿"},{"categories":["Applications"],"content":" 饥荒联机版现在我们要征服永恒领域了，首先的问题就是搭建一个服务器。我们下载服务器很简单，就是一行用了很多遍的命令 sudo -u steam -H ${steam}/steamcmd +force_install_dir ${dst} +login anonymous +app_update ${dst_id} validate +quit 下载好了当然没有用，我们需要去 Klei 官网 生成一份服务器 token。在 Games 一栏选择 DST Game Server 即可，输入 Cluster Name 添加生成服务器配置，还有 token，请务必保存好。 ","date":"02-22","objectID":"/2021/steam_apps/:4:0","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#饥荒联机版"},{"categories":["Applications"],"content":" 目录结构DST 服务器在运行时，全部数据都会存储在 $HOME/.klei 下，因此我们只需要常备份这个目录即可。饥荒服务器的目录结构如下： ├── Cluster1/ │ ├── cluster.ini │ ├── cluster_token.txt │ ├── Caves/ │ │ ├── server.ini │ │ ├── leveldataoverride.lua │ │ ├── modoverrides.lua │ ├── Master/ │ │ ├── server.ini │ │ ├── leveldataoverride.lua │ │ ├── modoverrides.lua 这是一个完整的位面所需要配置的文件 cluster_token.txt 存储的是从 Klei 官网拿到的 token cluster.ini 即这个位面的主配置，主要配置游戏模式、游戏人数、服务器名、密码、 steam 组等等 Master/server.ini 与 Caves/server.ini 配置基本相同，主要是世界的端口号和主次世界设置 modoverrides.lua 配置开启的 mod 的设置 leveldataoverride.lua 配置这个世界的资源、设置 另外在安装目录中有个 mods 的目录，其中可以添加服务器模组，当然仅仅是安装到服务器上。用不用还是需要看世界中 modoverrides.lua 的设置。 如果需要详细设置哪些人为管理员，可以在目录下添加 adminlist.txt 文件，文件中使用 Klei Account ID 区分玩家。如果你不知道 ID 的话可以在启动服务器后生成的 server_log.txt 与 server_chat_log.txt 中查找。这两个文件在启动对应的世界后在目录下生成。 ","date":"02-22","objectID":"/2021/steam_apps/:4:1","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#目录结构"},{"categories":["Applications"],"content":" 配置文件 服务器配置基本上在 Klei 官网所生成的 cluster.ini 与 cluster_token.txt 基本够用了，额外需要注意的是如果需要设置为组服务器 [STEAM] steam_group_admins=false # 设置组成员是否为管理员，接受 true / false steam_group_id=123456789 # 设置组 id steam_group_only=true # 设置是否仅组成员可用，接受 true / false 至于 server.ini 则分别在世界的目录下，基本无需多修改 (除非多个世界分主次或改端口) 世界设置DST 服务器最复杂的是世界设置文件，可以先在自己电脑上配置好你想要的世界设置，还有服务器 mod，然后把它们复制进对应的世界目录就好了。 当然还可以自己手动修改配置文件，比如说 Maxwell 雕像的数量、虫洞类型等等在界面无法修改的东西。 地上世界基础配置文件 return { desc=\"The standard Don't Starve experience.\", hideminimap=false, id=\"SURVIVAL_TOGETHER\", location=\"forest\", max_playlist_position=999, min_playlist_position=0, name=\"Survival\", numrandom_set_pieces=4, -- 雕像的数量 override_level_string=false, overrides={ -- 资源设置的可选项分别为: ----- \"never\" 无 ----- \"rare\" 较少 ----- \"default\" 默认 ----- \"often\" 较多 ----- \"always\" 大量 -- 全局设置 specialevent=\"default\", --- 特殊事件: none (无); default (自动) crow_carnival=\"default\", --- 盛夏鸦年华: default (自动); enabled (启用) hallowed_nights=\"default\", --- 万圣之夜: default (自动); enabled (启用) winters_feast=\"default\", --- 冬季盛宴: default (自动); enabled (启用) year_of_the_gobbler=\"default\", --- 火鸡之年: default (自动); enabled (启用) year_of_the_varg=\"default\", --- 座狼之年: default (自动); enabled (启用) year_of_the_pig=\"default\", --- 猪王之年: default (自动); enabled (启用) year_of_the_carrat=\"default\", --- 胡萝卜鼠之年: default (自动); enabled (启用) year_of_the_beefalo=\"default\", --- 皮弗娄牛之年: default (自动); enabled (启用) year_of_the_catcoon=\"default\", --- 浣猫之年: default (自动); enabled (启用) autumn=\"default\", --- 秋季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random winter=\"default\", --- 冬季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random spring=\"default\", --- 春季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random summer=\"default\", --- 夏季, noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random day=\"default\", --- 天类型: default (默认); --- longday (加长白天); longdusk (加长黄昏); longnight (加长黑夜); --- noday (无白天); nodusk (无黄昏); nonight (无黑夜); --- onlyday (只有白天); onlydusk (只有黄昏); onlynight (只有黑夜) spawnmode=\"fixed\", --- 出生点: fixed (固定大门); scatter (随机) ghostenabled=\"always\", --- 死亡变为鬼魂: none (关闭); always (启用) portalresurection=\"none\", --- 大门复活: none (关闭); always (启用) ghostsanitydrain=\"always\", --- 队友死亡扣除精神: none (关闭); always (启用) resettime=\"default\", --- 死亡重置时间: none (无); slow (慢); default (正常); fast (快); always (非常快) beefaloheat=\"default\", --- 野牛发情 krampus=\"default\", --- 坎普斯 -- 求生者设置 extrastartingitems=\"default\", --- 额外起始资源: 0 (总是); 5 (5 天后); default (10 天后); 15 (15 天后); 20 (20 天后); none (从不) seasonalstartingitems=\"default\", --- 季节起始物品: never; default spawnprotection=\"default\", --- 出生保护: never (无); default (自动); always (总是) dropeverythingondespawn=\"default\", --- 退出掉落物品: default (默认); always (所有) healthpenalty=\"always\", --- 生命惩罚: none (关闭); always (启用) lessdamagetaken=\"none\", --- 伤害减免: always (减免); none (无); more (增加) temperaturedamage=\"default\", --- 温度伤害: default (默认); nonlethal (非致命) hunger=\"default\", --- 饥饿伤害: default (默认); nonlethal (非致命) darkness=\"default\", --- 黑暗伤害: default (默认); nonlethal (非致命) brightmarecreatures=\"default\", --- 启蒙怪兽数量 shadowcreatures=\"default\", --- 理智怪兽数量 -- 世界设置 petrification=\"default\", --- 石化: none (无); few (慢); default (默认); many (快); max (极快) frograin=\"default\", --- 蛙雨 hounds=\"default\", --- 猎犬来袭频率 summerhounds=\"default\", --- 火狗: never (禁止); default (默认) winterhounds=\"default\", --- 冰狗: never (禁止); default (默认) alternatehunt=\"default\", --- 追猎惊喜 hunt=\"default\", --- 狩猎 lightning=\"default\", --- 闪电 meteorshowers=\"default\", --- 流星 weather=\"default\", --- 雨 wildfires=\"default\", --- 野火 -- 资源再生的可选项分别为: ----- \"never\" 从不 ----- \"veryslow\" 非常慢 ----- \"slow\" 缓慢 ----- \"default\" 默认 ----- \"fast\" 快速 ----- \"veryfast\" 非常快 -- 资源再生设置 regrowth=\"d","date":"02-22","objectID":"/2021/steam_apps/:4:2","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#配置文件"},{"categories":["Applications"],"content":" 配置文件 服务器配置基本上在 Klei 官网所生成的 cluster.ini 与 cluster_token.txt 基本够用了，额外需要注意的是如果需要设置为组服务器 [STEAM] steam_group_admins=false # 设置组成员是否为管理员，接受 true / false steam_group_id=123456789 # 设置组 id steam_group_only=true # 设置是否仅组成员可用，接受 true / false 至于 server.ini 则分别在世界的目录下，基本无需多修改 (除非多个世界分主次或改端口) 世界设置DST 服务器最复杂的是世界设置文件，可以先在自己电脑上配置好你想要的世界设置，还有服务器 mod，然后把它们复制进对应的世界目录就好了。 当然还可以自己手动修改配置文件，比如说 Maxwell 雕像的数量、虫洞类型等等在界面无法修改的东西。 地上世界基础配置文件 return { desc=\"The standard Don't Starve experience.\", hideminimap=false, id=\"SURVIVAL_TOGETHER\", location=\"forest\", max_playlist_position=999, min_playlist_position=0, name=\"Survival\", numrandom_set_pieces=4, -- 雕像的数量 override_level_string=false, overrides={ -- 资源设置的可选项分别为: ----- \"never\" 无 ----- \"rare\" 较少 ----- \"default\" 默认 ----- \"often\" 较多 ----- \"always\" 大量 -- 全局设置 specialevent=\"default\", --- 特殊事件: none (无); default (自动) crow_carnival=\"default\", --- 盛夏鸦年华: default (自动); enabled (启用) hallowed_nights=\"default\", --- 万圣之夜: default (自动); enabled (启用) winters_feast=\"default\", --- 冬季盛宴: default (自动); enabled (启用) year_of_the_gobbler=\"default\", --- 火鸡之年: default (自动); enabled (启用) year_of_the_varg=\"default\", --- 座狼之年: default (自动); enabled (启用) year_of_the_pig=\"default\", --- 猪王之年: default (自动); enabled (启用) year_of_the_carrat=\"default\", --- 胡萝卜鼠之年: default (自动); enabled (启用) year_of_the_beefalo=\"default\", --- 皮弗娄牛之年: default (自动); enabled (启用) year_of_the_catcoon=\"default\", --- 浣猫之年: default (自动); enabled (启用) autumn=\"default\", --- 秋季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random winter=\"default\", --- 冬季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random spring=\"default\", --- 春季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random summer=\"default\", --- 夏季, noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random day=\"default\", --- 天类型: default (默认); --- longday (加长白天); longdusk (加长黄昏); longnight (加长黑夜); --- noday (无白天); nodusk (无黄昏); nonight (无黑夜); --- onlyday (只有白天); onlydusk (只有黄昏); onlynight (只有黑夜) spawnmode=\"fixed\", --- 出生点: fixed (固定大门); scatter (随机) ghostenabled=\"always\", --- 死亡变为鬼魂: none (关闭); always (启用) portalresurection=\"none\", --- 大门复活: none (关闭); always (启用) ghostsanitydrain=\"always\", --- 队友死亡扣除精神: none (关闭); always (启用) resettime=\"default\", --- 死亡重置时间: none (无); slow (慢); default (正常); fast (快); always (非常快) beefaloheat=\"default\", --- 野牛发情 krampus=\"default\", --- 坎普斯 -- 求生者设置 extrastartingitems=\"default\", --- 额外起始资源: 0 (总是); 5 (5 天后); default (10 天后); 15 (15 天后); 20 (20 天后); none (从不) seasonalstartingitems=\"default\", --- 季节起始物品: never; default spawnprotection=\"default\", --- 出生保护: never (无); default (自动); always (总是) dropeverythingondespawn=\"default\", --- 退出掉落物品: default (默认); always (所有) healthpenalty=\"always\", --- 生命惩罚: none (关闭); always (启用) lessdamagetaken=\"none\", --- 伤害减免: always (减免); none (无); more (增加) temperaturedamage=\"default\", --- 温度伤害: default (默认); nonlethal (非致命) hunger=\"default\", --- 饥饿伤害: default (默认); nonlethal (非致命) darkness=\"default\", --- 黑暗伤害: default (默认); nonlethal (非致命) brightmarecreatures=\"default\", --- 启蒙怪兽数量 shadowcreatures=\"default\", --- 理智怪兽数量 -- 世界设置 petrification=\"default\", --- 石化: none (无); few (慢); default (默认); many (快); max (极快) frograin=\"default\", --- 蛙雨 hounds=\"default\", --- 猎犬来袭频率 summerhounds=\"default\", --- 火狗: never (禁止); default (默认) winterhounds=\"default\", --- 冰狗: never (禁止); default (默认) alternatehunt=\"default\", --- 追猎惊喜 hunt=\"default\", --- 狩猎 lightning=\"default\", --- 闪电 meteorshowers=\"default\", --- 流星 weather=\"default\", --- 雨 wildfires=\"default\", --- 野火 -- 资源再生的可选项分别为: ----- \"never\" 从不 ----- \"veryslow\" 非常慢 ----- \"slow\" 缓慢 ----- \"default\" 默认 ----- \"fast\" 快速 ----- \"veryfast\" 非常快 -- 资源再生设置 regrowth=\"d","date":"02-22","objectID":"/2021/steam_apps/:4:2","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#服务器配置"},{"categories":["Applications"],"content":" 配置文件 服务器配置基本上在 Klei 官网所生成的 cluster.ini 与 cluster_token.txt 基本够用了，额外需要注意的是如果需要设置为组服务器 [STEAM] steam_group_admins=false # 设置组成员是否为管理员，接受 true / false steam_group_id=123456789 # 设置组 id steam_group_only=true # 设置是否仅组成员可用，接受 true / false 至于 server.ini 则分别在世界的目录下，基本无需多修改 (除非多个世界分主次或改端口) 世界设置DST 服务器最复杂的是世界设置文件，可以先在自己电脑上配置好你想要的世界设置，还有服务器 mod，然后把它们复制进对应的世界目录就好了。 当然还可以自己手动修改配置文件，比如说 Maxwell 雕像的数量、虫洞类型等等在界面无法修改的东西。 地上世界基础配置文件 return { desc=\"The standard Don't Starve experience.\", hideminimap=false, id=\"SURVIVAL_TOGETHER\", location=\"forest\", max_playlist_position=999, min_playlist_position=0, name=\"Survival\", numrandom_set_pieces=4, -- 雕像的数量 override_level_string=false, overrides={ -- 资源设置的可选项分别为: ----- \"never\" 无 ----- \"rare\" 较少 ----- \"default\" 默认 ----- \"often\" 较多 ----- \"always\" 大量 -- 全局设置 specialevent=\"default\", --- 特殊事件: none (无); default (自动) crow_carnival=\"default\", --- 盛夏鸦年华: default (自动); enabled (启用) hallowed_nights=\"default\", --- 万圣之夜: default (自动); enabled (启用) winters_feast=\"default\", --- 冬季盛宴: default (自动); enabled (启用) year_of_the_gobbler=\"default\", --- 火鸡之年: default (自动); enabled (启用) year_of_the_varg=\"default\", --- 座狼之年: default (自动); enabled (启用) year_of_the_pig=\"default\", --- 猪王之年: default (自动); enabled (启用) year_of_the_carrat=\"default\", --- 胡萝卜鼠之年: default (自动); enabled (启用) year_of_the_beefalo=\"default\", --- 皮弗娄牛之年: default (自动); enabled (启用) year_of_the_catcoon=\"default\", --- 浣猫之年: default (自动); enabled (启用) autumn=\"default\", --- 秋季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random winter=\"default\", --- 冬季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random spring=\"default\", --- 春季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random summer=\"default\", --- 夏季, noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random day=\"default\", --- 天类型: default (默认); --- longday (加长白天); longdusk (加长黄昏); longnight (加长黑夜); --- noday (无白天); nodusk (无黄昏); nonight (无黑夜); --- onlyday (只有白天); onlydusk (只有黄昏); onlynight (只有黑夜) spawnmode=\"fixed\", --- 出生点: fixed (固定大门); scatter (随机) ghostenabled=\"always\", --- 死亡变为鬼魂: none (关闭); always (启用) portalresurection=\"none\", --- 大门复活: none (关闭); always (启用) ghostsanitydrain=\"always\", --- 队友死亡扣除精神: none (关闭); always (启用) resettime=\"default\", --- 死亡重置时间: none (无); slow (慢); default (正常); fast (快); always (非常快) beefaloheat=\"default\", --- 野牛发情 krampus=\"default\", --- 坎普斯 -- 求生者设置 extrastartingitems=\"default\", --- 额外起始资源: 0 (总是); 5 (5 天后); default (10 天后); 15 (15 天后); 20 (20 天后); none (从不) seasonalstartingitems=\"default\", --- 季节起始物品: never; default spawnprotection=\"default\", --- 出生保护: never (无); default (自动); always (总是) dropeverythingondespawn=\"default\", --- 退出掉落物品: default (默认); always (所有) healthpenalty=\"always\", --- 生命惩罚: none (关闭); always (启用) lessdamagetaken=\"none\", --- 伤害减免: always (减免); none (无); more (增加) temperaturedamage=\"default\", --- 温度伤害: default (默认); nonlethal (非致命) hunger=\"default\", --- 饥饿伤害: default (默认); nonlethal (非致命) darkness=\"default\", --- 黑暗伤害: default (默认); nonlethal (非致命) brightmarecreatures=\"default\", --- 启蒙怪兽数量 shadowcreatures=\"default\", --- 理智怪兽数量 -- 世界设置 petrification=\"default\", --- 石化: none (无); few (慢); default (默认); many (快); max (极快) frograin=\"default\", --- 蛙雨 hounds=\"default\", --- 猎犬来袭频率 summerhounds=\"default\", --- 火狗: never (禁止); default (默认) winterhounds=\"default\", --- 冰狗: never (禁止); default (默认) alternatehunt=\"default\", --- 追猎惊喜 hunt=\"default\", --- 狩猎 lightning=\"default\", --- 闪电 meteorshowers=\"default\", --- 流星 weather=\"default\", --- 雨 wildfires=\"default\", --- 野火 -- 资源再生的可选项分别为: ----- \"never\" 从不 ----- \"veryslow\" 非常慢 ----- \"slow\" 缓慢 ----- \"default\" 默认 ----- \"fast\" 快速 ----- \"veryfast\" 非常快 -- 资源再生设置 regrowth=\"d","date":"02-22","objectID":"/2021/steam_apps/:4:2","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#世界设置"},{"categories":["Applications"],"content":" 配置文件 服务器配置基本上在 Klei 官网所生成的 cluster.ini 与 cluster_token.txt 基本够用了，额外需要注意的是如果需要设置为组服务器 [STEAM] steam_group_admins=false # 设置组成员是否为管理员，接受 true / false steam_group_id=123456789 # 设置组 id steam_group_only=true # 设置是否仅组成员可用，接受 true / false 至于 server.ini 则分别在世界的目录下，基本无需多修改 (除非多个世界分主次或改端口) 世界设置DST 服务器最复杂的是世界设置文件，可以先在自己电脑上配置好你想要的世界设置，还有服务器 mod，然后把它们复制进对应的世界目录就好了。 当然还可以自己手动修改配置文件，比如说 Maxwell 雕像的数量、虫洞类型等等在界面无法修改的东西。 地上世界基础配置文件 return { desc=\"The standard Don't Starve experience.\", hideminimap=false, id=\"SURVIVAL_TOGETHER\", location=\"forest\", max_playlist_position=999, min_playlist_position=0, name=\"Survival\", numrandom_set_pieces=4, -- 雕像的数量 override_level_string=false, overrides={ -- 资源设置的可选项分别为: ----- \"never\" 无 ----- \"rare\" 较少 ----- \"default\" 默认 ----- \"often\" 较多 ----- \"always\" 大量 -- 全局设置 specialevent=\"default\", --- 特殊事件: none (无); default (自动) crow_carnival=\"default\", --- 盛夏鸦年华: default (自动); enabled (启用) hallowed_nights=\"default\", --- 万圣之夜: default (自动); enabled (启用) winters_feast=\"default\", --- 冬季盛宴: default (自动); enabled (启用) year_of_the_gobbler=\"default\", --- 火鸡之年: default (自动); enabled (启用) year_of_the_varg=\"default\", --- 座狼之年: default (自动); enabled (启用) year_of_the_pig=\"default\", --- 猪王之年: default (自动); enabled (启用) year_of_the_carrat=\"default\", --- 胡萝卜鼠之年: default (自动); enabled (启用) year_of_the_beefalo=\"default\", --- 皮弗娄牛之年: default (自动); enabled (启用) year_of_the_catcoon=\"default\", --- 浣猫之年: default (自动); enabled (启用) autumn=\"default\", --- 秋季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random winter=\"default\", --- 冬季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random spring=\"default\", --- 春季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random summer=\"default\", --- 夏季, noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random day=\"default\", --- 天类型: default (默认); --- longday (加长白天); longdusk (加长黄昏); longnight (加长黑夜); --- noday (无白天); nodusk (无黄昏); nonight (无黑夜); --- onlyday (只有白天); onlydusk (只有黄昏); onlynight (只有黑夜) spawnmode=\"fixed\", --- 出生点: fixed (固定大门); scatter (随机) ghostenabled=\"always\", --- 死亡变为鬼魂: none (关闭); always (启用) portalresurection=\"none\", --- 大门复活: none (关闭); always (启用) ghostsanitydrain=\"always\", --- 队友死亡扣除精神: none (关闭); always (启用) resettime=\"default\", --- 死亡重置时间: none (无); slow (慢); default (正常); fast (快); always (非常快) beefaloheat=\"default\", --- 野牛发情 krampus=\"default\", --- 坎普斯 -- 求生者设置 extrastartingitems=\"default\", --- 额外起始资源: 0 (总是); 5 (5 天后); default (10 天后); 15 (15 天后); 20 (20 天后); none (从不) seasonalstartingitems=\"default\", --- 季节起始物品: never; default spawnprotection=\"default\", --- 出生保护: never (无); default (自动); always (总是) dropeverythingondespawn=\"default\", --- 退出掉落物品: default (默认); always (所有) healthpenalty=\"always\", --- 生命惩罚: none (关闭); always (启用) lessdamagetaken=\"none\", --- 伤害减免: always (减免); none (无); more (增加) temperaturedamage=\"default\", --- 温度伤害: default (默认); nonlethal (非致命) hunger=\"default\", --- 饥饿伤害: default (默认); nonlethal (非致命) darkness=\"default\", --- 黑暗伤害: default (默认); nonlethal (非致命) brightmarecreatures=\"default\", --- 启蒙怪兽数量 shadowcreatures=\"default\", --- 理智怪兽数量 -- 世界设置 petrification=\"default\", --- 石化: none (无); few (慢); default (默认); many (快); max (极快) frograin=\"default\", --- 蛙雨 hounds=\"default\", --- 猎犬来袭频率 summerhounds=\"default\", --- 火狗: never (禁止); default (默认) winterhounds=\"default\", --- 冰狗: never (禁止); default (默认) alternatehunt=\"default\", --- 追猎惊喜 hunt=\"default\", --- 狩猎 lightning=\"default\", --- 闪电 meteorshowers=\"default\", --- 流星 weather=\"default\", --- 雨 wildfires=\"default\", --- 野火 -- 资源再生的可选项分别为: ----- \"never\" 从不 ----- \"veryslow\" 非常慢 ----- \"slow\" 缓慢 ----- \"default\" 默认 ----- \"fast\" 快速 ----- \"veryfast\" 非常快 -- 资源再生设置 regrowth=\"d","date":"02-22","objectID":"/2021/steam_apps/:4:2","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#启动程序"},{"categories":["Applications"],"content":" 配置文件 服务器配置基本上在 Klei 官网所生成的 cluster.ini 与 cluster_token.txt 基本够用了，额外需要注意的是如果需要设置为组服务器 [STEAM] steam_group_admins=false # 设置组成员是否为管理员，接受 true / false steam_group_id=123456789 # 设置组 id steam_group_only=true # 设置是否仅组成员可用，接受 true / false 至于 server.ini 则分别在世界的目录下，基本无需多修改 (除非多个世界分主次或改端口) 世界设置DST 服务器最复杂的是世界设置文件，可以先在自己电脑上配置好你想要的世界设置，还有服务器 mod，然后把它们复制进对应的世界目录就好了。 当然还可以自己手动修改配置文件，比如说 Maxwell 雕像的数量、虫洞类型等等在界面无法修改的东西。 地上世界基础配置文件 return { desc=\"The standard Don't Starve experience.\", hideminimap=false, id=\"SURVIVAL_TOGETHER\", location=\"forest\", max_playlist_position=999, min_playlist_position=0, name=\"Survival\", numrandom_set_pieces=4, -- 雕像的数量 override_level_string=false, overrides={ -- 资源设置的可选项分别为: ----- \"never\" 无 ----- \"rare\" 较少 ----- \"default\" 默认 ----- \"often\" 较多 ----- \"always\" 大量 -- 全局设置 specialevent=\"default\", --- 特殊事件: none (无); default (自动) crow_carnival=\"default\", --- 盛夏鸦年华: default (自动); enabled (启用) hallowed_nights=\"default\", --- 万圣之夜: default (自动); enabled (启用) winters_feast=\"default\", --- 冬季盛宴: default (自动); enabled (启用) year_of_the_gobbler=\"default\", --- 火鸡之年: default (自动); enabled (启用) year_of_the_varg=\"default\", --- 座狼之年: default (自动); enabled (启用) year_of_the_pig=\"default\", --- 猪王之年: default (自动); enabled (启用) year_of_the_carrat=\"default\", --- 胡萝卜鼠之年: default (自动); enabled (启用) year_of_the_beefalo=\"default\", --- 皮弗娄牛之年: default (自动); enabled (启用) year_of_the_catcoon=\"default\", --- 浣猫之年: default (自动); enabled (启用) autumn=\"default\", --- 秋季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random winter=\"default\", --- 冬季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random spring=\"default\", --- 春季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random summer=\"default\", --- 夏季, noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random day=\"default\", --- 天类型: default (默认); --- longday (加长白天); longdusk (加长黄昏); longnight (加长黑夜); --- noday (无白天); nodusk (无黄昏); nonight (无黑夜); --- onlyday (只有白天); onlydusk (只有黄昏); onlynight (只有黑夜) spawnmode=\"fixed\", --- 出生点: fixed (固定大门); scatter (随机) ghostenabled=\"always\", --- 死亡变为鬼魂: none (关闭); always (启用) portalresurection=\"none\", --- 大门复活: none (关闭); always (启用) ghostsanitydrain=\"always\", --- 队友死亡扣除精神: none (关闭); always (启用) resettime=\"default\", --- 死亡重置时间: none (无); slow (慢); default (正常); fast (快); always (非常快) beefaloheat=\"default\", --- 野牛发情 krampus=\"default\", --- 坎普斯 -- 求生者设置 extrastartingitems=\"default\", --- 额外起始资源: 0 (总是); 5 (5 天后); default (10 天后); 15 (15 天后); 20 (20 天后); none (从不) seasonalstartingitems=\"default\", --- 季节起始物品: never; default spawnprotection=\"default\", --- 出生保护: never (无); default (自动); always (总是) dropeverythingondespawn=\"default\", --- 退出掉落物品: default (默认); always (所有) healthpenalty=\"always\", --- 生命惩罚: none (关闭); always (启用) lessdamagetaken=\"none\", --- 伤害减免: always (减免); none (无); more (增加) temperaturedamage=\"default\", --- 温度伤害: default (默认); nonlethal (非致命) hunger=\"default\", --- 饥饿伤害: default (默认); nonlethal (非致命) darkness=\"default\", --- 黑暗伤害: default (默认); nonlethal (非致命) brightmarecreatures=\"default\", --- 启蒙怪兽数量 shadowcreatures=\"default\", --- 理智怪兽数量 -- 世界设置 petrification=\"default\", --- 石化: none (无); few (慢); default (默认); many (快); max (极快) frograin=\"default\", --- 蛙雨 hounds=\"default\", --- 猎犬来袭频率 summerhounds=\"default\", --- 火狗: never (禁止); default (默认) winterhounds=\"default\", --- 冰狗: never (禁止); default (默认) alternatehunt=\"default\", --- 追猎惊喜 hunt=\"default\", --- 狩猎 lightning=\"default\", --- 闪电 meteorshowers=\"default\", --- 流星 weather=\"default\", --- 雨 wildfires=\"default\", --- 野火 -- 资源再生的可选项分别为: ----- \"never\" 从不 ----- \"veryslow\" 非常慢 ----- \"slow\" 缓慢 ----- \"default\" 默认 ----- \"fast\" 快速 ----- \"veryfast\" 非常快 -- 资源再生设置 regrowth=\"d","date":"02-22","objectID":"/2021/steam_apps/:4:2","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#opensuse-遇到的问题"},{"categories":["Applications"],"content":" 配置文件 服务器配置基本上在 Klei 官网所生成的 cluster.ini 与 cluster_token.txt 基本够用了，额外需要注意的是如果需要设置为组服务器 [STEAM] steam_group_admins=false # 设置组成员是否为管理员，接受 true / false steam_group_id=123456789 # 设置组 id steam_group_only=true # 设置是否仅组成员可用，接受 true / false 至于 server.ini 则分别在世界的目录下，基本无需多修改 (除非多个世界分主次或改端口) 世界设置DST 服务器最复杂的是世界设置文件，可以先在自己电脑上配置好你想要的世界设置，还有服务器 mod，然后把它们复制进对应的世界目录就好了。 当然还可以自己手动修改配置文件，比如说 Maxwell 雕像的数量、虫洞类型等等在界面无法修改的东西。 地上世界基础配置文件 return { desc=\"The standard Don't Starve experience.\", hideminimap=false, id=\"SURVIVAL_TOGETHER\", location=\"forest\", max_playlist_position=999, min_playlist_position=0, name=\"Survival\", numrandom_set_pieces=4, -- 雕像的数量 override_level_string=false, overrides={ -- 资源设置的可选项分别为: ----- \"never\" 无 ----- \"rare\" 较少 ----- \"default\" 默认 ----- \"often\" 较多 ----- \"always\" 大量 -- 全局设置 specialevent=\"default\", --- 特殊事件: none (无); default (自动) crow_carnival=\"default\", --- 盛夏鸦年华: default (自动); enabled (启用) hallowed_nights=\"default\", --- 万圣之夜: default (自动); enabled (启用) winters_feast=\"default\", --- 冬季盛宴: default (自动); enabled (启用) year_of_the_gobbler=\"default\", --- 火鸡之年: default (自动); enabled (启用) year_of_the_varg=\"default\", --- 座狼之年: default (自动); enabled (启用) year_of_the_pig=\"default\", --- 猪王之年: default (自动); enabled (启用) year_of_the_carrat=\"default\", --- 胡萝卜鼠之年: default (自动); enabled (启用) year_of_the_beefalo=\"default\", --- 皮弗娄牛之年: default (自动); enabled (启用) year_of_the_catcoon=\"default\", --- 浣猫之年: default (自动); enabled (启用) autumn=\"default\", --- 秋季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random winter=\"default\", --- 冬季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random spring=\"default\", --- 春季: noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 12); --- default (默认长度, 20); longseason (较长, 30); verylongseason (非常长, 50); random summer=\"default\", --- 夏季, noseason (无季节, 0); veryshortseason (非常短, 5); shortseason (较短, 10); --- default (默认长度, 15); longseason (较长, 22); verylongseason (非常长, 40); random day=\"default\", --- 天类型: default (默认); --- longday (加长白天); longdusk (加长黄昏); longnight (加长黑夜); --- noday (无白天); nodusk (无黄昏); nonight (无黑夜); --- onlyday (只有白天); onlydusk (只有黄昏); onlynight (只有黑夜) spawnmode=\"fixed\", --- 出生点: fixed (固定大门); scatter (随机) ghostenabled=\"always\", --- 死亡变为鬼魂: none (关闭); always (启用) portalresurection=\"none\", --- 大门复活: none (关闭); always (启用) ghostsanitydrain=\"always\", --- 队友死亡扣除精神: none (关闭); always (启用) resettime=\"default\", --- 死亡重置时间: none (无); slow (慢); default (正常); fast (快); always (非常快) beefaloheat=\"default\", --- 野牛发情 krampus=\"default\", --- 坎普斯 -- 求生者设置 extrastartingitems=\"default\", --- 额外起始资源: 0 (总是); 5 (5 天后); default (10 天后); 15 (15 天后); 20 (20 天后); none (从不) seasonalstartingitems=\"default\", --- 季节起始物品: never; default spawnprotection=\"default\", --- 出生保护: never (无); default (自动); always (总是) dropeverythingondespawn=\"default\", --- 退出掉落物品: default (默认); always (所有) healthpenalty=\"always\", --- 生命惩罚: none (关闭); always (启用) lessdamagetaken=\"none\", --- 伤害减免: always (减免); none (无); more (增加) temperaturedamage=\"default\", --- 温度伤害: default (默认); nonlethal (非致命) hunger=\"default\", --- 饥饿伤害: default (默认); nonlethal (非致命) darkness=\"default\", --- 黑暗伤害: default (默认); nonlethal (非致命) brightmarecreatures=\"default\", --- 启蒙怪兽数量 shadowcreatures=\"default\", --- 理智怪兽数量 -- 世界设置 petrification=\"default\", --- 石化: none (无); few (慢); default (默认); many (快); max (极快) frograin=\"default\", --- 蛙雨 hounds=\"default\", --- 猎犬来袭频率 summerhounds=\"default\", --- 火狗: never (禁止); default (默认) winterhounds=\"default\", --- 冰狗: never (禁止); default (默认) alternatehunt=\"default\", --- 追猎惊喜 hunt=\"default\", --- 狩猎 lightning=\"default\", --- 闪电 meteorshowers=\"default\", --- 流星 weather=\"default\", --- 雨 wildfires=\"default\", --- 野火 -- 资源再生的可选项分别为: ----- \"never\" 从不 ----- \"veryslow\" 非常慢 ----- \"slow\" 缓慢 ----- \"default\" 默认 ----- \"fast\" 快速 ----- \"veryfast\" 非常快 -- 资源再生设置 regrowth=\"d","date":"02-22","objectID":"/2021/steam_apps/:4:2","series":null,"tags":["Server","Steam","L4D2","DST"],"title":"使用 steamcmd 搭建游戏服务器","uri":"/2021/steam_apps/#小脚本"},{"categories":["ProgrammingLanguage"],"content":"GinShio | Elixir 学习笔记 003 - Intermediate","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/"},{"categories":["ProgrammingLanguage"],"content":" MixMix 是 Elixir 社区开发的集包管理、依赖管理、构建工具于一身的开发工具，扩展性极好，功能强大，自带对 Erlang 的支持，可以类比 Golang 自带的 go，详细的使用方式请参考 mix help 以及 mix 我们如果需要创建一个新项目，使用 mix new 命令即可，详细使用方法可以使用 mix help new 查看，对于新建项目，mix 会很友好的创建一系列文件 (其中还包含 .gitignore) mix new example 我们目前只需要关注其中的 mix.exs 就行了，它包含了配置应用、依赖、环境信息、版本等功能，project 函数设置项目相关信息， application 函数在生产应用文件的时候会用到，deps 函数则是定义项目的依赖项 ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:1:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#mix"},{"categories":["ProgrammingLanguage"],"content":" 管理依赖、环境我们需要把所需的依赖全部列入 deps 中，deps 返回一个列表，每一项依赖都写在元组中，格式如下 {app, requirement} {app, opts} {app, requirement, opts} app 是一个原子，是依赖项的名称 requirement 是一个字符串或正则表达式，用以设定版本 opts 是一个 keyword list，设置依赖相关操作 下面列出常用的添加依赖方式 {:plug, \"\u003e= 0.4.0\"}, # 从 hex.pm 安装版本大于等于 0.4.0 的依赖 {:gettext, git: \"https://github.com/elixir-lang/gettext.git\", tag: \"0.1\"}, # 从指定git仓库下载依赖 {:local_dep, path: \"/path/to/local/deps\"}, # 本地依赖项 {:telemetry, \"~\u003e 0.4\"}, # 从 hex.pm 安装版本 0.4 的依赖项 {:phoenix_view, github: \"phoenixframework/phoenix_view\", branch: \"master\"}, # 从 github 下载依赖 master 分支 {:cowboy, \"~\u003e 1.0\", only: [:dev, :test]}, # 安装依赖，并只在 dev 与 tst 环境启用 当依赖项写好之后，我们只需要执行命令获取依赖就行 mix deps.get 至于环境，mix 默认支持三种 :dev 开发环境，默认的环境 :test 测试环境，使用 mix test :prod 生产环境，把应用上线到生产环境下的配置项 可以写一个 mix.env 文件，来让 mix 从中获取环境，也可以在命令行中使用 MIX_ENV 来配置环境 MIX_ENV=prod mix compile ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:1:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#管理依赖-环境"},{"categories":["ProgrammingLanguage"],"content":" 测试Elixir 社区提供了相当多的工具，其中之前讲过了文档使用的 ExDoc，今天我们主要说一说测试工具 ExUnit 测试是通过 Elixir 脚本来执行的，所以测试文件的后缀必须是 .exs，在测试之前需要使用 ExUnit.start() 启动 ExUnit (一般 mix 中的 test/test_helper.exs 已经帮我们做了这一步)。当执行 mix test 时，就开始运行项目测试了，测试时除了 case 之外，还会执行文档测试 # test/example_test.exs defmodule ExampleTest do use ExUnit.Case doctest Example test \"greets the world\" do assert Example.hello() == :world end end 断言 (assert) 一般用于测试中检查表达式的值是否为真，表达式为假是则会抛出异常，测试失败，下面我们让表达式为假看看测试结果 1) test greets the world (ExampleTest) test/example_test.exs:4 Assertion with != failed, both sides are exactly equal code: assert Example.hello() != :world left: :world stacktrace: test/example_test.exs:5: (test) refute 与 assert 的关系就像 unless 与 if，所以它们正好是一对语义相反的断言 assert_raise 是错误处理中断言某个错误是否被抛出，而 assert_receive 则是并发当中断言小时是否被发送 capture_io 和 capture_log 都是检查应用是否正确输出，不过 _io 检查的是IO输出，_log 检查的是logger输出 ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:1:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#测试"},{"categories":["ProgrammingLanguage"],"content":" 与 Erlang 互操作Elixir 构建于 BEAM (Erlang VM) 之上，可以直接使用 Erlang 大量的库。Erlang 的模块用小写的原子变量表示，比如 :os 和 :timer # 使用 Erlang 的模块计算函数运行时间 defmodule Example do def time(fun, args) do {time, result} = :timer.tc(fun, args) IO.puts(\"Time: #{time} μs\\nResult: #{result}\") end end Example.time(\u0026(\u00261 * \u00261 * \u00261), [100]) # Time: 9 μs # Result: 1000000 至于 Erlang 的第三方模块，我们使用 mix 管理，如果模块不在 hex 中，我们可以将依赖的 git 仓库添加进来，之后就可以愉快的使用这些模块了 需要注意的是，有一些小坑需要注意，比如字符串, Elixir 的字符串是 UTF-8 编码的二进制数据，而 Erlang 的是字符列表 is_list('Example') # true is_list(\"Example\") # false is_binary(\"Example\") # true \u003c\u003c\"Example\"\u003e\u003e === \"Example\" # true is_list('Example'). %% false is_list(\"Example\"). %% true is_binary(\"Example\"). %% false is_binary(\u003c\u003c\"Example\"\u003e\u003e). %% true ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:2:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#与-erlang-互操作"},{"categories":["ProgrammingLanguage"],"content":" 错误处理Elixir 通常会返回 {:ok, result} 或 {:error, reason} 来表示错误，或者抛出异常， Elixir 社区在返回错误方面有一些约定 对于那些作为一个函数功能相关的错误，这个函数应当相应地返回元组表示错误，如用户输入了一个错误的日期类型值 对于那些和函数功能无关的错误，则需要抛出异常，如无法正确解析配置的参数 通常一些公开的 API 中会有一个带感叹号的版本，这些函数返回一个未封包的结果，或者抛出异常 ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:3:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#错误处理"},{"categories":["ProgrammingLanguage"],"content":" raise在学习如何处理异常之前，我们应该学习如何产生一个错误，最简单的方式就是 raise ，它接收错误消息，并产生一个错误；对于产生的错误，我们使用 try/rescue 与模式匹配来处理 try do if elixir, do: raise \"Oh NO!!!\" # (RuntimeError) Oh NO!!! # (ArgumentError) the argument value is invalid raise ArgumentError, message: \"the argument value is invalid\" rescue e in RuntimeError -\u003e \"A Runtime Error occurred: #{e.message}\" e in ArgumentError -\u003e \"An Argument Error occurred: #{e.message}\" end 对于无论是否产生错误，都需要在 try/rescue 之后进行的操作，我们使用 after 来执行 try do if elixir, do: raise \"Oh NO!!!\" rescue e in RuntimeError -\u003e \"A Runtime Error occurred: #{e.message}\" after IO.puts(\"The End!!!\") end # when elixir==true: # The End!!! # \"A Runtime Error occurred: Oh NO!!!\" 这种情况比较常见的用法是关闭文件、连接等（突然怀念 RAII ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:3:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#raise"},{"categories":["ProgrammingLanguage"],"content":" throw感觉遇到了老熟人， throw 和 try/catch ，这种错误处理可以直接抛出一个值，并从当前执行的流程中退出，catch 可以直接使用这个抛出的值，不过在 Elixir 新代码中用的很少了 try do for x \u003c- 1..10 do if rem(x, 3) == 0, do: throw x end catch x -\u003e \"Caught: #{x}\" end # \"Caught: 3\" ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:3:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#throw"},{"categories":["ProgrammingLanguage"],"content":" ExitingExiting 是 Elixir 提供的最后一种产生错误的方式，产生退出信号直接挂掉，这是 Elixir 容错机制的一部分 fn -\u003e exit \"Oops\" end.() # (exit) \"Oops\" try do exit \"Oops\" catch :exit, _ -\u003e \"EXIT!!!\" end 虽然 exit 可以被捕获，但是请不要这样做，把它交给 supervisor 去处理 ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:3:3","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#exiting"},{"categories":["ProgrammingLanguage"],"content":" New ErrorsElixir 提供了相当的内建错误类型，不过 Elixir 还是提供了自建错误类型的方法，使用 defexception/1 来创建新的错误类型，并通过 :message 来设置默认的错误消息 defmodule ExampleError do defexception message: \"an example error\" end try do raise ExampleError rescue e in ExampleError -\u003e e.message end # \"an example error\" ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:3:4","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#new-errors"},{"categories":["ProgrammingLanguage"],"content":" 并发得益于 BEAM (Erlang VM)，Elixir 对并发的支持很棒，并发模型是 Actors，通过消息传递交互的进程，BEAM 的进程是轻量级的，可以运行在所有 CPU 之上，类似于现在所说的协程。 创建新进程的方式很简单，和 Golang 中的 go 有异曲同工之妙，使用 spawn 即可完成，并且返回一个 pid (进程标识符) defmodule Example do def add(a, b), do: a + b end Example.add(2, 3) # 5 spawn(Example, :add, [2, 3]) # #PID\u003c0.124.0\u003e ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#并发"},{"categories":["ProgrammingLanguage"],"content":" 进程 消息传递BEAM 中的进程间方式仅有消息传递， send 允许我们向 PID 发送消息，而 receive 监听和匹配消息，如果没有匹配的消息，进程会被阻塞。如果你用过 Golang，那你一定熟悉它，因为这和 Golang 中的 chan 很像，但是消息传递中发送方不会被阻塞 defmodule Example do def listen do receive do {:ok, \"hello\"} -\u003e IO.puts(\"world\") end IO.puts(\"receive end\") listen() end end pid = spawn(Example, :listen, []) send(pid, {:ok, \"hello\"}) # world # receive end send(pid, :ok) # :ok 进程链接如果进程崩溃了，spawn 就会有问题，因为父进程不会知道子进程出错而导致程序异常，为了解决这个问题，我们需要将父子进程连接起来，这样它们可以收到相互退出的通知 defmodule Example do def explode, do: exit :iris end spawn(Example, :explode, []) # #PID\u003c0.150.0\u003e spawn_link(Example, :explode, []) # (EXIT from #PID\u003c0.107.0\u003e) shell process exited with reason: :iris 有时候我们不希望链接的进程导致当前进程跟着崩溃，这时候就要通过 Process.flag/2 函数捕捉进程的错误退出，这个函数用 Erlang 的 process_flag/2 的 trap_exit 信号。当捕获到被链接的进程发生错误退出时 (trap_exit 设为 true), 就会收到像 {:EXIT, from_pid, reason} 这样的三元组形式的退出信号 defmodule Example do def explode, do: exit :iris def run do Process.flag(:trap_exit, true) spawn_link(Example, :explode, []) receive do {:EXIT, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris 进程监控如果不希望链接两个进程，但是仍然希望获得错误信息通知，那么就需要监控这个进程，即 spawn_monitor，不需要捕获进程，也不会导致当前进程崩溃 defmodule Example do def explode, do: exit :iris def run do spawn_monitor(Example, :explode, []) receive do {:DOWN, _ref, :process, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#进程"},{"categories":["ProgrammingLanguage"],"content":" 进程 消息传递BEAM 中的进程间方式仅有消息传递， send 允许我们向 PID 发送消息，而 receive 监听和匹配消息，如果没有匹配的消息，进程会被阻塞。如果你用过 Golang，那你一定熟悉它，因为这和 Golang 中的 chan 很像，但是消息传递中发送方不会被阻塞 defmodule Example do def listen do receive do {:ok, \"hello\"} -\u003e IO.puts(\"world\") end IO.puts(\"receive end\") listen() end end pid = spawn(Example, :listen, []) send(pid, {:ok, \"hello\"}) # world # receive end send(pid, :ok) # :ok 进程链接如果进程崩溃了，spawn 就会有问题，因为父进程不会知道子进程出错而导致程序异常，为了解决这个问题，我们需要将父子进程连接起来，这样它们可以收到相互退出的通知 defmodule Example do def explode, do: exit :iris end spawn(Example, :explode, []) # #PID\u003c0.150.0\u003e spawn_link(Example, :explode, []) # (EXIT from #PID\u003c0.107.0\u003e) shell process exited with reason: :iris 有时候我们不希望链接的进程导致当前进程跟着崩溃，这时候就要通过 Process.flag/2 函数捕捉进程的错误退出，这个函数用 Erlang 的 process_flag/2 的 trap_exit 信号。当捕获到被链接的进程发生错误退出时 (trap_exit 设为 true), 就会收到像 {:EXIT, from_pid, reason} 这样的三元组形式的退出信号 defmodule Example do def explode, do: exit :iris def run do Process.flag(:trap_exit, true) spawn_link(Example, :explode, []) receive do {:EXIT, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris 进程监控如果不希望链接两个进程，但是仍然希望获得错误信息通知，那么就需要监控这个进程，即 spawn_monitor，不需要捕获进程，也不会导致当前进程崩溃 defmodule Example do def explode, do: exit :iris def run do spawn_monitor(Example, :explode, []) receive do {:DOWN, _ref, :process, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#消息传递"},{"categories":["ProgrammingLanguage"],"content":" 进程 消息传递BEAM 中的进程间方式仅有消息传递， send 允许我们向 PID 发送消息，而 receive 监听和匹配消息，如果没有匹配的消息，进程会被阻塞。如果你用过 Golang，那你一定熟悉它，因为这和 Golang 中的 chan 很像，但是消息传递中发送方不会被阻塞 defmodule Example do def listen do receive do {:ok, \"hello\"} -\u003e IO.puts(\"world\") end IO.puts(\"receive end\") listen() end end pid = spawn(Example, :listen, []) send(pid, {:ok, \"hello\"}) # world # receive end send(pid, :ok) # :ok 进程链接如果进程崩溃了，spawn 就会有问题，因为父进程不会知道子进程出错而导致程序异常，为了解决这个问题，我们需要将父子进程连接起来，这样它们可以收到相互退出的通知 defmodule Example do def explode, do: exit :iris end spawn(Example, :explode, []) # #PID\u003c0.150.0\u003e spawn_link(Example, :explode, []) # (EXIT from #PID\u003c0.107.0\u003e) shell process exited with reason: :iris 有时候我们不希望链接的进程导致当前进程跟着崩溃，这时候就要通过 Process.flag/2 函数捕捉进程的错误退出，这个函数用 Erlang 的 process_flag/2 的 trap_exit 信号。当捕获到被链接的进程发生错误退出时 (trap_exit 设为 true), 就会收到像 {:EXIT, from_pid, reason} 这样的三元组形式的退出信号 defmodule Example do def explode, do: exit :iris def run do Process.flag(:trap_exit, true) spawn_link(Example, :explode, []) receive do {:EXIT, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris 进程监控如果不希望链接两个进程，但是仍然希望获得错误信息通知，那么就需要监控这个进程，即 spawn_monitor，不需要捕获进程，也不会导致当前进程崩溃 defmodule Example do def explode, do: exit :iris def run do spawn_monitor(Example, :explode, []) receive do {:DOWN, _ref, :process, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#进程链接"},{"categories":["ProgrammingLanguage"],"content":" 进程 消息传递BEAM 中的进程间方式仅有消息传递， send 允许我们向 PID 发送消息，而 receive 监听和匹配消息，如果没有匹配的消息，进程会被阻塞。如果你用过 Golang，那你一定熟悉它，因为这和 Golang 中的 chan 很像，但是消息传递中发送方不会被阻塞 defmodule Example do def listen do receive do {:ok, \"hello\"} -\u003e IO.puts(\"world\") end IO.puts(\"receive end\") listen() end end pid = spawn(Example, :listen, []) send(pid, {:ok, \"hello\"}) # world # receive end send(pid, :ok) # :ok 进程链接如果进程崩溃了，spawn 就会有问题，因为父进程不会知道子进程出错而导致程序异常，为了解决这个问题，我们需要将父子进程连接起来，这样它们可以收到相互退出的通知 defmodule Example do def explode, do: exit :iris end spawn(Example, :explode, []) # #PID\u003c0.150.0\u003e spawn_link(Example, :explode, []) # (EXIT from #PID\u003c0.107.0\u003e) shell process exited with reason: :iris 有时候我们不希望链接的进程导致当前进程跟着崩溃，这时候就要通过 Process.flag/2 函数捕捉进程的错误退出，这个函数用 Erlang 的 process_flag/2 的 trap_exit 信号。当捕获到被链接的进程发生错误退出时 (trap_exit 设为 true), 就会收到像 {:EXIT, from_pid, reason} 这样的三元组形式的退出信号 defmodule Example do def explode, do: exit :iris def run do Process.flag(:trap_exit, true) spawn_link(Example, :explode, []) receive do {:EXIT, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris 进程监控如果不希望链接两个进程，但是仍然希望获得错误信息通知，那么就需要监控这个进程，即 spawn_monitor，不需要捕获进程，也不会导致当前进程崩溃 defmodule Example do def explode, do: exit :iris def run do spawn_monitor(Example, :explode, []) receive do {:DOWN, _ref, :process, _from_pid, reason} -\u003e IO.puts(\"Exit reason: #{reason}\") end end end Example.run() # Exit reason: iris ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#进程监控"},{"categories":["ProgrammingLanguage"],"content":" Promise/FutureAgent 是 同步的 Promise/Future 的抽象，函数的返回值就是 Agent 的状态，我们可以通过 PID 来获取它，当然也可以通过命名来获取 {:ok, agent} = Agent.start_link(fn -\u003e [1, 2, 3] end) # {:ok, #PID\u003c0.109.0\u003e} Agent.update(agent, fn (state) -\u003e state ++ [4, 5] end) Agent.get(agent, \u0026(\u00261)) # [1, 2, 3, 4, 5] {:ok, agent} = Agent.start_link(fn -\u003e [1, 2, 3] end, name: Numbers) Agent.get(Numbers, \u0026(\u00261)) # [1, 2, 3] Task 是 异步的 Promise/Future 抽象，与 Agent 类似，但是提供了更多的异步操作 defmodule Example do def add(a, b) do :timer.sleep(3000) # sleep 3 s a + b end end task = Task.async(Example, :add, [5, 6]) # 异步执行 Example.add :timer.sleep(2000) Task.await(task) # 获取 Example.add 的运行结果 ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#promise-future"},{"categories":["ProgrammingLanguage"],"content":" GenServerOTP server 包含了 GenServer 的主要行为和一系列 callbacks，GenServer 是一个专门监控和控制进程状态、启停等的抽象，属于 BEAM/OTP 的一部分，它是一个循环，每次迭代都会处理一个带有目标状态的请求 我们先学习最简单的 GenServer 的使用，即启动与初始化，我们使用 GenServer 简单的实现一个队列 defmodule SimpleQueue do use GenServer @doc \"\"\" Start our queue and link it. This is a helper function \"\"\" def start_link(state \\\\ []) do GenServer.start_link(__MODULE__, state, name: __MODULE__) end @doc \"\"\" GenServer.init/1 callback \"\"\" def init(state), do: {:ok, state} end 我们如果需要使用 GenServer 同步调用，则需要实现 GenServer.handle_call/3 函数，接受请求、调用者PID以及初始状态，期望的返回值为 {:reply, response, state} @doc \"\"\" GenServer.handle_call/3 callback \"\"\" def handle_call(:dequeue, _from, [value | state]), do: {:reply, value, state} def handle_call(:dequeue, _from, []), do: {:reply, nil, []} def handle_call(:queue, _from, state), do: {:reply, state, state} def queue(), do: GenServer.call(__MODULE__, :queue) def dequeue(), do: GenServer.call(__MODULE__, :dequeue) 同步操作时，当调用 :dequeue 函数，就会从队列中取出头部，并将尾部保存为状态等待下一次使用，最终返回头部，当队列为空时则什么都不做。:queue 函数则只会展示当前状态，不会改变状态 SimpleQueue.start_link([1, 2, 3]) # {:ok, #PID\u003c0.136.0\u003e} SimpleQueue.dequeue() # 1 SimpleQueue.queue() # [2, 3] SimpleQueue.dequeue() # 2 SimpleQueue.dequeue() # 3 SimpleQueue.queue() # [] 如果想实现异步操作，那么需要实现类似的 handle_cast/2 函数，它不接受调用者为参数，且没有返回值，剩下的与同步调用时几乎一致 @doc \"\"\" GenServer.handle_cast/2 callback \"\"\" def handle_cast({:enqueue, value}, state), do: {:noreply, state ++ [value]} def enqueue(value), do: GenServer.cast(__MODULE__, {:enqueue, value}) ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:3","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#genserver"},{"categories":["ProgrammingLanguage"],"content":" OTP SupervisorsSupervisors 是一种特殊的进程，它专门监控其他进程，并自动重启出错的子进程，从而实现容错性高的程序。Supervisors 的魔力主要在 Supervisor.start_link/2 函数，这个函数除了能启动 supervisor 和子进程之外，它还允许我们设置管理子进程的策略 使用 mix new simple_queue --sup 命令，我们创建了拥有 supervisor 树的新项目， SimpleQueue 的代码放在 lib/simple_queue.ex，supervisor 的代码我们将添加到 lib/simple_queue/application.ex 中 defmodule SimpleQueue.Application do use Application def start(_type, _args) do children = [SimpleQueue] # 如果有配置项，可以使用元组来配置 children # children = [{SimpleQueue, [1, 2, 3]}] opts = [strategy: :one_for_one, name: SimpleQueue.Supervisor] Supervisor.start_link(children, opts) end end 如果我们的 SimpleQueue 进程崩溃了，或者被终止了，Supervisor 会自动重启这个进程，重启策略有三种 :one_for_one：只重启失败的子进程 :one_for_all：当错误事件出现时，重启所有子进程 :rest_for_one：重启失败的子进程，以及所有在它后面启动的进程 子进程 Specification当 Supervisor 进程启动后，它必须知道如何操作子进程，所以每个子模块都应该有 child_spec/1 函数来定义操作行为，不过幸运的是，如果使用了 use GenServer / use Supervisor 和 use Agent 会自动帮我们定义好这些行为，如果需要自己定义的话 def child_spec(opts) do %{ id: SimpleQueue, start: {__MODULE__, :start_link, [opts]}, shutdown: 5_000, restart: :permanent, type: :worker, } end 我们下来来说说这些参数都是什么 id：Supervisor 用于定位子进程的 specification，必选 start：被 Supervisor 启动时，需要调用的 Module / Function / Arguments，必选 shutdown：子进程关闭时的行为，可选 :brutal_kill 子进程立即停止 :infinity Supervisor 将会无限期等待，这是 :supervisor 进程类型的默认值 任意正整数，以 ms 为单位的等待时间，超时后将杀掉子进程， :work 进程类型的默认值为 5000 restart：子进程崩溃时的处理方式，可选 :permanent 总是重启子进程，这是默认值 :temporary 绝不重启子进程 :transient 只有在非正常中止的时候才重启子进程 type：进程的类型 :worker 或 :supervisor ，默认 :worker，可选 DynamicSupervisorSupervisor 通常在应用启动时伴随子进程启动，但有时候被监管的子进程在应用启动时还是 未知的 (如 web 应用中启动了一个新进程处理用户连接)，我们需要一个能按需启动子进程的 Supervisor，这正是 DynamicSupervisor 的使用场景 我们不指定子进程，我们只要定义好运行时的选项即可，不过 DynamicSupervisor 只支持 :one_for_one 这一种监管策略 options = [strategy: :one_for_one, name: SimpleQueue.Supervisor] DynamicSupervisor.start_link(options) 我们需要使用 start_child/2 函数来动态启动新的 SimpleQueue 子进程，这个函数接收一个 supervisor 和子进程 specification 作为参数 (SimpleQueue 使用了 use GenServer，所以子进程的 specification 已经定义好了) {:ok, pid} = DynamicSupervisor.start_child(SimpleQueue.Supervisor, SimpleQueue) Task SupervisorTask 有自己特殊的 Supervisor，它是专门为动态创建的任务而设计的 supervisor，内部实际使用的是 DynamicSupervisor Task.Supervisor 与其他 Supervisor 在使用上没有什么区别，与 Supervisor 主要的区别时默认重启策略的不同，Task.Supervisor 默认重启策略为 :temporary children = [ {Task.Supervisor, name: ExampleApp.TaskSupervisor, restart: :transient} ] {:ok, pid} = Supervisor.start_link(children, [strategy: :one_for_one]) 当创建好 Task.Supervisor 后，我们可以使用 start_child/2 来创建受监管的 task。如果我们的任务过早地崩溃掉，它会被自动启动。这个功能在处理大量涌来的请求或者后台工作的时候非常有用 {:ok, pid} = Task.Supervisor.start_child(ExampleApp.TaskSupervisor, fn -\u003e background_work() end) ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:4","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#otp-supervisors"},{"categories":["ProgrammingLanguage"],"content":" OTP SupervisorsSupervisors 是一种特殊的进程，它专门监控其他进程，并自动重启出错的子进程，从而实现容错性高的程序。Supervisors 的魔力主要在 Supervisor.start_link/2 函数，这个函数除了能启动 supervisor 和子进程之外，它还允许我们设置管理子进程的策略 使用 mix new simple_queue --sup 命令，我们创建了拥有 supervisor 树的新项目， SimpleQueue 的代码放在 lib/simple_queue.ex，supervisor 的代码我们将添加到 lib/simple_queue/application.ex 中 defmodule SimpleQueue.Application do use Application def start(_type, _args) do children = [SimpleQueue] # 如果有配置项，可以使用元组来配置 children # children = [{SimpleQueue, [1, 2, 3]}] opts = [strategy: :one_for_one, name: SimpleQueue.Supervisor] Supervisor.start_link(children, opts) end end 如果我们的 SimpleQueue 进程崩溃了，或者被终止了，Supervisor 会自动重启这个进程，重启策略有三种 :one_for_one：只重启失败的子进程 :one_for_all：当错误事件出现时，重启所有子进程 :rest_for_one：重启失败的子进程，以及所有在它后面启动的进程 子进程 Specification当 Supervisor 进程启动后，它必须知道如何操作子进程，所以每个子模块都应该有 child_spec/1 函数来定义操作行为，不过幸运的是，如果使用了 use GenServer / use Supervisor 和 use Agent 会自动帮我们定义好这些行为，如果需要自己定义的话 def child_spec(opts) do %{ id: SimpleQueue, start: {__MODULE__, :start_link, [opts]}, shutdown: 5_000, restart: :permanent, type: :worker, } end 我们下来来说说这些参数都是什么 id：Supervisor 用于定位子进程的 specification，必选 start：被 Supervisor 启动时，需要调用的 Module / Function / Arguments，必选 shutdown：子进程关闭时的行为，可选 :brutal_kill 子进程立即停止 :infinity Supervisor 将会无限期等待，这是 :supervisor 进程类型的默认值 任意正整数，以 ms 为单位的等待时间，超时后将杀掉子进程， :work 进程类型的默认值为 5000 restart：子进程崩溃时的处理方式，可选 :permanent 总是重启子进程，这是默认值 :temporary 绝不重启子进程 :transient 只有在非正常中止的时候才重启子进程 type：进程的类型 :worker 或 :supervisor ，默认 :worker，可选 DynamicSupervisorSupervisor 通常在应用启动时伴随子进程启动，但有时候被监管的子进程在应用启动时还是 未知的 (如 web 应用中启动了一个新进程处理用户连接)，我们需要一个能按需启动子进程的 Supervisor，这正是 DynamicSupervisor 的使用场景 我们不指定子进程，我们只要定义好运行时的选项即可，不过 DynamicSupervisor 只支持 :one_for_one 这一种监管策略 options = [strategy: :one_for_one, name: SimpleQueue.Supervisor] DynamicSupervisor.start_link(options) 我们需要使用 start_child/2 函数来动态启动新的 SimpleQueue 子进程，这个函数接收一个 supervisor 和子进程 specification 作为参数 (SimpleQueue 使用了 use GenServer，所以子进程的 specification 已经定义好了) {:ok, pid} = DynamicSupervisor.start_child(SimpleQueue.Supervisor, SimpleQueue) Task SupervisorTask 有自己特殊的 Supervisor，它是专门为动态创建的任务而设计的 supervisor，内部实际使用的是 DynamicSupervisor Task.Supervisor 与其他 Supervisor 在使用上没有什么区别，与 Supervisor 主要的区别时默认重启策略的不同，Task.Supervisor 默认重启策略为 :temporary children = [ {Task.Supervisor, name: ExampleApp.TaskSupervisor, restart: :transient} ] {:ok, pid} = Supervisor.start_link(children, [strategy: :one_for_one]) 当创建好 Task.Supervisor 后，我们可以使用 start_child/2 来创建受监管的 task。如果我们的任务过早地崩溃掉，它会被自动启动。这个功能在处理大量涌来的请求或者后台工作的时候非常有用 {:ok, pid} = Task.Supervisor.start_child(ExampleApp.TaskSupervisor, fn -\u003e background_work() end) ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:4","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#子进程-specification"},{"categories":["ProgrammingLanguage"],"content":" OTP SupervisorsSupervisors 是一种特殊的进程，它专门监控其他进程，并自动重启出错的子进程，从而实现容错性高的程序。Supervisors 的魔力主要在 Supervisor.start_link/2 函数，这个函数除了能启动 supervisor 和子进程之外，它还允许我们设置管理子进程的策略 使用 mix new simple_queue --sup 命令，我们创建了拥有 supervisor 树的新项目， SimpleQueue 的代码放在 lib/simple_queue.ex，supervisor 的代码我们将添加到 lib/simple_queue/application.ex 中 defmodule SimpleQueue.Application do use Application def start(_type, _args) do children = [SimpleQueue] # 如果有配置项，可以使用元组来配置 children # children = [{SimpleQueue, [1, 2, 3]}] opts = [strategy: :one_for_one, name: SimpleQueue.Supervisor] Supervisor.start_link(children, opts) end end 如果我们的 SimpleQueue 进程崩溃了，或者被终止了，Supervisor 会自动重启这个进程，重启策略有三种 :one_for_one：只重启失败的子进程 :one_for_all：当错误事件出现时，重启所有子进程 :rest_for_one：重启失败的子进程，以及所有在它后面启动的进程 子进程 Specification当 Supervisor 进程启动后，它必须知道如何操作子进程，所以每个子模块都应该有 child_spec/1 函数来定义操作行为，不过幸运的是，如果使用了 use GenServer / use Supervisor 和 use Agent 会自动帮我们定义好这些行为，如果需要自己定义的话 def child_spec(opts) do %{ id: SimpleQueue, start: {__MODULE__, :start_link, [opts]}, shutdown: 5_000, restart: :permanent, type: :worker, } end 我们下来来说说这些参数都是什么 id：Supervisor 用于定位子进程的 specification，必选 start：被 Supervisor 启动时，需要调用的 Module / Function / Arguments，必选 shutdown：子进程关闭时的行为，可选 :brutal_kill 子进程立即停止 :infinity Supervisor 将会无限期等待，这是 :supervisor 进程类型的默认值 任意正整数，以 ms 为单位的等待时间，超时后将杀掉子进程， :work 进程类型的默认值为 5000 restart：子进程崩溃时的处理方式，可选 :permanent 总是重启子进程，这是默认值 :temporary 绝不重启子进程 :transient 只有在非正常中止的时候才重启子进程 type：进程的类型 :worker 或 :supervisor ，默认 :worker，可选 DynamicSupervisorSupervisor 通常在应用启动时伴随子进程启动，但有时候被监管的子进程在应用启动时还是 未知的 (如 web 应用中启动了一个新进程处理用户连接)，我们需要一个能按需启动子进程的 Supervisor，这正是 DynamicSupervisor 的使用场景 我们不指定子进程，我们只要定义好运行时的选项即可，不过 DynamicSupervisor 只支持 :one_for_one 这一种监管策略 options = [strategy: :one_for_one, name: SimpleQueue.Supervisor] DynamicSupervisor.start_link(options) 我们需要使用 start_child/2 函数来动态启动新的 SimpleQueue 子进程，这个函数接收一个 supervisor 和子进程 specification 作为参数 (SimpleQueue 使用了 use GenServer，所以子进程的 specification 已经定义好了) {:ok, pid} = DynamicSupervisor.start_child(SimpleQueue.Supervisor, SimpleQueue) Task SupervisorTask 有自己特殊的 Supervisor，它是专门为动态创建的任务而设计的 supervisor，内部实际使用的是 DynamicSupervisor Task.Supervisor 与其他 Supervisor 在使用上没有什么区别，与 Supervisor 主要的区别时默认重启策略的不同，Task.Supervisor 默认重启策略为 :temporary children = [ {Task.Supervisor, name: ExampleApp.TaskSupervisor, restart: :transient} ] {:ok, pid} = Supervisor.start_link(children, [strategy: :one_for_one]) 当创建好 Task.Supervisor 后，我们可以使用 start_child/2 来创建受监管的 task。如果我们的任务过早地崩溃掉，它会被自动启动。这个功能在处理大量涌来的请求或者后台工作的时候非常有用 {:ok, pid} = Task.Supervisor.start_child(ExampleApp.TaskSupervisor, fn -\u003e background_work() end) ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:4","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#dynamicsupervisor"},{"categories":["ProgrammingLanguage"],"content":" OTP SupervisorsSupervisors 是一种特殊的进程，它专门监控其他进程，并自动重启出错的子进程，从而实现容错性高的程序。Supervisors 的魔力主要在 Supervisor.start_link/2 函数，这个函数除了能启动 supervisor 和子进程之外，它还允许我们设置管理子进程的策略 使用 mix new simple_queue --sup 命令，我们创建了拥有 supervisor 树的新项目， SimpleQueue 的代码放在 lib/simple_queue.ex，supervisor 的代码我们将添加到 lib/simple_queue/application.ex 中 defmodule SimpleQueue.Application do use Application def start(_type, _args) do children = [SimpleQueue] # 如果有配置项，可以使用元组来配置 children # children = [{SimpleQueue, [1, 2, 3]}] opts = [strategy: :one_for_one, name: SimpleQueue.Supervisor] Supervisor.start_link(children, opts) end end 如果我们的 SimpleQueue 进程崩溃了，或者被终止了，Supervisor 会自动重启这个进程，重启策略有三种 :one_for_one：只重启失败的子进程 :one_for_all：当错误事件出现时，重启所有子进程 :rest_for_one：重启失败的子进程，以及所有在它后面启动的进程 子进程 Specification当 Supervisor 进程启动后，它必须知道如何操作子进程，所以每个子模块都应该有 child_spec/1 函数来定义操作行为，不过幸运的是，如果使用了 use GenServer / use Supervisor 和 use Agent 会自动帮我们定义好这些行为，如果需要自己定义的话 def child_spec(opts) do %{ id: SimpleQueue, start: {__MODULE__, :start_link, [opts]}, shutdown: 5_000, restart: :permanent, type: :worker, } end 我们下来来说说这些参数都是什么 id：Supervisor 用于定位子进程的 specification，必选 start：被 Supervisor 启动时，需要调用的 Module / Function / Arguments，必选 shutdown：子进程关闭时的行为，可选 :brutal_kill 子进程立即停止 :infinity Supervisor 将会无限期等待，这是 :supervisor 进程类型的默认值 任意正整数，以 ms 为单位的等待时间，超时后将杀掉子进程， :work 进程类型的默认值为 5000 restart：子进程崩溃时的处理方式，可选 :permanent 总是重启子进程，这是默认值 :temporary 绝不重启子进程 :transient 只有在非正常中止的时候才重启子进程 type：进程的类型 :worker 或 :supervisor ，默认 :worker，可选 DynamicSupervisorSupervisor 通常在应用启动时伴随子进程启动，但有时候被监管的子进程在应用启动时还是 未知的 (如 web 应用中启动了一个新进程处理用户连接)，我们需要一个能按需启动子进程的 Supervisor，这正是 DynamicSupervisor 的使用场景 我们不指定子进程，我们只要定义好运行时的选项即可，不过 DynamicSupervisor 只支持 :one_for_one 这一种监管策略 options = [strategy: :one_for_one, name: SimpleQueue.Supervisor] DynamicSupervisor.start_link(options) 我们需要使用 start_child/2 函数来动态启动新的 SimpleQueue 子进程，这个函数接收一个 supervisor 和子进程 specification 作为参数 (SimpleQueue 使用了 use GenServer，所以子进程的 specification 已经定义好了) {:ok, pid} = DynamicSupervisor.start_child(SimpleQueue.Supervisor, SimpleQueue) Task SupervisorTask 有自己特殊的 Supervisor，它是专门为动态创建的任务而设计的 supervisor，内部实际使用的是 DynamicSupervisor Task.Supervisor 与其他 Supervisor 在使用上没有什么区别，与 Supervisor 主要的区别时默认重启策略的不同，Task.Supervisor 默认重启策略为 :temporary children = [ {Task.Supervisor, name: ExampleApp.TaskSupervisor, restart: :transient} ] {:ok, pid} = Supervisor.start_link(children, [strategy: :one_for_one]) 当创建好 Task.Supervisor 后，我们可以使用 start_child/2 来创建受监管的 task。如果我们的任务过早地崩溃掉，它会被自动启动。这个功能在处理大量涌来的请求或者后台工作的时候非常有用 {:ok, pid} = Task.Supervisor.start_child(ExampleApp.TaskSupervisor, fn -\u003e background_work() end) ","date":"02-21","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/:4:4","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir Intermediate","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_003/#task-supervisor"},{"categories":["ProgrammingLanguage"],"content":"GinShio | Elixir 学习笔记 002 - 基本语法","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/"},{"categories":["ProgrammingLanguage"],"content":" 模块之前函数的时候也简单的见过模块了，Elixir 允许嵌套模块，这样可以轻松定义多层命名空间 defmodule Greeter.Greeting do def morning(name), do: \"Good morning, #{name}\" def evening(name), do: \"Good evening, #{name}\" end Greeter.Greeting.morning(\"iris\") # \"Good morning, iris\" 模块通常还会有一些属性，这些属性通常被用作常量 defmodule Example do @greeting \"Hello\" def greeting(name) do ~s(#{@greeting}, #{name}.) end end Example.greeting(\"iris\") # \"Hello, iris.\" 当然还有一些的属性，用于保留功能，比如 moduledoc 和 doc 作为文档，文档可以用 ExDoc 生成 HTML，而 ExMark 是一个 Markdown 分析器，最终我们可以使用 mix 来生成文档 defmodule Example do @moduledoc \"\"\" This is the Hello module. \"\"\" @moduledoc since: \"1.0.0\" @doc \"\"\" Says hello to the given `name`. Returns `:ok`. ## Examples iex\u003e Example.world(:john) :ok \"\"\" @doc since: \"1.3.0\" def world(name) do IO.puts(\"hello #{name}\") end end ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:1:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#模块"},{"categories":["ProgrammingLanguage"],"content":" 结构体 (Struct)在 Elixir 中结构体 Struct 是 Map 的特殊形式，它的键是预定义的，一般都有默认值，不过有个限制，Struct 只能定义在 Module 中，一般一个模块定义一个结构体 defmodule Example.User do defstruct name: \"GinShio\", roles: [] end default = %Example.User{} # %Example.User{name: \"GinShio\", roles: []} iris = %{default | name: \"iris\"} # %Example.User{name: \"iris\", roles: []} inspect(default) # \"%Example.User{name: \\\"GinShio\\\", roles: []}\" %Example.User{name: \"iris\"} = iris # pattern 可以看到 inspect 展示了 Struct 中的所有字段，如果我们想排除保护字段，可以使用 @derive 注解来实现这一功能 defmodule Example.User do @derive {Inspect, only: [:name]} # 只打印 :only 中的字段 # @derive {Inspect, except: [:roles]} # 排除 :except 中的字段 defstruct name: nil, roles: [] end inspect(default) # \"#Example.User\u003cname: \\\"GinShio\\\", ...\u003e\" ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:1:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#结构体--struct"},{"categories":["ProgrammingLanguage"],"content":" 组合 (Composition)Elixir 以组合的方式为模块添加全新的功能，并且也为我们提供了多种访问其他模块的方式 alias (别名) 通过别名访问其他模块，当别名有冲突时还可以使用 as 来设置别名解决冲突，当然也可以一次指定多个别名 defmodule Example.Source do def hello(name), do: \"Hello, #{name}!\" end defmodule Example.Alias do alias Example.Source def hello(), do: Source.hello(\"FooBar\") end defmodule Example.AliasMulti do alias Example.{Source, Alias} def hello(), do: Source.hello(\"Src\") end defmodule Other.Alias do alias Example.Alias, as: ExAlias def hello(), do: ExAlias.hello() end import (导入) 我们可以从其他模块导入函数，不过这有些污染名称空间 # last([1, 2, 3]) # (CompileError): undefined function last/1 List.last([1, 2, 3]) # 3 import List last([1, 2, 3]) # 3 好消息是，就像之前学习 Struct 时控制 inspect 输出一样，:only 与 :except 是我们控制 import 导出的好帮手 import List, only: [last: 1] # only import `last/1' first([1, 2, 3]) # (CompileError): undefined function first/1 last([1, 2, 3]) # 3 我们如果想导出所有的函数呢，这样太麻烦了吧，好在 Elixir 提供了一种特殊的方法， :functions 和 :macros 分别代表函数和宏，不过不能除外就是了 import List, only: :functions # 导出 List 中的所有函数 import List, only: :macros # 导出 List 中的所有宏 # import List, except: :functions # (CompileError): invalid :except option for import, expected value to be a list literal, got: :functions require (请求) 这是一个只对宏有效的指令，虽然不知道宏是什么，不过只要知道它只 import 模块中的宏而不是函数，目前来说就行了 use (使用) 这是一个修改当前模块的指令，我们在调用 use 时会执行指定模块中所定义的 __using__ 宏进行回调，当然现在不懂没关系，在学习了宏之后再来学习这里吧 (反正我也不懂 defmodule Hello do defmacro __using__(opts) do quote do def hello(name), do: \"Hi, #{name}\" end end end defmodule Example do use Hello end Example.hello(\"GinShio\") # \"Hi, GinShio\" 非常的神奇，当然宏还是可以带参数的，比如下面这个从 Elixir School 抄来的示例 (这个更看不懂了 defmodule Hello do defmacro __using__(opts) do greeting = Keyword.get(opts, :greeting, \"Hi\") quote do def hello(name), do: unquote(greeting) \u003c\u003e \", \" \u003c\u003e name end end end defmodule Example.En do use Hello end defmodule Example.Es do use Hello, greeting: \"Hola\" end Example.En.hello(\"GinShio\") # \"Hi, GinShio\" Example.Es.hello(\"GinShio\") # \"Hola, GinShio\" ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:1:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#组合--composition"},{"categories":["ProgrammingLanguage"],"content":" 注解Elixir 是一个动态语言，类型信息会被编译器忽略，这样完成一个程序会很麻烦，因此我们往往会寄希望于其他工具帮助我们来完成检查，降低复杂度，这时就需要注解来帮助我们。 Specification 可以理解为一个接口 (interface)，用于定义了函数的参数与返回值的类型，语法 @spec name(param list) :: return，简单用例子看一下怎么用吧 @spec sum_product(integer) :: integer def sum_product(a) do [1, 2, 3] |\u003e Enum.map(fn e -\u003e e * a end) |\u003e Enum.sum() end 我们可以正常的使用这个函数，毕竟它不被编译器所关注，Enum.sum() 将会返回一个 number 而不是 integer，如果想发现这些问题的话，我们需要使用 Dialyzer 这类静态分析器来帮我们解决这些问题 当我们使用一个 spec 时我们可能需要有一些很复杂的结构，如果每次都定义一遍实在太麻烦了，这时我们就需要类型相关的注解，好在 Elixir 提供了 @type 公开类型，类型的内部结构是公开的 @typep 私有类型，只能在模块定义的地方使用 @opaque 公开类型，但内部结构是私有的 当然类型也是可以带参数的 (有 Haskell 那味了)，当然别忘了和模块文档相似的 @typedoc (类型文档)，我们看看怎么用 defmodule Example.Type do defstruct first: nil, last: nil @type t(first, last) :: %Example.Type{first: first, last: last} @typedoc \"\"\" Type that represents Example struct with :first(integer) and :last(integer) \"\"\" @type t :: %Example.Type{first: integer, last: integer} end defmodule Example do @spec sum_times(integer, Example.Type.t()) :: integer def sum_times(a, params) do for i \u003c- params.first..params.last do i end |\u003e Enum.map(fn(e) -\u003e e * a end) |\u003e Enum.sum() |\u003e round end end ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:1:3","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#注解"},{"categories":["ProgrammingLanguage"],"content":" 字符串Elixir 字符串是 UTF-8 编码，底层是字节序列，即二进制字节表示，如果我们在字符串后添加一个字节 0 的话将看到字符串的底层字节 (\u003c\u003c\u003e\u003e 表示一个字节的值) en = \"hello\" zh = \"你好\" en_bin = en \u003c\u003e \u003c\u003c0\u003e\u003e # \u003c\u003c104, 101, 108, 108, 111, 0\u003e\u003e zh_bin = zh \u003c\u003e \u003c\u003c0\u003e\u003e # \u003c\u003c228, 189, 160, 229, 165, 189, 0\u003e\u003e 除了字符序列，Elixir 中还有一种字符列表，它们使用 'char list' 来表示，字符列表的值都是 UTF-8 码点 ，这与字符序列有很大不同。正如示例中的 你，码点是 20320，但是 UTF-8 编码中是三个字节 en_ = 'hello' # [104, 101, 108, 108, 111] zh_ = '你好' # [20320, 22909] ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:2:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#字符串"},{"categories":["ProgrammingLanguage"],"content":" 魔符 (Sigil)Sigil 是 Elixir 中用于 表示 和 处理 字面量的，可以自定义，当然也有一些内置的 Sigil Sigil 释义 ~C 不处理 插值和转义的 字符列表 ~c 处理 插值和转义的 字符列表 ~R 不处理 插值和转义的 正则表达式 ~r 处理 插值和转义的 正则表达式 ~S 不处理 插值和转义的 字符串 ~s 处理 插值和转义的 字符串 ~W 不处理 插值和转义的 单词列表 ~w 处理 插值和转义的 单词列表 ~N NaiveDateTime 格式的数据结构 ~U DateTime 格式的数据结构 在使用 Sigil 时需要设定字面量的范围，需要用到分隔符 \u003c...\u003e (尖括号) {...} (花括号) [...] (方括号) (...) (小括号) |...| (直线) /.../ (斜线) \"...\" (双引号) '...' (单引号) 接下来我们大概看看这些 Sigil 的用法 ~C/2 + 7 = #{2 + 7}/ # '2+7=\\#{2+7}' ~c/2 + 7 = #{2 + 7}/ # '2 + 7 = 9' \"Elixir\" =~ ~r/elixir/ # false \"elixir\" =~ ~r/elixir/ # true ~w/i love elixir school/ # [\"i\", \"love\", \"elixir\", \"school\"] ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:3:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#魔符--sigil"},{"categories":["ProgrammingLanguage"],"content":" 时间Elixir 内置了几个处理时间的模块，让我们试试最简单的，当前的 UTC 时间 t = Time.utc_now() # ~T[11:40:46.527943] t.hour # 11 t.minute # 40 t.second # 49 # t.day # (KeyError) key :day not found UTC Time 虽然可以使用 Sigil，但是它只有时间，没有日期信息，也没有时区信息，那我们试试日期吧，只有日期没有时间！！！ d = Date.utc_today() # ~D[2021-02-19] {:ok, date} = Date.new(2021, 03, 01) # {:ok, ~D[2021-03-01]} d.year # 2021 d.month # 2 d.day # 19 Date.day_of_week(d) # 5 Date.leap_year?(d) # false Sigil 创建 Date 和 Time 还挺方便，不过有个问题，它们都是最简单的 UTC 时间，并且仅有日期或时间，也没有时区，显得很不好用 还记得之前 Sigil 中列出的 ~N 吗，我们现在来看看这个 NaiveDateTime，它包含了日期与时间，不过还是缺少时区，所以它所表示的还是 UTC 时间 n = NaiveDateTime.utc_now() # ~N[2021-02-19 11:50:44.630064], UTC时间 NaiveDateTime.add(n, 30) # ~N[2021-02-19 11:51:14.630064], 增加30s l = NaiveDateTime.local_now() # ~N[2021-02-19 19:52:10], 本地时间 NaiveDateTime.to_iso8601(l) # \"2021-02-19T19:52:10\", 格式化到 iso8601 DateTime 是包含全部信息的时间数据结构，不过遗憾的是这个模块仅有一些转换函数和处理 UTC 的函数，因为 Elixir 还没有提供相关的 时区数据库，务必加上 tz / tzdata 这个时区数据库再来体验，不然只能 UTC 太痛苦了 DateTime.utc_now() # ~U[2021-02-19 11:57:59.244240Z] {:ok, u} = DateTime.from_naive(n, \"Etc/UTC\") # {:ok, ~U[2021-02-19 11:50:44.630064Z]} DateTime.from_unix(1613735444) # {:ok, ~U[2021-02-19 11:50:44Z]} DateTime.from_iso8601(\"2021-02-19T19:52:10Z\") # {:ok, ~U[2021-02-19 19:52:10Z], 0} 还是有点不爽？不爽的话试试 timex / calendar 这些功能强大的第三方时间库 ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:4:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#时间"},{"categories":["ProgrammingLanguage"],"content":" 推导推导表达式在函数式编程中很常见，它可以根据一定规则生成全新列表，甚至非函数式的语言中 (如 Python) 也可以看到它的影子，举一个简单的例子 for x \u003c- [1, 2, 3, 4, 5], do: x * x # [1, 4, 9, 16, 25] 生成器从列表依次获取值，然后根据规则生成全新的列表，不过不一定必须是列表，还可以是任意的可遍历类型 for {k, v} \u003c- %{foo: \"bar\", hello: \"world\"}, do: {k, v} # [foo: \"bar\", hello: \"world\"] for \u003c\u003cc \u003c- \"hello\"\u003e\u003e, do: \u003c\u003cc\u003e\u003e # [\"h\", \"e\", \"l\", \"l\", \"o\"] 推导表达式可以嵌套，并且支持模式匹配，不过没发现怎么并列 (GHC牛皮 # [{1, 6}, {1, 7}, {1, 8}, {2, 6}, {2, 7}, {2, 8}, {3, 6}, {3, 7}, {3, 8}] for x \u003c- [1,2,3], y \u003c- [6,7,8], do: {x,y} # [\"Hello\", \"World\"] for {:ok, v} \u003c- [ok: \"Hello\", error: \"Unknown\", ok: \"World\"], do: v 好消息，guard 可以在这里使用，推导表达式会为检查相应的变量，只有 guard 表达式为真时才会继续执行 require Integer for x \u003c- 1..10, Integer.is_even(x), do: x # [2, 4, 6, 8, 10] for x \u003c- 1..100, Integer.is_even(x), rem(x, 9) == 0, do: x # [18, 36, 54, 72, 90] 推导表达式可以不止生成列表，还能生成很多东西，任何 Collectable 协议的结构体！！！当然这个协议，现在不重要，重要的是，可以生成其他结构 # %{one: 1, three: 3, two: 2} for {k, v} \u003c- [one: 1, two: 2, three: 3], into: %{}, do: {k, v} # \"Hello,iris\" for c \u003c- [72, 101, 108, 108, 111, 44, 105, 114, 105, 115], into: \"\", do: \u003c\u003cc\u003e\u003e ","date":"02-19","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/:5:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 模块","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_002/#推导"},{"categories":["ProgrammingLanguage"],"content":"GinShio | Elixir 学习笔记 001 - 基本语法","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/"},{"categories":["ProgrammingLanguage"],"content":"好久没学习，随便写点东西，一直想学FP来着，不过之前 Haskell 整的有点难受，好难啊不太会，下次静下心来好好学一学吧，不过先试试 Erlang / Elixir，听说也很难？ 至于原因，莫名喜欢 Erlang，不知道为什么哈哈哈哈，得知有 Elixir 这个披着 Ruby 皮、用着 Beam 的 Lisp 觉得还不错？毕竟 Lisp 大法好！！ (虽然我不会 lisp) 不过 Elixir 名字好听 Logo 也好看 好了，前置吐槽就这么多吧，希望可以静下心好好学学 Elixir，呃，我也不知道可不可以啦，但是如果对 Elixir 感兴趣的话可以在 Elixir School 尝试学习一下，我也才开始从这里开始学习 ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:0:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#"},{"categories":["ProgrammingLanguage"],"content":" 基本类型 整数类型：在 Erlang 和 Elixir 中，整数类型都是高精度类型，不区分类型所占的字节，有点类似 Python 中的整数 Elixir 支持 二(0b)、八(0o)、十、十六(0x)进制的整数字面量，使用起来十分方便 255 # 十进制整数 255 0b10001000 # 二进制整数 136 0o7654321 # 八进制整数 2054353 0xFFFF # 十六进制整数 65535 浮点类型：嗯，它是 IEEE 754，好了就这样吧，介绍完了 布尔类型：true 和 false，不过有一点需要注意，在 Elixir 中除了 false 和 nil 之外的所有值都为 true 原子类型：名字和代表的值相同的常量，有点像 Ruby 和 Lisp 中的 Symbol。BTW 布尔值 true / false 实际对应的是原子 :true 和 :false :foo # 符号，名为 :foo :foo == :bar # false is_atom(true) # true is_boolean(:false) # true true == :true # true 大写字母开始的别名也是原子，模块名 (module) 也是原子，当然也可以使用原子直接引用 Erlang 标准库的模块 is_atom(MyAtom) # true is_atom(MyApp.MyModule) # true 字符串：这是一个 UTF-8 编码的字符串，用双引号包住，并且支持换行与转义字符 \"Hello\" # \"Hello\" \"这是一个\\nString\" #这是一个\\nString 当然要讲讲字符串的简单操作了，插值可以将变量插入到字符串中，这个操作有点类似 Ruby 和 Shell 中的操作；字符串拼接操作，可以将两个字符串拼接在一起 name = \"GinShio\" \"Hello #{name}\" # \"Hello GinShio\" \"Hello\" \u003c\u003e \" \" \u003c\u003e name # \"Hello GinShio\" ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:1:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#基本类型"},{"categories":["ProgrammingLanguage"],"content":" 基本操作","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:2:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#基本操作"},{"categories":["ProgrammingLanguage"],"content":" 算术运算Elixir 支持基本的 加 (+) 、 减 (-) 、 乘 (*) 、 除 (/)，还有 div 和 mod 两个函数用于整数的除法和取模运算 2 + 3 # 5 6 - 1 # 5 2 * 3 # 6 9 / 3 # 3.0 9 / 2 # 4.5 div(9, 2) # 4 rem(9, 2) # 1 ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:2:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#算术运算"},{"categories":["ProgrammingLanguage"],"content":" 逻辑运算Elixir 支持逻辑运算，和其他语言差不多的 和(\u0026\u0026) 、 或(||) 、 非(!) -20 || true # -20 false || 42 # 42 nil || true # true 42 \u0026\u0026 true # true 42 \u0026\u0026 false # false 42 \u0026\u0026 nil # nil true \u0026\u0026 false # false !42 # false !false # true 当然还有三个操作符 and 、 or 和 not ，不过这些操作符的地一个参数必须是布尔类型 true and 42 # 42 # 42 and true # (BadBooleanError) expected a boolean on left-side of \"and\" not true # false true and nil # nil # nil and true # (BadBooleanError) expected a boolean on left-side of \"and\" ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:2:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#逻辑运算"},{"categories":["ProgrammingLanguage"],"content":" 关系运算常见的关系运算 ==, !=, \u003c=, \u003c, \u003e= 和 \u003e，这与其他语言中的关系运算符相似 1 \u003e 2 # false 1 != 2 # true 2 == 2 # true 2 \u003c= 3 # true Elixir 还提供了两个关系运算符 === 和 !==，它们类似于 JS 中的严格比较 2 == 2.0 # true 3 == 3.0000000000000000000000001 # true 2 === 2.0 # false 2 === 2 # true 3 === 3.0000000000000000000000001 # false Elixir 中有一个很重要的特性，任意类型之间都可以比较，因为类型都有一个优先级，支持它们之间互相比较 信息 number \u003c atom \u003c reference \u003c function \u003c port \u003c pid \u003c tuple \u003c map \u003c list \u003c bitstring 3 \u003c :foo # true {:hello, :world} \u003e [1, 2, 3] # false 9 \u003e [1, 2, 3] # false :bar \u003c [1, 2, 3] # true ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:2:3","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#关系运算"},{"categories":["ProgrammingLanguage"],"content":" 集合","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:3:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#集合"},{"categories":["ProgrammingLanguage"],"content":" 列表 (List)列表是简单的集合，可以包含不同的数据类型，并且可以包含相同的值，内部使用 链表 实现，头插相较于尾插更快，获取长度也是 O(n) 的 头插使用 | (cons) 进行操作，可以将元素插入到列表的头部，而拼接操作 ++ 可以将两个列表拼接成一个，而减法操作 -- 是基于严格比较的依照顺序的删除元素 [3.14, :pie, \"Apple\"] # [3.14, :pie, \"Apple\"] [\"π\" | [3.14, :pie]] # [\"π\", 3.14, :pie] [[:foo, \"hello\"] | [\"bar\", :world]] # [[:foo, \"hello\"], \"bar\", :world] [3.14, :pie] ++ [\"Cherry\"] # [3.14, :pie, \"Cherry\"] [1,2,2,3,2,3] -- [1,2,3,2] # [2, 3] [1,2,2,3,2,3] -- [1,2,2,3,3] # [2] [2] -- [2.0] # [2] [2.0] -- [2.0] # [] 列表可以选取 头 (head) 和 尾 (tail)，头是列表的第一个元素，尾是除去第一个元素剩下的列表，也可以和 cons 结合起来获取列表的头部与尾部 hd [3.14, :pie, \"Apple\"] # 3.14 tl [3.14, :pie, \"Apple\"] # [:pie, \"Apple\"] [head | tail] = [3.14, :pie, \"Apple\"] # head = 3.14 # tail = [:pie, \"Apple\"] ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:3:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#列表--list"},{"categories":["ProgrammingLanguage"],"content":" 元组 (Tuple)元组与列表类似，不过元组使用的是连续内存实现，获取元组的长度很快，但修改很麻烦 (新的元组必须重新在内存中拷贝一份) 元组相较于列表没有那么多操作，元组更倾向于做一个不可变的数据类型，我们常常把二元组称为 pair，三元组称为 triple，而其他长度为n的元组称其为 N 元组 (n-tuple)，这个概念在其他语言中也很常见 {3.14, :pie, \"Apple\"} # {3.14, :pie, \"Apple\"} {:foo, \"bar\"} # pair {5, 3.14, :test} # triple ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:3:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#元组--tuple"},{"categories":["ProgrammingLanguage"],"content":" 关键字列表 (Keyword List)keywords 是一种特殊的列表，列表的元素是 pair，且 pair 的第一个元素必须是原子，其他行为与列表完全一致，不过 keywords 的语法可以不用写 pair 那么复杂，而是简便的 key: value 形式即可 keywords 有一些特殊的特性 键是 原子的 键是 有序的，即定义后顺序不会改变 键 不必唯一 [foo: \"bar\", hello: \"world\", pi: 3.14] # [foo: \"bar\", hello: \"world\", pi: 3.14] [{:foo, \"bar\"}, {:hello, \"world\"}, {:pi, 3.14}] # [foo: \"bar\", hello: \"world\", pi: 3.14] keywords = [foo: \"bar\", foo: \"baz\", hello: \"world\"] keywords[:foo] # \"bar\" ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:3:3","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#关键字列表--keyword-list"},{"categories":["ProgrammingLanguage"],"content":" 映射 (Map)映射也是键值对结构，与 keywords 类似，可以任意类型的数据为键，数据并不严格排序，但是键不能重复，重复的键会覆盖已有的键值对 map 的语法相对麻烦些，以 key =\u003e value 形式书写，好消息是如果键全是原子那么可以与 keywords 的语法类似。map可以像C++一样可以使用 operator[] 读取值，也可以使用 operator. 来读取值 (只可以用于读取原子键) map1 = %{:foo =\u003e \"bar\", \"hello\" =\u003e :world} %{foo: \"bar\", hello: \"world\"} == %{:foo =\u003e \"bar\", :hello =\u003e \"world\"} # true map2 = %{foo: \"bar\", hello: \"world\"} map1[\"hello\"] # :world # map1.hello # (KeyError) key :hello not found map1.foo # \"bar\" map2[:foo] # \"bar\" map2.hello # \"world\" map 提供了 operator| 来更新一个键值对，但仅限于已存在的键值对，如果要添加一个新的键值对则需要用到 put 方法，当然 put 也可以用于更新 map3 = %{foo: \"bar\", hello: \"world\"} %{map3 | foo: \"baz\"} # %{foo: \"baz\", hello: \"world\"} # %{map3 | a: \"b\"} # (KeyError) key :a not found Map.put(map3, :a, \"b\") # %{a: \"b\", foo: \"bar\", hello: \"world\"} ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:3:4","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#映射--map"},{"categories":["ProgrammingLanguage"],"content":" 语句小小的吐槽下，本身想把模式匹配放在控制语句之后，毕竟控制语句如果学过其他热门语言肯定是认识的，不过看到 case 时，它依赖模式匹配，好吧…那就先记模式匹配的笔记，好了，开始吧 ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:4:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#语句"},{"categories":["ProgrammingLanguage"],"content":" 模式匹配模式匹配经常被用于函数、case等地方，用的还是蛮多的，且方便，模式匹配中必须穷尽示例用以匹配，如果默认值需要使用变量 _ 来接收默认情况，类似 C 语言的 switch 语句中的 default 匹配我们一直没有讲 = 这个其他语言中的赋值符号，在 Erlang/Elixir 中这不止是赋值，准确的将，这是 匹配，接下来我们写一点 Erlang 的语句来体验一下匹配，Don't panic，这和我们已经学会的 Elixir 几乎一样，如果要一直学习 Elixir 的话 Erlang 是逃不掉的， Lisp 不知道能逃掉不 Var = 10. % 将 var 与 10 匹配 % Var = 5. % exception error: no match of right hand side value 5 10 = Var. % 10 % 5 = Var. % exception error: no match of right hand side value 10 Erlang 中可以看到变量 Var 与 10 匹配，匹配之后便不能与 5 匹配了，与 5 匹配将出现错误，这与 Elixir 中是类似的 list = [1, 2, 3] # [1, 2, 3] [1, 2, 3] = list # [1, 2, 3] # [] = list # (MatchError) no match of right hand side value: [1, 2, 3] 现在想想之前学习 list 时使用的 operator|，取 head 和 tail 时其实也是匹配，匹配时对于不关注的变量可以使用变量 _ 替代 [head | tail] = [1, 2, 3] # haed = 1, tail = [2, 3] [head | _] = [1, 2, 3] # head = 1 {:ok, value} = {:ok, \"Successful\"} # value = \"Successful\" # {:ok, value} = {:error, \"Error\"} # (MatchError) no match of right hand side value PinElixir 在匹配时，匹配操作会同时做赋值操作，但 Erlang 中不会，我们可以使用 Pin 操作符 ^ 来保持与 Erlang 中行为的一致 var = 10 # OK, var = 10 ^var = 5 # NO, (MatchError) no match of right hand side value var = 5 # OK, var = 5 5 = var # OK, match pin 也可以被用于常见的数据结构中 x = 1 {x, ^x} = {2, 1} # x = 2 # {^x, x} = {2, 1} # (MatchError) no match of right hand side value key = \"hello\" %{^key =\u003e value} = %{\"hello\" =\u003e \"world\"} # value = \"world\" # %{^key =\u003e value} = %{:hello =\u003e \"world\"} # (MatchError) no match of right hand side value ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:4:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#模式匹配"},{"categories":["ProgrammingLanguage"],"content":" 模式匹配模式匹配经常被用于函数、case等地方，用的还是蛮多的，且方便，模式匹配中必须穷尽示例用以匹配，如果默认值需要使用变量 _ 来接收默认情况，类似 C 语言的 switch 语句中的 default 匹配我们一直没有讲 = 这个其他语言中的赋值符号，在 Erlang/Elixir 中这不止是赋值，准确的将，这是 匹配，接下来我们写一点 Erlang 的语句来体验一下匹配，Don't panic，这和我们已经学会的 Elixir 几乎一样，如果要一直学习 Elixir 的话 Erlang 是逃不掉的， Lisp 不知道能逃掉不 Var = 10. % 将 var 与 10 匹配 % Var = 5. % exception error: no match of right hand side value 5 10 = Var. % 10 % 5 = Var. % exception error: no match of right hand side value 10 Erlang 中可以看到变量 Var 与 10 匹配，匹配之后便不能与 5 匹配了，与 5 匹配将出现错误，这与 Elixir 中是类似的 list = [1, 2, 3] # [1, 2, 3] [1, 2, 3] = list # [1, 2, 3] # [] = list # (MatchError) no match of right hand side value: [1, 2, 3] 现在想想之前学习 list 时使用的 operator|，取 head 和 tail 时其实也是匹配，匹配时对于不关注的变量可以使用变量 _ 替代 [head | tail] = [1, 2, 3] # haed = 1, tail = [2, 3] [head | _] = [1, 2, 3] # head = 1 {:ok, value} = {:ok, \"Successful\"} # value = \"Successful\" # {:ok, value} = {:error, \"Error\"} # (MatchError) no match of right hand side value PinElixir 在匹配时，匹配操作会同时做赋值操作，但 Erlang 中不会，我们可以使用 Pin 操作符 ^ 来保持与 Erlang 中行为的一致 var = 10 # OK, var = 10 ^var = 5 # NO, (MatchError) no match of right hand side value var = 5 # OK, var = 5 5 = var # OK, match pin 也可以被用于常见的数据结构中 x = 1 {x, ^x} = {2, 1} # x = 2 # {^x, x} = {2, 1} # (MatchError) no match of right hand side value key = \"hello\" %{^key =\u003e value} = %{\"hello\" =\u003e \"world\"} # value = \"world\" # %{^key =\u003e value} = %{:hello =\u003e \"world\"} # (MatchError) no match of right hand side value ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:4:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#匹配"},{"categories":["ProgrammingLanguage"],"content":" 模式匹配模式匹配经常被用于函数、case等地方，用的还是蛮多的，且方便，模式匹配中必须穷尽示例用以匹配，如果默认值需要使用变量 _ 来接收默认情况，类似 C 语言的 switch 语句中的 default 匹配我们一直没有讲 = 这个其他语言中的赋值符号，在 Erlang/Elixir 中这不止是赋值，准确的将，这是 匹配，接下来我们写一点 Erlang 的语句来体验一下匹配，Don't panic，这和我们已经学会的 Elixir 几乎一样，如果要一直学习 Elixir 的话 Erlang 是逃不掉的， Lisp 不知道能逃掉不 Var = 10. % 将 var 与 10 匹配 % Var = 5. % exception error: no match of right hand side value 5 10 = Var. % 10 % 5 = Var. % exception error: no match of right hand side value 10 Erlang 中可以看到变量 Var 与 10 匹配，匹配之后便不能与 5 匹配了，与 5 匹配将出现错误，这与 Elixir 中是类似的 list = [1, 2, 3] # [1, 2, 3] [1, 2, 3] = list # [1, 2, 3] # [] = list # (MatchError) no match of right hand side value: [1, 2, 3] 现在想想之前学习 list 时使用的 operator|，取 head 和 tail 时其实也是匹配，匹配时对于不关注的变量可以使用变量 _ 替代 [head | tail] = [1, 2, 3] # haed = 1, tail = [2, 3] [head | _] = [1, 2, 3] # head = 1 {:ok, value} = {:ok, \"Successful\"} # value = \"Successful\" # {:ok, value} = {:error, \"Error\"} # (MatchError) no match of right hand side value PinElixir 在匹配时，匹配操作会同时做赋值操作，但 Erlang 中不会，我们可以使用 Pin 操作符 ^ 来保持与 Erlang 中行为的一致 var = 10 # OK, var = 10 ^var = 5 # NO, (MatchError) no match of right hand side value var = 5 # OK, var = 5 5 = var # OK, match pin 也可以被用于常见的数据结构中 x = 1 {x, ^x} = {2, 1} # x = 2 # {^x, x} = {2, 1} # (MatchError) no match of right hand side value key = \"hello\" %{^key =\u003e value} = %{\"hello\" =\u003e \"world\"} # value = \"world\" # %{^key =\u003e value} = %{:hello =\u003e \"world\"} # (MatchError) no match of right hand side value ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:4:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#pin"},{"categories":["ProgrammingLanguage"],"content":" 控制语句控制语句主要分为3种 if / unless* if 与 unless 是条件语句，与其他语言的 if 语句类似，if 与 unless 语义相反，在 Elixir 中都是宏定义 if String.valid?(\"Hello\") do \"Valid String\" else \"Invalid String\" end # \"Valid String\" unless String.valid?(\"World\") do \"Invalid String\" else \"Valid String\" end # \"Valid String\" case case 是一种匹配语句，基于模式匹配 case {:ok, \"Hello World\"} do {:ok, result} -\u003e result {:error} -\u003e \"Uh oh!\" _ -\u003e \"Catch all\" end # Hello World cond 当我们需要匹配条件而不是值的时候，可以使用 cond，它的语法很像 case，按顺序匹配每一个条件，必须有一个为真的表达式，所以一般在结尾设置 true 匹配，有些像 Haskell 中的 守卫 表达式 cond do 2 + 2 == 5 -\u003e \"2+2==5\" 2 * 2 == 8 -\u003e \"2*2==8\" true -\u003e \"All Error\" end # \"All Error\" with with 类似于 case 语句，适用于嵌套的 case 语句，按照顺序一次匹配表达式，当失败时会返回对应的返回值 user = %{first: \"Xin\", last: \"Liu\"} with {:ok, first} \u003c- Map.fetch(user, :first), {:ok, last} \u003c- Map.fetch(user, :last) do last \u003c\u003e \", \" \u003c\u003e first end # \"Liu, Xin\" with {:ok, first} \u003c- Map.fetch(user, :first), {:ok, hello} \u003c- Map.fetch(user, :hello) do hello \u003c\u003e \", \" \u003c\u003e first end # :error with 支持 else 语句，当 with 出现不匹配时，将其返回值在 else 中进行匹配， else 是类似 case 语法的模式匹配，需要穷尽匹配 with {:ok, number} \u003c- Map.fetch(%{a: 1, b: 4}, :a), true \u003c- is_even(number) do IO.puts \"#{number} divided by 2 is #{div(number, 2)}\" :even else :error -\u003e IO.puts(\"We don't have this item in map\") :error _ -\u003e IO.puts(\"It's odd\") :odd end ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:4:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#控制语句"},{"categories":["ProgrammingLanguage"],"content":" 函数","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:5:0","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#函数"},{"categories":["ProgrammingLanguage"],"content":" 匿名函数先说说匿名函数吧，lambda 表达式是函数式编程语言的基础，lambda 演算 与 图灵机 堪称计算机程序设计语言的两大支柱，这里我们不学习那么深入，有兴趣嘛那就加油吧，我们简单说说 Elixir 中的 lambda，Elixir 中函数是一等公民，它们可以像变量一样使用与传递，威力十足！ 简单的语法即 fn (params) -\u003e statements，这便会定义一个匿名函数，这个函数可以赋值给一个对象，或者传递进一个参数中；如果需要使用，则需要 name.(param) 来调用 sum = fn (a, b) -\u003e a + b end sum.(2, 3) # 5 很简单吧，这更像是一个完整的函数定义，还有一种做法是像 Shell 中使用函数参数，即使用参数的顺序来确定形式参数的使用；这就让 lambda 简单多了，当然也更难理解参数的含义了。我们在此简单的说明下， \u0026() 这是一个匿名函数， \u00261 这是这个函数接收的第一个参数，以此类推 sum = \u0026(\u00261 + \u00262) sum.(2, 3) # 5 # sum.(\"String\", 666) # Error: (ArithmeticError) bad argument in arithmetic expression: \"String\" + 666 # sum.(2, 3, 4) # Error: (BadArityError) \u0026:erlang.+/2 with arity 2 called with 3 arguments 模式匹配可以用在函数中，我们先来看看匿名函数中的模式匹配，和 case 差不多，不过这是个函数 handle_result = fn {:ok, result} -\u003e IO.puts(\"Handling result...\") {:ok, _} -\u003e IO.puts(\"This would be never run as previous will be matched beforehand.\") {:error} -\u003e IO.puts(\"An error has occurred!\") end ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:5:1","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#匿名函数"},{"categories":["ProgrammingLanguage"],"content":" 命名函数命名函数一般被定义在模块中，使用关键字 def 定义，如果函数体与头在一行的使用可以使用 do: 来简单的书写 defmodule MyModule do def hello1(name) do \"Hello\" \u003c\u003e \", \" \u003c\u003e name end def hello2(name), do: \"Hello\" \u003c\u003e \". \" \u003c\u003e name end MyModule.hello1(\"GinShio\") # Hello, GinShio MyModule.hello2(\"GinShio\") # Hello. GinShio 函数式语言往往也可以进行函数重载，不过他们一般只按照函数参数的个数进行重载，这在 Elixir 中也适用，在 Elixir 中函数的全程一般是 name/param_num defmodule MyModule do def hello(), do: \"Hello, Everybody\" # hello/0 def hello(name), do: \"Hello\" \u003c\u003e \", \" \u003c\u003e name # hello/1 end MyModule.hello() # \"Hello, Everybody\" MyModule.hello(\"iris\") # \"Hello, iris\" 命名函数当然也可以很好的支持模式匹配，这样我们递归的实现会很简单 defmodule MyLength do def of([]), do: 0 def of([_ | tail]), do: 1 + of(tail) end MyLength.of([]) # 0 MyLength.of([1, 2, 3, 4, 5]) # 5 当然模式匹配与 Map 结合在一起，示例函数 hello/1 展示了只关注指定键的用法，当然我们也可以在关注指定键时接受整个 Map，即用模式匹配来接受 defmodule MyModule do def hello(%{name: person}), do: \"Hello, \" \u003c\u003e person def all_map(%{name: person_name} = person) do IO.puts \"Hello, \" \u003c\u003e person_name IO.inspect person end end MyModule.hello(%{name: \"Fred\", age: 95}) # \"Hello, Fred\" # MyModule.hello(%{age: 95}) # (FunctionClauseError) no function clause matching in MyModule.hello/1 MyModule.all_map(%{name: \"Fred\", age: 95}) # Hello, Fred # %{age: 95, name: \"Fred\"} Private (私有函数) 如果你不希望模块外调用某些函数，你可以使用 defp 来定义私有函数，这样定义的函数只能在模块内使用 defmodule MyFibonacci do def fib(0), do: 0 def fib(1), do: 1 def fib(n), do: fib(1, 1, n) defp fib(ans, pre, 2), do: ans defp fib(ans, pre, n), do: fib(ans + pre, ans, n - 1) end MyFibonacci.fib(10) # 55 MyFibonacci.fib(100) # 354224848179261915075 # MyFibonacci.fib(1, 1, 5) # (UndefinedFunctionError) function MyFibonacci.fib/3 is undefined or private. Pipe |\u003e (管道操作) 没错你没听错，就是 pipe，有没有想起使用 *nix 时使用的管道，Elixir 中的 pipe 与 *nix 中的类似，都是将前一个调用的结果传递给后一个，这就很爽了，我们来对比一下，管道简直是嵌套调用的救星 foo(bar(baz(do_something()))) # Normal do_something() |\u003e baz() |\u003e bar() |\u003e foo() # Pipe 那如果参数数量大于1怎么办，这些问题不大，带上参数就行，从 Pipe 来的参数优先入栈 \"Hello, World\" |\u003e String.split() # [\"Hello,\", \"World\"] \"Hello, World\" |\u003e String.split(\", \") # [\"Hello\", \"World\"] Guard (守卫表达式) 可以被用于 函数 和 case 当中，比方说我们现在有两个签名相同的函数 hello/1 ，我们需要通过 guard 来确定应该调用哪个函数 defmodule MyModule do def hello(names) when is_list(names) do names |\u003e Enum.join(\", \") |\u003e hello() end def hello(names) when is_binary(names) do \"Hello, \" \u003c\u003e names end end MyModule.hello(\"GinShio\") # \"Hello, GinShio\" MyModule.hello([\"GinShio\", \"iris\"]) # \"Hello, GinShio, iris\" 我们试试在 case 中使用 guard，更多用法请查看 Guard clauses case x do 1 -\u003e :one 2 -\u003e :two n when is_integer(n) and n \u003e 2 -\u003e :larger_than_two end Default (默认参数) 我们可以为函数设置一些默认值，使用语法 argument \\\\ value defmodule MyModule do def hello(name, language_code \\\\ \"en\"), do: phrase(language_code) \u003c\u003e name defp phrase(\"en\"), do: \"Hello, \" defp phrase(\"es\"), do: \"Hola, \" defp phrase(\"zh\"), do: \"你好，\" end MyModule.hello(\"iris\") # \"Hello, iris\" MyModule.hello(\"iris\", \"en\") # \"Hello, iris\" MyModule.hello(\"iris\", \"es\") # \"Hola, iris\" MyModule.hello(\"iris\", \"zh\") # \"你好，iris\" 不过我们在默认参数与守卫表达式一起使用时，往往会出现一些问题，先来看看问题代码 defmodule Greeter do def hello(names, language_code \\\\ \"en\") when is_list(names) do names |\u003e Enum.join(\", \") |\u003e hello() end def hello(names, language_code \\\\ \"en\") when is_binary(names) do phrase(language_code) \u003c\u003e names end defp phrase(\"en\"), do: \"Hello, \" defp phrase(\"es\"), do: \"Hola, \" end # (CompileError): def hello/2 defines defaults multiple times. # Elixir allows defaults to be declared once per definition. Instead of: # def foo(:first_clause, b \\\\ :default) do ... end # def foo(:second_clause, b \\\\ :default) do ... end 有多个函数同时匹配时，默认参数这种模式很容易混淆，它不被 Elixir 喜欢，至于解决方法嘛还是有的，我们需要先声明这个函数，有点像 C++ 使用默认参数的方法 defmodule Greeter do def hello(names, language_code \\\\ \"en\") def hello(names, language_code) when is_list(names) do names |\u003e Enum.join(\", \") |\u003e hello() end def hello(names, language_code) when is_binary(names) do phrase(language_code) \u003c\u003e names end defp phrase(\"en\"), do: \"Hello, \" defp phrase(\"es\"), do: \"Hola, \" end ","date":"02-16","objectID":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/:5:2","series":["Elixir 学习笔记"],"tags":["Note","Elixir","Guide"],"title":"Elixir 基本语法","uri":"/2021/elixir%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_001/#命名函数"},{"categories":["ProgrammingLanguage"],"content":"GinShio | Cpp Concurrency in Action (2rd) 第五章读书笔记","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/"},{"categories":["ProgrammingLanguage"],"content":" 原子操作原子操作是一个不可分割的操作，系统的所有线程不会观察到原子操作完成了一半。如果读取对象的加载操作是原子的，那么这个对象的所有修改操作也是原子的。 标准原子类型全部定义于头文件 atomic 中，这些类型的操作都是原子的，但是其内部实现可能使用原子操作或互斥量模拟，所以原子操作可以替代互斥量完成同步操作，但是如果内部使用互斥量实现那么不会有性能提升。 通常标准原子类型不能进行拷贝和赋值，但是可以隐式转化成对应的内置类型，使用 load()、exchange()、compare_exchange_weak() 和 compare_exchange_strong()，另外还有 store() 用以原子地赋值。每种函数类型的操作都有一个内存序参数，这个参数可以用来指定存储的顺序。 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:1:0","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#原子操作"},{"categories":["ProgrammingLanguage"],"content":" ::std::atomic_flag::std::atomic_flag 是最简单的原子类型，标准保证其实现是 lock-free 的，这个类型的对象可以在 设置 和 清除 间切换，对象必须被 ATOMIC_FLAG_INIT，初始化标志位为清除状态。初始化后，对象进可以执行销毁、清除、设置 ::std::atomic_flag f = ATOMIC_FLAG_INIT; // 设置为清除状态 (false) 由于 clear() 清除操作原子地设置标志为 false，test_and_set() 设置操作原子地设置标志为 true 并获得其先前值，所以可以简单地实现一个自旋锁 class spinlock_mutex { private: ::std::atomic_flag flag; public: spinlock_mutex() : flag(ATOMIC_FLAG_INIT) {} void lock() { while (flag.test_and_set(::std::memory_order_acquire)); } void unlock() { flag.clear(::std::memory_order_release); } }; ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:1:1","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#std-atomic-flag"},{"categories":["ProgrammingLanguage"],"content":" ::std::atomic::std::atomic 不再保证 lock-free，但相比 ::std::atomic_flag 有了更通用的操作， store() 是一个存储操作，load() 是一个加载操作，exchange() 是一个读-改-写操作。 ::std::atomic\u003cbool\u003e b; bool x = b.load(::std::memory_order_acquire); // 加载操作，x==false b.store(true); // 存储操作，b==true x = b.exchange(false, ::std::memory_order_acq_rel); // 读-改-写操作，b==false，x==true 原子操作中还有一种存储方式：当前值与预期值一致时存储新值，这种操作被称为 compare/exchange (比较/交换) 操作。compare/exchange 是原子类型编程的基础，它比较原子变量的当前值和预期值，两者相等时存储目标值，当两者不相等时预期值会被更新为原子变量中的值。它在C++中以 compare_exchange_weak() 和 compare_exchange_strong() 提供，将当前值与预期值的 对象表示/值表示 逐位比较，且复制是逐位的，比较相等将返回 true，否则返回 false。另外我们可以指定两个内存序，即成功修改原子变量的内存序与失败时可以是不同的，具体的内存序我们之后在学习。 compare_exchange_weak 是 弱CAS，可能出现伪失败，即当前值与预期值相等时，也可能出现比较失败的行为，为此通常配合循环使用 bool expected = false; extern ::std::atomic\u003cbool\u003e b = false; while (!b.compare_exchange_weak(expected, true) \u0026\u0026 !expected); compare_exchange_strong 是 强CAS，它保证不会出现伪失败，一般推荐使用强形式 ::std::atomic\u003cint\u003e ai = 3; int tst = 4; ai.compare_exchange_strong(tst, 5); // false, ai = 3, tst = 3 ai.compare_exchange_strong(tst, 5); // true, ai = 5, tst = 3 整数类型、指针类型、浮点类型 (C++20起) 都拥有原子数值计算的能力，并且整数类型还可以原子地按位运算，这些操作可以修改原子变量并返回修改前原子变量的值 int arr[5] = {0}; ::std::atomic\u003cint\u003e ai{5}; ::std::atomic\u003cint*\u003e aip{arr}; // 数值计算 ai += 4; // ai = 9 aip += 2; // aip = arr + 2 ai.fetch_sub(8); // ai = 1 --aip; // aip = arr + 1 // 按位运算 (仅整数类型特化) ai |= 2; // ai = 3 ai.fetch_and(4); // ai = 0 类型中包含 填充位、陷阱位 或为 同一值提供多个对象表示 (如浮点数的 NaN) 时推荐弱CAS，因为可能两个对象的值相等但CAS操作失败 struct TestCas { int i; char : 1; }; TestCas a = {65536}; // 假设对象表示 [0,0,1,0,1,0,0,0] ::std::atomic\u003cTestCas\u003e aa{{65536}}; // 假设对象表示 [0,0,1,0,0,0,0,0] aa.compare_exchange_strong(a, {1024}); // 修改失败，对象表示不一致 // C++20开始，修改成功，值表示一致 我们常使用 用户定义类型 (UDT)，原子类型会进行逐位比较或复制操作，所以 UDT 应该是 可平凡复制 的类型。通常 UDT 类型的原子类型会使用锁来完成原子操作，不过好消息是大多数平台对于 UDT 大小如同一个 int 或 void* 时，将使用原子指令实现原子操作 (即无锁操作)，有些平台甚至还支持两倍大小使用原子指令，即 双字比较和交换 (DWCAS) 指令 ::std::cout \u003c\u003c ::std::boolalpha \u003c\u003c aa.is_lock_free() \u003c\u003c ::std::endl; // 可能的输出：true ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:1:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#std-atomic"},{"categories":["ProgrammingLanguage"],"content":" 内存布局内存模型一方面是内存布局，另一方面是并发，并发的基本结构很重要，特别是低层原子操作，因为C++所有的对象都和内存位置有关。一个类是一个有多个子对象组成的对象，我们需要牢记四个原则 每个变量都是对象，包括其成员变量的对象 每个对象至少占有一个内存位置 基本类型都有确定的内存位置 相邻位域是相同内存中的一部分 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:2:0","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#内存布局"},{"categories":["ProgrammingLanguage"],"content":" 数据模型数据模型描述了C等编程语言的基本算术类型的位宽，以下是常见的数据模型 (之后我们以 LP64 为准) Model short int long long long ptr RunTime LP32 16 16 32 64 32 MSVC (16bit) ILP32 16 32 32 64 32 MSVC (32bit), *nix (32bit) LLP64 16 32 32 64 64 MSVC, MinGW LP64 16 32 64 64 64 *nix, Cygwin, z/OS ILP64 16 64 64 64 64 Solaris SILP64 64 64 64 64 64 UNICOS ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:2:1","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#数据模型"},{"categories":["ProgrammingLanguage"],"content":" 内存对齐内存对齐主要是为了提高内存的访问效率，也为了更好的移植性。但是这样会改变各个成员在类中的偏移量，类的大小也不再是简单的成员大小相加，幸好其中也有一些规律可循 对象的起始地址能够被其最宽的成员大小整除 成员相对于起始地址的偏移量能够被自身大小整除，否则在前一个成员后面填充字节 类的大小能够被最宽的成员的大小整除，否则在最后填充字节 如果是空类，按照标准该类的对象必须占有一个字节 (除非 空基类优化)，在C中空类的大小是 0 字节 当指定对齐的时候，类的大小将是指定对齐大小的倍数，如果不足则在最后填充字节 好的，那么我们看一些简单的例子，基本上都可以算出下面各个类占用多少内存，并可以知道在哪里填充内存 struct A {}; // sizeof(A): 1, alignof(A): 1 struct B { short b_a; // 2 bytes int b_b; char b_c; // 3 bytes }; // sizeof(B): 12, , alignof(B): 4 struct C : public A { int c_a; }; // sizeof(C): 4, alignof(C): 4 struct D { A d_a; // 3 bytes int d_b; }; // sizeof(D): 8, alignof(D): 4 我们加入一些数组进来, 数组中的元素当然也会符合内存对齐的规则 struct E { int e_a; char e_b[3]; // 1 byte }; // sizeof(E): 8, alignof(E): 4 struct F { char f_a[3]; // 5 bytes long long f_b; short f_c[3]; // 2 bytes F* f_d; }; // sizeof(F): 32, alignof(F): 8 指定内存对齐的方式时，必须是2的正幂，若指定的对齐方式弱于原生对齐方式，将忽略指定 struct alignas(16) G { char g_a[3]; // 1 byte short g_b; // 10 bytes }; // sizeof(G): 16, alignof(G): 16 struct H : public G { short h_a; // 2 bytes int h_b; // 10 bytes }; // sizeof(H): 32, alignof(H): 16 struct alignas(2) I { // 弱于原生对齐方式(4) C c; }; // sizeof(I): 4, alignof(I): 4 位域只会占有多个二进制位，而不会占有整个字节，不过位域也遵循内存对齐的原则 位域的宽度不能超过底层类型的宽度 多个位域可以共享同一底层类型，当底层类型不够存储位域时，将从下一分配单位开始存储 允许空位域用以占位，则将不使用空位域 大小为零的空位域将强制填充剩下的位，之后的位域将从新的分配单位开始存储 位域是否可以 跨字节 与 打包方式，由实现而定 struct J { int j_a :6; int j_b :4; // 与 j_c 共享字节 int j_c :4; // 与 j_b 共享字节 char j_d; // 1 byte }; // sizeof(J): 4, alignof(J): 4 struct K { int k_a :3; int :4; // 与 k_a 共享字节 int k_b :2; // 独占一字节 char k_c; // 1 byte }; // sizeof(K): 4, alignof(K): 4 struct L { int l_a :3; int :0; // 将 int 剩下的部分填充 char l_b :2; char l_c; // 2 bytes }; // sizeof(L): 8, alignof(L): 4 一个内存位置是 一个标量类型 (算术类型、指针类型、枚举类型或 ::std::nullptr_t) 对象 或非零长位域的最大相接序列 C++的各种功能特性，例如引用和虚函数，可能涉及到程序不可访问，但为实现所管理的额外内存位置。 struct S { char a; // 内存位置 #1 int b : 5; // 内存位置 #2 int c : 11, // 内存位置 #2 （延续） : 0, d : 8; // 内存位置 #3 struct { int ee : 8; // 内存位置 #4 } e; } obj; // 对象 'obj' 由 4 个分离的内存位置组成 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:2:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#内存对齐"},{"categories":["ProgrammingLanguage"],"content":" 内存位置与并发当多个线程访问不同的内存位置时将不会存在任何问题，但是当写入与读取在同一内存位置时将会产生数据竞争，当然也有特例不会引起数据竞争 两个同一内存位置上的操作在 同一线程 上 冲突是 原子操作 一个冲突操作发生 早于 另一个 对象都有在初始化开始阶段确定好修改顺序的，大多数情况下，这个顺序不同于执行中的顺序，但在给定的程序中，所有线程都需要遵守这个顺序。如果对象不是原子类型，必须确保有足够的同步操作，确定线程都遵守了修改顺序。 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:2:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#内存位置与并发"},{"categories":["ProgrammingLanguage"],"content":" 同步操作和强制排序","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:0","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#同步操作和强制排序"},{"categories":["ProgrammingLanguage"],"content":" 相关术语线程间同步和内存顺序决定表达式的求值和副效应如何在不同的执行线程间排序 同步于 (synchronizes-with) 修改原子对象 M 的求值 A，对于其他线程可见 先序于 (sequenced-before) 在同一线程中求值 A 可以先序于求值 B 携带依赖 (carries dependency) 在同一线程中若下列任一为真，则先序于求值 B 的求值 A 可能也会将依赖带入 B A 的值被用作 B 的运算数，除了 B 是对 ::std::kill_dependency 的调用 A 是内建 \u0026\u0026、||、?: 或 , 运算符的左运算数 A 写入标量对象 M，B 从 M 读取 A 将依赖携带入另一求值 X，而 X 将依赖携带入 B 修改顺序 (modification order) 对任何特定的原子变量的修改，以限定于此一原子变量的单独全序出现，对所有原子操作保证下列四个要求 写写连贯: 若修改某原子对象 M 的求值 A 先发于 修改 M 的求值 B，则 A 在 M 的修改顺序中早于 B 出现 读读连贯: 若某原子对象 M 的值计算 A 先发于 对 M 的值计算 B，且 A 的值来自对 M 的写操作 X，则 B 的值要么是 X 所存储的值，要么是在 M 的修改顺序中后于 X 出现的 M 上的副效应 Y 所存储的值 读写连贯: 若某原子对象 M 的值计算 A 先发于 修改 M 的求值 B，则 A 的值来自 M 的修改顺序中早于 B 出现的副效应 X 写读连贯: 若某原子对象 M 上的副效应 X 先发于 M 的值计算 B，则求值 B 应从 X 或从 M 的修改顺序中后随 X 的副效应 Y 取得其值 释放序列 (release sequence) 在原子对象 M 上执行一次释放操作 A 之后，M 的修改顺序的最长连续子序列被称为以 A 为首的释放序列 由执行 A 的同一线程所执行的写操作 (C++20前) 任何线程对 M 的原子的读-改-写操作 依赖先序于 (dependency-ordered before) 在线程间若下列任一为真，则求值 A 依赖先序于求值 B A 在某原子对象 M 上进行释放操作，而不同的线程中 B 在同一原子对象 M 上进行消费操作，而 B 读取 A (所引领的释放序列的任何部分 (C++20 前)) 所写入的值 A 依赖先序于 X 且 X 携带依赖到 B 线程间先行发生 (inter-thread happens-before) 在线程间若下列任一为真，则求值 A 线程间先行发生于求值 B A 同步于 B A 依赖先序于 B A 同步于某求值 X，且 X 先序于 B A 先序于某求值 X，且 X 线程间先行发生于 B A 线程间先行发生于某求值 X，且 X 线程间先行发生于 B 先行发生 (happens-before) 先行发生无关乎线程，若下列任一为真，则求值 A 先发生于求值 B A 先序于 B A 线程间先发生于 B ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:1","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#相关术语"},{"categories":["ProgrammingLanguage"],"content":" 内存顺序C++ 标准定义了六种原子操作的内存顺序，它们代表了四种内存模型： 自由序 (Relaxed ordering) memory_order_relaxed，没有同步或顺序制约，仅对此操作要求原子性 消费-释放序 (Consume-Release ordering) memory_order_consume，当前线程中依赖于当前加载的该值的读或写不能被重排到此加载前；其他释放同一原子变量的线程对数据依赖变量的写入，为当前线程所可见 memory_order_release，当前线程中读或写不能被重排到此存储后；当前线程的所有写入，对其他获取同一原子变量的线程可见，对原子变量的带依赖写入变得对其他消费同一原子对象的线程可见 获取-释放序 (Acquire-Release ordering) memory_order_acquire，当前线程中读或写不能被重排到此加载前；其他释放同一原子变量的线程的所有写入，能为当前线程所见 memory_order_acq_rel，所有释放同一原子变量的线程的写操作在当前线程修改前可见，当前线程改操作对其他获取同一原子变量的线程可见 顺序一致性 (Sequentially-consistent ordering) memory_order_seq_cst，原子操作的默认内存序，所有线程以同一顺序观测到所有修改 在不同的原子操作上，可以用到的内存序也有所不同 store (写操作) memory_order_relaxed, memory_order_release, memory_order_seq_cst load (读操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_seq_cst read-modify-write (读改写操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_release, memory_order_acq_rel, memory_order_seq_cst 自由序带标签 memory_order_relaxed 的原子操作，它们不会在同时的内存访问间强加顺序，它们只保证原子性和修改顺序一致性，典型应用场景为 计数器自增 // x = {0}, y = {0} // thread 1 r1 = y.load(::std::memory_order_relaxed); // A x.store(r1, ::std::memory_order_relaxed); // B // thread 2 r2 = x.load(::std::memory_order_relaxed); // C y.store(42, ::std::memory_order_relaxed); // D assert(r1 != 42 || r2 != 42); // E A 先序于 B，C 先序于 D，但 D 在 y 上的副效应可能对 A 可见，同时 B 在 x 上的副效应可能对 C 可见，所以允许 E 断言失败。 另外自由序中，当前线程可能看到别的线程的更新，但是更新频率不一定是均匀的，但其值一定是递增的。详细例子可以查看 C++ Concurrency in Action (2rd) 中电话计数员的例子。 消费-释放序若线程 A 中的原子存储带标签 memory_order_release 而线程 B 中来自同一对象的读取存储值的原子加载带标签 memory_order_consume，则线程 A 视角中先发生于原子存储的所有内存写入，会在线程 B 中该加载操作所携带依赖进入的操作中变成可见副效应，即一旦完成原子加载，则保证线程 B 中使用从该加载获得的值的运算符和函数能见到线程 A 写入内存的内容。同步仅在释放和消费同一原子对象的线程间建立，其他线程能见到与被同步线程的一者或两者相异的内存访问顺序。 此顺序的典型使用情景，涉及对 很少被写入 的数据结构的同时时读取，和 有指针中介发布 的 发布者-订阅者 情形，即当生产者发布消费者能通过其访问信息的指针之时：无需令生产者写入内存的所有其他内容对消费者可见。这种场景的例子之一是 rcu 解引用。 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_consume))); assert(*p2 == \"Hello\"); // 断言成功 (*p2 从 ptr 携带依赖) assert(data == 42); // 断言可能失败 (data 不从 ptr 携带依赖) } 获取-释放序若线程 A 中的一个原子存储带标签 memory_order_release，而线程 B 中来自同一变量的原子加载带标签 memory_order_acquire，则从线程 A 的视角先发生于原子存储的所有内存写入，在线程 B 中成为可见副效应，即一旦原子加载完成保证线程 B 能观察到线程 A 写入内存的所有内容。此顺序的典型使用场景是 互斥量 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_acquire))); assert(*p2 == \"Hello\"); // 断言成功 assert(data == 42); // 断言成功 } 顺序一致性带标签 memory_order_seq_cst 的原子操作不仅以与释放/获得顺序相同的方式排序内存 (在一个线程中先发生于存储的任何结果都变成进行加载的线程中的可见副效应)，还对所有带此标签的内存操作建立单独全序。 void write_x() { x.store(true, ::std::memory_order_seq_cst); } void write_y() { y.store(true, ::std::memory_order_seq_cst); } void read_x_then_y() { while (!x.load(::std::memory_order_seq_cst)); if (y.load(::std::memory_order_seq_cst)) { ++z; } } void read_y_then_x() { while (!y.load(::std::memory_order_seq_cst)); if (x.load(::std::memory_order_seq_cst)) { ++z; } } assert(z.load() != 0); // 断言成功 全序列顺序在所有多核系统上要求完全的内存栅栏 CPU 指令，这可能成为性能瓶颈，因为它强制受影响的内存访问传播到每个核心。 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#内存顺序"},{"categories":["ProgrammingLanguage"],"content":" 内存顺序C++ 标准定义了六种原子操作的内存顺序，它们代表了四种内存模型： 自由序 (Relaxed ordering) memory_order_relaxed，没有同步或顺序制约，仅对此操作要求原子性 消费-释放序 (Consume-Release ordering) memory_order_consume，当前线程中依赖于当前加载的该值的读或写不能被重排到此加载前；其他释放同一原子变量的线程对数据依赖变量的写入，为当前线程所可见 memory_order_release，当前线程中读或写不能被重排到此存储后；当前线程的所有写入，对其他获取同一原子变量的线程可见，对原子变量的带依赖写入变得对其他消费同一原子对象的线程可见 获取-释放序 (Acquire-Release ordering) memory_order_acquire，当前线程中读或写不能被重排到此加载前；其他释放同一原子变量的线程的所有写入，能为当前线程所见 memory_order_acq_rel，所有释放同一原子变量的线程的写操作在当前线程修改前可见，当前线程改操作对其他获取同一原子变量的线程可见 顺序一致性 (Sequentially-consistent ordering) memory_order_seq_cst，原子操作的默认内存序，所有线程以同一顺序观测到所有修改 在不同的原子操作上，可以用到的内存序也有所不同 store (写操作) memory_order_relaxed, memory_order_release, memory_order_seq_cst load (读操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_seq_cst read-modify-write (读改写操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_release, memory_order_acq_rel, memory_order_seq_cst 自由序带标签 memory_order_relaxed 的原子操作，它们不会在同时的内存访问间强加顺序，它们只保证原子性和修改顺序一致性，典型应用场景为 计数器自增 // x = {0}, y = {0} // thread 1 r1 = y.load(::std::memory_order_relaxed); // A x.store(r1, ::std::memory_order_relaxed); // B // thread 2 r2 = x.load(::std::memory_order_relaxed); // C y.store(42, ::std::memory_order_relaxed); // D assert(r1 != 42 || r2 != 42); // E A 先序于 B，C 先序于 D，但 D 在 y 上的副效应可能对 A 可见，同时 B 在 x 上的副效应可能对 C 可见，所以允许 E 断言失败。 另外自由序中，当前线程可能看到别的线程的更新，但是更新频率不一定是均匀的，但其值一定是递增的。详细例子可以查看 C++ Concurrency in Action (2rd) 中电话计数员的例子。 消费-释放序若线程 A 中的原子存储带标签 memory_order_release 而线程 B 中来自同一对象的读取存储值的原子加载带标签 memory_order_consume，则线程 A 视角中先发生于原子存储的所有内存写入，会在线程 B 中该加载操作所携带依赖进入的操作中变成可见副效应，即一旦完成原子加载，则保证线程 B 中使用从该加载获得的值的运算符和函数能见到线程 A 写入内存的内容。同步仅在释放和消费同一原子对象的线程间建立，其他线程能见到与被同步线程的一者或两者相异的内存访问顺序。 此顺序的典型使用情景，涉及对 很少被写入 的数据结构的同时时读取，和 有指针中介发布 的 发布者-订阅者 情形，即当生产者发布消费者能通过其访问信息的指针之时：无需令生产者写入内存的所有其他内容对消费者可见。这种场景的例子之一是 rcu 解引用。 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_consume))); assert(*p2 == \"Hello\"); // 断言成功 (*p2 从 ptr 携带依赖) assert(data == 42); // 断言可能失败 (data 不从 ptr 携带依赖) } 获取-释放序若线程 A 中的一个原子存储带标签 memory_order_release，而线程 B 中来自同一变量的原子加载带标签 memory_order_acquire，则从线程 A 的视角先发生于原子存储的所有内存写入，在线程 B 中成为可见副效应，即一旦原子加载完成保证线程 B 能观察到线程 A 写入内存的所有内容。此顺序的典型使用场景是 互斥量 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_acquire))); assert(*p2 == \"Hello\"); // 断言成功 assert(data == 42); // 断言成功 } 顺序一致性带标签 memory_order_seq_cst 的原子操作不仅以与释放/获得顺序相同的方式排序内存 (在一个线程中先发生于存储的任何结果都变成进行加载的线程中的可见副效应)，还对所有带此标签的内存操作建立单独全序。 void write_x() { x.store(true, ::std::memory_order_seq_cst); } void write_y() { y.store(true, ::std::memory_order_seq_cst); } void read_x_then_y() { while (!x.load(::std::memory_order_seq_cst)); if (y.load(::std::memory_order_seq_cst)) { ++z; } } void read_y_then_x() { while (!y.load(::std::memory_order_seq_cst)); if (x.load(::std::memory_order_seq_cst)) { ++z; } } assert(z.load() != 0); // 断言成功 全序列顺序在所有多核系统上要求完全的内存栅栏 CPU 指令，这可能成为性能瓶颈，因为它强制受影响的内存访问传播到每个核心。 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#自由序"},{"categories":["ProgrammingLanguage"],"content":" 内存顺序C++ 标准定义了六种原子操作的内存顺序，它们代表了四种内存模型： 自由序 (Relaxed ordering) memory_order_relaxed，没有同步或顺序制约，仅对此操作要求原子性 消费-释放序 (Consume-Release ordering) memory_order_consume，当前线程中依赖于当前加载的该值的读或写不能被重排到此加载前；其他释放同一原子变量的线程对数据依赖变量的写入，为当前线程所可见 memory_order_release，当前线程中读或写不能被重排到此存储后；当前线程的所有写入，对其他获取同一原子变量的线程可见，对原子变量的带依赖写入变得对其他消费同一原子对象的线程可见 获取-释放序 (Acquire-Release ordering) memory_order_acquire，当前线程中读或写不能被重排到此加载前；其他释放同一原子变量的线程的所有写入，能为当前线程所见 memory_order_acq_rel，所有释放同一原子变量的线程的写操作在当前线程修改前可见，当前线程改操作对其他获取同一原子变量的线程可见 顺序一致性 (Sequentially-consistent ordering) memory_order_seq_cst，原子操作的默认内存序，所有线程以同一顺序观测到所有修改 在不同的原子操作上，可以用到的内存序也有所不同 store (写操作) memory_order_relaxed, memory_order_release, memory_order_seq_cst load (读操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_seq_cst read-modify-write (读改写操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_release, memory_order_acq_rel, memory_order_seq_cst 自由序带标签 memory_order_relaxed 的原子操作，它们不会在同时的内存访问间强加顺序，它们只保证原子性和修改顺序一致性，典型应用场景为 计数器自增 // x = {0}, y = {0} // thread 1 r1 = y.load(::std::memory_order_relaxed); // A x.store(r1, ::std::memory_order_relaxed); // B // thread 2 r2 = x.load(::std::memory_order_relaxed); // C y.store(42, ::std::memory_order_relaxed); // D assert(r1 != 42 || r2 != 42); // E A 先序于 B，C 先序于 D，但 D 在 y 上的副效应可能对 A 可见，同时 B 在 x 上的副效应可能对 C 可见，所以允许 E 断言失败。 另外自由序中，当前线程可能看到别的线程的更新，但是更新频率不一定是均匀的，但其值一定是递增的。详细例子可以查看 C++ Concurrency in Action (2rd) 中电话计数员的例子。 消费-释放序若线程 A 中的原子存储带标签 memory_order_release 而线程 B 中来自同一对象的读取存储值的原子加载带标签 memory_order_consume，则线程 A 视角中先发生于原子存储的所有内存写入，会在线程 B 中该加载操作所携带依赖进入的操作中变成可见副效应，即一旦完成原子加载，则保证线程 B 中使用从该加载获得的值的运算符和函数能见到线程 A 写入内存的内容。同步仅在释放和消费同一原子对象的线程间建立，其他线程能见到与被同步线程的一者或两者相异的内存访问顺序。 此顺序的典型使用情景，涉及对 很少被写入 的数据结构的同时时读取，和 有指针中介发布 的 发布者-订阅者 情形，即当生产者发布消费者能通过其访问信息的指针之时：无需令生产者写入内存的所有其他内容对消费者可见。这种场景的例子之一是 rcu 解引用。 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_consume))); assert(*p2 == \"Hello\"); // 断言成功 (*p2 从 ptr 携带依赖) assert(data == 42); // 断言可能失败 (data 不从 ptr 携带依赖) } 获取-释放序若线程 A 中的一个原子存储带标签 memory_order_release，而线程 B 中来自同一变量的原子加载带标签 memory_order_acquire，则从线程 A 的视角先发生于原子存储的所有内存写入，在线程 B 中成为可见副效应，即一旦原子加载完成保证线程 B 能观察到线程 A 写入内存的所有内容。此顺序的典型使用场景是 互斥量 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_acquire))); assert(*p2 == \"Hello\"); // 断言成功 assert(data == 42); // 断言成功 } 顺序一致性带标签 memory_order_seq_cst 的原子操作不仅以与释放/获得顺序相同的方式排序内存 (在一个线程中先发生于存储的任何结果都变成进行加载的线程中的可见副效应)，还对所有带此标签的内存操作建立单独全序。 void write_x() { x.store(true, ::std::memory_order_seq_cst); } void write_y() { y.store(true, ::std::memory_order_seq_cst); } void read_x_then_y() { while (!x.load(::std::memory_order_seq_cst)); if (y.load(::std::memory_order_seq_cst)) { ++z; } } void read_y_then_x() { while (!y.load(::std::memory_order_seq_cst)); if (x.load(::std::memory_order_seq_cst)) { ++z; } } assert(z.load() != 0); // 断言成功 全序列顺序在所有多核系统上要求完全的内存栅栏 CPU 指令，这可能成为性能瓶颈，因为它强制受影响的内存访问传播到每个核心。 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#消费-释放序"},{"categories":["ProgrammingLanguage"],"content":" 内存顺序C++ 标准定义了六种原子操作的内存顺序，它们代表了四种内存模型： 自由序 (Relaxed ordering) memory_order_relaxed，没有同步或顺序制约，仅对此操作要求原子性 消费-释放序 (Consume-Release ordering) memory_order_consume，当前线程中依赖于当前加载的该值的读或写不能被重排到此加载前；其他释放同一原子变量的线程对数据依赖变量的写入，为当前线程所可见 memory_order_release，当前线程中读或写不能被重排到此存储后；当前线程的所有写入，对其他获取同一原子变量的线程可见，对原子变量的带依赖写入变得对其他消费同一原子对象的线程可见 获取-释放序 (Acquire-Release ordering) memory_order_acquire，当前线程中读或写不能被重排到此加载前；其他释放同一原子变量的线程的所有写入，能为当前线程所见 memory_order_acq_rel，所有释放同一原子变量的线程的写操作在当前线程修改前可见，当前线程改操作对其他获取同一原子变量的线程可见 顺序一致性 (Sequentially-consistent ordering) memory_order_seq_cst，原子操作的默认内存序，所有线程以同一顺序观测到所有修改 在不同的原子操作上，可以用到的内存序也有所不同 store (写操作) memory_order_relaxed, memory_order_release, memory_order_seq_cst load (读操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_seq_cst read-modify-write (读改写操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_release, memory_order_acq_rel, memory_order_seq_cst 自由序带标签 memory_order_relaxed 的原子操作，它们不会在同时的内存访问间强加顺序，它们只保证原子性和修改顺序一致性，典型应用场景为 计数器自增 // x = {0}, y = {0} // thread 1 r1 = y.load(::std::memory_order_relaxed); // A x.store(r1, ::std::memory_order_relaxed); // B // thread 2 r2 = x.load(::std::memory_order_relaxed); // C y.store(42, ::std::memory_order_relaxed); // D assert(r1 != 42 || r2 != 42); // E A 先序于 B，C 先序于 D，但 D 在 y 上的副效应可能对 A 可见，同时 B 在 x 上的副效应可能对 C 可见，所以允许 E 断言失败。 另外自由序中，当前线程可能看到别的线程的更新，但是更新频率不一定是均匀的，但其值一定是递增的。详细例子可以查看 C++ Concurrency in Action (2rd) 中电话计数员的例子。 消费-释放序若线程 A 中的原子存储带标签 memory_order_release 而线程 B 中来自同一对象的读取存储值的原子加载带标签 memory_order_consume，则线程 A 视角中先发生于原子存储的所有内存写入，会在线程 B 中该加载操作所携带依赖进入的操作中变成可见副效应，即一旦完成原子加载，则保证线程 B 中使用从该加载获得的值的运算符和函数能见到线程 A 写入内存的内容。同步仅在释放和消费同一原子对象的线程间建立，其他线程能见到与被同步线程的一者或两者相异的内存访问顺序。 此顺序的典型使用情景，涉及对 很少被写入 的数据结构的同时时读取，和 有指针中介发布 的 发布者-订阅者 情形，即当生产者发布消费者能通过其访问信息的指针之时：无需令生产者写入内存的所有其他内容对消费者可见。这种场景的例子之一是 rcu 解引用。 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_consume))); assert(*p2 == \"Hello\"); // 断言成功 (*p2 从 ptr 携带依赖) assert(data == 42); // 断言可能失败 (data 不从 ptr 携带依赖) } 获取-释放序若线程 A 中的一个原子存储带标签 memory_order_release，而线程 B 中来自同一变量的原子加载带标签 memory_order_acquire，则从线程 A 的视角先发生于原子存储的所有内存写入，在线程 B 中成为可见副效应，即一旦原子加载完成保证线程 B 能观察到线程 A 写入内存的所有内容。此顺序的典型使用场景是 互斥量 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_acquire))); assert(*p2 == \"Hello\"); // 断言成功 assert(data == 42); // 断言成功 } 顺序一致性带标签 memory_order_seq_cst 的原子操作不仅以与释放/获得顺序相同的方式排序内存 (在一个线程中先发生于存储的任何结果都变成进行加载的线程中的可见副效应)，还对所有带此标签的内存操作建立单独全序。 void write_x() { x.store(true, ::std::memory_order_seq_cst); } void write_y() { y.store(true, ::std::memory_order_seq_cst); } void read_x_then_y() { while (!x.load(::std::memory_order_seq_cst)); if (y.load(::std::memory_order_seq_cst)) { ++z; } } void read_y_then_x() { while (!y.load(::std::memory_order_seq_cst)); if (x.load(::std::memory_order_seq_cst)) { ++z; } } assert(z.load() != 0); // 断言成功 全序列顺序在所有多核系统上要求完全的内存栅栏 CPU 指令，这可能成为性能瓶颈，因为它强制受影响的内存访问传播到每个核心。 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#获取-释放序"},{"categories":["ProgrammingLanguage"],"content":" 内存顺序C++ 标准定义了六种原子操作的内存顺序，它们代表了四种内存模型： 自由序 (Relaxed ordering) memory_order_relaxed，没有同步或顺序制约，仅对此操作要求原子性 消费-释放序 (Consume-Release ordering) memory_order_consume，当前线程中依赖于当前加载的该值的读或写不能被重排到此加载前；其他释放同一原子变量的线程对数据依赖变量的写入，为当前线程所可见 memory_order_release，当前线程中读或写不能被重排到此存储后；当前线程的所有写入，对其他获取同一原子变量的线程可见，对原子变量的带依赖写入变得对其他消费同一原子对象的线程可见 获取-释放序 (Acquire-Release ordering) memory_order_acquire，当前线程中读或写不能被重排到此加载前；其他释放同一原子变量的线程的所有写入，能为当前线程所见 memory_order_acq_rel，所有释放同一原子变量的线程的写操作在当前线程修改前可见，当前线程改操作对其他获取同一原子变量的线程可见 顺序一致性 (Sequentially-consistent ordering) memory_order_seq_cst，原子操作的默认内存序，所有线程以同一顺序观测到所有修改 在不同的原子操作上，可以用到的内存序也有所不同 store (写操作) memory_order_relaxed, memory_order_release, memory_order_seq_cst load (读操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_seq_cst read-modify-write (读改写操作) memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_release, memory_order_acq_rel, memory_order_seq_cst 自由序带标签 memory_order_relaxed 的原子操作，它们不会在同时的内存访问间强加顺序，它们只保证原子性和修改顺序一致性，典型应用场景为 计数器自增 // x = {0}, y = {0} // thread 1 r1 = y.load(::std::memory_order_relaxed); // A x.store(r1, ::std::memory_order_relaxed); // B // thread 2 r2 = x.load(::std::memory_order_relaxed); // C y.store(42, ::std::memory_order_relaxed); // D assert(r1 != 42 || r2 != 42); // E A 先序于 B，C 先序于 D，但 D 在 y 上的副效应可能对 A 可见，同时 B 在 x 上的副效应可能对 C 可见，所以允许 E 断言失败。 另外自由序中，当前线程可能看到别的线程的更新，但是更新频率不一定是均匀的，但其值一定是递增的。详细例子可以查看 C++ Concurrency in Action (2rd) 中电话计数员的例子。 消费-释放序若线程 A 中的原子存储带标签 memory_order_release 而线程 B 中来自同一对象的读取存储值的原子加载带标签 memory_order_consume，则线程 A 视角中先发生于原子存储的所有内存写入，会在线程 B 中该加载操作所携带依赖进入的操作中变成可见副效应，即一旦完成原子加载，则保证线程 B 中使用从该加载获得的值的运算符和函数能见到线程 A 写入内存的内容。同步仅在释放和消费同一原子对象的线程间建立，其他线程能见到与被同步线程的一者或两者相异的内存访问顺序。 此顺序的典型使用情景，涉及对 很少被写入 的数据结构的同时时读取，和 有指针中介发布 的 发布者-订阅者 情形，即当生产者发布消费者能通过其访问信息的指针之时：无需令生产者写入内存的所有其他内容对消费者可见。这种场景的例子之一是 rcu 解引用。 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_consume))); assert(*p2 == \"Hello\"); // 断言成功 (*p2 从 ptr 携带依赖) assert(data == 42); // 断言可能失败 (data 不从 ptr 携带依赖) } 获取-释放序若线程 A 中的一个原子存储带标签 memory_order_release，而线程 B 中来自同一变量的原子加载带标签 memory_order_acquire，则从线程 A 的视角先发生于原子存储的所有内存写入，在线程 B 中成为可见副效应，即一旦原子加载完成保证线程 B 能观察到线程 A 写入内存的所有内容。此顺序的典型使用场景是 互斥量 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ptr.store(p, ::std::memory_order_release); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_acquire))); assert(*p2 == \"Hello\"); // 断言成功 assert(data == 42); // 断言成功 } 顺序一致性带标签 memory_order_seq_cst 的原子操作不仅以与释放/获得顺序相同的方式排序内存 (在一个线程中先发生于存储的任何结果都变成进行加载的线程中的可见副效应)，还对所有带此标签的内存操作建立单独全序。 void write_x() { x.store(true, ::std::memory_order_seq_cst); } void write_y() { y.store(true, ::std::memory_order_seq_cst); } void read_x_then_y() { while (!x.load(::std::memory_order_seq_cst)); if (y.load(::std::memory_order_seq_cst)) { ++z; } } void read_y_then_x() { while (!y.load(::std::memory_order_seq_cst)); if (x.load(::std::memory_order_seq_cst)) { ++z; } } assert(z.load() != 0); // 断言成功 全序列顺序在所有多核系统上要求完全的内存栅栏 CPU 指令，这可能成为性能瓶颈，因为它强制受影响的内存访问传播到每个核心。 ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#顺序一致性"},{"categories":["ProgrammingLanguage"],"content":" 栅栏栅栏操作会对内存序列进行约束，使其无法对任何数据进行修改，典型的做法是与使用 memory_order_relaxed 约束序的原子操作一起使用。栅栏属于全局操作，执行栅栏操作可以影响到在线程中的其他原子操作，因为这类操作就像画了一条任何代码都无法跨越的线一样，所以栅栏操作通常也被称为 内存栅栏 (memory barriers)。我们以下代码与 获取- 释放序 代码效果相同 ::std::atomic\u003c::std::string*\u003e ptr; int data; void producer() { ::std::string* p = new ::std::string(\"Hello\"); data = 42; ::std::atomic_thread_fence(::std::memory_order_release); ptr.store(p, ::std::memory_order_relaxed); } void consumer() { ::std::string* p2; while (!(p2 = ptr.load(::std::memory_order_relaxed))); ::std::atomic_thread_fence(::std::memory_order_acquire); assert(*p2 == \"Hello\"); // 断言成功 assert(data == 42); // 断言成功 } ","date":"12-05","objectID":"/2020/cpp_concurrency_atomic/:3:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Atomic"],"title":"原子操作","uri":"/2020/cpp_concurrency_atomic/#栅栏"},{"categories":["ProgrammingLanguage"],"content":"GinShio | Cpp Concurrency in Action (2rd) 第二、三、四章读书笔记","date":"12-01","objectID":"/2020/cpp_concurrency_std/","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/"},{"categories":["ProgrammingLanguage"],"content":" 线程管理","date":"12-01","objectID":"/2020/cpp_concurrency_std/:1:0","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#线程管理"},{"categories":["ProgrammingLanguage"],"content":" 创建线程新的线程会在 ::std::thread (头文件 thread 中) 对象创建的时候被启动，在函数执行完毕后，该线程也就结束了，提供的函数对象会复制到新线程的存储空间中，函数对象的执行与操作都在线程的内存空间中执行。在创建新线程时你可以指定一个函数作为任务，或者是 仿函数，当然也可以是 lambda 表达式 ::std::thread my_thread0{do_something}; struct Task { void operator()() const { do_something(); } }; ::std::thread my_thread1{Task()}; ::std::thread my_thread2{[]() { do_something(); }}; 线程启动后，需要指定是等待线程结束还是让其自主运行，如果 ::std::thread 对象销毁之前没有做出决定，程序就会终止，因此必须确保线程能够正确 汇入 (joined) 或 分离 (detached)。调用 join() 可以等待线程完成，并在线程结束时清理相关的内存，使 ::std::thread 对象不再与已完成线程有任何关联，所以一个线程一旦被汇入将不能再次汇入。调用 detach() 会使线程在后台运行，不再与主线程进行直接交互， ::std::thread 对象不再引用这个线程，分离的线程也不可被再次汇入，不过C++运行时库保证线程退出时可以正确回收相关资源。 在C++中 ::std::thread 对象是一种 可移动但不可复制 的资源，它可以交出它的所有权，但不能与其他对象共享线程的所有权。如果你希望对一个已持有线程的对象更改其行为，那你必须先汇入或分离已关联的线程，或者将已关联的线程的所有权交出。 ::std::thread t1{do_something}; ::std::thread t2 = std::move(t1); t1 = std::thread{some_other_function}; std::thread t3; t3 = std::move(t2); // t1 = std::move(t3); // 错误 ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:1:1","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#创建线程"},{"categories":["ProgrammingLanguage"],"content":" 传递参数向线程中传递参数十分简单，为 ::std::thread 构造函数附加参数即可，所有参数 将会拷贝到新线程的内存空间中，即使函数中的参数是引用 void f1(int i, const ::std::string\u0026 s); void f2(int i, ::std::string\u0026 s); ::std::thread t1{f1, 3, ::std::string{\"Hello\"}}; t1.join(); // ::std::thread t2{f2, 2, ::std::string{\"Hello\"}}; // Error // t2.join(); 这里 f2 期望传入一个::std::string的引用，传递参数时会将拷贝的参数以右值的方式进行传递 (为了支持移动的类型)，与函数期望的非常量引用不符，故会在编译期报错。不过我们可以使用 ::std::ref 将参数转换为引用的形式进行传递 auto s = ::std::string{\"Hello\"}; ::std::thread t2{f2, 6, ::std::ref(s)}; t2.join(); 当然也可以在一个线程上运行一个成员函数，做法也是很简单的，第一个参数传递成员函数的指针，第二个参数传递这个类的对象的指针，剩下的则是这个待运行的函数的参数 struct X { void do_something(int); } x; ::std::thread t3{\u0026X::do_something, \u0026x, 1}; ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:1:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#传递参数"},{"categories":["ProgrammingLanguage"],"content":" 线程标识线程的标识类型是 ::std:🧵:id ，可以使用 ::std::thread 对象的成员函数 get_id() 获取，当线程没有和任何执行线程关联时将返回默认值来表示 无线程; 也可以使用 ::std::this_thread::get_id() 来获取当前线程的标识。::std:🧵:id 对象可以拷贝或对比，因为标识符是可复用的，当两个标识符相等时代表同一个线程或这两个线程无关联线程 if (master_thread_id == ::std::this_thread::get_id()) { master_do_something(); } else { worker_do_something(); } ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:1:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#线程标识"},{"categories":["ProgrammingLanguage"],"content":" 共享数据涉及到共享数据时，问题就是因为共享数据的修改所导致，如果共享数据只读，那么不会影响到数据，更不会对数据进行修改，所有线程都会获得同样的数据。但当一个或多个线程要修改共享数据时，就会产生很多麻烦，需要小心谨慎，才能确保所有线程都正常工作。 ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:0","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#共享数据"},{"categories":["ProgrammingLanguage"],"content":" 条件竞争并发中的竞争条件，取决于一个以上线程的执行顺序，每个线程都抢着完成自己的任务，大多数情况下，即使改变执行顺序，也是良性竞争，结果是可以接受的。遗憾的是，当不变量遭到破坏时会产生条件竞争，通常是恶性竞争：并发的去修改一个独立对象。恶性竞争时对一个数据块进行修改时，其他线程可能同时对其进行访问，导致数据不一致或与预期不符，并且出现概率低且难复现。 避免恶性竞争，最简单的方法就是对数据结构采用某种保护机制，确保只有修改线程可以看到不变量的中间状态，其他线程观察结构时会发现其修改还未开始。另一方式就是对数据结构与不变量进行修改，修改后的结构可以完成一系列不可分割的变量，从而保证不变量的状态，即无锁编程。 ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:1","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#条件竞争"},{"categories":["ProgrammingLanguage"],"content":" 互斥量访问共享数据前将数据锁住，在访问结束后再将数据解锁，当线程使用互斥量锁住共享数据时，其他的线程都必须等到之前那个线程对数据进行解锁后，才能进行访问数据。 通过实例化 ::std::mutex (头文件 mutex 中) 创建互斥量实例，成员函数lock()可对互斥量上锁，unlock()为解锁，不过不推荐使用成员函数，因为你必须在函数的出口处正确的解锁，其中包括异常情况也必须保证正确解锁，否则互斥量可能无法正常使用。推荐的做法是使用互斥量RAII模板类 ::std::lock_guard (头文件 mutex 中)，构造时加锁并在析构时解锁，保证互斥量可以被正确的解锁。 下面例子中，如果多个线程访问add_n函数，那么互斥量mu就会保护变量 result ，在一个线程中修改它时其他线程将无法访问它， \\(result += i\\) 将会在线程中安全的执行，不会因为数据竞争导致线程看到的result脏值，从而污染结果 ::std::mutex mu; void add_n(const long long\u0026 n, long long\u0026 result) { for (long long i = 1ll; i \u003c= n; ++i) { ::std::lock_guard\u003c::std::mutex\u003e guard(mu); result += i; } } 不过通常互斥量会与需要保护的数据封装在同一个类中，让它们联系在一起，保证数据不变量的稳定状态。不过当类中某个方法返回保护数据的指针或者引用时，可能会破坏数据，此时需要谨慎的对接口进行设计，切勿将受保护数据的指针或引用传递到互斥锁作用域之外。 使用互斥量保护数据时，还需要考虑接口间的条件竞争，比如常使用的 ::std::stack，以下代码在单线程中是正确的，但是当 ::std::stack 是共享数据时，虽然每次调用接口时内部可能返回正确的结果，但是当用户使用时可能并非安全的。很明显代码中，top() 调用时很可能其他线程已经 pop() 了最后一个元素，虽然该线程访问到栈不为空，但是 top() 获取到错误的结果，top() 与 pop() 存在数据竞争关系 ::std::stack\u003cint\u003e s; if (!s.empty()) { const int value = s.top(); s.pop(); do_something(value); } 锁的粒度太小，恶性条件竞争已经出现，需要保护的操作并未全覆盖到; 如果锁的粒度太大，会抵消并发带来的性能提升。 死锁使用多个互斥量操作时需要注意 死锁，这会让两个线程互相等待，直到另一个解锁互斥量。死锁产生的必要条件: 互斥条件 一个资源每次只能被一个任务使用 占有且等待 因请求资源而阻塞时，对已获得的资源保持不放 不可剥夺 已获得的资源，在末使用完之前，不能强行剥夺 循环等待条件 若干任务之间形成一种头尾相接的循环等待资源关系 一般在C++使用互斥量时，避免循环等待即可，对多个互斥量可以使用标准库中的 ::std::lock 与 ::std::lock_guard 进行RAII锁定，可以按照一定的顺序对互斥量进行锁定，避免循环锁定。以下代码展示了一次锁定多个互斥量，::std::lock 锁定互斥量，并创建两个 ::std::lock_guard 对象对互斥量进行管理，::std::adopt_lock 表示 ::std::lock_guard 可以获取锁并将锁交给其管理，::std::lock_guard 对象不需要再构建新的锁。值得一提的是，::std::lock 可能会抛出异常，但是请放心，已锁定的锁会随着异常而自动释放，所以 ::std::lock 要么 全部锁住 要么 一个都不锁 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::lock(lhs.mu, rhs.mu); ::std::lock_guard\u003c::std::mutex\u003e lockl{lhs.mu, ::std::adopt_lock}; ::std::lock_guard\u003c::std::mutex\u003e lockr{rhs.mu, ::std::adopt_lock}; ::std::swap(lhs.data, rhs.data); } C++17 中提供了RAII模板类 ::std::scoped_lock (头文件 mutex 中) 用来支持这种情况，并且增加了 自动推导模板参数，是所以这种情况在 C++17 中将会更简单的实现 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::scoped_lock guard{lhs.mu, rhs.mu}; // 等价于: // ::std::scoped_lock\u003c::std::mutex, ::std::mutex\u003e guard{lhs.mu, rhs.mu}; ::std::swap(lhs.data, rhs.data); } 死锁通常是对锁的使用不当造成，当然也可以是其他情况，不过我们应该尽可能的避免死锁 避免嵌套锁 获取一个锁时就别再获取第二个，需要获取多个锁时应使用 ::std::lock 来完成 避免在持有锁时调用外部代码 代码是由外部提供的，我们无法确定外部的行为，可能会造成与第一条违反的情况 使用固定顺序获取锁 当有多个锁且无法使用 ::std::lock 时，应在每个线程上以固定的顺序获取锁 灵活的管理锁标准库提供了一种灵活的RAII管理锁的方式 ::std::unique_lock (头文件 mutex 中)，它允许使用 ::std::adopt_lock 假设已拥有互斥的所有权，也允许使用 ::std::defer_lock 假设不获取互斥的所有权，使用 ::std::unique_lock 会与 ::std::lock_guard 的实现方式等价。::std::unique_lock 对象中带有标志来确定是否持有互斥量，并确保正确地在析构函数中处理互斥量 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::unique_lock\u003c::std::mutex\u003e lockl{lhs.mu, ::std::defer_lock}; ::std::unique_lock\u003c::std::mutex\u003e lockr{rhs.mu, ::std::defer_lock}; ::std::lock(lockl, lockr); // 持有的互斥量并锁定 ::std::swap(lhs.data, rhs.data); } ::std::unique_lock 是一种可移动不可复制的类型，它可以交出已持有互斥量的所有权，使互斥量在不同作用域中传递 ::std::unique_lock\u003c::std::mutex\u003e get_lock() { extern ::std::mutex mu; ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; do_something(); return lk; } void other() { ::std::unique_lock\u003c::std::mutex\u003e lk{get_lock()}; do_something(); } ::std::unique_lock 还支持在对象销毁之前放弃持有互斥，这样可以提前为其他等待线程释放锁，增加性能。 锁的粒度是用来描述锁保护的数据量的大小，细粒度锁 (fine-grained lock) 能够保护较小的数据量，粗粒度锁 (coarse-grained lock) 能够保护较多的数据量。比如数据库中，对一行进行锁定的锁比对整张表锁定的锁粒度小，行锁相对于表锁性能更高，因为可以同时处理多行，但是也更不安全。 ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#互斥量"},{"categories":["ProgrammingLanguage"],"content":" 互斥量访问共享数据前将数据锁住，在访问结束后再将数据解锁，当线程使用互斥量锁住共享数据时，其他的线程都必须等到之前那个线程对数据进行解锁后，才能进行访问数据。 通过实例化 ::std::mutex (头文件 mutex 中) 创建互斥量实例，成员函数lock()可对互斥量上锁，unlock()为解锁，不过不推荐使用成员函数，因为你必须在函数的出口处正确的解锁，其中包括异常情况也必须保证正确解锁，否则互斥量可能无法正常使用。推荐的做法是使用互斥量RAII模板类 ::std::lock_guard (头文件 mutex 中)，构造时加锁并在析构时解锁，保证互斥量可以被正确的解锁。 下面例子中，如果多个线程访问add_n函数，那么互斥量mu就会保护变量 result ，在一个线程中修改它时其他线程将无法访问它， \\(result += i\\) 将会在线程中安全的执行，不会因为数据竞争导致线程看到的result脏值，从而污染结果 ::std::mutex mu; void add_n(const long long\u0026 n, long long\u0026 result) { for (long long i = 1ll; i \u003c= n; ++i) { ::std::lock_guard\u003c::std::mutex\u003e guard(mu); result += i; } } 不过通常互斥量会与需要保护的数据封装在同一个类中，让它们联系在一起，保证数据不变量的稳定状态。不过当类中某个方法返回保护数据的指针或者引用时，可能会破坏数据，此时需要谨慎的对接口进行设计，切勿将受保护数据的指针或引用传递到互斥锁作用域之外。 使用互斥量保护数据时，还需要考虑接口间的条件竞争，比如常使用的 ::std::stack，以下代码在单线程中是正确的，但是当 ::std::stack 是共享数据时，虽然每次调用接口时内部可能返回正确的结果，但是当用户使用时可能并非安全的。很明显代码中，top() 调用时很可能其他线程已经 pop() 了最后一个元素，虽然该线程访问到栈不为空，但是 top() 获取到错误的结果，top() 与 pop() 存在数据竞争关系 ::std::stack s; if (!s.empty()) { const int value = s.top(); s.pop(); do_something(value); } 锁的粒度太小，恶性条件竞争已经出现，需要保护的操作并未全覆盖到; 如果锁的粒度太大，会抵消并发带来的性能提升。 死锁使用多个互斥量操作时需要注意 死锁，这会让两个线程互相等待，直到另一个解锁互斥量。死锁产生的必要条件: 互斥条件 一个资源每次只能被一个任务使用 占有且等待 因请求资源而阻塞时，对已获得的资源保持不放 不可剥夺 已获得的资源，在末使用完之前，不能强行剥夺 循环等待条件 若干任务之间形成一种头尾相接的循环等待资源关系 一般在C++使用互斥量时，避免循环等待即可，对多个互斥量可以使用标准库中的 ::std::lock 与 ::std::lock_guard 进行RAII锁定，可以按照一定的顺序对互斥量进行锁定，避免循环锁定。以下代码展示了一次锁定多个互斥量，::std::lock 锁定互斥量，并创建两个 ::std::lock_guard 对象对互斥量进行管理，::std::adopt_lock 表示 ::std::lock_guard 可以获取锁并将锁交给其管理，::std::lock_guard 对象不需要再构建新的锁。值得一提的是，::std::lock 可能会抛出异常，但是请放心，已锁定的锁会随着异常而自动释放，所以 ::std::lock 要么 全部锁住 要么 一个都不锁 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::lock(lhs.mu, rhs.mu); ::std::lock_guard\u003c::std::mutex\u003e lockl{lhs.mu, ::std::adopt_lock}; ::std::lock_guard\u003c::std::mutex\u003e lockr{rhs.mu, ::std::adopt_lock}; ::std::swap(lhs.data, rhs.data); } C++17 中提供了RAII模板类 ::std::scoped_lock (头文件 mutex 中) 用来支持这种情况，并且增加了 自动推导模板参数，是所以这种情况在 C++17 中将会更简单的实现 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::scoped_lock guard{lhs.mu, rhs.mu}; // 等价于: // ::std::scoped_lock\u003c::std::mutex, ::std::mutex\u003e guard{lhs.mu, rhs.mu}; ::std::swap(lhs.data, rhs.data); } 死锁通常是对锁的使用不当造成，当然也可以是其他情况，不过我们应该尽可能的避免死锁 避免嵌套锁 获取一个锁时就别再获取第二个，需要获取多个锁时应使用 ::std::lock 来完成 避免在持有锁时调用外部代码 代码是由外部提供的，我们无法确定外部的行为，可能会造成与第一条违反的情况 使用固定顺序获取锁 当有多个锁且无法使用 ::std::lock 时，应在每个线程上以固定的顺序获取锁 灵活的管理锁标准库提供了一种灵活的RAII管理锁的方式 ::std::unique_lock (头文件 mutex 中)，它允许使用 ::std::adopt_lock 假设已拥有互斥的所有权，也允许使用 ::std::defer_lock 假设不获取互斥的所有权，使用 ::std::unique_lock 会与 ::std::lock_guard 的实现方式等价。::std::unique_lock 对象中带有标志来确定是否持有互斥量，并确保正确地在析构函数中处理互斥量 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::unique_lock\u003c::std::mutex\u003e lockl{lhs.mu, ::std::defer_lock}; ::std::unique_lock\u003c::std::mutex\u003e lockr{rhs.mu, ::std::defer_lock}; ::std::lock(lockl, lockr); // 持有的互斥量并锁定 ::std::swap(lhs.data, rhs.data); } ::std::unique_lock 是一种可移动不可复制的类型，它可以交出已持有互斥量的所有权，使互斥量在不同作用域中传递 ::std::unique_lock\u003c::std::mutex\u003e get_lock() { extern ::std::mutex mu; ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; do_something(); return lk; } void other() { ::std::unique_lock\u003c::std::mutex\u003e lk{get_lock()}; do_something(); } ::std::unique_lock 还支持在对象销毁之前放弃持有互斥，这样可以提前为其他等待线程释放锁，增加性能。 锁的粒度是用来描述锁保护的数据量的大小，细粒度锁 (fine-grained lock) 能够保护较小的数据量，粗粒度锁 (coarse-grained lock) 能够保护较多的数据量。比如数据库中，对一行进行锁定的锁比对整张表锁定的锁粒度小，行锁相对于表锁性能更高，因为可以同时处理多行，但是也更不安全。 ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#死锁"},{"categories":["ProgrammingLanguage"],"content":" 互斥量访问共享数据前将数据锁住，在访问结束后再将数据解锁，当线程使用互斥量锁住共享数据时，其他的线程都必须等到之前那个线程对数据进行解锁后，才能进行访问数据。 通过实例化 ::std::mutex (头文件 mutex 中) 创建互斥量实例，成员函数lock()可对互斥量上锁，unlock()为解锁，不过不推荐使用成员函数，因为你必须在函数的出口处正确的解锁，其中包括异常情况也必须保证正确解锁，否则互斥量可能无法正常使用。推荐的做法是使用互斥量RAII模板类 ::std::lock_guard (头文件 mutex 中)，构造时加锁并在析构时解锁，保证互斥量可以被正确的解锁。 下面例子中，如果多个线程访问add_n函数，那么互斥量mu就会保护变量 result ，在一个线程中修改它时其他线程将无法访问它， \\(result += i\\) 将会在线程中安全的执行，不会因为数据竞争导致线程看到的result脏值，从而污染结果 ::std::mutex mu; void add_n(const long long\u0026 n, long long\u0026 result) { for (long long i = 1ll; i \u003c= n; ++i) { ::std::lock_guard\u003c::std::mutex\u003e guard(mu); result += i; } } 不过通常互斥量会与需要保护的数据封装在同一个类中，让它们联系在一起，保证数据不变量的稳定状态。不过当类中某个方法返回保护数据的指针或者引用时，可能会破坏数据，此时需要谨慎的对接口进行设计，切勿将受保护数据的指针或引用传递到互斥锁作用域之外。 使用互斥量保护数据时，还需要考虑接口间的条件竞争，比如常使用的 ::std::stack，以下代码在单线程中是正确的，但是当 ::std::stack 是共享数据时，虽然每次调用接口时内部可能返回正确的结果，但是当用户使用时可能并非安全的。很明显代码中，top() 调用时很可能其他线程已经 pop() 了最后一个元素，虽然该线程访问到栈不为空，但是 top() 获取到错误的结果，top() 与 pop() 存在数据竞争关系 ::std::stack s; if (!s.empty()) { const int value = s.top(); s.pop(); do_something(value); } 锁的粒度太小，恶性条件竞争已经出现，需要保护的操作并未全覆盖到; 如果锁的粒度太大，会抵消并发带来的性能提升。 死锁使用多个互斥量操作时需要注意 死锁，这会让两个线程互相等待，直到另一个解锁互斥量。死锁产生的必要条件: 互斥条件 一个资源每次只能被一个任务使用 占有且等待 因请求资源而阻塞时，对已获得的资源保持不放 不可剥夺 已获得的资源，在末使用完之前，不能强行剥夺 循环等待条件 若干任务之间形成一种头尾相接的循环等待资源关系 一般在C++使用互斥量时，避免循环等待即可，对多个互斥量可以使用标准库中的 ::std::lock 与 ::std::lock_guard 进行RAII锁定，可以按照一定的顺序对互斥量进行锁定，避免循环锁定。以下代码展示了一次锁定多个互斥量，::std::lock 锁定互斥量，并创建两个 ::std::lock_guard 对象对互斥量进行管理，::std::adopt_lock 表示 ::std::lock_guard 可以获取锁并将锁交给其管理，::std::lock_guard 对象不需要再构建新的锁。值得一提的是，::std::lock 可能会抛出异常，但是请放心，已锁定的锁会随着异常而自动释放，所以 ::std::lock 要么 全部锁住 要么 一个都不锁 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::lock(lhs.mu, rhs.mu); ::std::lock_guard\u003c::std::mutex\u003e lockl{lhs.mu, ::std::adopt_lock}; ::std::lock_guard\u003c::std::mutex\u003e lockr{rhs.mu, ::std::adopt_lock}; ::std::swap(lhs.data, rhs.data); } C++17 中提供了RAII模板类 ::std::scoped_lock (头文件 mutex 中) 用来支持这种情况，并且增加了 自动推导模板参数，是所以这种情况在 C++17 中将会更简单的实现 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::scoped_lock guard{lhs.mu, rhs.mu}; // 等价于: // ::std::scoped_lock\u003c::std::mutex, ::std::mutex\u003e guard{lhs.mu, rhs.mu}; ::std::swap(lhs.data, rhs.data); } 死锁通常是对锁的使用不当造成，当然也可以是其他情况，不过我们应该尽可能的避免死锁 避免嵌套锁 获取一个锁时就别再获取第二个，需要获取多个锁时应使用 ::std::lock 来完成 避免在持有锁时调用外部代码 代码是由外部提供的，我们无法确定外部的行为，可能会造成与第一条违反的情况 使用固定顺序获取锁 当有多个锁且无法使用 ::std::lock 时，应在每个线程上以固定的顺序获取锁 灵活的管理锁标准库提供了一种灵活的RAII管理锁的方式 ::std::unique_lock (头文件 mutex 中)，它允许使用 ::std::adopt_lock 假设已拥有互斥的所有权，也允许使用 ::std::defer_lock 假设不获取互斥的所有权，使用 ::std::unique_lock 会与 ::std::lock_guard 的实现方式等价。::std::unique_lock 对象中带有标志来确定是否持有互斥量，并确保正确地在析构函数中处理互斥量 void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) { return; } ::std::unique_lock\u003c::std::mutex\u003e lockl{lhs.mu, ::std::defer_lock}; ::std::unique_lock\u003c::std::mutex\u003e lockr{rhs.mu, ::std::defer_lock}; ::std::lock(lockl, lockr); // 持有的互斥量并锁定 ::std::swap(lhs.data, rhs.data); } ::std::unique_lock 是一种可移动不可复制的类型，它可以交出已持有互斥量的所有权，使互斥量在不同作用域中传递 ::std::unique_lock\u003c::std::mutex\u003e get_lock() { extern ::std::mutex mu; ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; do_something(); return lk; } void other() { ::std::unique_lock\u003c::std::mutex\u003e lk{get_lock()}; do_something(); } ::std::unique_lock 还支持在对象销毁之前放弃持有互斥，这样可以提前为其他等待线程释放锁，增加性能。 锁的粒度是用来描述锁保护的数据量的大小，细粒度锁 (fine-grained lock) 能够保护较小的数据量，粗粒度锁 (coarse-grained lock) 能够保护较多的数据量。比如数据库中，对一行进行锁定的锁比对整张表锁定的锁粒度小，行锁相对于表锁性能更高，因为可以同时处理多行，但是也更不安全。 ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#灵活的管理锁"},{"categories":["ProgrammingLanguage"],"content":" 保护共享数据的方式 保护共享数据的初始化过程如果一个资源构造代价昂贵，我们可能会使用延迟初始化来构造它，不过这在单线程下是安全的，多线程下初始化是需要被保护的，不然可能会出现多次初始化的情况 void foo() { ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } lk.lock(); do_something(); } 双重检测锁定模式 (DCLP) 也是一种保护初始化的状态，不过遗憾的是，它存在潜在的条件竞争，即线程可能得知其他线程完成了初始化，但可能没有看到新创建的实例，在调用 do_something()时得到不正确的结果。Java引入了volatile关键字并安全地实现了DCLP， C++11开始我们也可以实现安全的DCLP。详细可以阅读 C++与双重检测锁定模式的风险，我们也可以在之后的学习中学习安全的DCLP实现 // DCLP void bar() { if (data.empty()) { ::std::lock_guard\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } } do_something(); // 数据竞争 } 不过我们可以不这么麻烦，C++标准库为我们提供了 ::std::once_flag 与 ::std::call_once (头文件 mutex 中) 来处理这种情况，并且相比使用互斥量所消耗的资源更少 ::std::once_flag once; void func() { ::std::call_once(once，[]() { data = new element(); }); do_something(); } 局部作用域中的static变量在声明后就已经完成初始化，对于C++11之前初始化的过程中存在条件竞争，但是从 C++11 开始初始化与定义完全在一个线程中发生 element get_element_instance() { static element instance; // C++11 开始为线程安全的初始化 return instance; } 保护不常更新的数据结构当有不常更新的数据结构时，我们希望在修改时线程可以独占并安全的修改内容，完成修改后可以并发的安全访问数据。使用 ::std::mutex 来保护这样的数据结构对于性能来说并不是一个很好的方法，这会削弱读取数据的性能。我们可以想象这样一种互斥量，它可以在 写 线程中独占访问，而允许 读 线程并发访问，这样的互斥量被称为 读写锁，读线程需要等写线程释放锁后才可以并发访问，而写线程必须等全部读线程放弃互斥量后才可以独占访问。 C++17标准库提供了 ::std::shared_mutex (头文件 shared_mutex 中)，C++14提供了 RAII模板类 ::std::shared_lock 与有时限的读写锁 ::std::shared_timed_mutex (头文件 shared_mutex 中)，可惜的是C++11中并没有提供相应的设施。timed_mutex系列互斥量相比普通互斥量，多了时限功能，在时限内可以获得锁则返回true并获得锁，否则返回false 并不能获得锁，不过普通的互斥量则相较有更高的性能。在读写锁的使用中，对于写线程可以使用 ::std::lock_guard\u003c::std::shared_mutex\u003e 或 ::std::unique_lock\u003c::std::shared_mutex\u003e 进行RAII管理，它们与普通的互斥量行为一致；对于读线程，则需要 ::std::shared_lock\u003c::std::shared_mutex\u003e 进行RAII管理 class DnsCache { ::std::map\u003c::std::string，::std::string\u003e entries_; mutable std::shared_mutex mu_; public: ::std::string find(const ::std::string\u0026 domain) const { ::std::shared_lock\u003c::std::shared_mutex\u003e lk{mu_}; auto it = entries_.find(domain); return (it == entries_.end()) ? \"\" : it-\u003esecond; } void update(const ::std::string\u0026 domain, const ::std::string\u0026 ip) { ::std::lock_guard\u003c::std::shared_mutex\u003e lk{mu_}; entries_[domain] = ip; } }; 重入锁在一个线程上，对已上锁的 ::std::mutex 再次上锁是错误的，会引起未定义行为，如果希望在线程上对一个互斥量在释放前进行多次上锁，则需要使用 ::std::recursive_mutex (头文件 mutex 中)。当然要牢记，你对其上锁了多少次，那一定需要解锁多少次，否则就会出现锁死其他线程的情况 (请善用 ::std::lock_guard 与 ::std::unique_lock) ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#保护共享数据的方式"},{"categories":["ProgrammingLanguage"],"content":" 保护共享数据的方式 保护共享数据的初始化过程如果一个资源构造代价昂贵，我们可能会使用延迟初始化来构造它，不过这在单线程下是安全的，多线程下初始化是需要被保护的，不然可能会出现多次初始化的情况 void foo() { ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } lk.lock(); do_something(); } 双重检测锁定模式 (DCLP) 也是一种保护初始化的状态，不过遗憾的是，它存在潜在的条件竞争，即线程可能得知其他线程完成了初始化，但可能没有看到新创建的实例，在调用 do_something()时得到不正确的结果。Java引入了volatile关键字并安全地实现了DCLP， C++11开始我们也可以实现安全的DCLP。详细可以阅读 C++与双重检测锁定模式的风险，我们也可以在之后的学习中学习安全的DCLP实现 // DCLP void bar() { if (data.empty()) { ::std::lock_guard\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } } do_something(); // 数据竞争 } 不过我们可以不这么麻烦，C++标准库为我们提供了 ::std::once_flag 与 ::std::call_once (头文件 mutex 中) 来处理这种情况，并且相比使用互斥量所消耗的资源更少 ::std::once_flag once; void func() { ::std::call_once(once，[]() { data = new element(); }); do_something(); } 局部作用域中的static变量在声明后就已经完成初始化，对于C++11之前初始化的过程中存在条件竞争，但是从 C++11 开始初始化与定义完全在一个线程中发生 element get_element_instance() { static element instance; // C++11 开始为线程安全的初始化 return instance; } 保护不常更新的数据结构当有不常更新的数据结构时，我们希望在修改时线程可以独占并安全的修改内容，完成修改后可以并发的安全访问数据。使用 ::std::mutex 来保护这样的数据结构对于性能来说并不是一个很好的方法，这会削弱读取数据的性能。我们可以想象这样一种互斥量，它可以在 写 线程中独占访问，而允许 读 线程并发访问，这样的互斥量被称为 读写锁，读线程需要等写线程释放锁后才可以并发访问，而写线程必须等全部读线程放弃互斥量后才可以独占访问。 C++17标准库提供了 ::std::shared_mutex (头文件 shared_mutex 中)，C++14提供了 RAII模板类 ::std::shared_lock 与有时限的读写锁 ::std::shared_timed_mutex (头文件 shared_mutex 中)，可惜的是C++11中并没有提供相应的设施。timed_mutex系列互斥量相比普通互斥量，多了时限功能，在时限内可以获得锁则返回true并获得锁，否则返回false 并不能获得锁，不过普通的互斥量则相较有更高的性能。在读写锁的使用中，对于写线程可以使用 ::std::lock_guard\u003c::std::shared_mutex\u003e 或 ::std::unique_lock\u003c::std::shared_mutex\u003e 进行RAII管理，它们与普通的互斥量行为一致；对于读线程，则需要 ::std::shared_lock\u003c::std::shared_mutex\u003e 进行RAII管理 class DnsCache { ::std::map\u003c::std::string，::std::string\u003e entries_; mutable std::shared_mutex mu_; public: ::std::string find(const ::std::string\u0026 domain) const { ::std::shared_lock\u003c::std::shared_mutex\u003e lk{mu_}; auto it = entries_.find(domain); return (it == entries_.end()) ? \"\" : it-\u003esecond; } void update(const ::std::string\u0026 domain, const ::std::string\u0026 ip) { ::std::lock_guard\u003c::std::shared_mutex\u003e lk{mu_}; entries_[domain] = ip; } }; 重入锁在一个线程上，对已上锁的 ::std::mutex 再次上锁是错误的，会引起未定义行为，如果希望在线程上对一个互斥量在释放前进行多次上锁，则需要使用 ::std::recursive_mutex (头文件 mutex 中)。当然要牢记，你对其上锁了多少次，那一定需要解锁多少次，否则就会出现锁死其他线程的情况 (请善用 ::std::lock_guard 与 ::std::unique_lock) ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#保护共享数据的初始化过程"},{"categories":["ProgrammingLanguage"],"content":" 保护共享数据的方式 保护共享数据的初始化过程如果一个资源构造代价昂贵，我们可能会使用延迟初始化来构造它，不过这在单线程下是安全的，多线程下初始化是需要被保护的，不然可能会出现多次初始化的情况 void foo() { ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } lk.lock(); do_something(); } 双重检测锁定模式 (DCLP) 也是一种保护初始化的状态，不过遗憾的是，它存在潜在的条件竞争，即线程可能得知其他线程完成了初始化，但可能没有看到新创建的实例，在调用 do_something()时得到不正确的结果。Java引入了volatile关键字并安全地实现了DCLP， C++11开始我们也可以实现安全的DCLP。详细可以阅读 C++与双重检测锁定模式的风险，我们也可以在之后的学习中学习安全的DCLP实现 // DCLP void bar() { if (data.empty()) { ::std::lock_guard\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } } do_something(); // 数据竞争 } 不过我们可以不这么麻烦，C++标准库为我们提供了 ::std::once_flag 与 ::std::call_once (头文件 mutex 中) 来处理这种情况，并且相比使用互斥量所消耗的资源更少 ::std::once_flag once; void func() { ::std::call_once(once，[]() { data = new element(); }); do_something(); } 局部作用域中的static变量在声明后就已经完成初始化，对于C++11之前初始化的过程中存在条件竞争，但是从 C++11 开始初始化与定义完全在一个线程中发生 element get_element_instance() { static element instance; // C++11 开始为线程安全的初始化 return instance; } 保护不常更新的数据结构当有不常更新的数据结构时，我们希望在修改时线程可以独占并安全的修改内容，完成修改后可以并发的安全访问数据。使用 ::std::mutex 来保护这样的数据结构对于性能来说并不是一个很好的方法，这会削弱读取数据的性能。我们可以想象这样一种互斥量，它可以在 写 线程中独占访问，而允许 读 线程并发访问，这样的互斥量被称为 读写锁，读线程需要等写线程释放锁后才可以并发访问，而写线程必须等全部读线程放弃互斥量后才可以独占访问。 C++17标准库提供了 ::std::shared_mutex (头文件 shared_mutex 中)，C++14提供了 RAII模板类 ::std::shared_lock 与有时限的读写锁 ::std::shared_timed_mutex (头文件 shared_mutex 中)，可惜的是C++11中并没有提供相应的设施。timed_mutex系列互斥量相比普通互斥量，多了时限功能，在时限内可以获得锁则返回true并获得锁，否则返回false 并不能获得锁，不过普通的互斥量则相较有更高的性能。在读写锁的使用中，对于写线程可以使用 ::std::lock_guard\u003c::std::shared_mutex\u003e 或 ::std::unique_lock\u003c::std::shared_mutex\u003e 进行RAII管理，它们与普通的互斥量行为一致；对于读线程，则需要 ::std::shared_lock\u003c::std::shared_mutex\u003e 进行RAII管理 class DnsCache { ::std::map\u003c::std::string，::std::string\u003e entries_; mutable std::shared_mutex mu_; public: ::std::string find(const ::std::string\u0026 domain) const { ::std::shared_lock\u003c::std::shared_mutex\u003e lk{mu_}; auto it = entries_.find(domain); return (it == entries_.end()) ? \"\" : it-\u003esecond; } void update(const ::std::string\u0026 domain, const ::std::string\u0026 ip) { ::std::lock_guard\u003c::std::shared_mutex\u003e lk{mu_}; entries_[domain] = ip; } }; 重入锁在一个线程上，对已上锁的 ::std::mutex 再次上锁是错误的，会引起未定义行为，如果希望在线程上对一个互斥量在释放前进行多次上锁，则需要使用 ::std::recursive_mutex (头文件 mutex 中)。当然要牢记，你对其上锁了多少次，那一定需要解锁多少次，否则就会出现锁死其他线程的情况 (请善用 ::std::lock_guard 与 ::std::unique_lock) ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#保护不常更新的数据结构"},{"categories":["ProgrammingLanguage"],"content":" 保护共享数据的方式 保护共享数据的初始化过程如果一个资源构造代价昂贵，我们可能会使用延迟初始化来构造它，不过这在单线程下是安全的，多线程下初始化是需要被保护的，不然可能会出现多次初始化的情况 void foo() { ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } lk.lock(); do_something(); } 双重检测锁定模式 (DCLP) 也是一种保护初始化的状态，不过遗憾的是，它存在潜在的条件竞争，即线程可能得知其他线程完成了初始化，但可能没有看到新创建的实例，在调用 do_something()时得到不正确的结果。Java引入了volatile关键字并安全地实现了DCLP， C++11开始我们也可以实现安全的DCLP。详细可以阅读 C++与双重检测锁定模式的风险，我们也可以在之后的学习中学习安全的DCLP实现 // DCLP void bar() { if (data.empty()) { ::std::lock_guard\u003c::std::mutex\u003e lk{mu}; if (data.empty()) { data = new element(); } } do_something(); // 数据竞争 } 不过我们可以不这么麻烦，C++标准库为我们提供了 ::std::once_flag 与 ::std::call_once (头文件 mutex 中) 来处理这种情况，并且相比使用互斥量所消耗的资源更少 ::std::once_flag once; void func() { ::std::call_once(once，[]() { data = new element(); }); do_something(); } 局部作用域中的static变量在声明后就已经完成初始化，对于C++11之前初始化的过程中存在条件竞争，但是从 C++11 开始初始化与定义完全在一个线程中发生 element get_element_instance() { static element instance; // C++11 开始为线程安全的初始化 return instance; } 保护不常更新的数据结构当有不常更新的数据结构时，我们希望在修改时线程可以独占并安全的修改内容，完成修改后可以并发的安全访问数据。使用 ::std::mutex 来保护这样的数据结构对于性能来说并不是一个很好的方法，这会削弱读取数据的性能。我们可以想象这样一种互斥量，它可以在 写 线程中独占访问，而允许 读 线程并发访问，这样的互斥量被称为 读写锁，读线程需要等写线程释放锁后才可以并发访问，而写线程必须等全部读线程放弃互斥量后才可以独占访问。 C++17标准库提供了 ::std::shared_mutex (头文件 shared_mutex 中)，C++14提供了 RAII模板类 ::std::shared_lock 与有时限的读写锁 ::std::shared_timed_mutex (头文件 shared_mutex 中)，可惜的是C++11中并没有提供相应的设施。timed_mutex系列互斥量相比普通互斥量，多了时限功能，在时限内可以获得锁则返回true并获得锁，否则返回false 并不能获得锁，不过普通的互斥量则相较有更高的性能。在读写锁的使用中，对于写线程可以使用 ::std::lock_guard\u003c::std::shared_mutex\u003e 或 ::std::unique_lock\u003c::std::shared_mutex\u003e 进行RAII管理，它们与普通的互斥量行为一致；对于读线程，则需要 ::std::shared_lock\u003c::std::shared_mutex\u003e 进行RAII管理 class DnsCache { ::std::map\u003c::std::string，::std::string\u003e entries_; mutable std::shared_mutex mu_; public: ::std::string find(const ::std::string\u0026 domain) const { ::std::shared_lock\u003c::std::shared_mutex\u003e lk{mu_}; auto it = entries_.find(domain); return (it == entries_.end()) ? \"\" : it-\u003esecond; } void update(const ::std::string\u0026 domain, const ::std::string\u0026 ip) { ::std::lock_guard\u003c::std::shared_mutex\u003e lk{mu_}; entries_[domain] = ip; } }; 重入锁在一个线程上，对已上锁的 ::std::mutex 再次上锁是错误的，会引起未定义行为，如果希望在线程上对一个互斥量在释放前进行多次上锁，则需要使用 ::std::recursive_mutex (头文件 mutex 中)。当然要牢记，你对其上锁了多少次，那一定需要解锁多少次，否则就会出现锁死其他线程的情况 (请善用 ::std::lock_guard 与 ::std::unique_lock) ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:2:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#重入锁"},{"categories":["ProgrammingLanguage"],"content":" 同步操作","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:0","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#同步操作"},{"categories":["ProgrammingLanguage"],"content":" 等待条件通过一条线程触发等待事件的机制是最基本的唤醒方式，这种机制被称为 条件变量，条件变量与多个事件或其他条件相关，并且一个或多个线程会等待条件的达成。当某些线程被终止时，为了唤醒等待线程，终止线程会向等待着的线程广播信息。 C++标准库实现了条件变量 (头文件 condition_variable 中) ::std::condition_variable 和 ::std::condition_variable_any ，它们需要与互斥量一起才能工作，前者需要和 ::std::mutex 一起工作，而 _any 后缀的条件变量可以和任何互斥量一起，但是相比普通条件变量更消耗系统资源。 ::std::mutex mu; ::std::queue\u003cdata_chunk\u003e q; ::std::condition_variable cond; void preparation() { while (more()) { const data_chunk data = get_data(); ::std::lock_guard\u003c::std::mutex\u003e lk{mu}; q.push(data); cond.notify_one(); } } void processing() { while (true) { ::std::unique_lock\u003c::std::mutex\u003e lk{mu}; cond.wait(lk, [] { return !q.empty(); }); data_chunk data = q.front(); q.pop(); lk.unlock(); process(data); if (is_last_chunk(data)) { break; } } } 以上代码就是一个条件变量的应用，执行情况如下 preparation 线程将获取数据，上锁互斥量并将数据压入队列 processing 线程必须对互斥量进行锁定，之后才能调用条件变量的成员函数 wait() 检查条件谓词，如果成立则继续，如果不成立将解锁互斥量并阻塞当前线程 preparation 线程调用 notify_one() 会唤醒 一个正在等待 的线程，调用后需要解锁互斥量，如果没有等待线程则无事发生，notify_one() 不会唤醒调用后开始等待的线程 如果 processing 线程被唤醒，则会重新获取锁，并再次进行条件谓词的检查 条件变量调用wait()的过程中，可能会多次检查条件谓词，并在谓词为true的情况下立即返回。另一点，等待线程可能会在不被其他线程通知的情况下被唤醒，这被称为 虚假唤醒，而虚假唤醒的数量和频率都是不确定的，所以条件谓词不建议有副作用。 ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:1","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#等待条件"},{"categories":["ProgrammingLanguage"],"content":" future当线程需要等待特定事件时，某种程度上来说就需要知道期望的结果，线程会周期性的等待或检查事件是否触发，检查期间也会执行其他任务。另外，等待任务期间也可以先执行另外的任务，直到对应的任务触发，而后等待future的状态会变为就绪状态。future可能是和数据相关，也可能不是，当事件发生时，这个future就不能重置了。 C++标准库提供了两种future (头文件 future 中) ::std::future 和 ::std::shared_future，它们与智能指针 ::std::shared_ptr 和 ::std::unique_ptr 十分类似。::std::future 只能与指定事件相关连，而 ::std::shared_future 可以关联多个事件，而实现中所有实例会同时变为就绪状态，并且可以访问与事件相关的数据。如果希望future与数据无关，则可以使用 void 的特化。future 像是线程通信，但是其本身并不提供同步访问，如果需要访问独立的future对象时则需要使用互斥量或类似同步机制进行保护，::std::shared_future 提供访问异步操作结果的机制，每个线程可以安全的访问自身 ::std::shared_future 对象的副本。 异步返回值我们可以使用 ::std::async (头文件 future 中) 和 ::std::future 启动一个异步任务，获取线程的返回值，当然等待返回值的线程会阻塞，直到 ::std::future 就绪为止 int async_func(); void do_something(); int main(void) { ::std::future\u003cint\u003e ret = ::std::async(async_func); // 异步执行 async_func do_something(); ::std::cout \u003c\u003c \"Return: \" \u003c\u003c ::std::flush // 立即打印 \u003c\u003c ret.get() \u003c\u003c ::std::endl; // 阻塞，直到 future 就绪 } ::std::future 是否需要等待取决于绑定的 ::std::async 是否启动一个线程，或是否有任务正在进行，大多数情况下在函数调用之前可以传递一个 ::std::launch 类型的对象 ::std::launch::defered 惰性求值，延迟到 wait() 或 get() 时进行求值 ::std::launch::async 异步求值，求值将在一个独立的线程上进行 ::std::launch::async \\(|\\) ::std::luanch::defered 默认行为，惰性求值或异步求值，具体求值方式由实现定义 auto f0 = ::std::async(::std::launch::async, func0); // 异步求值 auto f1 = ::std::async(::std::launch::defered, func1); // 惰性求值 auto f2 = ::std::async(::std::launch::async | ::std::launch::defered, func2); // 求值方式由实现定义 auto f3 = ::std::async(func3); // 求值方式由实现定义 绑定任务::std::packaged_task (头文件 future 中) 允许将 future 与可调用对象进行绑定，::std::packaged_task 的模板参数是一个可调用类型，在调用 ::std::packaged_task 时就会调用相关函数，而 future 状态就绪时则会存储返回值，通过 get_future() 获取绑定的 future 对象。 Promise大部分并发编程语言都实现了 Promise/Future 结构，起源于函数式编程和相关范例，目的是将值与其计算方式分离，从而允许更灵活地进行计算，特别是通过并行化。后来它在分布式计算中得到了应用，减少了通信往返的延迟。future是变量的 只读 占位符视图，而 promise是 可写 的单赋值容器，用于设置future的值。 类模板 ::std::promise (头文件 future 中) 提供存储值或异常的设施，之后通过 ::std::promise 对象所创建的 ::std::future 对象异步获得结果。::std::future 会阻塞等待线程，::std::promise 则会设置结果并将关联的 ::std::future 对象设置为就绪状态，不过 std::promise 只应当使用一次。 void accumulate(::std::vector\u003cint\u003e::iterator first, ::std::vector\u003cint\u003e::iterator last, ::std::promise\u003cint\u003e accumulate_promise) { int sum = ::std::accumulate(first, last, 0); accumulate_promise.set_value(sum); } int main(void) { ::std::vector\u003cint\u003e numbers = {1, 2, 3, 4, 5, 6}; ::std::promise\u003cint\u003e accumulate_promise; ::std::future\u003cint\u003e accumulate_future = accumulate_promise.get_future(); ::std::thread work_thread(accumulate, numbers.begin(), numbers.end(), ::std::move(accumulate_promise)); ::std::cout \u003c\u003c accumulate_future.get() \u003c\u003c ::std::endl; // 等待结果 work_thread.join(); } ::std::shared_future 可用于同时向多个线程发信息, 类似于 ::std::condition_variable::notify_all() ::std::promise\u003cvoid\u003e ready_promise, t1_promise, t2_promise; ::std::shared_future\u003cvoid\u003e ready_future{ready_promise.get_future()}; using high_resolution_clock = ::std::chrono::high_resolution_clock; using milli = ::std::chrono::duration\u003cdouble, ::std::milli\u003e; ::std::chrono::time_point\u003chigh_resolution_clock\u003e start; auto result1 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t1_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); auto result2 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t2_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); t1_promise.get_future().wait(); t2_promise.get_future().wait(); start = std::chrono::high_resolution_clock::now(); ready_promise.set_value(); std::cout \u003c\u003c \"Thread 1 received the signal \" \u003c\u003c result1.get().count() \u003c\u003c \" ms after start\\n\" \u003c\u003c \"Thread 2 received the signal \" \u003c\u003c result2.get().count() \u003c\u003c \" ms after start\\n\"; ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#future"},{"categories":["ProgrammingLanguage"],"content":" future当线程需要等待特定事件时，某种程度上来说就需要知道期望的结果，线程会周期性的等待或检查事件是否触发，检查期间也会执行其他任务。另外，等待任务期间也可以先执行另外的任务，直到对应的任务触发，而后等待future的状态会变为就绪状态。future可能是和数据相关，也可能不是，当事件发生时，这个future就不能重置了。 C++标准库提供了两种future (头文件 future 中) ::std::future 和 ::std::shared_future，它们与智能指针 ::std::shared_ptr 和 ::std::unique_ptr 十分类似。::std::future 只能与指定事件相关连，而 ::std::shared_future 可以关联多个事件，而实现中所有实例会同时变为就绪状态，并且可以访问与事件相关的数据。如果希望future与数据无关，则可以使用 void 的特化。future 像是线程通信，但是其本身并不提供同步访问，如果需要访问独立的future对象时则需要使用互斥量或类似同步机制进行保护，::std::shared_future 提供访问异步操作结果的机制，每个线程可以安全的访问自身 ::std::shared_future 对象的副本。 异步返回值我们可以使用 ::std::async (头文件 future 中) 和 ::std::future 启动一个异步任务，获取线程的返回值，当然等待返回值的线程会阻塞，直到 ::std::future 就绪为止 int async_func(); void do_something(); int main(void) { ::std::future ret = ::std::async(async_func); // 异步执行 async_func do_something(); ::std::cout \u003c\u003c \"Return: \" \u003c\u003c ::std::flush // 立即打印 \u003c\u003c ret.get() \u003c\u003c ::std::endl; // 阻塞，直到 future 就绪 } ::std::future 是否需要等待取决于绑定的 ::std::async 是否启动一个线程，或是否有任务正在进行，大多数情况下在函数调用之前可以传递一个 ::std::launch 类型的对象 ::std::launch::defered 惰性求值，延迟到 wait() 或 get() 时进行求值 ::std::launch::async 异步求值，求值将在一个独立的线程上进行 ::std::launch::async \\(|\\) ::std::luanch::defered 默认行为，惰性求值或异步求值，具体求值方式由实现定义 auto f0 = ::std::async(::std::launch::async, func0); // 异步求值 auto f1 = ::std::async(::std::launch::defered, func1); // 惰性求值 auto f2 = ::std::async(::std::launch::async | ::std::launch::defered, func2); // 求值方式由实现定义 auto f3 = ::std::async(func3); // 求值方式由实现定义 绑定任务::std::packaged_task (头文件 future 中) 允许将 future 与可调用对象进行绑定，::std::packaged_task 的模板参数是一个可调用类型，在调用 ::std::packaged_task 时就会调用相关函数，而 future 状态就绪时则会存储返回值，通过 get_future() 获取绑定的 future 对象。 Promise大部分并发编程语言都实现了 Promise/Future 结构，起源于函数式编程和相关范例，目的是将值与其计算方式分离，从而允许更灵活地进行计算，特别是通过并行化。后来它在分布式计算中得到了应用，减少了通信往返的延迟。future是变量的 只读 占位符视图，而 promise是 可写 的单赋值容器，用于设置future的值。 类模板 ::std::promise (头文件 future 中) 提供存储值或异常的设施，之后通过 ::std::promise 对象所创建的 ::std::future 对象异步获得结果。::std::future 会阻塞等待线程，::std::promise 则会设置结果并将关联的 ::std::future 对象设置为就绪状态，不过 std::promise 只应当使用一次。 void accumulate(::std::vector::iterator first, ::std::vector::iterator last, ::std::promise accumulate_promise) { int sum = ::std::accumulate(first, last, 0); accumulate_promise.set_value(sum); } int main(void) { ::std::vector numbers = {1, 2, 3, 4, 5, 6}; ::std::promise accumulate_promise; ::std::future accumulate_future = accumulate_promise.get_future(); ::std::thread work_thread(accumulate, numbers.begin(), numbers.end(), ::std::move(accumulate_promise)); ::std::cout \u003c\u003c accumulate_future.get() \u003c\u003c ::std::endl; // 等待结果 work_thread.join(); } ::std::shared_future 可用于同时向多个线程发信息, 类似于 ::std::condition_variable::notify_all() ::std::promise ready_promise, t1_promise, t2_promise; ::std::shared_future ready_future{ready_promise.get_future()}; using high_resolution_clock = ::std::chrono::high_resolution_clock; using milli = ::std::chrono::duration; ::std::chrono::time_point start; auto result1 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t1_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); auto result2 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t2_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); t1_promise.get_future().wait(); t2_promise.get_future().wait(); start = std::chrono::high_resolution_clock::now(); ready_promise.set_value(); std::cout \u003c\u003c \"Thread 1 received the signal \" \u003c\u003c result1.get().count() \u003c\u003c \" ms after start\\n\" \u003c\u003c \"Thread 2 received the signal \" \u003c\u003c result2.get().count() \u003c\u003c \" ms after start\\n\"; ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#异步返回值"},{"categories":["ProgrammingLanguage"],"content":" future当线程需要等待特定事件时，某种程度上来说就需要知道期望的结果，线程会周期性的等待或检查事件是否触发，检查期间也会执行其他任务。另外，等待任务期间也可以先执行另外的任务，直到对应的任务触发，而后等待future的状态会变为就绪状态。future可能是和数据相关，也可能不是，当事件发生时，这个future就不能重置了。 C++标准库提供了两种future (头文件 future 中) ::std::future 和 ::std::shared_future，它们与智能指针 ::std::shared_ptr 和 ::std::unique_ptr 十分类似。::std::future 只能与指定事件相关连，而 ::std::shared_future 可以关联多个事件，而实现中所有实例会同时变为就绪状态，并且可以访问与事件相关的数据。如果希望future与数据无关，则可以使用 void 的特化。future 像是线程通信，但是其本身并不提供同步访问，如果需要访问独立的future对象时则需要使用互斥量或类似同步机制进行保护，::std::shared_future 提供访问异步操作结果的机制，每个线程可以安全的访问自身 ::std::shared_future 对象的副本。 异步返回值我们可以使用 ::std::async (头文件 future 中) 和 ::std::future 启动一个异步任务，获取线程的返回值，当然等待返回值的线程会阻塞，直到 ::std::future 就绪为止 int async_func(); void do_something(); int main(void) { ::std::future ret = ::std::async(async_func); // 异步执行 async_func do_something(); ::std::cout \u003c\u003c \"Return: \" \u003c\u003c ::std::flush // 立即打印 \u003c\u003c ret.get() \u003c\u003c ::std::endl; // 阻塞，直到 future 就绪 } ::std::future 是否需要等待取决于绑定的 ::std::async 是否启动一个线程，或是否有任务正在进行，大多数情况下在函数调用之前可以传递一个 ::std::launch 类型的对象 ::std::launch::defered 惰性求值，延迟到 wait() 或 get() 时进行求值 ::std::launch::async 异步求值，求值将在一个独立的线程上进行 ::std::launch::async \\(|\\) ::std::luanch::defered 默认行为，惰性求值或异步求值，具体求值方式由实现定义 auto f0 = ::std::async(::std::launch::async, func0); // 异步求值 auto f1 = ::std::async(::std::launch::defered, func1); // 惰性求值 auto f2 = ::std::async(::std::launch::async | ::std::launch::defered, func2); // 求值方式由实现定义 auto f3 = ::std::async(func3); // 求值方式由实现定义 绑定任务::std::packaged_task (头文件 future 中) 允许将 future 与可调用对象进行绑定，::std::packaged_task 的模板参数是一个可调用类型，在调用 ::std::packaged_task 时就会调用相关函数，而 future 状态就绪时则会存储返回值，通过 get_future() 获取绑定的 future 对象。 Promise大部分并发编程语言都实现了 Promise/Future 结构，起源于函数式编程和相关范例，目的是将值与其计算方式分离，从而允许更灵活地进行计算，特别是通过并行化。后来它在分布式计算中得到了应用，减少了通信往返的延迟。future是变量的 只读 占位符视图，而 promise是 可写 的单赋值容器，用于设置future的值。 类模板 ::std::promise (头文件 future 中) 提供存储值或异常的设施，之后通过 ::std::promise 对象所创建的 ::std::future 对象异步获得结果。::std::future 会阻塞等待线程，::std::promise 则会设置结果并将关联的 ::std::future 对象设置为就绪状态，不过 std::promise 只应当使用一次。 void accumulate(::std::vector::iterator first, ::std::vector::iterator last, ::std::promise accumulate_promise) { int sum = ::std::accumulate(first, last, 0); accumulate_promise.set_value(sum); } int main(void) { ::std::vector numbers = {1, 2, 3, 4, 5, 6}; ::std::promise accumulate_promise; ::std::future accumulate_future = accumulate_promise.get_future(); ::std::thread work_thread(accumulate, numbers.begin(), numbers.end(), ::std::move(accumulate_promise)); ::std::cout \u003c\u003c accumulate_future.get() \u003c\u003c ::std::endl; // 等待结果 work_thread.join(); } ::std::shared_future 可用于同时向多个线程发信息, 类似于 ::std::condition_variable::notify_all() ::std::promise ready_promise, t1_promise, t2_promise; ::std::shared_future ready_future{ready_promise.get_future()}; using high_resolution_clock = ::std::chrono::high_resolution_clock; using milli = ::std::chrono::duration; ::std::chrono::time_point start; auto result1 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t1_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); auto result2 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t2_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); t1_promise.get_future().wait(); t2_promise.get_future().wait(); start = std::chrono::high_resolution_clock::now(); ready_promise.set_value(); std::cout \u003c\u003c \"Thread 1 received the signal \" \u003c\u003c result1.get().count() \u003c\u003c \" ms after start\\n\" \u003c\u003c \"Thread 2 received the signal \" \u003c\u003c result2.get().count() \u003c\u003c \" ms after start\\n\"; ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#绑定任务"},{"categories":["ProgrammingLanguage"],"content":" future当线程需要等待特定事件时，某种程度上来说就需要知道期望的结果，线程会周期性的等待或检查事件是否触发，检查期间也会执行其他任务。另外，等待任务期间也可以先执行另外的任务，直到对应的任务触发，而后等待future的状态会变为就绪状态。future可能是和数据相关，也可能不是，当事件发生时，这个future就不能重置了。 C++标准库提供了两种future (头文件 future 中) ::std::future 和 ::std::shared_future，它们与智能指针 ::std::shared_ptr 和 ::std::unique_ptr 十分类似。::std::future 只能与指定事件相关连，而 ::std::shared_future 可以关联多个事件，而实现中所有实例会同时变为就绪状态，并且可以访问与事件相关的数据。如果希望future与数据无关，则可以使用 void 的特化。future 像是线程通信，但是其本身并不提供同步访问，如果需要访问独立的future对象时则需要使用互斥量或类似同步机制进行保护，::std::shared_future 提供访问异步操作结果的机制，每个线程可以安全的访问自身 ::std::shared_future 对象的副本。 异步返回值我们可以使用 ::std::async (头文件 future 中) 和 ::std::future 启动一个异步任务，获取线程的返回值，当然等待返回值的线程会阻塞，直到 ::std::future 就绪为止 int async_func(); void do_something(); int main(void) { ::std::future ret = ::std::async(async_func); // 异步执行 async_func do_something(); ::std::cout \u003c\u003c \"Return: \" \u003c\u003c ::std::flush // 立即打印 \u003c\u003c ret.get() \u003c\u003c ::std::endl; // 阻塞，直到 future 就绪 } ::std::future 是否需要等待取决于绑定的 ::std::async 是否启动一个线程，或是否有任务正在进行，大多数情况下在函数调用之前可以传递一个 ::std::launch 类型的对象 ::std::launch::defered 惰性求值，延迟到 wait() 或 get() 时进行求值 ::std::launch::async 异步求值，求值将在一个独立的线程上进行 ::std::launch::async \\(|\\) ::std::luanch::defered 默认行为，惰性求值或异步求值，具体求值方式由实现定义 auto f0 = ::std::async(::std::launch::async, func0); // 异步求值 auto f1 = ::std::async(::std::launch::defered, func1); // 惰性求值 auto f2 = ::std::async(::std::launch::async | ::std::launch::defered, func2); // 求值方式由实现定义 auto f3 = ::std::async(func3); // 求值方式由实现定义 绑定任务::std::packaged_task (头文件 future 中) 允许将 future 与可调用对象进行绑定，::std::packaged_task 的模板参数是一个可调用类型，在调用 ::std::packaged_task 时就会调用相关函数，而 future 状态就绪时则会存储返回值，通过 get_future() 获取绑定的 future 对象。 Promise大部分并发编程语言都实现了 Promise/Future 结构，起源于函数式编程和相关范例，目的是将值与其计算方式分离，从而允许更灵活地进行计算，特别是通过并行化。后来它在分布式计算中得到了应用，减少了通信往返的延迟。future是变量的 只读 占位符视图，而 promise是 可写 的单赋值容器，用于设置future的值。 类模板 ::std::promise (头文件 future 中) 提供存储值或异常的设施，之后通过 ::std::promise 对象所创建的 ::std::future 对象异步获得结果。::std::future 会阻塞等待线程，::std::promise 则会设置结果并将关联的 ::std::future 对象设置为就绪状态，不过 std::promise 只应当使用一次。 void accumulate(::std::vector::iterator first, ::std::vector::iterator last, ::std::promise accumulate_promise) { int sum = ::std::accumulate(first, last, 0); accumulate_promise.set_value(sum); } int main(void) { ::std::vector numbers = {1, 2, 3, 4, 5, 6}; ::std::promise accumulate_promise; ::std::future accumulate_future = accumulate_promise.get_future(); ::std::thread work_thread(accumulate, numbers.begin(), numbers.end(), ::std::move(accumulate_promise)); ::std::cout \u003c\u003c accumulate_future.get() \u003c\u003c ::std::endl; // 等待结果 work_thread.join(); } ::std::shared_future 可用于同时向多个线程发信息, 类似于 ::std::condition_variable::notify_all() ::std::promise ready_promise, t1_promise, t2_promise; ::std::shared_future ready_future{ready_promise.get_future()}; using high_resolution_clock = ::std::chrono::high_resolution_clock; using milli = ::std::chrono::duration; ::std::chrono::time_point start; auto result1 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t1_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); auto result2 = ::std::async(::std::launch::async, [\u0026, ready_future]() -\u003e milli { t2_promise.set_value(); ready_future.wait(); // 等待来自 main() 的信号 return high_resolution_clock::now() - start; }); t1_promise.get_future().wait(); t2_promise.get_future().wait(); start = std::chrono::high_resolution_clock::now(); ready_promise.set_value(); std::cout \u003c\u003c \"Thread 1 received the signal \" \u003c\u003c result1.get().count() \u003c\u003c \" ms after start\\n\" \u003c\u003c \"Thread 2 received the signal \" \u003c\u003c result2.get().count() \u003c\u003c \" ms after start\\n\"; ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:2","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#promise"},{"categories":["ProgrammingLanguage"],"content":" 限时等待阻塞调用会将线程挂起一段不确定的时间，直到相应的事件发生，通常情况下这样的方式很不错，但是在一些情况下，需要限定线程等待的时间。 通常有两种指定超时方式：一种是 时间段，另一种是 时间点。第一种方式，需要指定一段时间；第二种方式，就是指定一个时间点。多数等待函数提供变量，对两种超时方式进行处理，处理持续时间的变量 (时间段) 以 _for 作为后缀，处理绝对时间的变量 (时间戳) 以 _until 作为后缀。 时钟时钟就是时间信息源，一个时钟的当前时间可由静态成员函数 now() 获取，特定的时间点的类型是成员类型 time_point 。时钟节拍被指定为1/x秒，这是由时间周期所决定，当时钟节拍均匀分布且不可修改时这种时钟被称为稳定时钟。 时间段时间段 ::std::chrono::duration (头文件 chrono 中) 由 Rep 类型的 计次数 和 Period 类型的 计次周期 组成，计次周期是一个编译期有理数常量，表示从一个计次到下一个的秒数，比如分钟的类型可以使用 ::std::chrono::duration\u003clong long, ::std::ratio\u003c60, 1\u003e\u003e 表示，而毫秒的类型可以使用 ::std::chrono::duration\u003clong long, ::std::ratio\u003c1, 1000\u003e\u003e 表示。不过为了方便起见，标准库定义了辅助类型来简化使用 ::std::chrono::nanoseconds (纳秒) ::std::chrono::microseconds (微秒) ::std::chrono::milliseconds (毫秒) ::std::chrono::seconds (秒) ::std::chrono::minutes (分) ::std::chrono::hours (时) C++20开始，标准库又增加了天、周、月、年来方便使用时间段。 C++14 中，::std::literals 中定义了一些 duration 字面量方便使用 using namespace ::std::literals; auto one_day = 24h; // 24小时 auto half_an_hour = 30min; // 30分钟 auto five_seconds = 5s; // 5秒 auto one_second = 1000ms; // 1000毫秒 auto ten_micros = 10us; // 10微秒 auto two_nanos = 2ns; // 2纳秒 时间戳时间戳 ::std::chrono::time_point (头文件 chrono 中) 由 Clock 类型的 时钟 和 Duration 类型的 时钟间隔 组成，并且可以通过算术运算调整时间戳。 std::chrono::system_clock::time_point now = std::chrono::system_clock::now(); std::time_t now_c = std::chrono::system_clock::to_time_t(now - std::chrono::hours(24)); std::cout \u003c\u003c \"24 hours ago, the time was \" \u003c\u003c std::put_time(std::localtime(\u0026now_c), \"%F %T\") \u003c\u003c ::std::endl; std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); std::cout \u003c\u003c \"Hello World\\n\"; std::chrono::steady_clock::time_point end = std::chrono::steady_clock::now(); std::cout \u003c\u003c \"Printing took \" \u003c\u003c std::chrono::duration_cast\u003cstd::chrono::microseconds\u003e(end - start).count() \u003c\u003c \"us.\\n\"; // 24 hours ago, the time was 2020-12-03 23:47:43 // Hello World // Printing took 4us. ","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#限时等待"},{"categories":["ProgrammingLanguage"],"content":" 限时等待阻塞调用会将线程挂起一段不确定的时间，直到相应的事件发生，通常情况下这样的方式很不错，但是在一些情况下，需要限定线程等待的时间。 通常有两种指定超时方式：一种是 时间段，另一种是 时间点。第一种方式，需要指定一段时间；第二种方式，就是指定一个时间点。多数等待函数提供变量，对两种超时方式进行处理，处理持续时间的变量 (时间段) 以 _for 作为后缀，处理绝对时间的变量 (时间戳) 以 _until 作为后缀。 时钟时钟就是时间信息源，一个时钟的当前时间可由静态成员函数 now() 获取，特定的时间点的类型是成员类型 time_point 。时钟节拍被指定为1/x秒，这是由时间周期所决定，当时钟节拍均匀分布且不可修改时这种时钟被称为稳定时钟。 时间段时间段 ::std::chrono::duration (头文件 chrono 中) 由 Rep 类型的 计次数 和 Period 类型的 计次周期 组成，计次周期是一个编译期有理数常量，表示从一个计次到下一个的秒数，比如分钟的类型可以使用 ::std::chrono::duration","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#时钟"},{"categories":["ProgrammingLanguage"],"content":" 限时等待阻塞调用会将线程挂起一段不确定的时间，直到相应的事件发生，通常情况下这样的方式很不错，但是在一些情况下，需要限定线程等待的时间。 通常有两种指定超时方式：一种是 时间段，另一种是 时间点。第一种方式，需要指定一段时间；第二种方式，就是指定一个时间点。多数等待函数提供变量，对两种超时方式进行处理，处理持续时间的变量 (时间段) 以 _for 作为后缀，处理绝对时间的变量 (时间戳) 以 _until 作为后缀。 时钟时钟就是时间信息源，一个时钟的当前时间可由静态成员函数 now() 获取，特定的时间点的类型是成员类型 time_point 。时钟节拍被指定为1/x秒，这是由时间周期所决定，当时钟节拍均匀分布且不可修改时这种时钟被称为稳定时钟。 时间段时间段 ::std::chrono::duration (头文件 chrono 中) 由 Rep 类型的 计次数 和 Period 类型的 计次周期 组成，计次周期是一个编译期有理数常量，表示从一个计次到下一个的秒数，比如分钟的类型可以使用 ::std::chrono::duration","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#时间段"},{"categories":["ProgrammingLanguage"],"content":" 限时等待阻塞调用会将线程挂起一段不确定的时间，直到相应的事件发生，通常情况下这样的方式很不错，但是在一些情况下，需要限定线程等待的时间。 通常有两种指定超时方式：一种是 时间段，另一种是 时间点。第一种方式，需要指定一段时间；第二种方式，就是指定一个时间点。多数等待函数提供变量，对两种超时方式进行处理，处理持续时间的变量 (时间段) 以 _for 作为后缀，处理绝对时间的变量 (时间戳) 以 _until 作为后缀。 时钟时钟就是时间信息源，一个时钟的当前时间可由静态成员函数 now() 获取，特定的时间点的类型是成员类型 time_point 。时钟节拍被指定为1/x秒，这是由时间周期所决定，当时钟节拍均匀分布且不可修改时这种时钟被称为稳定时钟。 时间段时间段 ::std::chrono::duration (头文件 chrono 中) 由 Rep 类型的 计次数 和 Period 类型的 计次周期 组成，计次周期是一个编译期有理数常量，表示从一个计次到下一个的秒数，比如分钟的类型可以使用 ::std::chrono::duration","date":"12-01","objectID":"/2020/cpp_concurrency_std/:3:3","series":["C++ 并发"],"tags":["Note","C++","Concurrency","Library"],"title":"并发标准库","uri":"/2020/cpp_concurrency_std/#时间戳"},{"categories":["Applications"],"content":"GinShio | deploy the mail service","date":"11-16","objectID":"/2020/mail_server/","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/"},{"categories":["Applications"],"content":"搭建邮局服务器的想法之前一直都有，不过一直没有尝试，国庆的时候从阿里云换到了腾讯云的时候尝试直接使用 postfix 和 dovecot 搭建，尝试了大概3天被劝退了，重新使用现成的解决方案也算终于搭建好了，可以愉快的使用自建邮箱了 (可以愉快的装逼了 信息 更新了 mailu 的搭建，虽然 mailu 相比 mailcow 可以使用宿主机的数据库，不过 mailu 配置 SMTPS / IMAPS / POP3S 不如 mailcow 简单方便，也没怎么研究，目前没有切换到 mailu 的打算 警告 打算在更换服务器之后不再维护邮箱服务，装逼不存在的 ","date":"11-16","objectID":"/2020/mail_server/:0:0","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#"},{"categories":["Applications"],"content":" 部署开始搭建服务器，以下采用域名 (example.com) 和 IP (1.1.1.1)，安装在 /mailcow，使用主机的nginx反向代理，部署之前我们首先定义一些Shell变量，以便之后使用，请根据自己的需求更改 path_to=\"/path/to\" mailcow_path=\"${path_to}/mailcow\" # mailcow 所在目录 mailu_path=\"${path_to}/mailu\" mail_host=\"mail.example.com\" mail_ip=\"1.1.1.1\" db_user=\"example_user\" # 数据库用户 (Mailu使用宿主机PostgreSQL时使用) db_passwd=\"example_password\" # 数据库密码 (Mailu使用宿主机PostgreSQL时使用) db_name=\"example_db\" # 数据库名称 (Mailu使用宿主机PostgreSQL时使用) http_port=\"8080\" https_port=\"8443\" cert_path=\"/ssl/path/to/cert/\" # 证书存放目录 cert_file=\"${cert_path}/cert.pem\" # 域名证书 key_file=\"${cert_path}/key.pem\" # 域名证书密钥 ca_file=\"${cert_path}/intermediate_CA.pem\" # 域名证书颁发者证书 另外，由于webmail对 S/MIME 与 PGP/MIME 的支持并不好，我们将在服务器上禁止 webmail，使用本地的邮件客户端收发邮件，以便更好的使用加密、签名功能，如有需要请自行开启webmail。 ","date":"11-16","objectID":"/2020/mail_server/:1:0","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#部署"},{"categories":["Applications"],"content":" DNSDNS设置是一个邮件服务器的重中之重，为了让我们可以发出邮件和收到邮件，防止邮件被拒收或者进入垃圾箱被识别成垃圾邮件等，当然不是配置好了就不会进垃圾邮箱，不配置肯定会有问题。 除了上述DNS解析之外，还需要配置 DKIM 和 PTR，DKIM在我们搭建好服务之后配置， PTR需要向运营商提交工单申请 (阿里云和腾讯云是这样的)，如果你没有配置ptr解析那么你可能会上一些黑名单。 DKIM 同样的也是 TXT 类型的 DNS 解析，在部署完成后由指定选择器生成 DKIM，之后设置 DNS 解析 类型 记录 记录值 TXT \u003cselector\u003e._domainkey DKIM_VALUE ","date":"11-16","objectID":"/2020/mail_server/:1:1","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#dns"},{"categories":["Applications"],"content":" 黑名单在互联网上发送邮件不是可以为所欲为的，邮局服务有一套反垃圾邮件机制，当你的IP上了黑名单时，从这个IP发出去的邮件很容易进入垃圾邮箱或拒收，请珍惜自己的IP，不过可以尝试在检测上了哪些服务商的黑名单，并尝试解除黑名单，以下给出一些检测或申请去除反垃圾邮件网址 MXToolBox http://multirbl.valli.org/ ","date":"11-16","objectID":"/2020/mail_server/:1:2","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#黑名单"},{"categories":["Applications"],"content":" Mailcow:dockerizedMailcow:dockerized 是一个使用docker搭建的标准邮件服务器，集成了邮局、webmail、管理以及反垃圾邮件等功能，过程相对全面，不过缺点是比较吃资源，并且不支持 Synology/QNAP 或 OpenVZ、LXC 等虚拟化方式，并且不能使用 CentOS 7/8 源中的 Docker 包，要求真多。。。消耗资源的主要原因是 ClamAV 和 Solr，即杀毒功能和搜索功能，如果不需要可以关闭。 资源 需求 CPU 1GHz RAM 最少4G (包含交换空间) 硬盘 20GiB (不包含邮件) 以下列出Mailcow:dockerized使用的端口 (HTTP和HTTPS为我们自定义的端口) 服务 协议 端口 容器 Postfix SMTP / SMTPS TCP 25 / 465 postfix-mailcow Postfix Submission TCP 587 postfix-mailcow Dovecot IMAP / IMAPS TCP 143 / 993 dovecot-mailcow Dovecot POP3 / POP3S TCP 110 / 995 dovecot-mailcow Dovecot ManageSieve TCP 4190 dovecot-mailcow HTTP / HTTPS TCP 80 / 443 nginx-mailcow 部署 Mailcow:dockerized现在开始正式的搭建邮箱服务器 cd ${path_to} git clone https://github.com/mailcow/mailcow-dockerized mailcow \u0026\u0026 cd mailcow echo ${email_host} | ./generate_config.sh sed -ie \"s/HTTP_PORT=.*/HTTP_PORT=${http_port}/\" mailcow.conf # HTTP端口 sed -ie \"s/HTTPS_PORT=.*/HTTPS_PORT=${https_port}/\" mailcow.conf # HTTPS端口 sed -i \"s/TZ=.*/TZ=Asia\\/Shanghai/\" mailcow.conf # 时区 sed -i \"s/SKIP_LETS_ENCRYPT=.*/SKIP_LETS_ENCRYPT=y/\" mailcow.conf # 证书申请 (不需要) sed -i \"s/SKIP_SOGO=.*/SKIP_SOGO=y/\" mailcow.conf # webmail (不需要) sed -i \"s/SKIP_SOLR=.*/SKIP_SOLR=n/\" mailcow.conf # 搜索 (不需要) sed -i \"s/enable_ipv6: true/enable_ipv6: false/\" docker-compose.yml # 关闭ipv6 下面给出Nginx配置文件，Apache 配置文件请参见 官方文档 server { listen 80; listen [::]:80; server_name mail.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name mail.example.com; ssl_certificate /ssl/domain/cert.pem; ssl_certificate_key /ssl/domain/key.pem; ssl_session_timeout 2h; ssl_session_cache shared:mailcow:16m; ssl_session_tickets off; # See https://ssl-config.mozilla.org/#server=nginx for the latest ssl settings recommendations # An example config is given below ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5:!SHA1:!kRSA; ssl_prefer_server_ciphers off; location /Microsoft-Server-ActiveSync { proxy_pass http://127.0.0.1:8080/Microsoft-Server-ActiveSync; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_connect_timeout 75; proxy_send_timeout 3650; proxy_read_timeout 3650; proxy_buffers 24 256k; client_body_buffer_size 512k; client_max_body_size 0; } location / { proxy_pass http://127.0.0.1:8080/; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 0; } } 以上全部完成后，mailcow 基本配置完成，只需要启动起服务即可，默认用户密码 admin / moohoo cd ${mailcow_path} docker-compose pull docker-compose up -d 为 Mailcow:dockerized 配置 TLS现在我们可以为SMTP与IMAP服务加入TLS，假设我们已经对域名 mail.example.com 申请了证书，对 postfix 与 dovecot 配置证书前，我们需要根据 postfix 文档先将我们自己的证书与提供商的证书按顺序存放在同一文件下，并且文件后缀为 .pem，并存放在 mailcow的ssl文件夹下 cat ${cert_file} ${ca_file} \u003e ${mailcow_path}/data/assets/ssl/cert.pem cp ${key_file} ${mailcow_path}/data/assets/ssl/key.pem 证书保存完毕后，对 postfix 与 dovecot 进行配置，配置完成重启服务即可 # postfix sed -i \"s/smtp_tls_security_level.*/smtp_tls_security_level = dane/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_CAfile.*/smtp_tls_CAfile = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_cert_file.*/smtp_tls_cert_file = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_key_file.*/smtp_tls_key_file = \\/etc\\/ssl\\/mail\\/key.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_security_level.*/smtpd_tls_security_level = may/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_CAfile.*/smtpd_tls_CAfile = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_cert_file.*/smtpd_tls_cert_file = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_key_file.*/smtpd_tls_key_file = \\/etc\\/ssl\\/mail\\/key.pem/\" data/conf/postfix/main.cf # dovecot sed -i \"s/ssl_cert.*/ssl_cert = \u003c\\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/dovecot/dovecot.","date":"11-16","objectID":"/2020/mail_server/:1:3","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#mailcow-dockerized"},{"categories":["Applications"],"content":" Mailcow:dockerizedMailcow:dockerized 是一个使用docker搭建的标准邮件服务器，集成了邮局、webmail、管理以及反垃圾邮件等功能，过程相对全面，不过缺点是比较吃资源，并且不支持 Synology/QNAP 或 OpenVZ、LXC 等虚拟化方式，并且不能使用 CentOS 7/8 源中的 Docker 包，要求真多。。。消耗资源的主要原因是 ClamAV 和 Solr，即杀毒功能和搜索功能，如果不需要可以关闭。 资源 需求 CPU 1GHz RAM 最少4G (包含交换空间) 硬盘 20GiB (不包含邮件) 以下列出Mailcow:dockerized使用的端口 (HTTP和HTTPS为我们自定义的端口) 服务 协议 端口 容器 Postfix SMTP / SMTPS TCP 25 / 465 postfix-mailcow Postfix Submission TCP 587 postfix-mailcow Dovecot IMAP / IMAPS TCP 143 / 993 dovecot-mailcow Dovecot POP3 / POP3S TCP 110 / 995 dovecot-mailcow Dovecot ManageSieve TCP 4190 dovecot-mailcow HTTP / HTTPS TCP 80 / 443 nginx-mailcow 部署 Mailcow:dockerized现在开始正式的搭建邮箱服务器 cd ${path_to} git clone https://github.com/mailcow/mailcow-dockerized mailcow \u0026\u0026 cd mailcow echo ${email_host} | ./generate_config.sh sed -ie \"s/HTTP_PORT=.*/HTTP_PORT=${http_port}/\" mailcow.conf # HTTP端口 sed -ie \"s/HTTPS_PORT=.*/HTTPS_PORT=${https_port}/\" mailcow.conf # HTTPS端口 sed -i \"s/TZ=.*/TZ=Asia\\/Shanghai/\" mailcow.conf # 时区 sed -i \"s/SKIP_LETS_ENCRYPT=.*/SKIP_LETS_ENCRYPT=y/\" mailcow.conf # 证书申请 (不需要) sed -i \"s/SKIP_SOGO=.*/SKIP_SOGO=y/\" mailcow.conf # webmail (不需要) sed -i \"s/SKIP_SOLR=.*/SKIP_SOLR=n/\" mailcow.conf # 搜索 (不需要) sed -i \"s/enable_ipv6: true/enable_ipv6: false/\" docker-compose.yml # 关闭ipv6 下面给出Nginx配置文件，Apache 配置文件请参见 官方文档 server { listen 80; listen [::]:80; server_name mail.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name mail.example.com; ssl_certificate /ssl/domain/cert.pem; ssl_certificate_key /ssl/domain/key.pem; ssl_session_timeout 2h; ssl_session_cache shared:mailcow:16m; ssl_session_tickets off; # See https://ssl-config.mozilla.org/#server=nginx for the latest ssl settings recommendations # An example config is given below ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5:!SHA1:!kRSA; ssl_prefer_server_ciphers off; location /Microsoft-Server-ActiveSync { proxy_pass http://127.0.0.1:8080/Microsoft-Server-ActiveSync; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_connect_timeout 75; proxy_send_timeout 3650; proxy_read_timeout 3650; proxy_buffers 24 256k; client_body_buffer_size 512k; client_max_body_size 0; } location / { proxy_pass http://127.0.0.1:8080/; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 0; } } 以上全部完成后，mailcow 基本配置完成，只需要启动起服务即可，默认用户密码 admin / moohoo cd ${mailcow_path} docker-compose pull docker-compose up -d 为 Mailcow:dockerized 配置 TLS现在我们可以为SMTP与IMAP服务加入TLS，假设我们已经对域名 mail.example.com 申请了证书，对 postfix 与 dovecot 配置证书前，我们需要根据 postfix 文档先将我们自己的证书与提供商的证书按顺序存放在同一文件下，并且文件后缀为 .pem，并存放在 mailcow的ssl文件夹下 cat ${cert_file} ${ca_file} \u003e ${mailcow_path}/data/assets/ssl/cert.pem cp ${key_file} ${mailcow_path}/data/assets/ssl/key.pem 证书保存完毕后，对 postfix 与 dovecot 进行配置，配置完成重启服务即可 # postfix sed -i \"s/smtp_tls_security_level.*/smtp_tls_security_level = dane/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_CAfile.*/smtp_tls_CAfile = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_cert_file.*/smtp_tls_cert_file = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_key_file.*/smtp_tls_key_file = \\/etc\\/ssl\\/mail\\/key.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_security_level.*/smtpd_tls_security_level = may/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_CAfile.*/smtpd_tls_CAfile = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_cert_file.*/smtpd_tls_cert_file = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_key_file.*/smtpd_tls_key_file = \\/etc\\/ssl\\/mail\\/key.pem/\" data/conf/postfix/main.cf # dovecot sed -i \"s/ssl_cert.*/ssl_cert = \u003c\\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/dovecot/dovecot.","date":"11-16","objectID":"/2020/mail_server/:1:3","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#部署-mailcow-dockerized"},{"categories":["Applications"],"content":" Mailcow:dockerizedMailcow:dockerized 是一个使用docker搭建的标准邮件服务器，集成了邮局、webmail、管理以及反垃圾邮件等功能，过程相对全面，不过缺点是比较吃资源，并且不支持 Synology/QNAP 或 OpenVZ、LXC 等虚拟化方式，并且不能使用 CentOS 7/8 源中的 Docker 包，要求真多。。。消耗资源的主要原因是 ClamAV 和 Solr，即杀毒功能和搜索功能，如果不需要可以关闭。 资源 需求 CPU 1GHz RAM 最少4G (包含交换空间) 硬盘 20GiB (不包含邮件) 以下列出Mailcow:dockerized使用的端口 (HTTP和HTTPS为我们自定义的端口) 服务 协议 端口 容器 Postfix SMTP / SMTPS TCP 25 / 465 postfix-mailcow Postfix Submission TCP 587 postfix-mailcow Dovecot IMAP / IMAPS TCP 143 / 993 dovecot-mailcow Dovecot POP3 / POP3S TCP 110 / 995 dovecot-mailcow Dovecot ManageSieve TCP 4190 dovecot-mailcow HTTP / HTTPS TCP 80 / 443 nginx-mailcow 部署 Mailcow:dockerized现在开始正式的搭建邮箱服务器 cd ${path_to} git clone https://github.com/mailcow/mailcow-dockerized mailcow \u0026\u0026 cd mailcow echo ${email_host} | ./generate_config.sh sed -ie \"s/HTTP_PORT=.*/HTTP_PORT=${http_port}/\" mailcow.conf # HTTP端口 sed -ie \"s/HTTPS_PORT=.*/HTTPS_PORT=${https_port}/\" mailcow.conf # HTTPS端口 sed -i \"s/TZ=.*/TZ=Asia\\/Shanghai/\" mailcow.conf # 时区 sed -i \"s/SKIP_LETS_ENCRYPT=.*/SKIP_LETS_ENCRYPT=y/\" mailcow.conf # 证书申请 (不需要) sed -i \"s/SKIP_SOGO=.*/SKIP_SOGO=y/\" mailcow.conf # webmail (不需要) sed -i \"s/SKIP_SOLR=.*/SKIP_SOLR=n/\" mailcow.conf # 搜索 (不需要) sed -i \"s/enable_ipv6: true/enable_ipv6: false/\" docker-compose.yml # 关闭ipv6 下面给出Nginx配置文件，Apache 配置文件请参见 官方文档 server { listen 80; listen [::]:80; server_name mail.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name mail.example.com; ssl_certificate /ssl/domain/cert.pem; ssl_certificate_key /ssl/domain/key.pem; ssl_session_timeout 2h; ssl_session_cache shared:mailcow:16m; ssl_session_tickets off; # See https://ssl-config.mozilla.org/#server=nginx for the latest ssl settings recommendations # An example config is given below ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5:!SHA1:!kRSA; ssl_prefer_server_ciphers off; location /Microsoft-Server-ActiveSync { proxy_pass http://127.0.0.1:8080/Microsoft-Server-ActiveSync; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_connect_timeout 75; proxy_send_timeout 3650; proxy_read_timeout 3650; proxy_buffers 24 256k; client_body_buffer_size 512k; client_max_body_size 0; } location / { proxy_pass http://127.0.0.1:8080/; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 0; } } 以上全部完成后，mailcow 基本配置完成，只需要启动起服务即可，默认用户密码 admin / moohoo cd ${mailcow_path} docker-compose pull docker-compose up -d 为 Mailcow:dockerized 配置 TLS现在我们可以为SMTP与IMAP服务加入TLS，假设我们已经对域名 mail.example.com 申请了证书，对 postfix 与 dovecot 配置证书前，我们需要根据 postfix 文档先将我们自己的证书与提供商的证书按顺序存放在同一文件下，并且文件后缀为 .pem，并存放在 mailcow的ssl文件夹下 cat ${cert_file} ${ca_file} \u003e ${mailcow_path}/data/assets/ssl/cert.pem cp ${key_file} ${mailcow_path}/data/assets/ssl/key.pem 证书保存完毕后，对 postfix 与 dovecot 进行配置，配置完成重启服务即可 # postfix sed -i \"s/smtp_tls_security_level.*/smtp_tls_security_level = dane/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_CAfile.*/smtp_tls_CAfile = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_cert_file.*/smtp_tls_cert_file = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtp_tls_key_file.*/smtp_tls_key_file = \\/etc\\/ssl\\/mail\\/key.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_security_level.*/smtpd_tls_security_level = may/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_CAfile.*/smtpd_tls_CAfile = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_cert_file.*/smtpd_tls_cert_file = \\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/postfix/main.cf sed -i \"s/smtpd_tls_key_file.*/smtpd_tls_key_file = \\/etc\\/ssl\\/mail\\/key.pem/\" data/conf/postfix/main.cf # dovecot sed -i \"s/ssl_cert.*/ssl_cert = \u003c\\/etc\\/ssl\\/mail\\/cert.pem/\" data/conf/dovecot/dovecot.","date":"11-16","objectID":"/2020/mail_server/:1:3","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#为-mailcow-dockerized-配置-tls"},{"categories":["Applications"],"content":" Mailu.ioMailu 是一个使用docker搭建的轻量级标准邮件服务器，继承自poste.io，支持x86架构，集成了邮局、webmail、管理以及反垃圾邮件等功能。webmail可以选用roundcube、 rainloop或禁止webmail，而数据库支持sqlite、MySQL与PostgreSQL，最重要的是 MySQL 和 PostgreSQL 可以选择使用镜像或宿主机 (1.9开始将删除docker镜像)。 资源 需求 CPU x86 RAM 建议2G 生成配置文件Mailu官方提供了 在线生成配置文件，可以根据我们的需求生成配置文件，我们将使用 Docker-Compose 搭建 master 版本，并将生成的配置文件下载到服务器上。 initial configuration 进行初始化的配置，比如路径、主域名、TLS、管理界面等，由于我个人喜好自己生成 TLS证书，所以选择 mail 禁止mailu帮我生成证书，但是对邮件进行TLS加密，如果需要mailu生成TLS证书选择带有 letsencrypt 的选项 pick some features 进行功能配置，我们禁用了webmail，可以根据个人喜好选择合适自己的webmail。剩下的三个选项分别是杀毒 (内存杀手)、WebDAV以及邮件代收，根据自己的需求选择 expose Mailu to the world 配置IP与主机名，监听地址填写自己的服务器IP，hostname填写服务器的长主机名 database preferences 数据库设置，这里我们选择使用宿主机的PostgreSQL，URL填写的是Docker在宿主机上默认开启的子网 部署 Mailu现在开始正式的搭建邮箱服务器，假设你已经将配置文件下载到了 mailu_path 中，我们修改一下配置文件 sed -ie \"s/MESSAGE_SIZE_LIMIT=.*/MESSAGE_SIZE_LIMIT=100000000/\" mailu.env sed -i \"/::1/d\" docker-compose.yml sed -ie \"s/${mail_ip}://g\" docker-compose.yml sed -ie \"s/80:80/${http_port}:80/\" docker-compose.yml # HTTP端口 sed -ie \"s/443:443/${https_port}:443/\" docker-compose.yml # HTTPS端口 因为mailu配置的TLS选项是mail，所以我们使宿主机的Nginx反向代理到mailu-front监听的 HTTP上即可 server { listen 80; listen [::]:80; server_name mail.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name mail.example.com; ssl_certificate /ssl/domain/cert.pem; ssl_certificate_key /ssl/domain/key.pem; # See https://ssl-config.mozilla.org/#server=nginx for the latest ssl settings recommendations ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5:!SHA1:!kRSA; ssl_prefer_server_ciphers off; ssl_session_timeout 2h; ssl_session_cache shared:mailu:8m; ssl_session_tickets off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; location / { proxy_pass http://127.0.0.1:8080/; } location /admin { proxy_pass http://127.0.0.1:8080/admin/; } # location /webmail { # proxy_pass http://127.0.0.1:8080/webmail/; # } } 宿主机的PostgreSQL也需要稍微配置一下 sudo adduser --disabled-login --gecos 'Mailu' ${db_user} sudo -u postgres -H psql -d template1 -c \"CREATE USER ${db_user} WITH PASSWORD '${db_passwd}' CREATEDB;\" sudo -u postgres -H psql -d template1 -c \"CREATE DATABASE ${db_name} OWNER ${db_user};\" sudo -u postgres -H psql -h localhost -d ${db_name} -c \"create extension citext;\" echo \"host ${db_name} ${db_user} 192.168.203.0/24 md5\" \u003e\u003e /etc/postgresql/12/main/pg_hba.conf sed -i \"s/#listen_addresses = 'localhost'/listen_addresses = '0.0.0.0'/\" /etc/postgresql/12/main/postgresql.conf systemctl restart postgresql 以上全部完成后 mailu 基本配置完成，只需要根据最后一步，启动起服务并设置管理员密码即可 cd ${mailu_path} docker-compose -p mailu up -d docker-compose -p mailu exec admin flask mailu admin admin ${mail_host#*.} PASSWORD ","date":"11-16","objectID":"/2020/mail_server/:1:4","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#mailu-dot-io"},{"categories":["Applications"],"content":" Mailu.ioMailu 是一个使用docker搭建的轻量级标准邮件服务器，继承自poste.io，支持x86架构，集成了邮局、webmail、管理以及反垃圾邮件等功能。webmail可以选用roundcube、 rainloop或禁止webmail，而数据库支持sqlite、MySQL与PostgreSQL，最重要的是 MySQL 和 PostgreSQL 可以选择使用镜像或宿主机 (1.9开始将删除docker镜像)。 资源 需求 CPU x86 RAM 建议2G 生成配置文件Mailu官方提供了 在线生成配置文件，可以根据我们的需求生成配置文件，我们将使用 Docker-Compose 搭建 master 版本，并将生成的配置文件下载到服务器上。 initial configuration 进行初始化的配置，比如路径、主域名、TLS、管理界面等，由于我个人喜好自己生成 TLS证书，所以选择 mail 禁止mailu帮我生成证书，但是对邮件进行TLS加密，如果需要mailu生成TLS证书选择带有 letsencrypt 的选项 pick some features 进行功能配置，我们禁用了webmail，可以根据个人喜好选择合适自己的webmail。剩下的三个选项分别是杀毒 (内存杀手)、WebDAV以及邮件代收，根据自己的需求选择 expose Mailu to the world 配置IP与主机名，监听地址填写自己的服务器IP，hostname填写服务器的长主机名 database preferences 数据库设置，这里我们选择使用宿主机的PostgreSQL，URL填写的是Docker在宿主机上默认开启的子网 部署 Mailu现在开始正式的搭建邮箱服务器，假设你已经将配置文件下载到了 mailu_path 中，我们修改一下配置文件 sed -ie \"s/MESSAGE_SIZE_LIMIT=.*/MESSAGE_SIZE_LIMIT=100000000/\" mailu.env sed -i \"/::1/d\" docker-compose.yml sed -ie \"s/${mail_ip}://g\" docker-compose.yml sed -ie \"s/80:80/${http_port}:80/\" docker-compose.yml # HTTP端口 sed -ie \"s/443:443/${https_port}:443/\" docker-compose.yml # HTTPS端口 因为mailu配置的TLS选项是mail，所以我们使宿主机的Nginx反向代理到mailu-front监听的 HTTP上即可 server { listen 80; listen [::]:80; server_name mail.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name mail.example.com; ssl_certificate /ssl/domain/cert.pem; ssl_certificate_key /ssl/domain/key.pem; # See https://ssl-config.mozilla.org/#server=nginx for the latest ssl settings recommendations ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5:!SHA1:!kRSA; ssl_prefer_server_ciphers off; ssl_session_timeout 2h; ssl_session_cache shared:mailu:8m; ssl_session_tickets off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; location / { proxy_pass http://127.0.0.1:8080/; } location /admin { proxy_pass http://127.0.0.1:8080/admin/; } # location /webmail { # proxy_pass http://127.0.0.1:8080/webmail/; # } } 宿主机的PostgreSQL也需要稍微配置一下 sudo adduser --disabled-login --gecos 'Mailu' ${db_user} sudo -u postgres -H psql -d template1 -c \"CREATE USER ${db_user} WITH PASSWORD '${db_passwd}' CREATEDB;\" sudo -u postgres -H psql -d template1 -c \"CREATE DATABASE ${db_name} OWNER ${db_user};\" sudo -u postgres -H psql -h localhost -d ${db_name} -c \"create extension citext;\" echo \"host ${db_name} ${db_user} 192.168.203.0/24 md5\" \u003e\u003e /etc/postgresql/12/main/pg_hba.conf sed -i \"s/#listen_addresses = 'localhost'/listen_addresses = '0.0.0.0'/\" /etc/postgresql/12/main/postgresql.conf systemctl restart postgresql 以上全部完成后 mailu 基本配置完成，只需要根据最后一步，启动起服务并设置管理员密码即可 cd ${mailu_path} docker-compose -p mailu up -d docker-compose -p mailu exec admin flask mailu admin admin ${mail_host#*.} PASSWORD ","date":"11-16","objectID":"/2020/mail_server/:1:4","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#生成配置文件"},{"categories":["Applications"],"content":" Mailu.ioMailu 是一个使用docker搭建的轻量级标准邮件服务器，继承自poste.io，支持x86架构，集成了邮局、webmail、管理以及反垃圾邮件等功能。webmail可以选用roundcube、 rainloop或禁止webmail，而数据库支持sqlite、MySQL与PostgreSQL，最重要的是 MySQL 和 PostgreSQL 可以选择使用镜像或宿主机 (1.9开始将删除docker镜像)。 资源 需求 CPU x86 RAM 建议2G 生成配置文件Mailu官方提供了 在线生成配置文件，可以根据我们的需求生成配置文件，我们将使用 Docker-Compose 搭建 master 版本，并将生成的配置文件下载到服务器上。 initial configuration 进行初始化的配置，比如路径、主域名、TLS、管理界面等，由于我个人喜好自己生成 TLS证书，所以选择 mail 禁止mailu帮我生成证书，但是对邮件进行TLS加密，如果需要mailu生成TLS证书选择带有 letsencrypt 的选项 pick some features 进行功能配置，我们禁用了webmail，可以根据个人喜好选择合适自己的webmail。剩下的三个选项分别是杀毒 (内存杀手)、WebDAV以及邮件代收，根据自己的需求选择 expose Mailu to the world 配置IP与主机名，监听地址填写自己的服务器IP，hostname填写服务器的长主机名 database preferences 数据库设置，这里我们选择使用宿主机的PostgreSQL，URL填写的是Docker在宿主机上默认开启的子网 部署 Mailu现在开始正式的搭建邮箱服务器，假设你已经将配置文件下载到了 mailu_path 中，我们修改一下配置文件 sed -ie \"s/MESSAGE_SIZE_LIMIT=.*/MESSAGE_SIZE_LIMIT=100000000/\" mailu.env sed -i \"/::1/d\" docker-compose.yml sed -ie \"s/${mail_ip}://g\" docker-compose.yml sed -ie \"s/80:80/${http_port}:80/\" docker-compose.yml # HTTP端口 sed -ie \"s/443:443/${https_port}:443/\" docker-compose.yml # HTTPS端口 因为mailu配置的TLS选项是mail，所以我们使宿主机的Nginx反向代理到mailu-front监听的 HTTP上即可 server { listen 80; listen [::]:80; server_name mail.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name mail.example.com; ssl_certificate /ssl/domain/cert.pem; ssl_certificate_key /ssl/domain/key.pem; # See https://ssl-config.mozilla.org/#server=nginx for the latest ssl settings recommendations ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5:!SHA1:!kRSA; ssl_prefer_server_ciphers off; ssl_session_timeout 2h; ssl_session_cache shared:mailu:8m; ssl_session_tickets off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; location / { proxy_pass http://127.0.0.1:8080/; } location /admin { proxy_pass http://127.0.0.1:8080/admin/; } # location /webmail { # proxy_pass http://127.0.0.1:8080/webmail/; # } } 宿主机的PostgreSQL也需要稍微配置一下 sudo adduser --disabled-login --gecos 'Mailu' ${db_user} sudo -u postgres -H psql -d template1 -c \"CREATE USER ${db_user} WITH PASSWORD '${db_passwd}' CREATEDB;\" sudo -u postgres -H psql -d template1 -c \"CREATE DATABASE ${db_name} OWNER ${db_user};\" sudo -u postgres -H psql -h localhost -d ${db_name} -c \"create extension citext;\" echo \"host ${db_name} ${db_user} 192.168.203.0/24 md5\" \u003e\u003e /etc/postgresql/12/main/pg_hba.conf sed -i \"s/#listen_addresses = 'localhost'/listen_addresses = '0.0.0.0'/\" /etc/postgresql/12/main/postgresql.conf systemctl restart postgresql 以上全部完成后 mailu 基本配置完成，只需要根据最后一步，启动起服务并设置管理员密码即可 cd ${mailu_path} docker-compose -p mailu up -d docker-compose -p mailu exec admin flask mailu admin admin ${mail_host#*.} PASSWORD ","date":"11-16","objectID":"/2020/mail_server/:1:4","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#部署-mailu"},{"categories":["Applications"],"content":" 安全我们已经配置了TLS，对于邮件的传输过程来说我们的邮件是安全的，但是对于服务提供商来说还是可以随意浏览我们的邮件内容的，如果你希望重要的内容不被服务商所浏览，可以尝试使用对邮件加密的方式。邮件加密并不是将邮件转换为一个带密码的文件，而是使用非对称加密套件，在MUA中进行加密、签名等，MTA只负责传输邮件而不能检测邮件的内容。如果你想使用加密的方式向我发送邮件，请保存以下公钥: OpenPGP S/MIME (iris@ginshio.org) S/MIME (ginshio78@gmail.com) 由于加密邮件是MUA行为，一般情况服务提供商的Webmail并不支持加密邮件，部分提供加密功能的提供商如果需要你上传私钥到他们的服务器，请保持警惕，私钥可以解密你的邮件。以下列出了常见的支持加密的MUA: Microsoft Outlook (S/MIME) Apple Mail (S/MIME) Mozilla Thunderbird (OpenPGP 和 S/MIME) KDE Kontact KMail (OpenPGP 和 S/MIME) GNOME Evolution (OpenPGP 和 S/MIME) Mutt (OpenPGP 和 S/MIME) ","date":"11-16","objectID":"/2020/mail_server/:2:0","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#安全"},{"categories":["Applications"],"content":" S/MIME安全多功能互联网邮件扩展 (S/MIME) 是基于 PKI 的符合 X.509 格式的非对称密钥协议，提供了数字签名、加密功能。发送邮件时，数字签名会以 smime.p7s 的附件跟随邮件发送，如GMail的网页端就支持验证签名，如果是加密邮件则整封邮件被加密后以 smime.p7m 的附件发送。双方互发信息之前，如果没有对方公钥那么无法加密邮件，需要先互相发送签名的邮件用以交换公钥，导入公钥后可以开始发送加密邮件。你可以在 Actalis 申请为期一年的免费 S/MIME 证书，为你邮件加密开启第一步，请保存好申请到的证书 (.pfx文件)、密码以及CRP。 从 Actalis 申请来的 S/MIME 证书是 PKCS #12 格式，这种格式被称为 安全包裹 ，通常这种文件用于打包私钥以及有关的 X.509 证书。我们可以使用 openssl 的 pkcs12 进行创建、解析、读取。 如果需要查看安全包裹信息，可以使用如下命令，这会输出所有证书和私钥 openssl pkcs12 -in file -info -nodes 如果你希望将私钥加密输出，可以去除 -nodes 参数，下表举例了输出 PKCS12 文件信息时的一些控制参数 参数 含义 参数 含义 参数 含义 -nokeys 不输出私钥 -nocerts 不输出证书 -clcerts 仅输出客户证书 -cacerts 仅输出 CA 证书 -noout 没有输出，仅验证 -descert 3DES 算法加密 -nodes 不加密私钥 对于导出到文件，可以添加 -out 参数来指定导出的文件，依然可以使用 -nokeys 与 -nocerts 来决定导出的是证书还是密钥，导出密钥时记得不要加密密钥 ","date":"11-16","objectID":"/2020/mail_server/:2:1","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#s-mime"},{"categories":["Applications"],"content":" OpenPGPOpenPGP标准是一种非对称的非对称密钥协议，提供了加密、签名等工程，OpenGPG是通过信任网络机制确保之间的密钥认证。相比于 S/MIME 而言，OpenGPG 在邮件方便被支持的更少，比如Gmail可以在webmail中验证S/MIME签名，但是并不支持 PGP/MIME。 ","date":"11-16","objectID":"/2020/mail_server/:2:2","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#openpgp"},{"categories":["Applications"],"content":" 推荐阅读 Outlook 反垃圾邮件策略指南 SPF 记录：原理、语法及配置方法简介 DMARC 是什么？ 了解 S/MIME 电子邮件加密指南 在 Thunderbird 中使用 OpenPGP —— 怎么做以及问题解答 Mailcow:dockerized官方文档 使用 mailcow:dockerized 搭建邮件服务器 Mailu.io官方文档 ","date":"11-16","objectID":"/2020/mail_server/:3:0","series":null,"tags":["Server","Mail"],"title":"搭建邮箱服务器","uri":"/2020/mail_server/#推荐阅读"},{"categories":["Applications"],"content":"GinShio |  GPG 入门教程","date":"11-14","objectID":"/2020/gpg_started_guide/","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/"},{"categories":["Applications"],"content":"Pretty Good Privacy (PGP)，是一套用于讯息加密、验证的应用程序，由 Phil Zimmermann 于1991年发布，由一系列散列、数据压缩、对称密钥加密以及公钥加密的算法组合而成。GNU Privacy Guard (GPG)，是一个用于加密、签名通信内容以及管理非对称密钥的自由软件，遵循IETF订定的 OpenPGP技术标准 设计，并与PGP保持兼容。 GPG的基于现代密码学，主要是对非对称加密的应用，由于自己本身是菜鸡，又没有学过密码学，所以对于以下加密方式进行简单的介绍，如有不准确请指正。 对称加密 又称私钥加密，这类算法在加密与解密时使用 相同的 的密钥，通信双方在通信之前需要协商一个密钥。对称加密简单、高效，加密强度随密钥长度的增加而增加，常见加密算法 DES、ChaCha20、AES 等 非对称加密 又称公开密钥加密，这类算法采用公钥加密私钥解密，公钥可以随意发布，私钥必须由用户严格保管，通信双方在通信时使用对方的公钥加密自己的信息。非对称加密的数学基础是超大整数的因数分解、整数有限域离散对数、椭圆曲线离散对数等问题的复杂性。数字签名也是基于非对称加密实现，简单地说即将文件散列后使用私钥加密生成签名，验证时散列文件并与公钥解密签名的值做对比进行验证，数字签名可以验证文件完整性，也有防止伪造的作用。常见的加密算法有 DSA、RSA、ECDSA 等 ","date":"11-14","objectID":"/2020/gpg_started_guide/:0:0","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#"},{"categories":["Applications"],"content":" 初体验","date":"11-14","objectID":"/2020/gpg_started_guide/:1:0","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#初体验"},{"categories":["Applications"],"content":" 生成使用 --generate-key 参数可以创建一个使用默认值的密钥对，如果想设置更多的值可以使用 --full-generate-key 参数，如果再加上 --expert 开启专家模式，专家模式允许你自己选择 不同的加密算法 与 不同的密钥种类，在此仅介绍 --full-generate-key 参数。 选择你希望的密钥种类 我们选择默认的 RSA and RSA，会生成采用RSA算法且拥有加密、签名、验证功能的密钥 密钥长度 NIST建议 2030年之前推荐的最小密钥长度，对称加密 128bit ，非对称加密 2048bit ，椭圆曲线密码学 224bit 使用期限 默认为永久(0)，在这里我们选择1天 (1) 我们生成了一个密钥对，可以看到一些关于新生成的密钥的信息，包括了密钥长度、uid、指纹，我们一般使用指纹来分别不同的密钥，指纹是用40位16进制数字表示的串，我们一般使用邮箱、整串或串的最后16位区分密钥。 ","date":"11-14","objectID":"/2020/gpg_started_guide/:1:1","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#生成"},{"categories":["Applications"],"content":" 备份我们采用最朴素的方式保存密钥 —— 本地存储，但是请记住一点，私钥一定不能丢失或外泄。为了以防万一，我们生成一份吊销证书，用以在特殊情况时吊销该密钥，当然吊销证书也应该妥善保管。 gpg -a --export EFC4B50FE8F8B2B3 \u003e test.pub # 导出公钥 gpg -a --export-secret-key EFC4B50FE8F8B2B3 \u003e test.sec # 导出私钥 gpg -a --gen-revoke EFC4B50FE8F8B2B3 \u003e test.rev # 生成吊销证书 ","date":"11-14","objectID":"/2020/gpg_started_guide/:1:2","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#备份"},{"categories":["Applications"],"content":" 发布 警告 将公钥发布到密钥服务器上是不可逆行为，请谨慎操作 首先列出常用的密钥服务器 sks-keyserver OpenPGP.org GnuPG.net MIT Ubuntu 我们可以从密钥服务器上查找、上传或导入公钥，如果我们已经上传了公钥，本地更新信息后需要再次上传将信息同步到服务器。需要注意的是，公钥服务器会不断同步公钥，不会因为你的密钥过期或吊销而删除。当你将公钥上传到服务器后，其他人可以很好获取你的公钥，完成一些实际用途。 gpg --keyserver pool.sks-keyservers.net --send-keys EFC4B50FE8F8B2B3 ","date":"11-14","objectID":"/2020/gpg_started_guide/:1:3","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#发布"},{"categories":["Applications"],"content":" 吊销 警告 吊销密钥是不可逆行为，请谨慎操作 吊销密钥是不可逆行为，当由于某些特殊原因，请吊销密钥并更新服务器上的密钥信息，尤其是私钥泄漏发生时请尽快吊销，吊销时将密钥生成的吊销证书导入 gpg 即可完成。 gpg --import test.rev # 吊销密钥 gpg --keyserver pool.sks-keyservers.net --send-keys EFC4B50FE8F8B2B3 # 更新吊销信息 ","date":"11-14","objectID":"/2020/gpg_started_guide/:1:4","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#吊销"},{"categories":["Applications"],"content":" 深♂入♂了♂解我们已经有了自己的密钥，那么接下来，我们先创建一个名为 alpha.txt 的文件，里面记录了大写字母A-Z，剩下的就交给GPG来做吧。 # 创建 alpha.txt echo \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \u003e alpha.txt ","date":"11-14","objectID":"/2020/gpg_started_guide/:2:0","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#深-入-了-解"},{"categories":["Applications"],"content":" 导入与删除刚刚我们接触到了如何生成密钥，接下来我们还需要导入一些密钥，可能是其他人的，也可能是我们自己的。我们可以从密钥服务器上查找一些公钥，并导入到本地，我们使用 Debian Key Server 上举例的公钥 673A03E4C1DB921F 做演示 gpg --keyserver pool.sks-keyservers.net --search-keys 673A03E4C1DB921F # 查找公钥 gpg --keyserver pool.sks-keyservers.net --recv-keys 673A03E4C1DB921F # 导入公钥 对于一些本地的密钥，我们可以使用 --import 导入密钥，在使用 --list-keys 展示密钥时你会发现，每个密钥都有一个信任级别，这是一个十分复杂的概念，你可以将好友的 GPG公钥签名后 (--sign-keys)，再上传到公钥服务器上，逐渐组成一个大的 信任网络 ，emmm…参见 Key Signing Party gpg --import test.sec # 导入之前备份的私钥 至于删除密钥，相对来说简单很多，--delete-keys 可以删除公钥， --delete-secert-keys 可以删除私钥 gpg --delete-keys EFC4B50FE8F8B2B3 ","date":"11-14","objectID":"/2020/gpg_started_guide/:2:1","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#导入与删除"},{"categories":["Applications"],"content":" 加密与解密加密与解密内容是我们申请RSA密钥的主要理由，我们可以将我们重要的数据加密、签名，然后发布到互联网上，这样没人可以知道你发布了什么，除非他获取到了你的私钥。 加密 参数 -e / --encrypt，加密时使用 -r / --recipient 指定密钥，加密文件时默认的加密文件为 alpha.txt.gpg gpg -er EFC4B50FE8F8B2B3 alpha.txt # 加密文件，输出到 alpha.txt.gpg gpg -er EFC4B50FE8F8B2B3 -o alpha.encrypt alpha.txt # 加密文件，输出到 alpha.encrypt md5sum alpha.txt | awk '{print $1}' - | gpg -aer EFC4B50FE8F8B2B3 - # 将文件的md5校验值加密输出到终端 解密 参数 -d / --decrypt，使用 -u / --local-user 指定密钥，解密文件时默认将解密的内容输出到终端中 gpg -du EFC4B50FE8F8B2B3 alpha.txt.gpg \u003e alpha.decrypt # 解密数据到 alpha.decrypt ","date":"11-14","objectID":"/2020/gpg_started_guide/:2:2","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#加密与解密"},{"categories":["Applications"],"content":" 签名与校验我们有时不需要对一些发布的文件进行加密，可以进行数字签名，表示这个文件是我发出的，并不是别人伪造的。签名时指定密钥的参数与解密相同 -u / --local-user ，数字签名的方式有多种，接下来依次介绍 二进制签名 参数 -s / --sign，这种签名将源内容与签名存放在同一文件下，并以二进制的形式保存文件，默认输出文件为 alpha.txt.gpg gpg -u EFC4B50FE8F8B2B3 --sign alpha.txt 文本签名 参数 --clear-sign，这种签名将源内容与签名存放在同一文件下，并以文本的形式保存文件，查看输出文件就可以发现源内容与签名存放在一起，默认输出文件为 alpha.txt.asc gpg -u EFC4B50FE8F8B2B3 --clear-sign alpha.txt 分离式签名 参数 -b / --detach-sign，这种签名将源内容与签名存放在不同文件，签名与源文件可以分别发布，默认签名为二进制形式，默认输出文件名为 alpha.txt.sig。如果需要文本形式的分离式签名可以加参数 -a / --armor，此时默认输出文件名为 alpha.txt.asc gpg -u EFC4B50FE8F8B2B3 -b -o alpha.bin.sig alpha.txt # 二进制分离式签名 gpg -u EFC4B50FE8F8B2B3 -ab -o alpha.asc.sig alpha.txt # 文本分离式签名 签名加密同时进行 以上签名形式只进行签名，没有加密，以下介绍的方式可以让签名加密同时进行，但验证签名时只能直接解密同时验证签名，默认输出文件名为 alpha.txt.gpg gpg -u EFC4B50FE8F8B2B3 -r EFC4B50FE8F8B2B3 -se alpha.txt 签名就此介绍完了，如果我们需要验证他人的文件，需要先获取他们的公钥才可以开始验证文件，使用参数 --verify 对签名文件进行验证 gpg --verify alpha.txt.asc # 验证混合签名文件 对于分离式签名，后缀是 .asc 和 .sig 的文件，gpg会默认查找去除后缀后的文件名作为数据文件进行验证，也可以手动指定待验证的数据文件 gpg --verify alpha.bin.sig # 错误，没有文件 alpha.bin.sig cp alpha.asc.sig alpha.txt.asc.sig gpg --verify alpha.txt.asc.sig # 验证文件 'alpha.txt.asc'，签名损坏 cp alpha.asc.sig alpha.txt.sig gpg --verify alpha.txt.sig # 验证文件 'alpha.txt'，签名完好 gpg --verify alpha.bin.sig alpha.txt # 指定数据文件为 'alpha.txt'，使用 'alpha.bin.sig' 进行验证 ","date":"11-14","objectID":"/2020/gpg_started_guide/:2:3","series":null,"tags":["GPG","Encrypt","Tool"],"title":"GPG 入门指北","uri":"/2020/gpg_started_guide/#签名与校验"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第四章 4.2 4.3 读书笔记","date":"11-03","objectID":"/2020/compilerprinciple_004/","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/"},{"categories":["CompilerPrinciple"],"content":"程序设计语言构造的语法可以使用 上下文无关文法 或者 BNF (巴库斯-瑙尔范式) 表示法来描述，文法为语言设计者和编译器编写者提供了很大便利: 文法给出了一个程序设计语言的精确易懂的语法归约 对于某些类型的文法，我们可以自动构造出高效的语法分析器，它能够确定一个源程序的语法结构。同时，语法分析器的构造过程可以揭示出语法的二义性，同时还可能发现一些容易在语言的初始设计阶段被忽略的问题 一个正确设计的文法给出了一个语言的结构，该结构有助于把源程序翻译为正确的目标代码，也有助于检测错误 一个文法支持逐步加入可以完成新任务的新语言构造，从而迭代地演化和开发程序语言。如果对语言的实现遵循语言的文法结构，那么在实现中加入这些新构造的工作就会变得更加容易 语法分析器从词法分析器获得一个词法单元组成的串，并验证这个串可以由源语言的文法生成，我们期望语法分析器能够以易于理解的方式报告语法错误，并能够从常见的错误中恢复并继续处理程序的其余部分。从概念上来说，对于良构的程序，语法分析器构造出一棵 语法分析树，并把它传递给编译器的其他部分进一步处理。我们并不需要显式地构造出语法分析树，对于源程序的检查和翻译工作可以和语法分析过程交替完成，因此语法分析器和其他部分可以用一个模块实现。 错误处理程序检测出错误后，必须报告在源程序的什么位置检测到错误，程序可能有不同层次的错误 词法错误 包括标识符、关键字或运算符拼写错误，或没有在n字符串文本上正确的添加引号 语法错误 包括分号、花括号的多余、缺失等，或 if-else 语句不匹配等 语义错误 包括运算符和运算分量之间的类型不匹配 逻辑错误 因程序员的错误推理而引起的任何错误，包括良构程序但结果不符合预期 语法分析器在检测出错误后，一般将自己恢复到某个状态，且有理由预期从那里开始输入将提供有意义的诊断信息，通常也会发现更多的错误，而不是检测到一个错误就退出程序，当然如果错误过多最好让编译器在达到某个错误数量上限后退出 panic 的恢复 语法分析器一旦发现错误就不断丢弃输入的符号，直到找到同步词法单元 (synchronizing token) 为止，同步词法单元通常是界限符 (如 ; 或 })，它们在源程序中清晰、无二义性。panic 的错误纠正方法常常会跳过大量输入，不检查跳过部分可能包含的错误，但是实现足够简单且不会让语法分析陷入死循环 短语层次的恢复 当发现错误时，语法分析器可以在余下的输入上进行局部性纠正，即将余下输入的某个前缀替换为另一个串，使语法分析器可以继续分析。这个方法难以处理实际错误发生在检测位置之前的情况 错误产生式 通过预测可能遇到的常见错误，在当前语言的文法中加入特殊的产生式，这些产生式可以生产含有错误的构造，语法分析器就能检测到一个预期的错误，生成适当的错误诊断信息 全局纠正 处理一个错误的输入串时通过最少的改动将其转换为语法正确的串 ","date":"11-03","objectID":"/2020/compilerprinciple_004/:0:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#"},{"categories":["CompilerPrinciple"],"content":" 上下文无关文法一个上下文无关文法由 终结符、非终结符、一个 开始符号 和一组 产生式 组成 终结符：组成串的基本符号，与术语 词法单元名 为同义词，如 if-else 结构中的 if 和 else 非终结符：表示串的集合的语法变量，它们表示的串集合用于定义由文法生成的语言 开始符号：某个非终结符号，这个符号表示的串集合就是这个文法生成的语言 产生式：将终结符和非终结符组合为串的方法，每个产生式由以下元素组成 一个被成为产生式头或左部的非终结符，头代表串的集合 符号 \\(\\rightarrow\\)，有时也使用 ::= 来表示 一个由零或多个终结符与非终结符组成的体或右部，体代表头所对应的串的某种构造方法 例如有一组生成式它们的头都是 E，我们可以将其组合在一起成 E \\(\\rightarrow\\) E + T | E - T | T 这种形式 \\[\\begin{aligned} E \u0026\\rightarrow E + T\\\\ E \u0026\\rightarrow E - T \\\\ E \u0026\\rightarrow T \\end{aligned}\\] ","date":"11-03","objectID":"/2020/compilerprinciple_004/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#上下文无关文法"},{"categories":["CompilerPrinciple"],"content":" 符号约定在对文法符号进行表示时，为了方便区分终结符与非终结符，我们对文法中的符号做以下约定 终结符 在字母表中排在前面的 小写字母，如a、b、c等 运算符，如+、-等 标点符号，如逗号、分号等 数字 黑体字符串 非终结符 在字母表中排在前面的 大写字母，如A、B、C等 字母S，它通常表示开始符号 小写的斜体字符串 字母表中排在后面的大写字母表示 文法符号，即表示非终结符或终结符，如X、Y、Z 等 字母表中排在后面的小写字母表示 可能为空的终结符号串，如x、y、z等 除非特殊说明，第一个产生式的头就是开始符号 例如以下文法中我们可知，E、T 和 F 是非终结符，其中E是开始符号，其余符号是终结符 \\[\\begin{aligned} E \u0026\\rightarrow E + T | E - T | T\\\\ T \u0026\\rightarrow T * F | T / F | F \\\\ F \u0026\\rightarrow (E) | \\textbf{id} \\end{aligned}\\] ","date":"11-03","objectID":"/2020/compilerprinciple_004/:1:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#符号约定"},{"categories":["CompilerPrinciple"],"content":" 推导推导就是由一连串的产生式组成，从开始符号开始，经过一系列产生式替换，从而形成了推导过程。考虑一个文法 \\(\\alpha A\\beta\\)，其中 \\(\\alpha\\) 和 \\(\\beta\\) 是任意的文法符号串，A是非终结符，假设 \\(A \\rightarrow \\gamma\\) 是一个产生式，那么可以推导出 \\(\\alpha A \\beta \\Rightarrow \\alpha\\gamma\\beta\\)，我们经常说的 经过零或多步推导出 使用符号 \\(\\xRightarrow{*}\\) 表示，经过一步或多步推导出 使用符号 \\(\\xRightarrow{+}\\) 表示，并且有以下推论 对于任何串 \\(\\alpha\\)，\\(\\alpha \\xRightarrow{*} \\alpha\\) 如果 \\(\\alpha \\xRightarrow{*} \\beta\\) 且 \\(\\beta \\xRightarrow{*} \\gamma\\)，那么 \\(\\alpha \\xRightarrow{*} \\gamma\\) 如果 \\(S \\xRightarrow{*} \\alpha\\)，其中 S 是文法G的开始符号，我们说 \\(\\alpha\\) 是 G 的一个 句型 (句型可能即包含终结符又包含非终结符，也可以是空串)，文法生成的语言是它所有句子的集合 (句子是不包含非终结符的句型)，由文法生成的语言被成为上下文无关语言，如果两个文法生成的语言相同那么这两个文法等价。推导过程有多种，我们最关心的是 最左推导 和 最右推导，即总是选择句型的最左/最右的非终结符进行替换，直到推导出句子，最左推导与最右推导存在一对一的关系，最左推导写作 \\(\\alpha \\xRightarrow[lm]{} \\beta\\)，最右推导写作 \\(\\alpha \\xRightarrow[rm]{} \\beta\\)。 语法分析树是推导过程的图形化表示，其中每个内部结点表示一个产生式的应用，标号为产生式的头，该结点的子结点的标号从左到右组成了推导过程中替换这个产生式的体。一棵树的叶子结点可以是终结符或非终结符， 从左到右将叶子结点排列起来就可以得到一个句型，这个句型被成为 结果 (yield) 或 边缘 (frontier)。例如产生式 E \\(\\rightarrow\\) E + E | E * E | -E | (E) | id ，则 -(id + id) 的语法分析树如下 ","date":"11-03","objectID":"/2020/compilerprinciple_004/:1:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#推导"},{"categories":["CompilerPrinciple"],"content":" 二义性如果一个文法可以为某个句子生成多棵语法分析树，那么它就是有 二义性 (ambiguity)，即对同一个句子存在多个最左或最右推导文法。语法分析器都期望文法是无二义性的，需要消除文法中的二义性，可以选择抛弃不需要的语法生成树为每个句子留下一棵语法分析树。譬如上面产生式，可以推导出两种 id + id * id 的语法分析树，很明显第一棵树是正确的，乘法优先于加法进行计算，第二棵语法分析树错误的处理了加法与乘法的优先级。 ","date":"11-03","objectID":"/2020/compilerprinciple_004/:1:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#二义性"},{"categories":["CompilerPrinciple"],"content":" 设计文法文法能够描述程序设计语言的大部分语法，语法分析器接受的词法单元序列构成了程序设计语言的超集，编译器后续步骤必须对语法分析器的输出进行分析，以保证源程序遵守那些没有被语法分析器检查的规则。 文法是比正则表达式表达能力更强的表示方法，每个可以使用正则表达式描述的构造都可以使用文法来描述，反之不成立。为什么使用正则表达式来定义一个语言的词法语法？ 将一个语言的语法结构分为词法和非词法两个部分，可以很方便的将编译器前端模块化，将编译器分为词法分析器和语法分析器两个大小适中的部分 一个语言的词法规则通常很简单，不需要使用像文法这样的功能强大的表示方法来描述 与文法相比，正则表达式通常提供了 简洁 且 易于理解 的表示词法单元的方法 根据正则表达式自动构造得到的词法分析器效率要高于任意文法自动构造的到的分析器 相较来说，正则表达式更适合描述如标识符、常量、关键字等这样的语言构造的结构，文法最适合描述 嵌套结构，这样的嵌套结构不适合正则表达式描述。 ","date":"11-03","objectID":"/2020/compilerprinciple_004/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#设计文法"},{"categories":["CompilerPrinciple"],"content":" 消除二义性一个二义性文法有时也可以被改写为一个无二义性的文法，给出一个 if-then-else 文法， other 表示任何其他语句，这个文法在 悬空-else 结构中会出现二义性 \\[\\begin{aligned} \\textit{stmt}\\ \u0026\\rightarrow\\ \\textbf{if}\\ \\textit{expr}\\ \\textbf{then}\\ \\textit{stmt}\\\\ \\ \u0026\\rightarrow\\ \\textbf{if}\\ \\textit{expr}\\ \\textbf{then}\\ \\textit{stmt}\\ \\textbf{else}\\ \\textit{stmt}\\\\ \\ \u0026\\rightarrow\\ \\textbf{other}\\\\ \\end{aligned}\\] 可以构造出条件语句 if \\(E_{1}\\) then if \\(E_{2}\\) then \\(S_{1}\\) else \\(S_{2}\\) 的两棵不同的语法分析树，通常规则是每个 else 和最近且尚未匹配的 then 匹配，这个消除二义性规则可以用一个文法直接表示，但实践中很少用产生式表示这个规则。 这里我们给出 if-then-else 结构无二义性的文法 \\[\\begin{aligned} \\textit{stmt}\\ \u0026\\rightarrow\\ \\textit{matched\\_stmt}\\ |\\ \\textit{open\\_stmt}\\\\ \\textit{matched\\_stmt}\\ \u0026\\rightarrow\\ \\textbf{if}\\ \\textit{expr}\\ \\textbf{then}\\ \\textit{matched\\_stmt}\\ \\textbf{else}\\ \\textit{matched\\_stmt}\\ |\\ \\textbf{other}\\\\ \\textit{open\\_stmt}\\ \u0026\\rightarrow\\ \\textbf{if}\\ \\textit{expr}\\ \\textbf{then}\\ \\textit{stmt}\\ |\\ \\textbf{if}\\ \\textit{expr}\\ \\textbf{then}\\ \\textit{matched\\_stmt}\\ \\textbf{else}\\ \\textit{open\\_stmt}\\\\ \\end{aligned}\\] ","date":"11-03","objectID":"/2020/compilerprinciple_004/:2:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#消除二义性"},{"categories":["CompilerPrinciple"],"content":" 消除左递归如果一个文法中存在一个非终结符A使得对某个串 \\(\\alpha\\) 存在一个推导 \\(A \\xRightarrow{+} A\\alpha\\) ，那么这个文法就是 左递归的，即产生式的右部的最左符号是非终结符A本身，自顶向下语法分析方法不能处理左递归的文法，因此需要一个方法来消除左递归。 左递归产生式 \\(A \\rightarrow A\\alpha\\,|\\,\\beta\\)，不断应用这个产生式将在 A 的右边生成一个 \\(\\alpha\\) 的序列，当 A 最终被替换为 \\(\\beta\\) 时，就得到一个在 \\(\\beta\\) 后跟0或多个 \\(\\alpha\\) 的序列。使用一个新的非终结符 R，并按照以下方法改写 A 的产生式可以达到同样的效果，对于新产生式 \\(R \\rightarrow \\alpha R\\) 来说这是一个 右递归的。 \\[\\begin{aligned} A\\ \u0026\\rightarrow\\ \\beta R\\\\ R\\ \u0026\\rightarrow\\ \\alpha R \\ |\\ \\varepsilon\\\\ \\end{aligned}\\] 现在我们给出消除左递归的算法，如果文法中不存在 环 (如 \\(A \\xRightarrow{+} A\\) 的推导) 或 \\(\\varepsilon\\) 产生式 (如 \\(A \\rightarrow \\varepsilon\\) 的产生式)，就能保证能够消除左递归，伪代码如下 按某个顺序将非终结符排序为 \\(A_{1}, A_{2}, \\cdots, A_{n}\\) for i in (1, n): for j in (1, i - 1): 将每个形如 \\(A_{i} \\rightarrow A_{i}\\gamma\\) 的产生式替换为产生式组 \\(A_{i} \\rightarrow \\delta_{1}\\gamma | \\delta_{2}\\gamma | \\cdots | \\delta_{k}\\gamma\\), 其中 \\(A_{j} \\rightarrow \\delta_{1} | \\delta_{2} | \\cdots | \\delta_{k}\\) 是所有的 \\(A_{j}\\) 产生式 消除 \\(A_{i}\\) 产生式之间的立即左递归 ","date":"11-03","objectID":"/2020/compilerprinciple_004/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#消除左递归"},{"categories":["CompilerPrinciple"],"content":" 提取左公因子提取左公因子是一种文法转换方法，它可以产生适用于预测分析技术或自顶向下分析技术的文法。当不清楚应用在两个A产生式中如何选择时，我们可以通过改写产生式来推后这个决定，等我们读入了足够多的输入，获得足够信息后再做出正确选择。 如有文法 \\(A \\rightarrow \\alpha\\beta_{1} | \\alpha\\beta_{2}\\)，输入的开头是从 \\(\\alpha\\) 推导得到的一个非空串，那么我们就不知道应该将A展开为 \\(\\alpha\\beta_{1}\\) 还是 \\(\\alpha\\beta_{2}\\)，我们可以先将 A 展开为 \\(\\alpha{}B\\)，从而将作出决定的时间推迟，在读入了从 \\(\\alpha\\) 推导得到的输入前缀之后，我们再决定将 B 展开为 \\(\\beta_{1}\\) 或 \\(\\beta_{2}\\)。 输入：文法G 输出：一个等价的提取了左公因子的文法 方法：对于每个非终结符A，找出它的两个或多个选项之间的最长公共前缀 \\(\\alpha\\) ，如果 \\(\\alpha \\neq \\varepsilon\\) ，那么存在一个非平凡的公共前缀将所有 A 的 e 产生式 \\(A \\rightarrow \\alpha\\beta_{1} | \\alpha\\beta_{2} | \\cdots | \\alpha\\beta_{n} | \\gamma\\) 替换为 \\[\\begin{aligned} A \u0026\\rightarrow \\alpha A’ | \\gamma\\\\ A’ \u0026\\rightarrow \\beta_{1} | \\beta_{2} | \\cdots | \\beta_{n} \\end{aligned}\\] 其中 \\(\\gamma\\) 表示所有不以 \\(\\alpha\\) 开头的产生式体，\\(A’\\) 代表新的非终结符，不断应用这个转换，直到所有非终结符的任意两个产生式体都不存在公共前缀为止。 ","date":"11-03","objectID":"/2020/compilerprinciple_004/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","SyntacticAnalysis"],"title":"语法分析 1","uri":"/2020/compilerprinciple_004/#提取左公因子"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第三章读书笔记","date":"10-17","objectID":"/2020/compilerprinciple_003/","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 2","uri":"/2020/compilerprinciple_003/"},{"categories":["CompilerPrinciple"],"content":" NFA的重要状态如果一个 NFA 状态有一个标号非 \\(\\varepsilon\\) 的离开转换，那么我们称这个状态为 重要状态 (important state)。子集构造法在计算 \\(\\varepsilon-closure(move(T，a))\\) 的时候，它只使用了集合T中的重要状态，也就是说只有当状态s是重要的，状态集合 \\(move(s,a)\\) 才可能是非空的。在子集构造法的应用过程中，两个NFA状态集合可以被认为是一致的条件是 具有相同的重要状态，且 要么都包含接受状态，要么都不包含接受状态 如果 NFA 是使用 McMaughton-Yamada-Thompson 算法根据一个正则表达式生成的，那么我们可以获得更多重要状态的性质 重要状态只包括在基础规则部分为正则表达式中某个特定符号位置引入的初始状态，即每个重要状态对应于正则表达式中的某个运算分量 NFA 只有一个接受状态，但该接受状态不是重要状态。我们可以在正则表达式r的右端连接一个独特的结束标记符 #，使得r的接收状态增加一个在 # 上的转换，使其成为 (r)# 的NFA的重要状态 NFA 的重要状态直接对应于正则表达式中存放了字母表中符号的位置，使用抽象语法树来表示扩展的正则表达式是非常有用的 ","date":"10-17","objectID":"/2020/compilerprinciple_003/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 2","uri":"/2020/compilerprinciple_003/#nfa的重要状态"},{"categories":["CompilerPrinciple"],"content":" 抽象语法树抽象语法树的叶子结点对应于运算分量，内部结点表示运算符。标号为 连接运算符 (\\(\\circ\\)) 的内部结点被称为 cat结点，并运算符 (\\(|\\)) 的内部结点被称为 or结点，= 星号运算符= (\\(*\\)) 的内部结点被称为 star结点，我们构建正则表达式 \\((a|b)^{*}abb\\#\\) 的抽象语法树。 抽象语法树的叶子结点可以标号为 \\(\\varepsilon\\)，也可以用字母表中的符号作为标号，对于每个标号不为 \\(\\varepsilon\\) 的叶子结点，我们赋予一个独立的整数，我们将这个整数称作叶子结点的 位置，同时也表示和它对应的符号的位置，当然一个符号可以有多个位置。抽象语法树中的这些位置对应构造出的 NFA 中的重要状态。 ","date":"10-17","objectID":"/2020/compilerprinciple_003/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 2","uri":"/2020/compilerprinciple_003/#抽象语法树"},{"categories":["CompilerPrinciple"],"content":" 计算函数要从一个正则表达式直接构造出 DFA，我们要先构造出它的抽象语法树，然后计算如下四个函数：nullable、firstpos、lastpos 和 followpos，且这四个函数都用到了扩展正则表达式 (r)# 的抽象语法树。 nullable(n) 当且仅当此结点代表的子表达式的语言中包含空串 \\(\\varepsilon\\) 时抽象语法树结点n为真，即：这个子表达式可以生成空串或本身就是空串，即使它也可能表示其他串 firstpos(n) 定义了以结点n为根的子树中的位置集合，这些位置对应于以n为根的子表达式的语言中某个串的 第一个符号 lastpos(n) 定义了以结点n为根的子树中的位置集合，这些位置对应于以n为根的子表达式的语言中某个串的 最后一个符号 followpos(p) 定义了一个和位置p相关的、抽象语法树中的某些位置的集合。当且仅当存在 L((r)#) 中的某个串 \\(x=a_{1}a_{2}\\cdots a_{n}\\)，使得我们在解释为什么x属于 L((r)#) 时，可以将x中的某个 \\(a_{i}\\) 和抽象语法树中的位置p匹配，且将位置 \\(a_{i+1}\\) 和位置q 匹配，那么位置q在 \\(followpos(p)\\) 中。简单地说，该函数计算出位置n之后可以跟随的其他位置 在计算函数时，我们先给出较为简单的 nullable 、 firstpos 和 lastpos 的计算方式，可以使用一个对树的高度直接进行递归的过程来计算它们，计算方式如下。 一个标号为 \\(\\varepsilon\\) 的叶子结点 nullable(n): true firstpos(n): \\(\\emptyset\\) lastpos(n): \\(\\emptyset\\) 一个位置为 i 的叶子结点 nullable(n): false firstpos(n): {i} lastpos(n): {i} 一个 or 结点 (\\(n = c_{1}\\mid c_{2}\\)) nullable(n): \\(nullable(c_{1})\\) or \\(nullable(c_{2})\\) firstpos(n): \\(firstpos(c_{1}) \\cup firstpos(c_{2})\\) lastpos(n): \\(lastpos(c_{1}) \\cup lastpos(c_{2})\\) 一个 cat 结点 (\\(n = c_{1}c_{2}\\)) nullable(n): \\(nullable(c_{1})\\) and \\(nullable(c_{2})\\) firstpos(n): if \\(nullable(c_{1})\\) then \\(firstpos(c_{1}) \\cup firstpos(c_{2})\\) else \\(firstpos(c_{1})\\) lastpos(n): if \\(nullable(c_{2})\\) then \\(lastpos(c_{1}) \\cup lastpos(c_{2})\\) else \\(lastpos(c_{2})\\) 一个 star 结点 (\\(n=(c_{1})^{*}\\)) nullable(n): true firstpos(n): \\(firstpos(c_{1})\\) lastpos(n): \\(lastpos(c_{1})\\) followpos 的概念有些复杂，我们先来了解如何计算 followpos，只有两种情况会使得正则表达式的某个位置跟在另一个位置之后 如果 n 是 cat 结点，且其左右子结点分别是 \\(c_{1}\\) 和 \\(c_{2}\\)，那么对于 \\(lastpos(c_{1})\\) 中的每个位置 i， \\(firstpos(c_{2})\\) 中的所有位置都在 \\(followpos(i)\\) 中 如果 n 是 star 结点，且 i 是 \\(lastpos(n)\\) 中的一个位置，那么 \\(firstpos(n)\\) 中的所有位置都在 \\(followpos(i)\\) 中 四个函数如何计算都已经给出，现在我们用正则表达式 \\((a|b)^{*}abb\\#\\) 练练手，下图给出构建出的语法分析树，结点左边给出其 firstpos，结点右边给出其 lastpos followpos 的计算规则1要求我们查看每个cat结点，并将它的右子结点的firstpos中的每个位置放到它的左子结点的lastpos中各个位置的followpos中；计算规则2要求我们查看每个 star 结点，并将它的firstpos中的所有位置放到它的lastpos中各个位置的followpos中。例如上图中最下面的一个 cat 结点，根据规则1，将位置3加入到 followpos(1) 和 followpos(2) 中。 位置n followpos(n) 1 {1,2,3} 2 {1,2,3} 3 {4} 4 {5} 5 {6} 6 \\(\\emptyset\\) 我们可以创建有向图来表示函数 followpos，其中每个位置有一个对应的结点，当且仅当j 在followpos(i)中时从位置i到位置j有一条有向边。那么这个表示followpos函数的有向图几乎就是相应正则表达式的不含 \\(\\varepsilon\\) 转换的NFA，我们经过以下处理即可由有向图得到NFA 将根结点的firstpos中的所有位置设置为开始状态 在每条从i到j的有向边上添加位置i上的符号作为标号 把和结尾 # 相关的位置当作唯一的接收状态 ","date":"10-17","objectID":"/2020/compilerprinciple_003/:3:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 2","uri":"/2020/compilerprinciple_003/#计算函数"},{"categories":["CompilerPrinciple"],"content":" 从正则表达式构造DFA接下来我们给出算法，直接从正则表达式构造DFA 信息 输入：一个正则表达式 r 输出：一个识别 L(r) 的 DFA D 方法： 根据扩展的正则表达式 (r)# 构造出一颗抽象语法树 T 计算T的函数 nullable，firstpos，lastpos 和 followpos 构造出 D 的 状态集 \\(D_{states}\\) 和 D 的 转换函数 \\(D_{tran}\\)，D的状态就是T中的位置集合，开始状态是 \\(firstpos(n_{0})\\) (\\(n_{0}\\) 是T的根节点)，接受状态集合是那些包含了和结束标记#对应的位置的状态。每个状态最初都是 未标记的，当我们开始考虑某个状态的离开转换时，该状态就变为 已标记的 构造的伪代码如下: while Dstates 中存在未标记的状态S: 标记 S for 每个输入符号a: 令 U 为 S 中和 a 对应的所有位置p的 followpos(p) 的并集 if U 不在 Dstates 中: 将 U 作为未标记的状态加入 Dstates 中 Dtran[S，a] = U 依然以 \\((a|b)^{*}abb\\) 为例构造 DFA，正则表达式所构造出的语法分析树上面已有，分析语法分析树可知只有 star 结点的 nullable 为真。 这颗树的根结点的 firstpos 集为 {1,2,3} ，即 DFA 的开始状态集合，我们称这个集合为 A。计算 \\(D_{tran}[A，a]\\) 和 \\(D_{tran}[A，b]\\) ，A中1和3对应于a，2对应于b，所有 \\(D_{tran}[A，a] = followpos(1) \\cup followpos(3) = {1，2，3，4}\\)，\\(D_{tran}[A，b] = followpos(2) = {1,2,3}\\) 以此类推，构造出该正则表达式的 DFA。 名称 集合 a b A {1,2,3} B A B {1,2,3,4} B C C {1,2,3,5} B D D {1,2,3,6} B A ","date":"10-17","objectID":"/2020/compilerprinciple_003/:4:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 2","uri":"/2020/compilerprinciple_003/#从正则表达式构造dfa"},{"categories":["CompilerPrinciple"],"content":" 最小化DFA对于同一个语言，可以存在多个识别此语言的DFA。对于不同的DFA，各个状态的的名字可能不同，状态的个数也可能不一样，如果我们使用DFA实现词法分析器，则希望DFA的状态数尽可能的少，因为词法分析器的转换表需要为每个状态分配条目。 状态名如果不同，但只改变状态名就可以将一个自动机转换为另一个自动机，那么这两个自动机是同构的，反之则不是。有一个重要结论：任何正则语言都有一个唯一的且状态数目最少的DFA，而且从任意接受相同正则语言的DFA出发，通过分组合并等价状态，我们总可以构造出状态数最少的 DFA。 我们以正则表达式 \\((a|b)^{*}abb\\) 的两个已经构造出的DFA来讲解最小化，其中最小化的 DFA是本篇中由正则表达式直接构造出的DFA，另一个非同构DFA是上一篇中由NFA转换来的 DFA。 在最小化DFA之前，先说明输入串是如何区分各个状态的，如果分别从状态s和t出发，沿着标号为x的路径到达的两个状态只有一个是接受状态，则串x 区分状态 s 和 t；如果状态 s 和 t 存在能够区分它们的串，那么它们就是 可区分的。空串 \\(\\varepsilon\\) 可以区分如何一个接受状态和非接受状态。串 bb 区分状态 A 和 B，因为从 A 出发经过标号 bb 的路径会到达非接受状态 C，而从B出发可以到达接受状态。 DFA状态最小化的工作原理是将一个DFA的状态集合划分为多个组，每个组中的各个状态相互不可区分，但不同组的状态是可区分的，每个组中的状态合并为最小DFA的一个状态，当任意一个组都不能再被分解为更小的组时这个划分结束，此时我们就得到了状态最少的DFA。具体方法如下 首先构造包含两个组 F 和 S-F 的初始划分 \\(\\Pi\\)，这两个组分别是D的接受状态组和非接受状态组 应用以下方法构造新的分划 \\(\\Pi_{new}\\) Pi_new = Pi for Pi 中的每个组 G: 将 G 划分为更小的组，当且仅当对于所有的输入符号a，使得两个状态s和t在同一小组中，状态s和t在a上的转换都到达 Pi 中的同一组 在 Pi_new 中将 G 替换为对 G 进行划分得到的那些小组 如果 \\(\\Pi_{new} = \\Pi\\)，令 \\(\\Pi_{final} = \\Pi\\) 并执行步骤4，否则用 \\(\\Pi_{new}\\) 替换 \\(\\Pi\\) 并重复步骤2 在划分 \\(\\Pi_{final}\\) 的每个组中选取一个状态作为该组的代表，这些代表构成了状态最少 DFA 的状态。最小状态DFA \\(D’\\) 的其他部分按如下步骤构造 a. \\(D’\\) 的开始状态是包含了 D 的开始状态的组的代表 b. \\(D’\\) 的接受状态是那些包含了 D 的接受状态的组的代表。每个组要么只包含了接受状态，要么只包含了非接受状态，因为我们一开始将这两类状态分开了 c. 令 s 是 \\(\\Pi_{final}\\) 中某个组 G 的代表，并令 DFA 中正在输入 a 上离开 s 的转换到达状态 t，令 r 为 t 所在组 H 的代表，那么在 \\(D’\\) 中存在一个从 s 到 r 在输入 a 上的转换 上述算法可能会产生一个带有 死状态 的DFA，所谓死状态是在所有输入符号上都转向自己的非接受状态。我们可以消除掉死状态，使这个DFA可能会变为缺少某些转换的自动机。 ","date":"10-17","objectID":"/2020/compilerprinciple_003/:5:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 2","uri":"/2020/compilerprinciple_003/#最小化dfa"},{"categories":["Applications"],"content":"GinShio | 在服务器上搭建一些自己用的到的服务","date":"10-13","objectID":"/2020/service/","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/"},{"categories":["Applications"],"content":"个人使用的是腾讯云的轻量服务器，系统镜像选择的是 Debian 11，搭建的服务有 博客 HUGO 、私有网盘 Nextcloud 以及 Git服务器 GitLab 目前使用的是 Debian GNU/Linux 11 (bullseye) 搭建服务器，当然用的是 fish 作为 shell bash -c \"cat \u003c\u003c- EOF | sudo tee /etc/apt/sources.list deb https://mirrors.ustc.edu.cn/debian/ bullseye main contrib non-free deb https://mirrors.ustc.edu.cn/debian/ bullseye-updates main contrib non-free deb https://mirrors.ustc.edu.cn/debian/ bullseye-backports main contrib non-free deb https://mirrors.ustc.edu.cn/debian-security bullseye-security main contrib non-free # deb-src https://mirrors.ustc.edu.cn/debian/ bullseye main contrib non-free # deb-src https://mirrors.ustc.edu.cn/debian/ bullseye-updates main contrib non-free # deb-src https://mirrors.ustc.edu.cn/debian/ bullseye-backports main contrib non-free # deb-src https://mirrors.ustc.edu.cn/debian-security bullseye-security main contrib non-free EOF\" sudo apt update -y \u0026\u0026 sudo apt upgrade -y 一下服务搭建时，域名统一使用 example.com，请根据自己的情况修改对应的配置，用到一些基础依赖请自行安装 Nginx Git PHP PostgreSQL Redis ","date":"10-13","objectID":"/2020/service/:0:0","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#"},{"categories":["Applications"],"content":" NextcloudNextcloud 是 ownCloud 项目的一个分支，一个开源的私有云盘应用，官方提供了包括 桌面以及移动系统的客户端。 定义一些变量 set fpm_path /etc/php/7.4/fpm set nextcloud_version 23.0.4 set nextcloud_path /path/to/nextcloud set nextcloud_host example.com set nextcloud_db_user nextcloud set nextcloud_db_passwd YourPassword set nextcloud_db_name nextcloud ","date":"10-13","objectID":"/2020/service/:1:0","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#nextcloud"},{"categories":["Applications"],"content":" Nextcloud 依赖Nextcloud 依赖 PHP 运行时以及数据库 (MySQL 5.7+ / MariaDB 10.2+ 或 PostgreSQL) apt install -y nginx postgresql \\ php php-fpm php-cli php-mysql php-pgsql php-sqlite3 php-redis \\ php-apcu php-memcached php-bcmath php-intl php-mbstring php-json php-xml \\ php-curl php-imagick php-gd php-zip php-gmp php-ctype php-dom php-iconv PHP 修改 php-fpm 的配置文件 cd $fpm_path # php config sed -i \"s/memory_limit = .*/memory_limit = 512M/\" php.ini sed -i \"s/;date.timezone.*/date.timezone = UTC/\" php.ini sed -i \"s/;cgi.fix_pathinfo=1/cgi.fix_pathinfo=1/\" php.ini sed -i \"s/upload_max_filesize = .*/upload_max_filesize = 4096M/\" php.ini sed -i \"s/post_max_size = .*/post_max_size = 4096M/\" php.ini sed -i \"s/max_input_time = .*/max_input_time = 480/\" php.ini sed -i \"s/max_execution_time = .*/max_execution_time = 360/\" php.ini sed -i \"s/pm.max_children = .*/pm.max_children = 32/\" pool.d/www.conf sed -i \"s/pm.min_spare_servers = .*/pm.min_spare_servers = 1/\" pool.d/www.conf sed -i \"s/pm.max_spare_servers = .*/pm.max_spare_servers = 8/\" pool.d/www.conf sed -i \"s/pm.start_servers = .*/pm.start_servers = 4/\" pool.d/www.conf sed -i \"s/;clear_env = no/clear_env = no/\" pool.d/www.conf # opcache config sed -i \"s/;opcache.enable=1/opcache.enable=1/\" php.ini sed -i \"s/;opcache.memory_consumption=128/opcache.memory_consumption=128/\" php.ini sed -i \"s/;opcache.interned_strings_buffer=8/opcache.interned_strings_buffer=8/\" php.ini sed -i \"s/;opcache.max_accelerated_files=10000/opcache.max_accelerated_files=10000/\" php.ini sed -i \"s/;opcache.revalidate_freq=2/opcache.revalidate_freq=1/\" php.ini # apc config bash -c \"cat \u003c\u003c-EOF | sudo tee -a php.ini [apc] apc.cache_by_default = on apc.enable_cli = on apc.enable = on apc.file_update_protection = 2 EOF\" echo \"apc.enable_cli=on\" |sudo tee -a /etc/php/7.4/cli/conf.d/20-apcu.ini # restart systemctl restart php7.4-fpm PostgreSQL 创建用户 nextcloud 和数据库 nextcloud_db sudo useradd $nextcloud_db_user --create-home --shell /sbin/nologin sudo -u postgres -H psql -c \"CREATE USER $nextcloud_db_user WITH PASSWORD '$nextcloud_db_passwd'\" sudo -u postgres -H psql -c \"CREATE DATABASE $nextcloud_db_name OWNER $nextcloud_db_user\" Nginxm 配置网站，修改 官方示例配置文件 并保存在 /etc/nginx/sites-available/nextcloud ln -sf /etc/nginx/sites-available/nextcloud /etc/nginx/sites-enabled/ systemctl restart nginx ","date":"10-13","objectID":"/2020/service/:1:1","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#nextcloud-依赖"},{"categories":["Applications"],"content":" Nextcloud 安装下载 你需要的版本并解压到目录中 sudo -E wget https://download.nextcloud.com/server/releases/nextcloud-\"$nextcloud_version\".tar.bz2 sudo mkdir -p $nextcloud_path sudo tar -jxvf nextcloud-\"$nextcloud_version\".tar.bz2 -C $nextcloud_path sudo chown -R www-data:www-data $nextcloud_path 准备工作完成后，进入网页，设置管理员帐号和数据库。 ","date":"10-13","objectID":"/2020/service/:1:2","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#nextcloud-安装"},{"categories":["Applications"],"content":" Nextcloud 开启邮件服务配置邮箱服务器前需要先修改nextcloud的代码，如下 cd /path/to/nextcloud sed -i \\ \"s/\\$streamContext = .*;/\\$streamContext = stream_context_create(array('ssl'=\u003e['verify_peer'=\u003efalse, 'verify_peer_name'=\u003efalse, 'allow_self_signed'=\u003etrue]));/\" \\ 3rdparty/swiftmailer/swiftmailer/lib/classes/Swift/Transport/StreamBuffer.php systemctl restart php7.4-fpm 登录管理员帐号进行邮箱服务器配置即可 字段 值 发送模式 SMTP 加密 SSL/TLS 来自地址 noreply@example.com 认证方式 登录 需要认证 true 服务器地址 mail.example.com:465 证书 noreply@example.com 密码 YourPassword ","date":"10-13","objectID":"/2020/service/:1:3","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#nextcloud-开启邮件服务"},{"categories":["Applications"],"content":" Nextcloud 优化优化最常见的手段是添加缓存，这里是在配置文件中添加 Redis (uds) 和 APCu 支持 'memcache.local' =\u003e '\\OC\\Memcache\\APCu', 'memcache.distributed' =\u003e '\\OC\\Memcache\\Redis', 'memcache.locking' =\u003e '\\OC\\Memcache\\Redis', 'redis' =\u003e [ 'host' =\u003e '/var/run/redis/redis.sock', 'port' =\u003e 0, 'dbindex' =\u003e 0, 'timeout' =\u003e 2, ], 还有就是开启缩略图，比如最大只有 512x512 的缩略图，可以提高访问速度，但是访问可能体验极差，毕竟点开图片你就给我看这个 'enable_previews' =\u003e true, 'preview_max_x' =\u003e 512, 'preview_max_y' =\u003e 512, 'preview_max_scale_factor' =\u003e 1, 至于默认的 JPEG 质量是 90，有点高的话，你可以用以下命令调低 (比如到 60) occ config:app:set preview jpeg_quality --value=\"60\" 上传大文件的话，可以看看官方的相关文档 ","date":"10-13","objectID":"/2020/service/:1:4","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#nextcloud-优化"},{"categories":["Applications"],"content":" Nextcloud 插件Nextcloud 不仅自带了很多功能，也提供了插件用于扩展功能，官方称作 Apps，以下个人推荐一些插件，欢迎补充 Announcement center 公告发布 Registration 注册功能 File access control 文件访问控制，可以添加规则来控制管理用户对文件的操作，参见 工作流 Music 音乐播放器 Full text search 全文搜索 ","date":"10-13","objectID":"/2020/service/:1:5","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#nextcloud-插件"},{"categories":["Applications"],"content":" GitLabGitLab 是开源的基于git的 web DevOps生命周期工具，提供了Git仓库、问题追踪和CI/CD等功能。分为社区版和企业版，使用相同内核，部分功能社区版没有提供。 Gitlab 相较消耗资源，官方推荐的最低要求为 4C4G 可以最多支持500用户， 8C8G 最多支持1000用户，具体的使用受到用户的活跃程度、CI/CD、修改大小等因素影响。 由于暂时不需要，没有安装 Gitlab Pages，Gitlab的安装依赖 git 用户，以下是目录结构 /home/git ├── gitaly ├── gitlab ├── gitlab-shell ├── gitlab-workhorse ├── go └── repositories ","date":"10-13","objectID":"/2020/service/:2:0","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#gitlab"},{"categories":["Applications"],"content":" Gitlab 组件 Gitaly 处理所有的 git 操作 GitLab Shell 处理基于 SSH 的 git 会话 与 SSH密钥 GitLab Workhorse 反向代理服务器，处理与Rails无关的请求，Git Pull/Push 请求 和 到Rails的连接，减轻Web服务的压力，帮助整体加快Gitlab的速度 Unicorn / Puma Gitlab 自身的 Web 服务器，提供面向用户的功能，Gitlab 13.0 起默认使用 Puma Sidekiq 后台任务服务器，从Redis队列中提取任务并进行处理 GitLab Pages 允许直接从仓库发布静态网站 Gitlab Runner Gitlab CI/CD 所关联的任务处理器 Nginx Web 服务器 PostgreSQL 数据库 ","date":"10-13","objectID":"/2020/service/:2:1","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#gitlab-组件"},{"categories":["Applications"],"content":" Gitlab 依赖目前Gitlab最新版本为 v14.8 ，下列表是部分依赖，细明详见 安装最低要求 Software Minimum Version Notes Ruby 2.7 自 GitLab 13.6 起最低要求 v2.7，暂不支持 Ruby 3.0 GoLang 1.16 Node.js 12.22.1 GitLab 使用 webpack 编译前端资源，推荐使用 Node.js 14.x yarn 1.22.x GitLab 使用 Yarn 管理 Js 依赖，暂不支持 Yarn 2 Redis 4.0 GitLab 13.0 起最低要求 4.0，推荐使用 Redis 6.0 PostgreSQL 13 GitLab 13.0 最低要求 pgsql 12，14.0 最低要求 pgsql 13 Git 2.33.x Nginx 另外需要注意，GitLab 支持的 pgsql 扩展主要有三个，在不同版本有所支持 pg_trgm 最低要求 GitLab 版本为 8.6 plpgsql 最低要求 GitLab 版本为 11.7 btree_gist 最低要求 GitLab 版本为 13.1 安装相关依赖，建立数据库，将 Redis 设置为 Unix Domain Socket (UDS) 连接 定义变量 set gitlab_version 14-10-stable set gitlab_path /home/git/gitlab set gitaly_path /home/git/gitaly set gitlab_host example.com set gitlab_db_passwd YourPassword 安装相关依赖 # install dependencies sudo apt install -y \\ build-essential zlib1g-dev libyaml-dev libssl-dev libgdbm-dev libre2-dev \\ libreadline-dev libncurses5-dev libffi-dev curl openssh-server libxml2-dev \\ libxslt1-dev libcurl4-openssl-dev libicu-dev logrotate rsync python3-docutils \\ pkg-config cmake runit-systemd libkrb5-dev graphicsmagick libimage-exiftool-perl 安装 Git # install git sudo apt install -y \\ libcurl4-openssl-dev libexpat1-dev gettext libz-dev \\ libssl-dev libpcre2-dev build-essential git-core git clone https://gitlab.com/gitlab-org/gitaly.git -b $gitlab_version /tmp/gitaly cd /tmp/gitaly sudo make git GIT_PREFIX=/usr/local sudo apt remove -y git-core sudo apt autoremove 需要注意，这种方法安装的 Git 需要修改配置文件 config/gitlab.yml 中的 git 路径 git: bin_path: /usr/local/bin/git 安装 Ruby mkdir /tmp/ruby \u0026\u0026 cd /tmp/ruby curl --remote-name --location --progress-bar \"https://cache.ruby-lang.org/pub/ruby/2.7/ruby-2.7.4.tar.gz\" echo '3043099089608859fc8cce7f9fdccaa1f53a462457e3838ec3b25a7d609fbc5b ruby-2.7.4.tar.gz' | sha256sum -c - \u0026\u0026 tar xzf ruby-2.7.4.tar.gz cd ruby-2.7.4 ./configure --disable-install-rdoc --enable-shared make sudo make install 安装 GoLang sudo rm -rf /usr/local/go curl --remote-name --location --progress-bar \"https://go.dev/dl/go1.16.10.linux-amd64.tar.gz\" echo '414cd18ce1d193769b9e97d2401ad718755ab47816e13b2a1cde203d263b55cf go1.16.10.linux-amd64.tar.gz' | shasum -a256 -c - \u0026\u0026 \\ sudo tar -C /usr/local -xzf go1.16.10.linux-amd64.tar.gz sudo ln -sf /usr/local/go/bin/{go,gofmt} /usr/local/bin/ rm go1.16.10.linux-amd64.tar.gz 安装 Node # install node v16.x curl --location \"https://deb.nodesource.com/setup_16.x\" | sudo bash - sudo apt-get install -y nodejs npm install --global yarn 安装并配置数据库 sudo useradd git --create-home --shell /sbin/nologin sudo apt install -y postgresql postgresql-client libpq-dev postgresql-contrib sudo -u postgres psql -d template1 -c \"CREATE USER git WITH PASSWORD '$gitlab_db_passwd' CREATEDB;\" sudo -u postgres psql -d template1 -c \"CREATE EXTENSION IF NOT EXISTS pg_trgm;\" sudo -u postgres psql -d template1 -c \"CREATE EXTENSION IF NOT EXISTS btree_gist\"; sudo -u postgres psql -d template1 -c \"CREATE DATABASE gitlabhq_production OWNER git;\" 检查 pg_trgm 扩展是否启用 SELECT true AS enabled FROM pg_available_extensions WHERE name = 'pg_trgm' AND installed_version IS NOT NULL; 检查 btree_gist 扩展是否启用 SELECT true AS enabled FROM pg_available_extensions WHERE name = 'btree_gist' AND installed_version IS NOT NULL; 安装并配置 Redis sudo apt install redis-server sudo cp /etc/redis/redis.conf /etc/redis/redis.conf.orig sudo sed 's/^port .*/port 0/' /etc/redis/redis.conf.orig | sudo tee /etc/redis/redis.conf echo 'unixsocket /var/run/redis/redis.sock' | sudo tee -a /etc/redis/redis.conf echo 'unixsocketperm 770' | sudo tee -a /etc/redis/redis.conf sudo mkdir -p /var/run/redis sudo chown redis:redis /var/run/redis sudo chmod 755 /var/run/redis if test -d /etc/tmpfiles.d echo 'd /var/run/redis 0755 redis redis 10d -' | sudo tee -a /etc/tmpfiles.d/redis.conf end sudo systemctl restart redis sudo usermod -aG redis git ","date":"10-13","objectID":"/2020/service/:2:2","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#gitlab-依赖"},{"categories":["Applications"],"content":" Gitlab 安装 克隆 GitLab 并配置相关文件 # clone gitlab src cd ~git sudo -u git -H -E git clone https://gitlab.com/gitlab-org/gitlab-foss.git -b $gitlab_version gitlab cd $gitlab_path # config gitlab.yml sudo -u git -H cp config/gitlab.yml.example config/gitlab.yml sudo -u git -H editor config/gitlab.yml # config permissions sudo -u git -H cp config/secrets.yml.example config/secrets.yml sudo -u git -H chmod 0600 config/secrets.yml sudo chown -R git log/ sudo chown -R git tmp/ sudo chmod -R u+rwX,go-w log/ sudo chmod -R u+rwX tmp/ sudo chmod -R u+rwX tmp/pids/ sudo chmod -R u+rwX tmp/sockets/ sudo -u git -H mkdir -p public/uploads/ sudo chmod 0700 public/uploads sudo chmod -R u+rwX builds/ sudo chmod -R u+rwX shared/artifacts/ sudo chmod -R ug+rwX shared/pages/ # config puma sudo -u git -H cp config/puma.rb.example config/puma.rb sudo -u git -H editor config/puma.rb # config git sudo -u git -H git config --global core.autocrlf input sudo -u git -H git config --global gc.auto 0 sudo -u git -H git config --global repack.writeBitmaps true sudo -u git -H git config --global receive.advertisePushOptions true sudo -u git -H git config --global core.fsyncObjectFiles true # config Redis sudo -u git -H cp config/resque.yml.example config/resque.yml sudo -u git -H cp config/cable.yml.example config/cable.yml sudo -u git -H editor config/resque.yml config/cable.yml # config database sudo -u git cp config/database.yml.postgresql config/database.yml sudo -u git -H editor config/database.yml sudo -u git -H chmod o-rwx config/database.yml 安装 GitLab 相关程序 # install gems sudo -u git -H bundle config set --local deployment 'true' sudo -u git -H bundle config set --local without 'development test mysql aws kerberos' sudo -u git -H -E bundle install # install gitlab-shell sudo -u git -H -E bundle exec rake gitlab🐚install RAILS_ENV=production sudo -u git -H editor ~git/gitlab-shell/config.yml # install gitlab-workhorse sudo -u git -H -E bundle exec rake \"gitlab:workhorse:install[/home/git/gitlab-workhorse]\" RAILS_ENV=production # install gitaly cd /home/git/gitlab sudo -u git -H -E bundle exec rake \\ \"gitlab:gitaly:install[/home/git/gitaly,/home/git/repositories]\" RAILS_ENV=production sudo chmod 0700 $gitlab_path/tmp/sockets/private sudo chown git $gitlab_path/tmp/sockets/private sudo -u git -H editor $gitaly_path/config.toml # install service sudo mkdir -p /usr/local/lib/systemd/system sudo cp lib/support/systemd/* /usr/local/lib/systemd/system/ sudo systemctl daemon-reload sudo systemctl enable gitlab.target # setup logrotate sudo cp lib/support/logrotate/gitlab /etc/logrotate.d/gitlab 如果主机上还有 PostgreSQL 以及 Redis，可以在 gitlab-*.service 文件 Unit 单元的 Wants 与 After 两个字段追加下面两个服务 [Unit] Wants=redis-server.service postgresql.service After=redis-server.service postgresql.service 启动相关项目 # start gitaly sudo systemctl start gitlab-gitaly.service # init database and activate advanced features sudo -u git -H bundle exec rake gitlab:setup RAILS_ENV=production force=yes 检测应用状态 sudo -u git -H bundle exec rake gitlab:env:info RAILS_ENV=production 编译前端文件 # GetText PO files sudo -u git -H bundle exec rake gettext:compile RAILS_ENV=production # Assets sudo -u git -H yarn install --production --pure-lockfile sudo -u git -H bundle exec rake gitlab:assets:compile \\ RAILS_ENV=production NODE_ENV=production \\ NODE_OPTIONS=\"--max_old_space_size=8192\" if test 0 -ne $status echo \"\\033[31m compile assets error. desc '--max_old_space_size' \\033[0m\" exit 64 end 设置 Nginx sudo cp lib/support/nginx/gitlab-ssl /etc/nginx/sites-available/gitlab sudo ln -sf /etc/nginx/sites-available/gitlab /etc/nginx/sites-enabled/gitlab sudo editor /etc/nginx/sites-available/gitlab sudo nginx -t if test 0 -ne $status echo \"nginx config error. editor /etc/nginx/sites-available/gitlab\" exit 64 end 二次检查程序状态 sudo systemctl restart nginx.service gitlab.target sudo -u git -H bundle exec rake gitlab:check RAILS_ENV=production Gitlab基本配置完成，登录网站设置默认管理员密码即可登录，默认管理员帐号为 root ","date":"10-13","objectID":"/2020/service/:2:3","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#gitlab-安装"},{"categories":["Applications"],"content":" 更新之后每次更新与安装类似，因此可以写一个小脚本 function check_and_print_command_status if test 0 -eq $argv[1] echo \"$argv[2]: \"(set_color green)\"consistency\"(set_color normal) else echo \"$argv[2]: \"(set_color red)\"difference\"(set_color normal) end end function get_next_branch cd $gitlab_path sudo -u git -H bundle exec rake gitlab:backup:create RAILS_ENV=production systemctl stop gitlab.target sudo -u git -H -E git fetch --all --prune sudo -u git -H git checkout -- Gemfile.lock db/structure.sql locale sudo -u git -H git checkout $next_branch end function check_configuration_files cd $gitlab_path set files \\ \"config/gitlab.yml.example\" \"lib/support/nginx/gitlab-ssl\" \\ \"lib/support/systemd/gitlab-gitaly.service\" \"lib/support/systemd/gitlab-pages.service\" \\ \"lib/support/systemd/gitlab-sidekiq.service\" \"lib/support/systemd/gitlab-mailroom.service\" \\ \"lib/support/systemd/gitlab-puma.service\" \"lib/support/systemd/gitlab-workhorse.service\" \\ \"lib/support/systemd/gitlab.target\" \"lib/support/systemd/gitlab.slice\" for f in $files git diff --exit-code origin/$curr_branch:$f origin/$next_branch:$f check_and_print_command_status $status \"$f\" end end function install_and_migration cd $gitlab_path # If you haven't done so during installation or a previous upgrade already sudo -u git -H bundle config set --local deployment 'true' sudo -u git -H bundle config set --local without 'development test mysql aws kerberos' # Update gems sudo -u git -H -E bundle install # Optional: clean up old gems sudo -u git -H bundle clean # Run database migrations sudo -u git -H bundle exec rake db:migrate RAILS_ENV=production # Compile GetText PO files sudo -u git -H bundle exec rake gettext:compile RAILS_ENV=production # Update node dependencies and recompile assets sudo -u git -H -E bundle exec rake yarn:install gitlab:assets:clean gitlab:assets:compile RAILS_ENV=production NODE_ENV=production NODE_OPTIONS=\"--max_old_space_size=6144\" # Clean up cache sudo -u git -H bundle exec rake cache:clear RAILS_ENV=production end function upgrade_shell cd $shell_path sudo -u git -H git fetch --all --tags --prune sudo -u git -H git checkout v(cat $gitlab_path/GITLAB_SHELL_VERSION) sudo -u git -H make build end function upgrade_workhorse cd $gitlab_path sudo -u git -H bundle exec rake \"gitlab:workhorse:install[$work_path]\" RAILS_ENV=production end function upgrade_gitaly cd $gitlab_path sudo -u git -H bundle exec rake \"gitlab:gitaly:install[$gitaly_path,$repos_path]\" RAILS_ENV=production end function check_status cd $gitlab_path sudo -u git -H bundle exec rake gitlab:env:info RAILS_ENV=production sudo -u git -H bundle exec rake gitlab:check RAILS_ENV=production end set -g base_path /home/git set -g gitlab_path $base_path/gitlab set -g gitaly_path $base_path/gitaly set -g shell_path $base_path/gitlab-shell set -g work_path $base_path/gitlab-workhorse set -g repos_path $base_path/repositories set -g curr_branch (git --git-dir=$gitlab_path/.git symbolic-ref --short -q HEAD) set -g next_branch \"15-0-stable\" argparse 'c/check' 'u/update' -- $argv if set --query _flag_update get_next_branch install_and_migration upgrade_shell upgrade_workhorse upgrade_gitaly end if set --query _flag_check check_configuration_files check_status exit 0 end ","date":"10-13","objectID":"/2020/service/:2:4","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#更新"},{"categories":["Applications"],"content":" GitLab CI/CD接下来安装 Gitlab Runner 最新版，使 CI/CD 可用 cd /tmp curl -L \"https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh\" | sudo bash sudo -E apt install docker docker-compose gitlab-runner Gitlab Runner 提供了几种模式，分别是 shell docker docker-ssh ssh parallels virtualbox docker+machine docker-ssh+machine kubernetes virtualbox、parallels、docker-ssh 都是使用 SSH 的方式连接到镜像。而通常采用 docker 执行器就可以轻量化的实现各种环境的构建；如果你需要执行 CI 任务时多时少则可以选用 docker+machine；shell 就很暴力了，直接在主机上执行，但是需要自行搭建环境，这不如 container 来的灵活自在。 在管理员面板的 Overview \u003e Runners 中获取到注册 runner 要用的 token。注册 docker 执行器的 runner sudo gitlab-runner register --executor docker --docker-image alpine:latest 或许以后会改为 k8s 搭建 runner。先将 docker build 映射到 /tmp，只需要修改配置文件中的 volumes 为 volumes = [\"/cache\", \"/tmp:/builds:rw\"] 如果对 GitLab Runner 的配置感兴趣，可以参考官方文档。 ","date":"10-13","objectID":"/2020/service/:2:5","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#gitlab-ci-cd"},{"categories":["Applications"],"content":" GitLab 开启邮件服务我们的GitLab使用的是源码安装，需要修改 config/gitlab.yml 开启 emil gitlab_email_from=\"noreply@example.com\" gitlab_email_reply=\"noreply@example.com\" cd /home/git/gitlab sed -i \"s/email_enabled:.*/email_enabled: true/\" config/gitlab.yml sed -ie \"s/email_from:.*/email_from: ${gitlab_email_from}\" config/gitlab.yml sed -ie \"s/email_reply_to:.*/email_reply_to: ${gitlab_email_reply}\" config/gitlab.yml cp config/initializers/smtp_settings.rb.sample config/initializers/smtp_settings.rb 将email启用后，还需要配置smtp，可以参考 官方教程，修改配置文件 config/initializers/smtp_settings.rb，将 ActionMailer::Base.smtp_settings 修改为以下内容 enable: true, address: \"mail.example.com\", port: 465, user_name: \"noreply@example.com\", password: \"YourPassword\", domain: \"mail.example.com\", authentication: :login, enable_starttls_auto: true, tls: true, openssl_verify_mode: 'none' 开启对邮件的 S/MIME 签名服务，将你的S/MIME私钥保存到 $gitlab_path/.gitlab_smime_key，公钥保存到 $gitlab_path/.gitlab_smime_cert sed -i \"103s/# enabled:.*/enabled: true/\" config/gitlab.yml 配置完成后重启服务即可，如果需要验证SMTP是否工作，可以使用以下命令 echo \"Notify.test_email('$gitlab_email_reply', 'Message Subject', 'Message Body').deliver_now\" | \\ sudo -u git -H bundle exec rails console -e production ","date":"10-13","objectID":"/2020/service/:2:6","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#gitlab-开启邮件服务"},{"categories":["Applications"],"content":" LycheeLychee 现在是由 LycheeOrg 维护的开源项目，旨在实现一个简单易用的照片管理系统，我们搭建服务将使用 4.x 版本作为示例 Lychee 是目前为数不多的支持 PostgreSQL 的图床 set lychee_path /path/to/lychee set lychee_version v4.0.8 set lychee_host example.com set lychee_db_user lychee set lychee_db_passwd YourPassword set lychee_db_name lychee ","date":"10-13","objectID":"/2020/service/:3:0","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#lychee"},{"categories":["Applications"],"content":" Lychee 依赖由于 Lychee 同样也是 PHP 开发的服务，所以已经安装 Nextcloud 的情况下，并不需要再安装 PHP 相关的其他 package (当然除了 PHP 的依赖管理器 composer) PHP \u003e= 7.4，依赖的扩展 (可以使用命令 php -m 查看已安装的扩展) BCMath Ctype Exif Ffmpeg (optional — to generate video thumbnails) Fileinfo GD Imagick (optional — to generate better thumbnails) JSON Mbstring OpenSSL PDO Tokenizer XML ZIP ","date":"10-13","objectID":"/2020/service/:3:1","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#lychee-依赖"},{"categories":["Applications"],"content":" Lychee 安装我们先创建数据库用户，Lychee 支持 MySQL (\u003e 5.7.8) / MariaDB (\u003e 10.2) / PostgreSQL (\u003e 9.2)，我们继续使用 PostgreSQL 就好 sudo useradd $lychee_db_user --create-home --shell /sbin/nologin sudo -u postgres -H psql -c \"CREATE USER $lychee_db_user WITH PASSWORD '$lychee_db_passwd'\" sudo -u postgres -H psql -c \"CREATE DATABASE $lychee_db_name OWNER $lychee_db_user\" 我们将安装 v4.0.8，更多详细版本信息请浏览 更新日志 git clone https://www.github.com/LycheeOrg/Lychee -b $lychee_version $lychee_path cd $lychee_path composer install --no-dev chown -R www-data:www-data $lychee_path 关于 Web 服务器的配置官方已经给出了 Nginx 和 Apache 的相关配置 ","date":"10-13","objectID":"/2020/service/:3:2","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#lychee-安装"},{"categories":["Applications"],"content":" Lychee 配置Lychee 相关的环境配置在 .env 中 bash -c \"cat \u003c\u003c- EOF \u003e $lychee_path/.env APP_NAME=Lychee APP_ENV=production APP_DEBUG=false APP_URL=http://$lychee_host APP_KEY= DEBUGBAR_ENABLED=false LOG_CHANNEL=stack DB_CONNECTION=pgsql DB_HOST=localhost DB_PORT=5432 DB_DATABASE=$lychee_db_name DB_USERNAME=$lychee_db_user DB_PASSWORD=$lychee_db_passwd DB_LOG_SQL=false TIMEZONE=Asia/Shanghai BROADCAST_DRIVER=log CACHE_DRIVER=file SESSION_DRIVER=file SESSION_LIFETIME=120 QUEUE_DRIVER=sync REDIS_HOST=/var/run/redis/redis.sock REDIS_PASSWORD=null REDIS_PORT=0 MAIL_DRIVER=smtp MAIL_HOST= MAIL_PORT= MAIL_USERNAME= MAIL_PASSWORD= MAIL_ENCRYPTION= PUSHER_APP_ID= PUSHER_APP_KEY= PUSHER_APP_SECRET= PUSHER_APP_CLUSTER=mt1 MIX_PUSHER_APP_KEY=\\\"\\${PUSHER_APP_KEY}\\\" MIX_PUSHER_APP_CLUSTER=\\\"\\${PUSHER_APP_CLUSTER}\\\" EOF\" chown www-data:www-data $lychee_path/.env sudo -u www-data php artisan key:generate 最后只需要配置 Nginx 相关内容即可 ","date":"10-13","objectID":"/2020/service/:3:3","series":null,"tags":["Server","NextCloud","GitLab","Lychee"],"title":"在服务器上部署一些服务","uri":"/2020/service/#lychee-配置"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第三章读书笔记","date":"07-16","objectID":"/2020/compilerprinciple_002/","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/"},{"categories":["CompilerPrinciple"],"content":"词法分析是编译器的第一阶段，主要负责读取源程序的输入字符，将它们组成 词素，生成并输出一个词法单元序列，每个词法单元对应一个词素，这个词法单元序列将被语法分析器进行语法分析。除此之外，词法分析器还会过滤源程序中的注释和空白，生成错误信息与源程序的位置关联起来，有时还会进行宏扩展。 学习词法分析时，需要分清以下三个相关但有区别的术语 词法单元 由一个词法单元名和一个可选的属性值组成，词法单元名是一个表示某种词法单位的抽象符号，比如关键字，或标识符的输入字符序列 词素 源程序中的字符序列，它和某一词法单元的模式匹配，并被词法分析器识别为该词法单元的一个实例 模式 描述了一个词法单元的词素可能具有的形式。对于关键词它是组成关键字的字符序列；对于标识符和其他词法单元，模式是一个更加复杂的结构，可以和很多符号串匹配 比如 printf(\"Total=%d\\n\"，source); 中，printf 和 source 都是和词法单元 id 的模式匹配的词素，而字符串则是一个和 literal 匹配的词素，以下表格为词法单元的示例 词法单元 非正式描述 词素示例 if 关键字，字符 i/f if else 关键字，字符 e/l/s/e else comparison 比较运算符 \u003c，\u003c= id 普通标识符 pi，D2，source number 数字常量 3.1415926，1024 literal 字符串常量 “hello world!” ","date":"07-16","objectID":"/2020/compilerprinciple_002/:0:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#"},{"categories":["CompilerPrinciple"],"content":" 词法单元的规约","date":"07-16","objectID":"/2020/compilerprinciple_002/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#词法单元的规约"},{"categories":["CompilerPrinciple"],"content":" 串和语言字母表 (alphabet) 是一个有限的符号集合，符号的典型示例是包括字母、数字和标点符号，常见的字母表如 ASCII 和 Unicode。 串 (string) 是某个字母表中符号的一个有穷序列，串 s 的长度，表示 s 中符号出现的次数，记作 \\(|s|\\)，长度为 0 的串被称为空串，记作 \\(\\varepsilon\\)。 语言 (language) 是某个给定字母表上一个任意的可数的串的集合，此外空集 \\(\\varnothing\\) 和 仅包含空串的集合都是语言。 词法分析中，最重要的语言上的运算是 并、连接 和 闭包。连接是将一个串附加到另一个串的后面形成新串，例如 \\(x=dog, y=house\\)，那么 x、y 的连接 \\(xy=doghouse\\) ；空串是连接运算的 单位元，即对于任意串 \\(s\\varepsilon = \\varepsilon s = s\\)。两个串的连接可以被看作乘积，那么可以定义串的指数运算： \\(s^0=\\varepsilon，s^i = s^{i-1}s(i \u003e 0)\\) 。Kleene 闭包 (closure)，记作 \\(L^{*}\\)，即将 L 连接 0 次或多次后得到的串集；正闭包 与闭包基本相同，但不包括 \\(L^0\\)，也就是说，除非 \\(\\varepsilon\\) 属于 L，否则 \\(\\varepsilon \\notin L\\)。 运算 定义和表示 L 和 M 的并 \\(L \\cup M = \\{s \\mid s \\in L \\ or\\ s \\in M\\}\\) L 和 M 的连接 \\(LM = \\{st \\mid s \\in L \\ and\\ t \\in M\\}\\) L 的 Kleene 闭包 \\(L^{*} = \\cup_{i=0}^{\\infty} L^i\\) L 的正闭包 \\(L^{+} = \\cup_{i=1}^{\\infty} L^i\\) 信息 令 L = {A, B, \\(\\ldots\\), Z, a, b, \\(\\ldots\\), z}，令 D = {0, 1, \\(\\ldots\\), 9}，这是两个字母表，也可以认为是两个串长都为 1 的语言，对他们进行上述 4 种运算 \\(L \\cup D\\) 是字母和数字的集合，结果是 62 个长度为 1 的串 \\(LD\\) 是包含 520 个长度为 2 的集合，每个串都是一个字母跟一个数字 \\(L^4\\) 是由四个字母构成的串的集合 \\(L^{*}\\) 是由字母构成的串的集合，包含空串 \\(\\varepsilon\\) \\(D^{+}\\) 是由一个或多个数字构成的串的集合，不包含空串 ","date":"07-16","objectID":"/2020/compilerprinciple_002/:1:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#串和语言"},{"categories":["CompilerPrinciple"],"content":" 正则表达式正则表达式由常量和运算构成，它们分别是字符串的集合和在这些集合上的运算，正则表达式可以由较小的正则表达式按照一定规则递归地构建。 归纳基础 a. \\(\\varepsilon\\) 是一个正则表达式， \\(L(\\varepsilon) = \\{\\varepsilon\\}\\)，即该语言仅包含空串 b. 如果 a 是 \\(\\Sigma\\) 上的一个符号，那么 a 是一个正则表达式，并且 \\(L(\\textbf{a}) = \\{a\\}\\) ，即该语言仅包含一个长度为 1 的字符串 a 归纳步骤：假定 r 和 s 都是正则表达式，分别表示语言 \\(L( r)\\) 和 \\(L(s)\\) ，那么 a. \\(( r)|(s)\\) 是一个正则表达式，表示语言 \\(L( r) \\cup L(s)\\) b. \\(( r)(s)\\) 是一个正则表达式，表示语言 \\(L( r)L(s)\\) c. \\(( r)^{*}\\) 是一个正则表达式，表示语言 \\((L( r))^{*}\\) d. \\(( r)\\) 是一个正则表达式，表示语言 \\(L( r)\\) 按照以上定义，正则表达式经常会包含一些不必要的括号，一般正则表达式有如下优先级 一元运算符 \\(*\\) 具有最高优先级，是左结合的 连接具有次高优先级，是左结合的 \\(|\\) 优先级最低，是左结合的 以下表格列出正则表达式中常用定律 定律 描述 \\(r\\mid s = s\\mid r\\) \\(\\mid\\) 满足交换律 \\(r\\mid(s \\mid t) = (r \\mid s) \\mid t\\) \\(\\mid\\) 满足结合律 \\(r(st) = (rs)t\\) 连接满足结合律 \\(r(s \\mid t) = rs \\mid rt; (s \\mid t)r = sr \\mid tr\\) 连接对 \\(\\mid\\) 满足分配率 \\(\\varepsilon r = r\\varepsilon = r\\) \\(\\varepsilon\\) 是连接的单位元 \\(r^{*} = (r\\mid\\varepsilon)^{*}\\) Kleene 闭包中一定包含 ε \\(r^{**} = r^{*}\\) \\(*\\) 具有幂等性 ","date":"07-16","objectID":"/2020/compilerprinciple_002/:1:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#正则表达式"},{"categories":["CompilerPrinciple"],"content":" 正则定义如果 \\(\\Sigma\\) 是 基本符号集，那么一个 正则定义 (regular definition) 是具有如下形式的定义序列 \\[ \\begin{aligned} d_1 \\rightarrow r_1 \\\\ d_2 \\rightarrow r_2 \\\\ \\dots \\\\ d_n \\rightarrow r_n \\end{aligned} \\] 每个 \\(d_i\\) 都是一个新符号，它们都不在 \\(\\Sigma\\) 中，并且各不相同 每个 \\(r_i\\) 是字母表 \\(\\Sigma \\cup \\{d_1，d_2，\\ldots，d_n\\}\\) 上的正则表达式 C 语言的标识符是由字母或下划线开头，字母、数字和下划线组成的串，正则定义如下 \\[ \\begin{aligned} \\textit{letter}\\_ \u0026 \\rightarrow A | B | \\ldots | Z | a | b | \\ldots | z | \\_ \\\\ \\textit{digit} \u0026 \\rightarrow 0 | 1 | \\ldots | 9 \\\\ \\textit{id} \u0026 \\rightarrow \\textit{letter\\_}(\\textit{letter\\_}|dight)^{*} \\end{aligned} \\] 在进行词法分析器的规约时，现有的正则定义太过于麻烦，于是对其做了一些扩展，当然除了以下介绍的 GNU、Perl 等都有互不兼容的正则表达式扩展 一个或多个实例 (+)，表示一个正则表达式及其语言的正闭包，+ 与 * 具有相同的优先级与结合性 零个或一个实例 (?)，表示一个正则表达式及其语言出现零或一次，\\(r? = r|\\varepsilon\\)，? 与 * 具有相同的优先级与结合性 字符类，一个正则表达式 \\(a_1 | a_2 | \\ldots | a_n\\) 可以缩写为 \\([a_1a_2\\ldots a_n]\\)，如果 \\(a_1\\) 到 \\(a_n\\) 是连接的序列时可以缩写为 \\([a_1-a_n]\\) C 语言的数字字面量可以分为 整型字面量 与 浮点型字面量，以下给出它们的正则定义 \\[ \\begin{aligned} \\textit{digit}\u0026\\rightarrow [0-9] \\\\ \\textit{digits}\u0026\\rightarrow digit^{+} \\\\ \\textit{number}\u0026\\rightarrow [+-](\\textit{digits}.?\\textit{digit}^{*}|.\\textit{digits})([eE][+-]?\\textit{digits})? \\end{aligned} \\] ","date":"07-16","objectID":"/2020/compilerprinciple_002/:1:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#正则定义"},{"categories":["CompilerPrinciple"],"content":" 状态转换图将模式首先需要转换为具有特定风格的流图，我们称为 状态转化图 (transition diagram)，它有一组被称为 状态 (state) 的结点，词法分析器扫描输入串的过程中寻找和某个模式匹配的词素，状态图上的每个状态代表一个可能在过程中出现的情况，结点包含了我们在进行词法分析时需要的全部信息。状态图的 边 (edge) 从图的一个状态指向另一个状态，每条边的标号包含了一个或多个符号。例如我们现在处于状态 s 下，下一个输入的符号为 a，那么我们就会在状态图中寻找一条从 s 离开且符号为 a 的边，并进入这条边所指向的下一个状态。关于状态转移图的重要约定如下 某些状态被称为 接受状态 或 最终状态，在图中用双层圈表示，如果该状态要执行一个动作，通常是向语法分析器返回一个词法单元和相关属性值 如果要回退一个位置，我们一般在该状态上加一个 *，如果要回退多个位置则需要加相应数量的 * 一个状态被称为 开始状态 或 初始状态，该状态由一条没有出发结点的、标号为 start 的边指明，在读入任何符号之前，状态图总是位于它的起始状态 我们用 SQL 中的关系运算符来举个例子 词素 词法单元名 属性值 \u003c relop LT \u003c= relop LE = relop EQ \u003c\u003e relop NE \u003e relop GT \u003e= relop GE 对于符号来说很简单，但对于关键字来说，它们是被保留的，但它们看起来很像标识符，因此我们常常使用两种方法来处理长的很像标识符的关键字 初始化时将各个保留字填入符号表，符号表中的某个字段会指明这些串并非普通的标识符，并指出它们所代表的词法单元 为每个保留字建立单独的状态转换图，并设立词法单元的优先级，当同时匹配关键字模式与 id 模式时优先识别保留字的词法单元 ","date":"07-16","objectID":"/2020/compilerprinciple_002/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#状态转换图"},{"categories":["CompilerPrinciple"],"content":" 有穷自动机一些词法分析其生成程序使用了 有穷自动机 (finite automata) 这种表示方式，其在本质上是与状态转换图类似的图，但有如下不同 有穷自动机不是识别器，它们只能对每个可能输入的串进行简单的回答是或否 分为两类 不确定有穷自动机 (Nondeterministic Finite Automata，NFA)，它们对其边上的标号没有任何限制，一个符号标记离开同一状态的多条边，并且空串也可以作为标记 确定有穷自动机 (Deterministic Finite Automata，DFA)，对于每个状态及自动机输入字母表的每个符号，有且只有一条离开的状态、以该符号为标点的边 确定与不确定的有穷自动机能识别的语言的集合是相同的，这些语言集合正好是能够用正则表达式描述的语言的集合，这个集合中的语言被称为 正则语言 (regular language)。 ","date":"07-16","objectID":"/2020/compilerprinciple_002/:3:0","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#有穷自动机"},{"categories":["CompilerPrinciple"],"content":" 不确定的有穷状态机首先，一个 NFA 由以下几部分组成 一个有穷的状态集合 \\(S\\) 一个输入符号集 \\(\\Sigma\\) ，即输入字母表，我们假设 \\(\\varepsilon \\notin \\Sigma\\) 一个 转换函数 (transition function)，它为每个状态和 \\(\\Sigma \\cup \\{\\varepsilon\\}\\) 中的每个符号都给出了相应的 后续状态 (next state) 的集合 \\(S\\) 中一个状态 \\(s_0\\) 被指定为初始状态 \\(S\\) 中一个子集 \\(F\\) 被指定为接受状态集合 我们可以将 NFA 表示为一个转换图，图中的结点是状态，带有标号的边表示自动机的转换函数，这个图与转台转换图十分相似，但还是有一些区别的 同一个符号可以标记从同一状态出发到达多个目标状态的多条边 一条边的符号不仅可以是输入字母表中的符号，也可以是空串 除了转换图，我们也可以将 NFA 表示为一张转换表，表的各行对应与状态，各列对应于输入符号和 \\(\\varepsilon\\) 。对应于一个给定状态和给定输出的条目是将 NFA 的转换函数应用于这些参数后得到的值，如果转换函数没有没有相关信息，那么我们就将 \\(\\emptyset\\) 填入相应的位置。如下表就是上图的转换表形式 状态 a b \\(\\varepsilon\\) 0 {0, 1} {0} \\(\\emptyset\\) 1 \\(\\emptyset\\) {2} \\(\\emptyset\\) 2 \\(\\emptyset\\) {3} \\(\\emptyset\\) 3 \\(\\emptyset\\) \\(\\emptyset\\) \\(\\emptyset\\) 在转换表上，我们可以很容易确定，一个给定状态和一个输入符号相对应的转换；但是如果输入字母表很大，且大多数状态在大多数输入字符上没有转换时，转换表需要占用大量的空间 ","date":"07-16","objectID":"/2020/compilerprinciple_002/:3:1","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#不确定的有穷状态机"},{"categories":["CompilerPrinciple"],"content":" 确定的有穷状态机DFA 是 NFA 的一个特例，主要体现在 没有输入 \\(\\varepsilon\\) 之上的转换动作 对每个状态 s 和每个输入符号 a，有且只有一条标号为 a 的边离开 s NFA 抽象地表示了用来识别某个语言中的串的算法，DFA 则是一个简单具体的识别串的算法，在构造词法分析器的时候我们使用的是 DFA。 ","date":"07-16","objectID":"/2020/compilerprinciple_002/:3:2","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#确定的有穷状态机"},{"categories":["CompilerPrinciple"],"content":" 从正则表达式构造NFA现在我们给出一个算法，将任何正则表达式转换为接受相同语言的NFA，这个算法是 语法制导 的，对于每个子表达式该算法构造一个只有一个接受状态的NFA。 信息 输入：字母表 \\(\\Sigma\\) 上的一个正则表达式 r 输出：一个接受 L(r) 的 NFA N 方法：首先对r进行语法分析，分解出组成它的子表达式。构造一个NFA的规则分为 基本规则 和 归纳规则 。基本规则处理不包含运算符的子表达式，而归纳规则根据一个给定的表达式的直接子表达式的NFA构造出这个表达式的NFA 基本规则 构造NFA，其中 i 是一个新状态，也是这个NFA的开始状态；f 是另一个新状态，也是这个NFA的接受状态。对于表达式 \\(\\varepsilon\\) 以及字母表 \\(\\Sigma\\) 中的子表达式 a，构造以下 NFA 归纳规则 假设正则表达式 s 和 t 的 NFA 分别为 N(s) 和 N(t)，表达式 r 的 NFA 为 N(r) 假设 r = s|t，构造 N(r)，可以得到从 i 到 N(s) 或 N(t) 的开始状态各有一个 \\(\\varepsilon\\) 转换，从 N(s) 和 N(t) 的接受状态到 f 也各有一个 \\(\\varepsilon\\) 转换。因为从 i 到 f 的任何路径要么只通过 N(s)，要么只通过 N(t)，且离开 i 或进入 f 的 \\(\\varepsilon\\) 转换都不会改变路径上的标号，因此我们可以判定 N(r) 识别 \\(L(s) \\cup L(t)\\)，即 \\(L( r)\\) 假设 r = st ，构造 N(r)，N(s) 的开始状态变为了 N(r) 的开始状态，N(t) 的接受状态变成了 N(r) 唯一接受状态，N(s) 的接受状态和 N(t) 的开始状态合并为一个状态，合并后的状态拥有原来进入和离开合并前的两个状态的全部转换。 假设 r = \\(s^{*}\\)，构造 N(r)，i 和 f 是两个新状态，分别为 N(r) 的开始状态和唯一的接受状态。要从i到达f我们需要沿着新引入的标号为 \\(\\varepsilon\\) 的路径前进，这个路径对应 \\(L(s)^{0}\\) 中的一个串。我们也可以到达 N(s) 的开始状态，然后经过该 NFA，在零次或多次从它的接受状态回到它的开始状态并重复上述过程。 r = (s) ，那么 L(r) = L(s)，我们可以直接把 N(s) 当作 N(r)。 N(r) 接受语言 L(r) 之外，构造得到的 NFA 还具有以下性质: N(r) 的状态数最多为 r 中出现的 运算符 和 运算分量 的总数的 2倍，因为算法的每一个构造步骤最多只引入两个新状态 N(r) 有且只有一个开始状态和一个接受状态 N(r) 中除接受状态之外的每个状态要么有一条其标号为 \\(\\Sigma\\) 中符号的出边，要么有两条标号为 \\(\\varepsilon\\) 的出边 ","date":"07-16","objectID":"/2020/compilerprinciple_002/:3:3","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#从正则表达式构造nfa"},{"categories":["CompilerPrinciple"],"content":" NFA 到 DFA我们需要将 NFA 转换为 DFA，一般采用 子集构造法 直接模拟 NFA。子集构造法的基本思想是让构造得到的 DFA 的每个状态对应于 NFA 的一个状态集合。DFA 的状态数有可能是 NFA 状态数的指数，不过对于真实的语言，NFA 与 DFA 的状态数量大致相同。 信息 输入：一个 NFA N 输出：一个接受同样语言的 DFA D 方法：我们为 D 构造一个转换表 Dtran。D的每个状态是一个 NFA 的状态集，我们构造 Dtran 使得 D 并行的模拟 N 在遇到一个给定输入串时可能执行的所有动作。在读入第一个输入符号之前，N 位于 \\(\\varepsilon-closure(s_0)\\) 中的任何状态上。假定 N 在读入字符串 x 后位于集合 T 的状态上，那么下一个输入符号 a，N 可以移动到集合 \\(move(T， a)\\) 中的任何状态。 \\(\\scriptsize \\varepsilon-closure(s)\\) 从 NFA 的状态 s 开始只通过 \\(\\varepsilon\\) 转换到达的 NFA 状态集合 \\(\\scriptsize \\varepsilon-closure(T)\\) 从 T 中某个 NFA 状态 s 开始只通过 \\(\\varepsilon\\) 转换到达的 NFA 状态集合，即 \\(\\cup_{s \\in T} \\varepsilon-closure(s)\\) \\(move(T,a)\\) 从 T 中某个状态 s 出发通过标号 a 的转换到达的 NFA 状态的集合 简单的说，NFA 中起始状态与起始状态经过 \\(\\varepsilon\\) 转换后所到达的所有状态，这些状态所组成的集合就是转换成 DFA 的起始状态，而这个集合中的所有状态分别经过某一路径转换和转换后再经过 \\(\\varepsilon\\) 转换的状态组成了另一个 DFA 状态，以此下去构成了所有 DFA 中的所有状态 我们继续以上图 \\((a|b)^{*}abb\\) 为例进行从 NFA 到 DFA 的装换，起始状态 A 为 \\(\\varepsilon-closure(0)\\)，即 \\(A=\\{0，1，2，4，7\\}\\)，而输入字母表为 \\(\\{a，b\\}\\)，那么接下来分别计算 \\(Dtran[A，a] = \\varepsilon-closure(move(A,a))\\) 以及 \\(Dtran[A，b] = \\varepsilon-closure(move(A,b))\\) 分别得到 DFA 的状态 B 与状态 C，最终依次计算，我们会得到一张 NFA 与 DFA 对应关系表 (下表)，这样就可能很轻松的完成 NFA 向 DFA 的转换 DFA 状态 NFA 状态集 经过 a 转换得到的状态 经过 b 转换得到的状态 A {0,1,2,4,7} B C B {1,2,3,4,6,7,8} B D C {1,2,4,5,6,7} B C D {1,2,4,5,6,7,9} B E E {1,2,4,5,6,7,10} B C ","date":"07-16","objectID":"/2020/compilerprinciple_002/:3:4","series":["龙书学习笔记"],"tags":["Note","DragonBook","LexicalAnalysis"],"title":"词法分析 1","uri":"/2020/compilerprinciple_002/#nfa-到-dfa"},{"categories":["CompilerPrinciple"],"content":"GinShio | 编译原理第一章读书笔记","date":"07-14","objectID":"/2020/compilerprinciple_001/","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/"},{"categories":["CompilerPrinciple"],"content":" 编译器编译器是一个 程序，它可以阅读以某一 源语言 编写的程序，并把该程序翻译成一个 等价的、用 目标语言 编写的程序; 解释器是另一种语言处理器，它直接利用用户提供的输入执行源程序中指定的操作。 编译器产生的机器语言目标程序通常比一个解释器 快 得多，但是解释器的 错误诊断效果 比编译器更好，因为解释器是逐个语句地执行源程序。 ","date":"07-14","objectID":"/2020/compilerprinciple_001/:1:0","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#编译器"},{"categories":["CompilerPrinciple"],"content":" 基本组成编译器是由 预处理器 (preprocessor)、编译器 (compiler)、汇编器 (assembler)、链接器 (linker) 这几大主要部分组成，最后生成一个可执行程序 (executable)。 预处理器：主要负责文本替换或巨集展开 编译器：可能产生一个汇编语言的中间代码作为其输出，因为汇编语言比较容易 输出 和 调试 汇编器：将编译器产生的中间结果生成 可重新定位的 机器代码 链接器：将一个或多个由编译器或汇编器生成的目标文件外加库，链接为一个可执行文件 现代编译器中，基本可以分步骤调用编译器的各个部分，生成所需要的阶段输出，以 gcc 和 clang 为例 预处理 (-E)：输出文件经过预处理器生成的源代码，一般以 .i 作为文件扩展名 编译 (-S)：将源代码或预处理文件编译生成汇编代码，汇编代码后缀名 .s 汇编 (-c)：将源代码或之前步骤生成的中间代码汇编生成可重新定位的机器码，文件后缀名为 .o 链接：将源文件或之前步骤生成的中间代码链接生成可执行程序 ","date":"07-14","objectID":"/2020/compilerprinciple_001/:1:1","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#基本组成"},{"categories":["CompilerPrinciple"],"content":" 结构编译器由两部分组成，分析 部分和 综合 部分 分析：将源程序分解成为多个组成元素，并在要素之上加上语法结构。分析部分被称为编译器的 前端 综合：根据中间表示和符号表中的信息来构造用户期待的目标程序。综合部分被称为编译器的 后端 现代编译器，比如 LLVM 以设计精良的 IR 作为中端输出，语言只需要实现前端和中端，最终由 LLVM 实现 IR 到二进制的转换，可以做到只优化 LLVM 就完成对其上所有语言的优化。 词法分析 (lexical analysis) 词法分析器读入组成源程序的字符流，并将它们组成成为有意义的 词素 (lexeme) 的序列。对于每个词素，词法分析器产生如下形式的 词法单元 (token) 作为输出 \u003ctoken-name，attribute-value\u003e 语法分析 (syntax analysis) 语法分析器使用由词法分析器生成的各个词法单元的第一个分量来创建树形的中间表示，该中间表示给出了词法分析产生的词法单元流的语法结构。常用表示方法为 语法树 (syntax tree)，树中的每个内部接点表示一个运算，而该结点的子结点表示该运算的分量 语义分析 (semantic analysis) 使用语法树和符号表中的信息来检查源程序是否和语言定义的语义一致。同时也会收集类型信息，并把这些信息存放在语法树或符号表中 中间代码生成 编译器一般在语法分析、语义分析结束之后，会生成一个明确的低级的或类机器语言的中间表示，该中间表示应该 易于生成、且可以被 轻松翻译 为目标机器语言 代码优化 机器无关的代码优化步骤试图改进中间代码，以便生成 更好 的目标代码 代码生成 以源程序的中间表示形式作为输入，并把它映射到目标语言，代码生成必须要 合理分配寄存器 符号表管理 记录源程序中使用的变量名称，并收集和每个名字的各种属性有关的信息，这些属性一般包含 存储分配、类型、作用域 等 ","date":"07-14","objectID":"/2020/compilerprinciple_001/:1:2","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#结构"},{"categories":["CompilerPrinciple"],"content":" 构造工具除通用软件开发工具外，编译器的实现一般需要专业的工具来实现，这些专用工具使用专用的语言来 描述 和 实现 特定的组件，这些生成器会隐藏相当复杂的生成算法细节，并生成易于与其他部分集成的组件 语法分析器的生成器 可以根据一个程序设计语言的语法描述自动生成语法分析器 扫描器的生成器 可以根据一个语言的语法单元的正则表达式描述生成词法分析器 语法执导的翻译引擎 可以生成一组用于遍历分析树并生成中间代码的程序 代码生成器的生成器 根据一组关于如何把中间语言的每个运算翻译成为目标机上的机器语言的规则，生成一个代码生成器 数据流分析引擎 可以帮助收集数据流信息，即程序中的值如何从程序的一部分传递到另一部分，这是代码优化的重要部分 编译器构造工具集 用于构造编译器不同阶段的例程的完整集合 ","date":"07-14","objectID":"/2020/compilerprinciple_001/:1:3","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#构造工具"},{"categories":["CompilerPrinciple"],"content":" 程序设计语言20 世纪 40 年代，第一台计算机问世，它使用 01 序列组成的机器语言编程，直到现在计算机的最底层依然以这种方式运行。但这种编程速度 慢 且 枯燥，容易出错，写出的程序 难以 修改与理解。 20 世纪 50 年代早期，人们开始对助记汇编语言开发，汇编语言已开始仅是对机器语言的助记表示，后来加入了 宏指令，可以为频繁使用的机器指令序列定义带有参数的缩写。 之后，程序设计语言从汇编语言开始走向高级语言，用于科学计算的 Fortran、用于商业数据处理的 Cobol、用于符号计算的 Lisp 等等，随着时间的推移，越来越多带着新特性的高级语言被开发出来，它们更加 简单、自然、强大。 根据时间与应用关系，龙书将程序设计语言分为了 5 代 第一代：机器语言 第二代：汇编语言 第三代：高级程序设计语言，例如 Fortran、C、C++、Java 等 第四代：为特定应用设计的语言，例如用于数据库查询的 SQL，用于文字排版的 Postscript 第五代：基于逻辑和约束的语言，例如 Prolog 和 OPS5 根据程序编程范式的不同，分为 2 种 强制式 (imperative) 又称 命令式，程序指明如何完成一个计算任务，所有强制式语言都有表示 程序状态 和 语句 的表示方法，语句可以改变程序状态，例如 C、C++等 声明式 (declarative) 程序指明需要进行哪些运算，例如 函数式程序设计语言 和 Prolog 等 ","date":"07-14","objectID":"/2020/compilerprinciple_001/:2:0","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#程序设计语言"},{"categories":["CompilerPrinciple"],"content":" 环境与状态标识符 (identifier) 是一个字符串，它通常由子母、数字和下划线组成，它被用来标记一个 实体，例如 数据对象、过程、类型 等。变量指向存储中的某一个特定位置，同一个标识符可能被多次声明 (例如在递归过程中的局部变量)，每一个这样的声明都会引入一个新的变量。所有的标识符都是名字，不过名字不一定是标识符，比如 x.y 这样的名字被称为 受限名字 (qualified name)，表示变量 x 所指向结构中的字段 y。 名字和内存 (存储) 位置的关联，以及之后和值的关联可以用两个映射来描述，这两个映射随着程序的运行而改变。环境 (environment) 是从一个名字到存储位置的映射，例如 C 语言中的右值；状态 (state) 是一个内存位置到它们值的映射，例如 C 语言中左值所对应的右值 大多数环境和状态是 动态绑定 的，一般全局变量的环境映射是静态的，编译器可以在生成目标代码的时候为其分配一个地址；常量的声明一般其状态是静态绑定的，我们看到这个语句时就能确定绑定关系，并且在程序的运行时这个绑定不能改变。 ","date":"07-14","objectID":"/2020/compilerprinciple_001/:2:1","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#环境与状态"},{"categories":["CompilerPrinciple"],"content":" 静态与动态允许编译器静态决定某个问题时，或者说这个问题可以在 编译时 (compile time) 决定，我们称这个语言使用了 静态策略 (static policy)。一个问题只允许在 运行时 (run time) 做出决定，那么称之为 动态策略 (dynamic policy)。比如 C++中的模板计算就是静态策略，而多继承中的多态则是动态策略。 作用域 (scope) 也需要关注静态还是动态，如果仅通过阅读程序即可确定一个声明的作用域，即在编译时就可确定其作用域，那么这个语言使用 静态作用域，或者说是 词法作用域 (lexical scope)。否则，这个语言使用 动态作用域，如果使用动态作用域，在程序运行时，同一个对 x 的使用会指向 x 的几个声明之一。或者简单的说，静态作用域关注在 何处定义，动态作用域关注 何处声明 或 何处调用 a=1; function foo() { echo $a; # 静态作用域输出1，动态作用域输出2 } function bar() { a=2; foo; } bar; ","date":"07-14","objectID":"/2020/compilerprinciple_001/:2:2","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#静态与动态"},{"categories":["CompilerPrinciple"],"content":" 参数的传递机制 值调用 (call-by-value) 在调用过程中会对实参进行求值或拷贝，被调用过程中所有有关形式参数的计算被局限于这一过程中，实参本身不会被影响 引用调用 (call-by-reference) 在调用过程中，以实参的地址作为形参的值传递给被调用者，在使用时就会直接使用这个内存地址，因此形参被修改会影响到实参本身 ","date":"07-14","objectID":"/2020/compilerprinciple_001/:2:3","series":["龙书学习笔记"],"tags":["Note","DragonBook"],"title":"编译器与程序设计语言","uri":"/2020/compilerprinciple_001/#参数的传递机制"},{"categories":null,"content":" 个人 好友 Treamn 友链 我的博客信息 名称： GinShio 简介： VENI VIDI VICI 地址： https://blog.ginshio.org 头像： https://blog.ginshio.org/avatar.wepb ","date":"01-01","objectID":"/links/:0:0","series":null,"tags":null,"title":"友人帐","uri":"/links/#"},{"categories":null,"content":" 个人 好友 Treamn 友链 我的博客信息 名称： GinShio 简介： VENI VIDI VICI 地址： https://blog.ginshio.org 头像： https://blog.ginshio.org/avatar.wepb ","date":"01-01","objectID":"/links/:0:0","series":null,"tags":null,"title":"友人帐","uri":"/links/#个人"},{"categories":null,"content":" 个人 好友 Treamn 友链 我的博客信息 名称： GinShio 简介： VENI VIDI VICI 地址： https://blog.ginshio.org 头像： https://blog.ginshio.org/avatar.wepb ","date":"01-01","objectID":"/links/:0:0","series":null,"tags":null,"title":"友人帐","uri":"/links/#好友"},{"categories":null,"content":" 个人 好友 Treamn 友链 我的博客信息 名称： GinShio 简介： VENI VIDI VICI 地址： https://blog.ginshio.org 头像： https://blog.ginshio.org/avatar.wepb ","date":"01-01","objectID":"/links/:0:0","series":null,"tags":null,"title":"友人帐","uri":"/links/#友链"},{"categories":null,"content":" 个人 好友 Treamn 友链 我的博客信息 名称： GinShio 简介： VENI VIDI VICI 地址： https://blog.ginshio.org 头像： https://blog.ginshio.org/avatar.wepb ","date":"01-01","objectID":"/links/:0:0","series":null,"tags":null,"title":"友人帐","uri":"/links/#我的博客信息"},{"categories":null,"content":" Hi there 👋 is Xin LiuI’m Xin “Russell” Liu, bachelor degree, from Xi’an, China. Working as a software development engineer. Currently I’m learning modern C++ and modern cmake, of course git is also need to be learned. I would like to learn some functional programming languages if I can. My personal development environment is openSUSE Tumbleweed Doom Emacs You can contact me via my personal email ginshio78@gmail.com. Statistics Public Key personal OpenPGP NO Email S/MIME Personal Account ","date":"01-01","objectID":"/about/:0:1","series":null,"tags":null,"title":"","uri":"/about/#hi-there--is-xin-liu"},{"categories":null,"content":" Hi there 👋 is Xin LiuI’m Xin “Russell” Liu, bachelor degree, from Xi’an, China. Working as a software development engineer. Currently I’m learning modern C++ and modern cmake, of course git is also need to be learned. I would like to learn some functional programming languages if I can. My personal development environment is openSUSE Tumbleweed Doom Emacs You can contact me via my personal email ginshio78@gmail.com. Statistics Public Key personal OpenPGP NO Email S/MIME Personal Account ","date":"01-01","objectID":"/about/:0:1","series":null,"tags":null,"title":"","uri":"/about/#statistics"},{"categories":null,"content":" Hi there 👋 is Xin LiuI’m Xin “Russell” Liu, bachelor degree, from Xi’an, China. Working as a software development engineer. Currently I’m learning modern C++ and modern cmake, of course git is also need to be learned. I would like to learn some functional programming languages if I can. My personal development environment is openSUSE Tumbleweed Doom Emacs You can contact me via my personal email ginshio78@gmail.com. Statistics Public Key personal OpenPGP NO Email S/MIME Personal Account ","date":"01-01","objectID":"/about/:0:1","series":null,"tags":null,"title":"","uri":"/about/#public-key"},{"categories":null,"content":" Hi there 👋 is Xin LiuI’m Xin “Russell” Liu, bachelor degree, from Xi’an, China. Working as a software development engineer. Currently I’m learning modern C++ and modern cmake, of course git is also need to be learned. I would like to learn some functional programming languages if I can. My personal development environment is openSUSE Tumbleweed Doom Emacs You can contact me via my personal email ginshio78@gmail.com. Statistics Public Key personal OpenPGP NO Email S/MIME Personal Account ","date":"01-01","objectID":"/about/:0:1","series":null,"tags":null,"title":"","uri":"/about/#personal-account"}]